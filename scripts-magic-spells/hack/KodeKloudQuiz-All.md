# KodeKloud Quiz
- ref: https://gist.github.com/debakarr/4edd7dddd5278ec7b97603f54a6771da

## test

**1. What is the command to view the version of docker engine installed?**


* [ ] `docker --version`
* [ ] `docker version`
* [ ] `docker engine info`
* [ ] `docker info engine`

**Correct answer:**
* [x] `docker version`

**Documentation Link**: htts://docs.docker.com/engine/reference/commandline/docker/

**2. testing question includes links and answers.**


* [ ] answer
* [ ] false
* [ ] true
* [ ] right

**Correct answer:**
* [x] answer

**Code**: 
terraform state list

**Explaination**: The terraform state command is used for advanced state management. As your Terraform usage becomes more advanced, there are some cases where you may need to modify the Terraform state. Rather than modify the state directly, the terraform state commands can be used in many cases instead.

**Documentation Link**: https://www.terraform.io/cli/commands/state

**3. Which operator you can use to perform string concatenation?**


* [ ] \/
* [ ] \*
* [ ] \+
* [ ] \-

**Correct answer:**
* [x] \+

---


## Docker Architecture

**1. Which component  is a read-only template and used for creating a Docker container**


* [ ] Docker Network
* [ ] Docker Images
* [ ] Container
* [ ] Docker Volume

**Correct answer:**
* [x] Docker Images

**2. What is the command to view the version of docker engine installed?**


* [ ] `docker --version`
* [ ] `docker version`
* [ ] `docker engine info`
* [ ] `docker info engine`

**Correct answer:**
* [x] `docker version`

**Documentation Link**: https://docs.docker.com/engine/reference/commandline/docker/

---


## Mock Exam 1

**1. If you have one manager in your swarm cluster, is it possible to demote it to a worker node?**


* [ ] Yes
* [ ] No

**Correct answer:**
* [x] No

**Explaination**: You should have at least 1 manager in ur cluster. If you try, you will have an error " error: code = FailedPrecondition desc = attempting to demote the last manager of the 
Swarm"

**2. We have a single manager 2 worker node swarm cluster. All three nodes are hosting workload. What is the sequence of activities to remove the manager node from the swarm cluster?**


* [ ] Drain the node, and run docker swarm leave
* [ ] Demote to a worker node, drain the node and run docker swarm leave.
* [ ] Promote a worker node to master, demote manager to worker, drain the node and run docker swarm leave
* [ ] Add a new worker node, drain the manager node, and run docker swarm leave

**Correct answer:**
* [x] Promote a worker node to master, demote manager to worker, drain the node and run docker swarm leave

**3. The <code>webapp:v1</code> had some bugs and we fixed them in <code>webapp:v2</code>. We want to update the service to use the image <code>webapp:v2</code>. What is the right command?**


* [ ] <code>docker service update --image=webapp:v1 webapp</code>
* [ ] <code> docker service update --image=webapp:v2 webapp</code>
* [ ] <code>docker service update webapp webapp:v1</code>
* [ ] <code>docker service update webapp webapp:v2</code>

**Correct answer:**
* [x] <code> docker service update --image=webapp:v2 webapp</code>

**4. Create a swarm service named <code>webapp</code> and this service should be connected to <code>my-overlay network</code> and add a custom DNS <code>8.8.8.8</code> to it.**


* [ ] <code>docker service create --name=webapp --dns=8.8.8.8 nginx</code>
* [ ] <code>docker service create --name=webapp --dns=8.8.8.8 --network=my-overlay nginx</code>
* [ ] <code>docker service create --name=webapp --network=my-overlay nginx</code>

**Correct answer:**
* [x] <code>docker service create --name=webapp --dns=8.8.8.8 --network=my-overlay nginx</code>

**5. Which statement best describes Quorum?**


* [ ] Quorum is the minimum number of nodes that must be available for the cluster to function properly.
* [ ] In the case of 3 manager nodes, the quorum is 3
* [ ] You should maintain an odd number of managers in the swarm to support manager node failures.

**Correct answer:**
* [x] Quorum is the minimum number of nodes that must be available for the cluster to function properly.

**6. Which node is responsible for maintaining the desired state of the swarm cluster and taking necessary actions if a node was to fail or a new node was added to the cluster?**


* [ ] manager node
* [ ] worker node
* [ ] slave node
* [ ] worker, slave nodes

**Correct answer:**
* [x] manager node

**7. Which command can be used to create and start containers in the foreground using a docker compose file?**


* [ ] <code>docker-compose up</code>
* [ ] <code> docker-compose ps</code>
* [ ] <code>docker-compose logs</code>
* [ ] <code>docker-compose stop</code>

**Correct answer:**
* [x] <code>docker-compose up</code>

**8. To list the services created by a stack, run …**


* [ ] <code> docker stack deploy</code>
* [ ] <code>docker stack ls</code>
* [ ] <code> docker stack services</code>
* [ ] <code>docker stack ps</code>

**Correct answer:**
* [x] <code> docker stack services</code>

**9. Which of the following are correct commands to create config maps? Select all the answers that apply.**


* [ ] <code>kubectl create configmap CONFIGMAP-NAME --from-literal=KEY1=VALUE1 --from-literal=KEY2=VALUE2</code>
* [ ] <code>kubectl create configmap CONFIGMAP-NAME --from-file=/tmp/env</code>
* [ ] <code>kubectl create configmap CONFIGMAP-NAME --file=/tmp/env</code>
* [ ] <code>kubectl create configmap CONFIGMAP-NAME --literal=KEY1=VALUE1 KEY2=VALUE2</code>

**Correct answer:**
* [x] <code>kubectl create configmap CONFIGMAP-NAME --from-literal=KEY1=VALUE1 --from-literal=KEY2=VALUE2</code>
* [x] <code>kubectl create configmap CONFIGMAP-NAME --from-file=/tmp/env</code>

**10. Which command can be used to deploy exactly one instance of the application on all the nodes in the cluster?**


* [ ] <code>docker service create --replicas=1 webapp</code>
* [ ] <code>docker service create --mode=replicated --replicas=1 webapp</code>
* [ ] <code>docker service create --mode=global --replicas=1 webapp</code>
* [ ] <code>docker service create --mode=global webapp</code>

**Correct answer:**
* [x] <code>docker service create --mode=global webapp</code>

**11. Create a swarm service webapp with image <code>httpd</code> and expose port <code>8080</code> on host to port <code>80</code> in container.**


* [ ] <code>docker container run --name=webapp -p 8080:80 httpd</code>
* [ ] <code>docker service create --name=webapp -p 8080:80 httpd</code>
* [ ] <code>docker service create --name=webapp -p 80:8080 httpd</code>
* [ ] <code>docker service create --replicas=3 httpd</code>

**Correct answer:**
* [x] <code>docker service create --name=webapp -p 8080:80 httpd</code>

**12. Where do you configure the <code>configMapKeyRef</code> in a pod to use environment variables defined in a ConfigMap?**


* [ ] <code>spec.containers.env</code>
* [ ] <code>spec.env.valueFrom</code>
* [ ] <code>spec.containers.valueFrom</code>
* [ ] <code>spec.containers.env.valueFrom</code>

**Correct answer:**
* [x] <code>spec.containers.env.valueFrom</code>

**13. Which statements best describe kubernetes secrets?**


* [ ] Kubernetes secrets let you store and manage sensitive information, such as passwords, OAuth tokens, and ssh keys.
* [ ] Storing confidential information in a Secret is safer.
* [ ] Secrets may be created by Users or the System itself.
* [ ] It is a best practice to check-in secrets into source code repositories.

**Correct answer:**
* [x] Kubernetes secrets let you store and manage sensitive information, such as passwords, OAuth tokens, and ssh keys.
* [x] Storing confidential information in a Secret is safer.
* [x] Secrets may be created by Users or the System itself.

**14. What is the command to deploy a service named <code>webapp</code> on a node which has a <code>type=cpu-optimized</code> label?**


* [ ] <code> docker service create --constraint=node.labels.type==cpu-optimized webapp</code>
* [ ] <code>docker service create --labels type==cpu-optimized webapp</code>
* [ ] <code>docker service create --container-label type==cpu-optimized webapp</code>

**Correct answer:**
* [x] <code> docker service create --constraint=node.labels.type==cpu-optimized webapp</code>

**15. How do you configure all key-value pairs in a Secret object as environment variables within a container?**


* [ ] <code>env.secreRef</code>
* [ ] <code>envFrom.secret</code>
* [ ] <code>envFrom.secretRef</code>
* [ ] <code>envFrom.secretRefKey</code>

**Correct answer:**
* [x] <code>envFrom.secretRef</code>

**Explaination**: Use envFrom to define all of the Secret's data as container environment variables. The key from the Secret becomes the environment variable name in the Pod.

**16. How many IP addresses are consumed by the pod when it’s created?**


* [ ] 1
* [ ] 2
* [ ] 3
* [ ] 4

**Correct answer:**
* [x] 1

**Code**: 
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
spec:
  containers: 
   - name: nginx-container
     image: nginx
   - name: agent
     image: agent

**17. An application you are developing requires an httpd server as frontend, a python application as the backend API server, a MongoDB database and a worker developed in Python. What is the recommended approach in building images for these containers?**


* [ ] Build httpd, python API server, MongoDB database and Python worker into a single image to allow ease of deployment
* [ ] Build httpd into an image, MongoDB database to another and Python API and worker together into a single image
* [ ] Build separate images for each component of the application

**Correct answer:**
* [x] Build separate images for each component of the application

**18. Which of the following commands may be used to list all images matching the <code>com.example.version</code> label?**


* [ ] <code>docker images --label=”com.example.version”</code>
* [ ] <code>docker images --filter "com.example.version"</code>
* [ ] <code>docker images --filter "label=com.example.version"</code>
* [ ] <code>docker images --format "label=com.example.version"</code>

**Correct answer:**
* [x] <code>docker images --filter "label=com.example.version"</code>

**19. Choose the correct statement regarding the following compose file.**


* [ ] Compose will pull the latest version of the mysql image
* [ ] <code>Volumes</code> must be under <code>services</code> section and not at the root-level 
* [ ] The <code>app</code> service will be able to successfully connect to the <code>mysql</code> service
* [ ] The <code>app</code> service will not be able to connect to the <code>mysql</code> service as the host and password specified are incorrect

**Correct answer:**
* [x] The <code>app</code> service will not be able to connect to the <code>mysql</code> service as the host and password specified are incorrect

**Code**: 
version: "3.7"
services:
  app:
    image: node:12-alpine
    working_dir: /app
    environment:
      MYSQL_HOST: localhost
      MYSQL_USER: root
      MYSQL_PASSWORD: secret
      MYSQL_DB: foo
  mysql:
    image: mysql:5.6
    volumes:
      - foo-mysql-data: /var/lib/mysql
    environment: 
      MYSQL_ROOT_PASSWORD: password
      MYSQL_DATABASE: foo
volumes:

**20. What is the command to change the tag of <code>httpd:latest</code> to <code>httpd:v1</code>**


* [ ] <code> docker container image retag httpd:latest httpd:v1</code>
* [ ] <code>docker container image tag httpd:latest httpd:v1</code>
* [ ] <code>docker image retag httpd:latest httpd:v1</code>
* [ ] <code>docker image tag httpd:latest httpd:v1</code>

**Correct answer:**
* [x] <code>docker image tag httpd:latest httpd:v1</code>

**21. When you log in to a registry using the <code>docker login</code> command, the credentials are stored locally at…**


* [ ] <code>$HOME/.docker/config.json</code>
* [ ] <code>/etc/docker/.docker/config.json</code>
* [ ] <code>/var/lib/docker/.docker/config.json</code>
* [ ] <code>/var/lib/docker/containers/.docker/config.json</code>

**Correct answer:**
* [x] <code>$HOME/.docker/config.json</code>

**22. Which of the following is the correct docker image address to be used to access an image named <code>payapp</code> hosted under the organization <code>payroll</code> at a private registry <code>registry.company.io</code>?**


* [ ] <code>registry.company.io/payapp/payroll</code>
* [ ] <code>payroll/registry.company.io/payapp</code>
* [ ] <code>payapp/registry.company.io/payroll</code>
* [ ] <code>registry.company.io/payroll/payapp</code>

**Correct answer:**
* [x] <code>registry.company.io/payroll/payapp</code>

**23. What is a recommended best practice for installing packages and libraries using the <code>apt-get</code> package manager while building an image?**


* [ ]  Download packages on the host and use <code>ADD</code> instructions to add them to the image.
* [ ] Use the <code>ADD</code> instruction to provide a URL to the package on the remote host.
* [ ] Use the <code>RUN</code> instruction and have the apt-get update and apt-get install commands on the same instruction
* [ ] Use the <code>RUN</code> instruction and have the <code>apt-get update</code> and <code>apt-get install</code> commands as separate instructions

**Correct answer:**
* [x] Use the <code>RUN</code> instruction and have the apt-get update and apt-get install commands on the same instruction

**24. What is the command used to find images with a name containing <code>postgres</code>, with at least 12 stars?**


* [ ] <code>docker find  --filter=stars=12 postgres</code>
* [ ] <code> docker search --filter=stars=12 postgres</code>
* [ ] <code> docker find  --limit=12 postgres</code>
* [ ] <code> docker search --limit=12 postgres</code>

**Correct answer:**
* [x] <code> docker search --filter=stars=12 postgres</code>

**25. After building the below code with an image named <code>webapp</code>, What will happen when you run <code>docker run webapp sleep 1000</code>?**


* [ ] docker overrides the <code>ENTRYPOINT</code> instruction with <code>sleep 1000</code>
* [ ] docker overrides the <code>CMD</code> instruction with <code>sleep 1000</code>
* [ ] docker override ENTRYPOINT instruction with <code>sleep</code> and <code>CMD</code> instruction with <code>1000</code>

**Correct answer:**
* [x] docker overrides the <code>CMD</code> instruction with <code>sleep 1000</code>

**Code**: 
FROM ubuntu:18.04
COPY . /app
RUN make /app
CMD python /app/app.py

**26. Which of the below is a recommended best practice while taking backups of a swarm cluster?**


* [ ] Perform the backup operations from a swarm manager node that is a leader
* [ ]  Perform the backup operations from a swarm worker node
* [ ] Perform the backup operations from a swarm manager node that is not a leader

**Correct answer:**
* [x] Perform the backup operations from a swarm manager node that is not a leader

**Explaination**: It is better to perform a backup operation on a swarm node that is not the leader to avoid leader re-election.

**27. What are the recommended hardware requirements to install DTR in a production environment?**


* [ ] 16GB RAM, 2vCPUs and 100GB of free disk space.
* [ ] 16GB RAM, 4vCPUs and 25-100GB of free disk space.
* [ ] 8GB RAM, 4vCPUs and 25-100GB of free disk space.
* [ ] 8GB RAM, 2vCPUs and 100GB of free disk space.

**Correct answer:**
* [x] 16GB RAM, 4vCPUs and 25-100GB of free disk space.

**28. Which of the following steps are required to add a worker node in the UCP cluster?**


* [ ] Provision a node and Install Docker enterprise engine on it.
* [ ] Run the <code>docker swarm join</code> command to join the new node to the cluster.
* [ ] Deploy an instance of the ucp-agent on the new node.
* [ ] ucp-agent then installs the necessary components on the worker node.

**Correct answer:**
* [x] Provision a node and Install Docker enterprise engine on it.
* [x] Run the <code>docker swarm join</code> command to join the new node to the cluster.
* [x] ucp-agent then installs the necessary components on the worker node.

**29. Docker Content Trust (DCT) provides the ability to use digital signatures for data sent to and received from remote Docker registries.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**30. What will happen if the container consumes more memory than its limit?**


* [ ] the container will not be killed
* [ ] the container will be killed with an Out of Memory exception
* [ ] the container’s memory usage will be throttled

**Correct answer:**
* [x] the container will be killed with an Out of Memory exception

**31. To take a backup of UCP, which docker image do you need to run along with the backup command?**


* [ ] docker/ucp-backup
* [ ] docker/ucp
* [ ] docker/backup
* [ ] docker/backup-ucp

**Correct answer:**
* [x] docker/ucp

**32. What will happen if the <code>--memory-swap</code> is set to 0?**


* [ ] the container does not have access to swap
* [ ] the container is allowed to use unlimited swap
* [ ] the setting is ignored, and the value is treated as unset

**Correct answer:**
* [x] the setting is ignored, and the value is treated as unset

**33. What is the command to create an overlay network that can also be connected by standalone containers that were not created as part of a swarm service.**


* [ ] <code>docker network create --driver overlay --attachable my-overlay-network</code>
* [ ] <code>docker network create --driver overlay --subnet 10.15.0.0/16 my-overlay-network</code>
* [ ] <code>docker network create --driver overlay --opt encrypted my-overlay-network</code>
* [ ] <code>docker network create --driver overlay my-overlay-network</code>

**Correct answer:**
* [x] <code>docker network create --driver overlay --attachable my-overlay-network</code>

**34. Unless specified otherwise, docker publishes the exposed port on all network interfaces on the host.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**35. When you create a swarm service and do not specify a user-defined overlay network, it connects to the ... network by default**


* [ ] host
* [ ] bridge
* [ ] macvlan
* [ ] ingress

**Correct answer:**
* [x] ingress

**36. Which command is used to get the events of the container named <code>webapp</code>?**


* [ ] <code>docker system events since 10m</code>
* [ ] <code>docker system events --filter 'container=webapp'</code>
* [ ] <code>docker system events --filter 'image=webapp'</code>

**Correct answer:**
* [x] <code>docker system events --filter 'container=webapp'</code>

**37. How does docker map a port on a container to a port on the host?**


* [ ] Using an internal load balancer
* [ ] FirewallD Rules
* [ ] Using an external load balancer
* [ ] IPTables Rules

**Correct answer:**
* [x] IPTables Rules

**38. An application deployed in a kubernetes cluster has 2 tiers - a <code>web</code> service that must be externally accessible by users on the nodes interfaces and a <code>database</code> service that must be accessible within the cluster only. What service types should be configured for each?**


* [ ] Web - NodePort, Database - LoadBalancer
* [ ] Web - ClusterIP, Database - ClusterIP
* [ ] Web - NodePort, Database - ClusterIP
* [ ] Web - ClusterIP, Database - NodePort

**Correct answer:**
* [x] Web - NodePort, Database - ClusterIP

**39. What is the type and the name of the network created for the DTR services to communicate with each other?**


* [ ] overlay/dtr
* [ ] overlay/dtr-ol
* [ ] bridge/dtr
* [ ] bridge/dtr-ol

**Correct answer:**
* [x] overlay/dtr-ol

**40. Which of the following solutions support network policies?**


* [ ] kube-router
* [ ] Calico
* [ ] Flannel
* [ ] Weave-Net

**Correct answer:**
* [x] kube-router
* [x] Calico
* [x] Weave-Net

**41. What is a Linux feature that prevents a process within the container from performing filesystem related operations such as altering attributes of certain files?**


* [ ] Control Groups (CGroups)
* [ ] Namespaces
* [ ] Kernel Capabilities
* [ ] Network Namespaces

**Correct answer:**
* [x] Kernel Capabilities

**42. What flags are used to configure encryption on docker daemon without any authentication?**


* [ ]  tlsverify, tlscert, tlskey
* [ ] key, cert, tls
* [ ] tls, tlscert, tlskey
* [ ] host, key, cert, tls

**Correct answer:**
* [x] tls, tlscert, tlskey

**43. In which of the following will image scanning look for known vulnerabilities**


* [ ] OS packages
* [ ] Libraries 
* [ ] Other dependencies that are defined in a container image

**Correct answer:**
* [x] OS packages
* [x] Libraries 
* [x] Other dependencies that are defined in a container image

**44. Which of the statements best describes "Subjects" in the Access Control Model?**


* [ ] A subject represents a user, team, organization
* [ ] A subject does not represent a service account.
* [ ] A subject can be granted a role that defines permitted operations against one or more resource sets.

**Correct answer:**
* [x] A subject represents a user, team, organization
* [x] A subject can be granted a role that defines permitted operations against one or more resource sets.

**45. Which of the statements best describe "Grants" in the Access Control Model?**


* [ ] Grants define which users can access what resources in what way.
* [ ] A grant is made up of a role and a resource set.
* [ ] A grant is made up of a subject, a role, and a resource set.
* [ ] Grants are effectively Access Control Lists (ACLs) which provide comprehensive access policies for an entire organization when grouped together.

**Correct answer:**
* [x] Grants are effectively Access Control Lists (ACLs) which provide comprehensive access policies for an entire organization when grouped together.
* [x] Grants define which users can access what resources in what way.
* [x] A grant is made up of a subject, a role, and a resource set.

**46. Which of the following statements best describes users?**


* [ ] Users are shared across UCP and DTR.
* [ ] When you create a new user in UCP, that user becomes available in DTR.
* [ ] When you create a new user in DTR, that user does not become available in UCP.

**Correct answer:**
* [x] Users are shared across UCP and DTR.
* [x] When you create a new user in UCP, that user becomes available in DTR.

**47. The … allows you to authorize a remote Docker engine to a specific user account managed in Docker EE, absorbing all associated RBAC controls in the process**


* [ ] DTR
* [ ] UCP
* [ ] RBAC
* [ ] Client bundle

**Correct answer:**
* [x] Client bundle

**48. Which component is responsible for performing all of these operations: Maintaining the layered architecture, creating a write-able layer, moving files across layers to enable Copy-OnWrite etc?**


* [ ] Namespaces
* [ ] Storage drivers
* [ ] Control groups
* [ ] LibContainer

**Correct answer:**
* [x] Storage drivers

**49. Which of the following modes is used to configure the device-mapper storage driver**


* [ ] loop-lvm
* [ ] direct-lvm
* [ ] CoW
* [ ] ReadWriteMany

**Correct answer:**
* [x] loop-lvm
* [x] direct-lvm

**50. You are developing an e-commerce application. The application must store cart details of users temporarily as long as the user’s session is active. What is the recommended approach to storing the cart details with the application deployed as a docker container?**


* [ ] Store the cart details in the /tmp directory of the container
* [ ] Store the cart details in the memory of the container
* [ ] Store the cart details in a volume backed by an in-memory cache service like redis

**Correct answer:**
* [x] Store the cart details in a volume backed by an in-memory cache service like redis

**Explaination**: as containers are ephemeral in nature, you will lose the data.

**51. Which among the below is a correct command to start a webapp container with the volume vol2, mounted to the destination directory /app**


* [ ] <code>docker run -d --name webapp -v vol2:/app httpd</code>
* [ ] <code> docker run -d --name webapp --volume vol2:/app httpd</code>
* [ ] <code> docker run -d --name webapp --storage vol2:/app httpd</code>
* [ ] `docker run -d --name webapp --mount source=vol2,target=/app httpd`

**Correct answer:**
* [x] <code>docker run -d --name webapp -v vol2:/app httpd</code>
* [x] <code> docker run -d --name webapp --volume vol2:/app httpd</code>
* [x] `docker run -d --name webapp --mount source=vol2,target=/app httpd`

**52. Which statement best describes a kubernetes storage class?**


* [ ] A StorageClass provides a way for administrators to describe the "classes" of storage they offer
* [ ] Each StorageClass contains the fields provisioner, parameters, and reclaimPolicy.
* [ ] The StorageClass objects can use a provisioner that can dynamically provision storage on supported storage providers.
* [ ] A StorageClass requires an equal number of Persistent Volumes pre-provisioned before being used by a Persistent Volume Claim.

**Correct answer:**
* [x] A StorageClass provides a way for administrators to describe the "classes" of storage they offer
* [x] Each StorageClass contains the fields provisioner, parameters, and reclaimPolicy.
* [x] The StorageClass objects can use a provisioner that can dynamically provision storage on supported storage providers.

**53. What is the status of a persistent volume when it is associated with a claim?**


* [ ] Available
* [ ] Bound 
* [ ] Released
* [ ] Failed 

**Correct answer:**
* [x] Bound 

**54. What are the different access modes configurable on a persistent volume?**


* [ ] ReadOnlyMany
* [ ] ReadWrite
* [ ] ReadWriteMany
* [ ] ReadOnly
* [ ] ReadWriteOnce

**Correct answer:**
* [x] ReadOnlyMany
* [x] ReadWriteMany
* [x] ReadWriteOnce

**55. Which statements best describe a PersistentVolumeClaim?**


* [ ] A PersistentVolumeClaim (PVC) is a request for storage by a user. 
* [ ] A PVC will be automatically bound to a PV on creation when a PV is available
* [ ] Claims can request specific size and access modes
* [ ] Each PVC contains the fields provisioner, parameters, and reclaimPolicy.

**Correct answer:**
* [x] A PersistentVolumeClaim (PVC) is a request for storage by a user. 
* [x] A PVC will be automatically bound to a PV on creation when a PV is available
* [x] Claims can request specific size and access modes

---


## Mock Exam 2

**1. Which command can be used to return the current autolock key used to lock a docker swarm cluster?**


* [ ] <code>docker swarm lock-key</code>
* [ ] <code>docker swarm lock --autolock=true</code>
* [ ] <code>docker swarm unlock --autolock=true</code>
* [ ] <code>docker swarm unlock-key</code>

**Correct answer:**
* [x] <code>docker swarm unlock-key</code>

**2. The ... assigns tasks to nodes in Docker Swarm.**


* [ ] scheduler
* [ ] dispatcher
* [ ] orchestrator
* [ ] allocator

**Correct answer:**
* [x] dispatcher

**3. How many manager nodes must be online in a cluster with 13 manager nodes for the swarm cluster to continue to operate?**


* [ ] 3
* [ ] 1
* [ ] 6
* [ ] 7

**Correct answer:**
* [x] 7

**4. ... is defined as the minimum number of managers required to be present for carrying out cluster management tasks.**


* [ ] Fault tolerance
* [ ] Quorum
* [ ] Consensus
* [ ] RAFT

**Correct answer:**
* [x] Quorum

**5. What is the command to change the role of a manager node named `manager1` to a worker node in a Docker Swarm cluster?**


* [ ] <code>docker swarm node demote manager1</code>
* [ ] <code>docker demote node manager1</code>
* [ ] <code>docker node demote manager1</code>
* [ ] <code>docker node demote manager1 worker</code>

**Correct answer:**
* [x] <code>docker node demote manager1</code>

**6. We deployed a container called `webapp`. What is the command to inspect this container to get the property `IPPrefixLen` on the container**


* [ ] <code>docker container inspect webapp | grep IPPrefixLen</code>
* [ ] <code>docker container top webapp | grep IPPrefixLen</code>
* [ ] <code>docker container run webapp | grep IPPrefixLen</code>
* [ ] <code>docker container logs webapp | grep IPPrefixLen</code>

**Correct answer:**
* [x] <code>docker container inspect webapp | grep IPPrefixLen</code>

**7. Which command can be used to remove a stack named `webapp`?**


* [ ] <code>docker stack deploy webapp</code>
* [ ] <code>docker stack ls webapp</code>
* [ ] <code>docker stack services webapp</code>
* [ ]  <code>docker stack rm webapp</code>

**Correct answer:**
* [x]  <code>docker stack rm webapp</code>

**8. When you create a new container, you add a new read-only  layer on top of the underlying layers.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**Explaination**: you add a new writable layer on top of the underlying layers.

**9. Which of the below are features/functionalities offered by Kubernetes?**


* [ ] Self-healing & Batch execution
* [ ] Container Image Management
* [ ] Automated rollouts and rollbacks
* [ ] Auto scaling

**Correct answer:**
* [x] Self-healing & Batch execution
* [x] Automated rollouts and rollbacks
* [x] Auto scaling

**Explaination**: Other features are Automated Scheduling, Horizontal Scaling & Load balancing, Storage Orchestration etc.

**10. Which option of the docker service command can be used to update 4 replicas at a time of a service named `mywebapp`?**


* [ ] `--update-delay 4`
* [ ] `--update-parallelism 4`
* [ ] `--placement-pref-add 4`
* [ ] `--replicas 4`

**Correct answer:**
* [x] `--update-parallelism 4`

**11. In Docker Swarm, with a global service, you can specify a minimum number of replicas of the service to be provisioned on each node.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**Explaination**: With the replicated service

**12. Where do you specify image names in a pod definition YAML file to be deployed on Kubernetes?**


* [ ] <code>containers.image</code>
* [ ] <code>spec.containers.image</code>
* [ ] <code>template.containers.image</code>
* [ ] <code>kind.containers.image</code>

**Correct answer:**
* [x] <code>spec.containers.image</code>

**13. What is the command to rebalance the docker swarm cluster workloads?**


* [ ] <code>docker service update <service-name></code>
* [ ] <code>docker service update --force <service-name></code>
* [ ] <code>docker update service <service-name></code>
* [ ] <code>docker update service --force <service-name></code>

**Correct answer:**
* [x] <code>docker service update --force <service-name></code>

**Explaination**: If you really need to rebalance the cluster, then you could run the "docker service update" command with the force flag for each service.

**14. Where do you specify labels to a pod in a pod definition YAML file to be deployed on Kubernetes?**


* [ ] <code>labels</code>
* [ ] <code>spec.labels</code>
* [ ] <code>spec.containers.labels</code>
* [ ] <code>metadata.labels</code>

**Correct answer:**
* [x] <code>metadata.labels</code>

**15. How do you inject configmap into a pod in Kubernetes?**


* [ ] Using <code>envFrom</code> and <code>configMapRef</code>
* [ ] Using <code>env</code> and <code>configMapRef</code>
* [ ] Using <code>envFrom</code> and <code>configMap</code>
* [ ] Using <code>env</code> and <code>configMap</code>

**Correct answer:**
* [x] Using <code>envFrom</code> and <code>configMapRef</code>

**Explaination**: Use envFrom to define all of the ConfigMap's data as container environment variables. The key from the ConfigMap becomes the environment variable name in the Pod.

**16. Refer to the below specification and identify which of the statements are true?**


* [ ] This is an invalid configuration because the selector matchLabel nginx does not match the label web set on the deployment
* [ ] This is an invalid configuration because there are more than 1 containers configured in the template
* [ ] This is an invalid configuration because the selector field must come under the template section and not directly under spec
* [ ] This is an invalid configuration because the API version is not set correctly
* [ ] This is a valid configuration

**Correct answer:**
* [x] This is a valid configuration

**Code**: 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-application
  labels:
    app: web
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80
      - name: logger
        image: log-agent:1.2
      - name: monitor
        image: monitor-agent:1.0

**17. Choose the correct instruction to add the command instruction with the command `echo "Hello World"` in the Dockerfile**


* [ ] <code>CMD [echo "Hello World"]</code>
* [ ] <code>CMD ["echo", "Hello World"]</code>
* [ ] <code>CMD ["Hello World"]</code>
* [ ] <code>CMD Hello World </code>

**Correct answer:**
* [x] <code>CMD ["echo", "Hello World"]</code>

**18. Which is the correct statement referring to the following Compose file?**


* [ ] The depends_on configuration is not supported in Compose version 3
* [ ] db and redis services will be started before web service
* [ ] web service will be started before db and redis services

**Correct answer:**
* [x] db and redis services will be started before web service

**Code**: 

version: "3.8"
services:
  web:
    build: .
    depends_on:
      - db
      - redis
    volumes:
      - .:/code
      - logvolume01:/var/log
    ports:
      - "8080:80"
  redis:
    image: redis
  db:
    image: postgres
volumes:
  logvolume01: {}

**19. What is the right instruction to download a file from https://file.tar.xz and auto-extract it into /testdir in the image**


* [ ] <code> ADD https://file.tar.xz  /testdir</code>
* [ ] <code> COPY https://file.tar.xz  /testdir</code>
* [ ]  <code>RUN https://file.tar.xz  /testdir</code>
* [ ] <code>Volume https://file.tar.xz  /testdir</code>

**Correct answer:**
* [x] <code> ADD https://file.tar.xz  /testdir</code>

**Explaination**: COPY instruction only supports the basic copying of local files into the container.


**20. Which command is used to delete the stopped containers?**


* [ ] <code>docker container remove $(docker container ls -aq)</code>
* [ ] <code>docker container prune</code>
* [ ] <code>docker rm $(docker ps --filter status=exited -q)</code>

**Correct answer:**
* [x] <code>docker container prune</code>
* [x] <code>docker rm $(docker ps --filter status=exited -q)</code>

**21. Which of the below can help minimize the image size?**


* [ ] Only install necessary packages within the image
* [ ] Avoid sending unwanted files to the build context using .dockerignore
* [ ] Combine multiple dependent instructions into a single instruction and cleanup temporary files 
* [ ] Move the instructions that are likely to change most frequently to the bottom of the Dockerfile
* [ ] Use multi-stage builds

**Correct answer:**
* [x] Only install necessary packages within the image
* [x] Combine multiple dependent instructions into a single instruction and cleanup temporary files 
* [x] Use multi-stage builds
* [x] Avoid sending unwanted files to the build context using .dockerignore

**22. What is the command to find images with a name containing `busybox`, at least `3 stars` and are `official` builds**


* [ ] <code> docker find  --filter is-official=true --filter stars=3 busybox</code>
* [ ] <code>docker search --filter is-official=true --filter stars=3 busybox</code>
* [ ] <code>docker find  --filter is-official=true --limit=3 busybox</code>
* [ ] <code>docker search --filter is-official=true --limit=3 busybox</code>

**Correct answer:**
* [x] <code>docker search --filter is-official=true --filter stars=3 busybox</code>

**23. Which of the following statements best describe Docker Trusted Registry (DTR)?**


* [ ] Docker Trusted Registry (DTR) is Mirantis’s enterprise-grade image storage solution. 
* [ ] Installed behind the firewall only on-premises
* [ ] DTR provides a secure environment from which users can store and manage Docker images.
* [ ]  Using DTR in Docker EE we can control who can access and make changes to your cluster and applications

**Correct answer:**
* [x] Docker Trusted Registry (DTR) is Mirantis’s enterprise-grade image storage solution. 
* [x] DTR provides a secure environment from which users can store and manage Docker images.

**24. A government facility runs a secure data center with no internet connectivity. A new application requires access to docker images hosted on docker hub. What is the best approach to solve this?**


* [ ] Get the Dockerfile of the image and build a local version from within the restricted environment.
* [ ] Establish a secure link between the host in the restricted environment and docker hub
* [ ] Pull docker images from a host with access to docker hub, convert to a tarball using docker image save command, and copy to the restricted environment and extract the tarball
* [ ] Pull docker images from a host with access to docker hub, then push to a registry hosted within the restricted environment.

**Correct answer:**
* [x] Pull docker images from a host with access to docker hub, convert to a tarball using docker image save command, and copy to the restricted environment and extract the tarball

**25. Which of the below commands may be used to change the default logging driver to splunk?**


* [ ] <code>echo ‘{"splunk": "log-driver"}’ > /etc/docker/daemon.json</code>
* [ ] <code>echo ‘{"log-driver": "splunk"}’ > /var/lib/docker/daemon.json</code>
* [ ] <code>echo ‘{"splunk": "log-driver"}’ > /var/lib/docker/daemon.json</code>
* [ ] <code>echo ‘{"log-driver": "splunk"}’ > /etc/docker/daemon.json</code>

**Correct answer:**
* [x] <code>echo ‘{"log-driver": "splunk"}’ > /etc/docker/daemon.json</code>

**26. Refer to the Dockerfile below and identify which value should be added to the `--from= ` option in the second stage to copy the application build from the first stage.**


* [ ] 0
* [ ] builder
* [ ] golang:1.7.3
* [ ] /app

**Correct answer:**
* [x] 0
* [x] builder

**Code**: 

FROM golang:1.7.3 AS builder
WORKDIR /go/src/github.com/alexellis/href-counter/
RUN go get -d -v golang.org/x/net/html  
COPY app.go    .
RUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o app .

FROM alpine
RUN apk --no-cache add ca-certificates
WORKDIR /root/
COPY --from=???? /go/src/github.com/alexellis/href-counter/app .
CMD ["./app"]


**27. Run a webapp container, and make sure that No logs are configured for this container**


* [ ] <code>docker run -it --logging-driver none webapp</code>
* [ ] <code>docker run -it webapp</code>
* [ ] <code>docker run -it --log-driver none webapp</code>
* [ ] <code>docker run -it --log none webapp</code>

**Correct answer:**
* [x] <code>docker run -it --log-driver none webapp</code>

**28. What are the steps to be followed to backup the Docker swarm database?**


* [ ] Create a tar backup of the swarm data at /var/lib/docker/db and restart the docker service.
* [ ] Stop docker service, create a tar backup of the swarm data at /var/lib/docker/swarm, start the docker.
* [ ] Stop docker service, run the `docker backup` command and start the docker
* [ ] Create a tar backup of the swarm data at `/var/lib/docker`, stop docker service

**Correct answer:**
* [x] Stop docker service, create a tar backup of the swarm data at /var/lib/docker/swarm, start the docker.

**29. The communication between UCP server on the manager node and its associated agent services running on worker nodes are encrypted and properly authenticated using certificate-based client-server authentication by default.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**30. Which command can be used to enable the debugging mode on the Docker Host?**


* [ ] <code>echo '{"debug": true}' > /etc/docker/daemon.json</code>
* [ ] <code>echo '{"debug"}' > /etc/docker/daemon.json</code>
* [ ] <code>echo '{"debug": true}' > /var/lib/docker/daemon.json</code>
* [ ] <code>echo '{"debug"}' > /var/lib/docker/daemon.json</code>

**Correct answer:**
* [x] <code>echo '{"debug": true}' > /etc/docker/daemon.json</code>

**31. Which command can be used to start the docker engine enterprise service on a systemctl configured system?**


* [ ] <code>sudo systemctl start docker-ee</code>
* [ ] <code>sudo systemctl start docker</code>
* [ ] <code>sudo systemctl docker start</code>
* [ ] <code>sudo systemctl docker-ee start</code>

**Correct answer:**
* [x] <code>sudo systemctl start docker</code>

**32. What is the high level command to restore the DTR from a backup tar named `dtr-metadata-backup.tar` ?**


* [ ] <code>docker run -i --rm  docker/dtr-restore < dtr-metadata-backup.tar </code>
* [ ] <code>docker run -i --rm docker/dtr restore < dtr-metadata-backup.tar </code>
* [ ] <code>docker run -i --rm docker/restore-dtr < dtr-metadata-backup.tar </code>
* [ ] <code>docker run -i --rm docker/restore dtr < dtr-metadata-backup.tar </code>

**Correct answer:**
* [x] <code>docker run -i --rm docker/dtr restore < dtr-metadata-backup.tar </code>

**33. How can you mitigate the risk of system instability due to OOME?**


* [ ] Perform tests to understand the memory requirements of your application before placing it into production.
* [ ] Ensure that your application runs only on hosts with adequate resources.
* [ ] Limit the amount of memory your container can use.

**Correct answer:**
* [x] Perform tests to understand the memory requirements of your application before placing it into production.
* [x] Ensure that your application runs only on hosts with adequate resources.
* [x] Limit the amount of memory your container can use.

**34. What IPTables chains does Docker modify to configure port mapping on a host?**


* [ ] INPUT
* [ ] FORWARD
* [ ] DOCKER
* [ ] OUTPUT

**Correct answer:**
* [x] DOCKER

**35. ETCD by default listens on port 2780.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**Explaination**: When you start ETCD it will by default listens on port 2379

**36. What types of networks will be created when you initialize a swarm or join a Docker host to an existing swarm?**


* [ ] host
* [ ] bridge
* [ ] macvlan
* [ ] ingress

**Correct answer:**
* [x] bridge
* [x] ingress

**37. Service Discovery allows containers and Services to locate and communicate with each other with their names.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**38. What is the command to create an overlay network driver called `my-overlay` with subnet `10.15.0.0/16` ?**


* [ ] <code>docker network create my-overlay</code>
* [ ] <code>docker network create --driver overlay --subnet 10.15.0.0/16 my-overlay</code>
* [ ] <code>docker network create -d overlay -subnet 10.15.0.0/16</code>
* [ ] <code>docker network create overlay my-overlay</code>

**Correct answer:**
* [x] <code>docker network create --driver overlay --subnet 10.15.0.0/16 my-overlay</code>

**39. Which command returns events since the past 30 minutes?**


* [ ] <code>docker system events since 30m</code>
* [ ] <code>docker system events --since 30m</code>
* [ ] <code>docker container events --since 30m</code>
* [ ]  <code>docker container events since 30m</code>

**Correct answer:**
* [x] <code>docker system events --since 30m</code>

**40. Which of the following are valid service types in kubernetes?**


* [ ] NodePort
* [ ] ClusterIP
* [ ] LoadBalancer
* [ ] ExternalName
* [ ] ElasticLoadBalancer

**Correct answer:**
* [x] NodePort
* [x] ClusterIP
* [x] LoadBalancer
* [x] ExternalName

**41. In DTR, when a user creates a repository, by default other users will also have permissions to make changes to the repository.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**Explaination**: By default, anonymous users can only pull images from public repositories. They can’t create new repositories or push to existing ones. You can then grant permissions to enforce fine-grained access control to image repositories.
When a user creates a repository, only that user has permissions to make changes to the repository.


**42. The organization is a group of teams that share a specific set of permissions, defined by the roles of the organization**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**43. To scan an image, DTR ________________.**


* [ ] Extracts a copy of the image layers from backend storage.
* [ ] Extracts the files from the layer into a working directory inside the dtr-jobrunner container.
* [ ] Executes the scanner against the files in this working directory, collecting a series of scanning data.
* [ ] Once the scanning data is collected, the working directory for the layer will not be removed automatically.
* [ ] Once the scanning data is collected, the working directory for the layer is removed.

**Correct answer:**
* [x] Extracts a copy of the image layers from backend storage.
* [x] Extracts the files from the layer into a working directory inside the dtr-jobrunner container.
* [x] Executes the scanner against the files in this working directory, collecting a series of scanning data.
* [x] Once the scanning data is collected, the working directory for the layer is removed.

**44. What feature of the Docker Enterprise Edition provides the functionality to create users and group them into teams which are nothing but group of users and tie them up with organization.**


* [ ] UCP Bundles
* [ ] Docker Content Trust
* [ ] Image Scanning
* [ ] RBAC

**Correct answer:**
* [x] RBAC

**45. Universal Control Plane (UCP), lets you authorize users to view, edit, and use cluster resources by granting role-based permissions against resource sets.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**46. Which option is used to change the default storage driver to use `devicemapper`?**


* [ ] <code>{"storage-driver": "devicemapper"}</code>
* [ ] <code>{"driver": "devicemapper"}</code>
* [ ] <code>{"dev-storage": "devicemapper"}</code>

**Correct answer:**
* [x] <code>{"storage-driver": "devicemapper"}</code>

**47. Which of the following are a valid storage driver supported by Docker?**


* [ ] AUFS
* [ ] S3
* [ ] overlay2
* [ ] Device Mapper

**Correct answer:**
* [x] AUFS
* [x] overlay2
* [x] Device Mapper

**48. By default all files created inside a container are stored on a writable container layer.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**Explaination**: This means that: The data doesn’t persist when that container no longer exists, and it can be difficult to get the data out of the container if another process needs it.

**49. What is the command to create a volume with the name my-vol**


* [ ] <code>docker volume create my-vol</code>
* [ ] <code>docker create volume my-vol</code>
* [ ] <code>docker volume prune</code>
* [ ] <code>docker volume rm all</code>

**Correct answer:**
* [x] <code>docker volume create my-vol</code>

**50. What is the command to list storage classes in Kubernetes?**


* [ ] <code>kubectl list sc</code>
* [ ] <code>kubectl get sc</code>
* [ ] <code>kubectl get storageclass</code>
* [ ] <code>kubectl list storageclass</code>

**Correct answer:**
* [x] <code>kubectl get sc</code>
* [x] <code>kubectl get storageclass</code>

**51. Which of the following statements best describes ETCD? Select the correct answer**


* [ ] Etcd serves as the backing datastore for kubernetes cluster data
* [ ] ETCD must be deployed on all worker nodes in the cluster
* [ ] ETCD is a distributed reliable key-value store

**Correct answer:**
* [x] Etcd serves as the backing datastore for kubernetes cluster data
* [x] ETCD is a distributed reliable key-value store

**Explaination**: A distributed key-value data store, It helps to maintain the cluster configuration and state. Intended for permanent data storage and retrieval, etcd stores metadata and other information consistently with a fault-tolerant method, including full replication.

- Option B is incorrect because it is not a worker node component, it can be installed and configured in Master Nodes or can be configured on a separate node.


**52. Which statements best describe Persistent Volume in kubernetes?**


* [ ] A PersistentVolume (PV) is a piece of storage in the cluster that has been provisioned by an administrator or dynamically provisioned using Storage Class
* [ ]  It is a resource in the cluster just like a node is a cluster resource.
* [ ] We can create PVs with the same name in multiple namespaces.

**Correct answer:**
* [x] A PersistentVolume (PV) is a piece of storage in the cluster that has been provisioned by an administrator or dynamically provisioned using Storage Class
* [x]  It is a resource in the cluster just like a node is a cluster resource.

**53. Once the Persistent Volume Claim is created, you need to manually bind the persistent volumes to claim.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**Explaination**: A PVC will be automatically bound to a PV on creation when a PV is available


**54. Which statement best describes docker volume plugin?**


* [ ] Docker Engine volume plugins enables Engine deployments to be integrated with external storage systems such as Amazon EBS
* [ ] The local volume plugin helps to create a volume on Docker host and store its data under the /var/lib/docker/volumes/ directory.
* [ ] ZFS, BTRFS and Device Mapper are some of the supported volume drivers

**Correct answer:**
* [x] Docker Engine volume plugins enables Engine deployments to be integrated with external storage systems such as Amazon EBS
* [x] The local volume plugin helps to create a volume on Docker host and store its data under the /var/lib/docker/volumes/ directory.

**55. How can the web application address redis?**


* [ ] Using the container ID generated by redis
* [ ]  Using the name redis
* [ ] Using the internal IP address of the redis container
* [ ] By exposing port 6379 of redis container on the host and then using hosts IP

**Correct answer:**
* [x]  Using the name redis

**Code**: 

version: "3.8"
services:
  web:
    build: .
    depends_on:
      - db
      - redis
    volumes:
      - .:/code
      - logvolume01:/var/log
    ports:
      - "8080:80"
  redis:
    image: redis
  db:
    image: postgres
volumes:
  logvolume01: {}

---


## Mock Exam 3

**1. Which command can be used to run an instance on swarm?**


* [ ] <code>docker container run</code>
* [ ] <code>docker container create</code>
* [ ] <code>docker service create</code>
* [ ] <code>docker swarm  service create</code>

**Correct answer:**
* [x] <code>docker service create</code>

**2. What option may be used to change the default behaviour of a failed task during an update in swarm?**


* [ ] `--update-failure-action`
* [ ]  `--update-parallelism`
* [ ]  `--update-delay`
* [ ]  `--placement-pref-add`

**Correct answer:**
* [x] `--update-failure-action`

**3. Which formula can be used to calculate the Quorum  of N nodes?**


* [ ] N + 1
* [ ] N+1 / 2
* [ ] N / 2 +1
* [ ] N /2 -1

**Correct answer:**
* [x] N / 2 +1

**4. What component is responsible for instructing a worker to run a task?**


* [ ] scheduler
* [ ] dispatcher
* [ ] orchestrator
* [ ] allocater

**Correct answer:**
* [x] scheduler

**5. Which command can be used to check the restart policy of a container named `webapp`?**


* [ ] <code>docker container inspect webapp </code>
* [ ]  <code>docker container info webapp</code>
* [ ] <code>docker container check webapp</code>

**Correct answer:**
* [x] <code>docker container inspect webapp </code>

**6. Which command can be used to list the tasks in a stack named `webapp`?**


* [ ] <code>docker stack deploy webapp</code>
* [ ] <code>docker stack ls webapp</code>
* [ ] <code>docker stack services webapp</code>
* [ ] <code>docker stack ps webapp</code>

**Correct answer:**
* [x] <code>docker stack ps webapp</code>

**7. The routing mesh enables each node in the swarm to accept connections on published ports for any service running in the swarm, even if there’s no task running on the node.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**8. Which command can be used to increase the number of replicas from 2 to 4 of a service named `webapp`? Select the all right answer**


* [ ] <code>docker service update --replicas=4 webapp</code>
* [ ] <code>docker service update --replicas=2 webapp</code>
* [ ] <code>docker service scale webapp=2</code>
* [ ] <code>docker service scale webapp=4</code>

**Correct answer:**
* [x] <code>docker service update --replicas=4 webapp</code>
* [x] <code>docker service scale webapp=4</code>

**9. What is the flag that we can use to define a literal value from the command line while creating a ConfigMap?**


* [ ]  `--env`
* [ ]  `--from-literal`
* [ ]  `--literal`
* [ ]  `--text`

**Correct answer:**
* [x]  `--from-literal`

**Explaination**: You can use kubectl create configmap with the --from-literal argument to define a literal value from the command line:

**10. What is the command to apply `disk=ssd` label to `worker1` in a swarm cluster.**


* [ ] <code>docker node update --label-add disk=ssd worker1</code>
* [ ] <code>docker node update --label-rm disk=ssd worker1</code>
* [ ] <code>docker service update  --labels disk=ssd worker1</code>
* [ ] <code>docker service update --container-label disk=ssd worker1</code>

**Correct answer:**
* [x] <code>docker node update --label-add disk=ssd worker1</code>

**11. After an update to a service named `webapp` we realized that something is wrong with the new version and we want to revert back to the old version. How can we achieve that?**


* [ ] <code>docker service update rollback webapp</code>
* [ ] <code>docker service rollback webapp</code>
* [ ] <code>docker service rm webapp</code>
* [ ] <code>docker service leave webapp</code>

**Correct answer:**
* [x] <code>docker service rollback webapp</code>

**12. Which command can be used to get the logs of a swarm service?**


* [ ] <code>docker container logs SERVICE-NAME</code>
* [ ] <code>docker service logs SERVICE-NAME</code>
* [ ] <code>docker swarm log SERVICE-NAME</code>
* [ ] <code>docker swarm logs SERVICE-NAME</code>

**Correct answer:**
* [x] <code>docker service logs SERVICE-NAME</code>

**13. Create a service using the `my-web-server` image and map UDP port `80` in the container to port `5000` on the overlay network.**


* [ ] <code>docker service create -p 80:5000/udp my-web-server</code>
* [ ] <code>docker service create --publish published=80,target=5000,protocol=udp my-web-server</code>
* [ ] <code>docker service create -p 5000:80/udp my-web-server</code>
* [ ] <code>docker service create --publish published=5000,target=80,protocol=udp my-web-server</code>

**Correct answer:**
* [x] <code>docker service create -p 5000:80/udp my-web-server</code>
* [x] <code>docker service create --publish published=5000,target=80,protocol=udp my-web-server</code>

**14. What is the recommended approach to load a set of configurations into the pod in the form of a file to the path `/var/configs`?**


* [ ] Add a separate env parameter for each config and use a startup script to write to a file
* [ ] Create a ConfigMap with the required configurations, configure it as a volume in the pod definition file and then mount the volume as a file at /var/configs
* [ ] Create a ConfigMap with the required configurations, configure it as an env variable in the pod definition file and use a startup script to write to a file

**Correct answer:**
* [x] Create a ConfigMap with the required configurations, configure it as a volume in the pod definition file and then mount the volume as a file at /var/configs

**15. What are the 4 top level fields a kubernetes definition file for POD contains?**


* [ ] <code>apiVersion</code>
* [ ] <code>templates</code>
* [ ] <code>metadata</code>
* [ ] <code>labels</code>
* [ ] <code>kind</code>
* [ ] <code>spec</code>
* [ ] <code>namespaces</code>
* [ ] <code>containers</code>

**Correct answer:**
* [x] <code>apiVersion</code>
* [x] <code>metadata</code>
* [x] <code>kind</code>
* [x] <code>spec</code>

**Explaination**: apiversion, kind, metadata and spec are the four top level fields a kubernetes definition file consists of.

- Option A and B are incorrect because these are not part of the top level fields that a definition file consists of.

**16. Which of the below statements are correct?**


* [ ] Traffic to port 39376 on the node hosting the pod in the cluster is routed to port 9376 on a POD with the label app web on the same node
* [ ] Traffic to port 39376 on all nodes in the cluster is routed to port 9376 on a random POD with the label app web
* [ ] Traffic to port 80 on the service is routed to port 9376 on a random POD with the label app web
* [ ] Traffic to port 80 on the node is routed to port 9376 on the service

**Correct answer:**
* [x] Traffic to port 39376 on all nodes in the cluster is routed to port 9376 on a random POD with the label app web
* [x] Traffic to port 80 on the service is routed to port 9376 on a random POD with the label app web

**Code**: 
apiVersion: v1
kind: Service
metadata:
  name: web-service
  labels:
    obj: web-service
    app: web
spec:
  selector:
    app: web
  type: NodePort
  ports:
    - protocol: TCP
      port: 80
      targetPort: 9376
      nodePort: 39376

**17. Which of the following is the correct format for CMD instruction**


* [ ] CMD ["executable","param1","param2"]
* [ ] CMD ["param1","param2"]
* [ ] CMD command param1 param2
* [ ] CMD param1 param2

**Correct answer:**
* [x] CMD ["executable","param1","param2"]
* [x] CMD ["param1","param2"]
* [x] CMD command param1 param2

**18. What is the command to stop all running containers on the host?**


* [ ] <code>docker container stop $(docker container ls)</code>
* [ ] <code>docker container rm $(docker container ls -q)</code>
* [ ] <code>docker container stop $(docker container ls -q)</code>
* [ ] <code>docker container stop --all</code>

**Correct answer:**
* [x] <code>docker container stop $(docker container ls -q)</code>

**19. While building a docker image from code stored in a remote URL, which command will be used to build from a directory called `docker` in the branch `dev`?**


* [ ] <code>docker build https://github.com/kk/dca.git#dev:docker</code>
* [ ] <code>docker build https://github.com/kk/dca.git#docker:dev</code>
* [ ] <code>docker build https://github.com/kk/dca.git:dev</code>
* [ ] <code>docker build https://github.com/kk/dca.gitdev:#docker</code>

**Correct answer:**
* [x] <code>docker build https://github.com/kk/dca.git#dev:docker</code>

**20. Using `RUN apt-get update && apt-get install -y` ensures your Dockerfile installs the latest package versions everytime an image is built. This technique is known as …..**


* [ ] Fast build
* [ ] Cache busting
* [ ] Version pinning
* [ ] Build-context

**Correct answer:**
* [x] Cache busting

**21. Print the value of 'Architecture' and 'Os' of an image named `webapp`**


* [ ] `docker image inspect webapp -f '{{.Os}}' -f '{{.Architecture}}'`
* [ ] `docker image inspect webapp -f '{{.Os}} {{.Architecture}}'`
* [ ] `docker image inspect webapp -f '{{.Os}}', -f '{{.Architecture}}'`
* [ ] `docker image inspect webapp -f '{{.Os .Architecture}}'`

**Correct answer:**
* [x] `docker image inspect webapp -f '{{.Os}} {{.Architecture}}'`

**22. What are the features of docker trusted registry (DTR)?**


* [ ] Built-in Access Control
* [ ] Image and Job Management
* [ ] Security Scanning
* [ ] Auto scaling applications
* [ ] Image Signing

**Correct answer:**
* [x] Built-in Access Control
* [x] Image and Job Management
* [x] Security Scanning
* [x] Image Signing

**23. Choose the correct statement regarding the following compose snippet**


* [ ] When the service is deployed it creates 4 containers in total
* [ ] The `webapp` service has a health check configured that tests if the web application is alive every 45 seconds
* [ ] Each time during the health check the `webapp` service waits for 60 seconds to receive a positive response from the curl command
* [ ] The `webapp` can reach the database using the name `dbservice`

**Correct answer:**
* [x] The `webapp` service has a health check configured that tests if the web application is alive every 45 seconds
* [x] The `webapp` can reach the database using the name `dbservice`

**Code**: 
version: "3.8"

services:
  webapp:
    image: webapp
    ports:
      - "8080:80"
    networks:
      - app-net
    deploy:
      mode: replicated
      replicas: 2
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost"]
      interval: 45s
      timeout: 20s
      retries: 3
      start_period: 60s

  dbservice:
    image: mysql
    volumes:
       - db-data:/var/lib/mysql/data
    networks:
       - app-net

volumes:
  db-data:

networks:
  app-net:


**24. What is the user/account and image/repository name for the image `company/nginx`?**


* [ ] image=company, user=nginx
* [ ] image=company, user=company
* [ ] image=nginx, user=nginx
* [ ]  image=nginx, user=company

**Correct answer:**
* [x]  image=nginx, user=company

**25. When a container is created using the image built with the following Dockerfile, what is the command used to RUN the application inside it**


* [ ] <code>pip install flask</code>
* [ ] <code>docker run app.py</code>
* [ ] <code>app.py</code>
* [ ] <code>python app.py</code>

**Correct answer:**
* [x] <code>python app.py</code>

**Code**: 
FROM python:3.6
RUN pip install flask
COPY . /opt/
EXPOSE 8080
WORKDIR /opt
ENTRYPOINT ["python", "app.py"]

**26. Which image is used to deploy the Docker Trusted Registry?**


* [ ] dtr
* [ ] docker/dtr
* [ ] ucp
* [ ] docker/ucp

**Correct answer:**
* [x] docker/dtr

**27. Where is the log of the webapp container, with id `78373635`, stored on the Docker Host?**


* [ ] <code>/var/lib/docker/containers/78373635/78373635.json</code>
* [ ] <code>/var/log/docker/78373635.json</code>
* [ ] <code>/etc/docker/78373635.json</code>
* [ ] <code>/var/lib/docker/tmp/78373635/78373635.json</code>

**Correct answer:**
* [x] <code>/var/lib/docker/containers/78373635/78373635.json</code>

**28. Which of the statements best describes `Roles` in the Access control model?**


* [ ] Roles define what operations can be done by whom.
* [ ] A role is a group of teams that share a specific set of permissions.
* [ ] Most organizations use multiple roles to fine-tune the appropriate access.

**Correct answer:**
* [x] Roles define what operations can be done by whom.
* [x] Most organizations use multiple roles to fine-tune the appropriate access.

**Explaination**: A role is a set of permitted operations against a type of resource, like a container or volume, which is assigned to a user or a team with a grant.

**29. Which of the following are the major components of Docker Engine - Enterprise?**


* [ ] A server which is a type of long-running program called a daemon process (the dockerd command).
* [ ] A REST API which specifies interfaces that programs can use to talk to the daemon and instruct it what to do.
* [ ] A command line interface (CLI) client (the docker command).

**Correct answer:**
* [x] A server which is a type of long-running program called a daemon process (the dockerd command).
* [x] A REST API which specifies interfaces that programs can use to talk to the daemon and instruct it what to do.
* [x] A command line interface (CLI) client (the docker command).

**30. What are the prerequisites for restoring swarm?**


* [ ] You must restore the backup on the same Docker Engine version.
* [ ] You must use the same IP as the node from which you made the backup.
* [ ] If auto-lock was enabled on the old Swarm, the unlock key is required to perform the restore.

**Correct answer:**
* [x] You must restore the backup on the same Docker Engine version.
* [x] You must use the same IP as the node from which you made the backup.
* [x] If auto-lock was enabled on the old Swarm, the unlock key is required to perform the restore.

**31. Which of the following steps are required on each manager node to restore data to a new swarm?**


* [ ] Shut down the Docker Engine on the node you select for the restore
* [ ] Remove the /var/lib/docker directory on the new Swarm if it exists.
* [ ] Remove the contents of the /var/lib/docker/swarm directory on the new Swarm if it exists.
* [ ] Restore the /var/lib/docker/swarm directory with the contents of the backup
* [ ] Start Docker on the new node. Unlock the swarm if necessary
* [ ] Re-initialize the swarm so that the node does not attempt to connect to nodes that were part of the old swarm, and presumably no longer exist.

**Correct answer:**
* [x] Shut down the Docker Engine on the node you select for the restore
* [x] Remove the contents of the /var/lib/docker/swarm directory on the new Swarm if it exists.
* [x] Restore the /var/lib/docker/swarm directory with the contents of the backup
* [x] Start Docker on the new node. Unlock the swarm if necessary
* [x] Re-initialize the swarm so that the node does not attempt to connect to nodes that were part of the old swarm, and presumably no longer exist.

**32. What is a linux feature that allows isolation of containers from the Docker host?**


* [ ] Control Groups (CGroups)
* [ ] Namespaces
* [ ] Kernel Capabilities
* [ ] LXC

**Correct answer:**
* [x] Namespaces

**33. Which environment variable will be used to connect a remote docker server?**


* [ ] DOCKER_REMOTE
* [ ] DOCKER_HOST
* [ ] DOCKER_CONFIG
* [ ] DOCKER_SERVICE

**Correct answer:**
* [x] DOCKER_HOST

**34. What is the command to perform a backup of DTR node?**


* [ ] Run the <code>docker/dtr backup <dtr-cli-backup></code> command
* [ ] Run the <code>docker/dtr-backup <dtr-cli-backup></code> command
* [ ] Run the <code>docker/backup-dtr <dtr-cli-backup></code> command
* [ ] Run the <code>docker/backup dtr <dtr-cli-backup></code> command

**Correct answer:**
* [x] Run the <code>docker/dtr backup <dtr-cli-backup></code> command

**35. Which component is responsible to serve the UCP components such as the web ui, the authentication api,metrics server, proxy and data stores used by UCP in the form of containers?**


* [ ] UCP Agent
* [ ] Docker Enterprise Edition
* [ ] Docker Community Edition
* [ ] Docker Trusted Registry (DTR)

**Correct answer:**
* [x] UCP Agent

**36. By default, all containers get the same share of CPU cycles. How to modify the shares?**


* [ ] <code>docker container run --cpu-shares=512 webapp</code>
* [ ] </code>docker container run --cpuset-cups=512 webapp</code>
* [ ] <code>docker container run --cpu-quota=512 webapp</code>
* [ ] <code>docker container run --cpus=512 webapp</code>

**Correct answer:**
* [x] <code>docker container run --cpu-shares=512 webapp</code>

**37. What component is responsible for managing CPU resources and allocating the time of the CPU between different processes?**


* [ ] Allocater
* [ ] CFS
* [ ] Controller
* [ ] Allocater, Controller

**Correct answer:**
* [x] CFS

**38. When you initialize a Docker Swarm cluster it creates a new network of type overlay which is an internal private network that spans across all the nodes participating in the swarm cluster.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**39. The built-in DNS server in Docker always runs at IP address …**


* [ ] 127.0.0.11
* [ ] 127.0.0.1
* [ ] 172.17.0.3
* [ ] 172.17.0.1

**Correct answer:**
* [x] 127.0.0.11

**40. What is the command to create an `overlay` network driver called `my-overlay`?**


* [ ] <code>docker network create my-overlay</code>
* [ ] <code>docker create network my-overlay</code>
* [ ] <code>docker network create -d overlay my-overlay</code>
* [ ] <code>docker network create overlay my-overlay</code>

**Correct answer:**
* [x] <code>docker network create -d overlay my-overlay</code>

**41. If the service type is NodePort, then Kubernetes will allocate a port on every worker node.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**42. Which among the following statements are true without any change made to the default behaviour of network policies in the namespace?**


* [ ] As soon as a network policy is associated with a POD traffic between all PODs in the namespace is denied
* [ ] As soon as a network policy is associated with a POD all ingress and egress traffic to that POD are denied except allowed by the network policy
* [ ] As soon as a network policy is associated with a POD all ingress and egress traffic to that POD are allowed except for the the ones blocked by the network policy

**Correct answer:**
* [x] As soon as a network policy is associated with a POD all ingress and egress traffic to that POD are denied except allowed by the network policy

**43. What is the default range of ports that Kubernetes uses for NodePort if one is not specified?**


* [ ] 32767-64000
* [ ] 30000-32767
* [ ] 32000-3276
* [ ] 80-8080

**Correct answer:**
* [x] 30000-32767

**44. Which of the statements best describe "Resource sets" in Access Control Model?**


* [ ] To control user access, cluster resources are grouped into Docker Swarm collections or Kubernetes namespaces.
* [ ] Together, collections and namespaces are named resource sets.
* [ ] A group of teams that share a specific set of permissions

**Correct answer:**
* [x] To control user access, cluster resources are grouped into Docker Swarm collections or Kubernetes namespaces.
* [x] Together, collections and namespaces are named resource sets.

**45. The communication between the swarm related services on different nodes in the swarm cluster are not secured by default.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**Explaination**: The communication between the nodes in the swarm cluster are secured by authentication and encryption using TLS/SSL certificates

**46. In which service does the DTR image scanning occur?**


* [ ] A service known as the dtr-jobrunner container
* [ ] A service known as the dtr-registry container
* [ ] A service known as the dtr-api container
* [ ] A service known as the dtr-runner container

**Correct answer:**
* [x] A service known as the dtr-jobrunner container

**Explaination**: When you install DTR on a node, the <code>dtr-jobrunner</code> started and runs cleanup jobs in the background

**47. Which of the following is a common workflow for RBAC in Docker EE is**


* [ ] Create users, teams and organization
* [ ] Create custom roles with set of permissions
* [ ] Combine resources sets using collection
* [ ] Create a grant that combines subjects, roles and resource sets.

**Correct answer:**
* [x] Create users, teams and organization
* [x] Create custom roles with set of permissions
* [x] Combine resources sets using collection
* [x] Create a grant that combines subjects, roles and resource sets.

**48. UCP has its own built-in authentication mechanism and integrates with LDAP and AD services.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**49. A client bundle is a group of certificates downloadable directly from the Docker Trusted Registry (DTR) user interface within the admin section for “My Profile”**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**50. overlay2, aufs, and devicemapper all operate at the file level rather than the block level.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**Explaination**: devicemapper operate at the block level rather than the file level


**51. The volumes are mounted as “readonly” by default inside the container if no options are specified.**


* [ ] True
* [ ] False 

**Correct answer:**
* [x] False 

**Explaination**: The volumes are mounted as “readwrite” by default…

**52. Which among the below is a correct command to start a container named `webapp` with the volume `vol3`, mounted to the destination directory `/opt` in `readonly` mode?**


* [ ] <code>docker run -d --name webapp --mount source=vol3,target=/opt,readonly httpd</code>
* [ ] <code>docker run -d --name webapp -v vol3:/opt:ro httpd</code>
* [ ] <code>docker run -d --name webapp -v vol3:/opt:readonly httpd</code>
* [ ] <code>docker run -d --name webapp --volume vol3:/opt:ro httpd</code>
* [ ] <code>docker run -d --name webapp --mount source=vol3,target=/opt,ro httpd</code>

**Correct answer:**
* [x] <code>docker run -d --name webapp --mount source=vol3,target=/opt,readonly httpd</code>
* [x] <code>docker run -d --name webapp -v vol3:/opt:ro httpd</code>
* [x] <code>docker run -d --name webapp --volume vol3:/opt:ro httpd</code>

**53. Which statements best describe <code>emptyDir</code> volume type?**


* [ ] An <code>emptyDir</code> volume is first created when a Pod is assigned to a node, and still exists after a pod termination.
* [ ] An <code>emptyDir</code> volume is first created when a Pod is assigned to a node, and exists as long as that Pod is running on that node. 
* [ ] The <code>emptyDir</code> volume is initially empty
* [ ] When a Pod is removed from a node for any reason, the data in the <code>emptyDir</code> is deleted permanently

**Correct answer:**
* [x] An <code>emptyDir</code> volume is first created when a Pod is assigned to a node, and exists as long as that Pod is running on that node. 
* [x] The <code>emptyDir</code> volume is initially empty
* [x] When a Pod is removed from a node for any reason, the data in the <code>emptyDir</code> is deleted permanently

**54. What is the sequence of operations to be followed while configuring a storage class for an application?**


* [ ] Create a storage class with a provisioner, create a persistent volume with definition using the storage class, create a PVC and then use the PVC in the volumes section in the pod definition file
* [ ] Create a storage class with a provisioner, create a PVC with the storage class, and then use the PVC in the volumes section in the pod definition file
* [ ] Create a storage class, and use it directly in the volumes section in the pod definition file

**Correct answer:**
* [x] Create a storage class with a provisioner, create a PVC with the storage class, and then use the PVC in the volumes section in the pod definition file

**55. What is the command to delete the persistent volumes?**


* [ ] <code>kubectl delete pv PV-NAME</code>
* [ ] <code>kubectl del pv PV-NAME</code>
* [ ] <code>kubectl rm pv PV-NAME</code>
* [ ] <code>kubectl erase pv PV-NAME</code>

**Correct answer:**
* [x] <code>kubectl delete pv PV-NAME</code>

---


## Mock Exam 3 duplicate

**1. What component is responsible for instructing a worker to run a task?**


* [ ] scheduler
* [ ] dispatcher
* [ ] orchestrator
* [ ] allocater

**Correct answer:**
* [x] scheduler

**2. What option may be used to change the default behaviour of a failed task during an update in swarm?**


* [ ] `--update-failure-action`
* [ ]  `--update-parallelism`
* [ ]  `--update-delay`
* [ ]  `--placement-pref-add`

**Correct answer:**
* [x] `--update-failure-action`

**3. Which command can be used to check the restart policy of a container named `webapp`?**


* [ ] <code>docker container inspect webapp </code>
* [ ]  <code>docker container info webapp</code>
* [ ] <code>docker container check webapp</code>

**Correct answer:**
* [x] <code>docker container inspect webapp </code>

**4. Which command can be used to list the tasks in a stack named `webapp`?**


* [ ] <code>docker stack deploy webapp</code>
* [ ] <code>docker stack ls webapp</code>
* [ ] <code>docker stack services webapp</code>
* [ ] <code>docker stack ps webapp</code>

**Correct answer:**
* [x] <code>docker stack ps webapp</code>

**5. Which formula can be used to calculate the Quorum  of N nodes?**


* [ ] N + 1
* [ ] N+1 / 2
* [ ] N / 2 +1
* [ ] N /2 -1

**Correct answer:**
* [x] N / 2 +1

**6. Which command can be used to run an instance on swarm?**


* [ ] <code>docker container run</code>
* [ ] <code>docker container create</code>
* [ ] <code>docker service create</code>
* [ ] <code>docker swarm  service create</code>

**Correct answer:**
* [x] <code>docker service create</code>

**7. The routing mesh enables each node in the swarm to accept connections on published ports for any service running in the swarm, even if there’s no task running on the node.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**8. Which command can be used to increase the number of replicas from 2 to 4 of a service named `webapp`? Select the all right answer**


* [ ] <code>docker service update --replicas=4 webapp</code>
* [ ] <code>docker service update --replicas=2 webapp</code>
* [ ] <code>docker service scale webapp=2</code>
* [ ] <code>docker service scale webapp=4</code>

**Correct answer:**
* [x] <code>docker service update --replicas=4 webapp</code>
* [x] <code>docker service scale webapp=4</code>

**9. What is the command to apply `disk=ssd` label to `worker1` in a swarm cluster.**


* [ ] <code>docker node update --label-add disk=ssd worker1</code>
* [ ] <code>docker node update --label-rm disk=ssd worker1</code>
* [ ] <code>docker service update  --labels disk=ssd worker1</code>
* [ ] <code>docker service update --container-label disk=ssd worker1</code>

**Correct answer:**
* [x] <code>docker node update --label-add disk=ssd worker1</code>

**10. After an update to a service named `webapp` we realized that something is wrong with the new version and we want to revert back to the old version. How can we achieve that?**


* [ ] <code>docker service update rollback webapp</code>
* [ ] <code>docker service rollback webapp</code>
* [ ] <code>docker service rm webapp</code>
* [ ] <code>docker service leave webapp</code>

**Correct answer:**
* [x] <code>docker service rollback webapp</code>

**11. Which command can be used to get the logs of a swarm service?**


* [ ] <code>docker container logs SERVICE-NAME</code>
* [ ] <code>docker service logs SERVICE-NAME</code>
* [ ] <code>docker swarm log SERVICE-NAME</code>
* [ ] <code>docker swarm logs SERVICE-NAME</code>

**Correct answer:**
* [x] <code>docker service logs SERVICE-NAME</code>

**12. Create a service using the `my-web-server` image and map UDP port `80` in the container to port `5000` on the overlay network.**


* [ ] <code>docker service create -p 80:5000/udp my-web-server</code>
* [ ] <code>docker service create --publish published=80,target=5000,protocol=udp my-web-server</code>
* [ ] <code>docker service create -p 5000:80/udp my-web-server</code>
* [ ] <code>docker service create --publish published=5000,target=80,protocol=udp my-web-server</code>

**Correct answer:**
* [x] <code>docker service create -p 5000:80/udp my-web-server</code>
* [x] <code>docker service create --publish published=5000,target=80,protocol=udp my-web-server</code>

**13. What are the 4 top level fields a kubernetes definition file for POD contains?**


* [ ] <code>apiVersion</code>
* [ ] <code>templates</code>
* [ ] <code>metadata</code>
* [ ] <code>labels</code>
* [ ] <code>kind</code>
* [ ] <code>spec</code>
* [ ] <code>namespaces</code>
* [ ] <code>containers</code>

**Correct answer:**
* [x] <code>apiVersion</code>
* [x] <code>metadata</code>
* [x] <code>kind</code>
* [x] <code>spec</code>

**Explaination**: apiversion, kind, metadata and spec are the four top level fields a kubernetes definition file consists of.

- Option A and B are incorrect because these are not part of the top level fields that a definition file consists of.

**14. What is the flag that we can use to define a literal value from the command line while creating a ConfigMap?**


* [ ]  `--env`
* [ ]  `--from-literal`
* [ ]  `--literal`
* [ ]  `--text`

**Correct answer:**
* [x]  `--from-literal`

**Explaination**: You can use kubectl create configmap with the --from-literal argument to define a literal value from the command line:

**15. What is the recommended approach to load a set of configurations into the pod in the form of a file to the path `/var/configs`?**


* [ ] Add a separate env parameter for each config and use a startup script to write to a file
* [ ] Create a ConfigMap with the required configurations, configure it as a volume in the pod definition file and then mount the volume as a file at /var/configs
* [ ] Create a ConfigMap with the required configurations, configure it as an env variable in the pod definition file and use a startup script to write to a file

**Correct answer:**
* [x] Create a ConfigMap with the required configurations, configure it as a volume in the pod definition file and then mount the volume as a file at /var/configs

**16. Which of the below statements are correct?**


* [ ] Traffic to port 39376 on the node hosting the pod in the cluster is routed to port 9376 on a POD with the label app web on the same node
* [ ] Traffic to port 39376 on all nodes in the cluster is routed to port 9376 on a random POD with the label app web
* [ ] Traffic to port 80 on the service is routed to port 9376 on a random POD with the label app web
* [ ] Traffic to port 80 on the node is routed to port 9376 on the service

**Correct answer:**
* [x] Traffic to port 39376 on all nodes in the cluster is routed to port 9376 on a random POD with the label app web
* [x] Traffic to port 80 on the service is routed to port 9376 on a random POD with the label app web

**Code**: 
apiVersion: v1
kind: Service
metadata:
  name: web-service
  labels:
    obj: web-service
    app: web
spec:
  selector:
    app: web
  type: NodePort
  ports:
    - protocol: TCP
      port: 80
      targetPort: 9376
      nodePort: 39376

**17. Choose the correct statement regarding the following compose snippet**


* [ ] When the service is deployed it creates 4 containers in total
* [ ] The `webapp` service has a health check configured that tests if the web application is alive every 45 seconds
* [ ] Each time during the health check the `webapp` service waits for 60 seconds to receive a positive response from the curl command
* [ ] The `webapp` can reach the database using the name `dbservice`

**Correct answer:**
* [x] The `webapp` service has a health check configured that tests if the web application is alive every 45 seconds
* [x] The `webapp` can reach the database using the name `dbservice`

**Code**: 
version: "3.8"

services:
  webapp:
    image: webapp
    ports:
      - "8080:80"
    networks:
      - app-net
    deploy:
      mode: replicated
      replicas: 2
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost"]
      interval: 45s
      timeout: 20s
      retries: 3
      start_period: 60s

  dbservice:
    image: mysql
    volumes:
       - db-data:/var/lib/mysql/data
    networks:
       - app-net

volumes:
  db-data:

networks:
  app-net:


**18. Which of the following is the correct format for CMD instruction**


* [ ] CMD ["executable","param1","param2"]
* [ ] CMD ["param1","param2"]
* [ ] CMD command param1 param2
* [ ] CMD param1 param2

**Correct answer:**
* [x] CMD ["executable","param1","param2"]
* [x] CMD ["param1","param2"]
* [x] CMD command param1 param2

**19. Print the value of 'Architecture' and 'Os' of an image named `webapp`**


* [ ] `docker image inspect webapp -f '{{.Os}}' -f '{{.Architecture}}'`
* [ ] `docker image inspect webapp -f '{{.Os}} {{.Architecture}}'`
* [ ] `docker image inspect webapp -f '{{.Os}}', -f '{{.Architecture}}'`
* [ ] `docker image inspect webapp -f '{{.Os .Architecture}}'`

**Correct answer:**
* [x] `docker image inspect webapp -f '{{.Os}} {{.Architecture}}'`

**20. What is the command to stop all running containers on the host?**


* [ ] <code>docker container stop $(docker container ls)</code>
* [ ] <code>docker container rm $(docker container ls -q)</code>
* [ ] <code>docker container stop $(docker container ls -q)</code>
* [ ] <code>docker container stop --all</code>

**Correct answer:**
* [x] <code>docker container stop $(docker container ls -q)</code>

**21. While building a docker image from code stored in a remote URL, which command will be used to build from a directory called `docker` in the branch `dev`?**


* [ ] <code>docker build https://github.com/kk/dca.git#dev:docker</code>
* [ ] <code>docker build https://github.com/kk/dca.git#docker:dev</code>
* [ ] <code>docker build https://github.com/kk/dca.git:dev</code>
* [ ] <code>docker build https://github.com/kk/dca.gitdev:#docker</code>

**Correct answer:**
* [x] <code>docker build https://github.com/kk/dca.git#dev:docker</code>

**22. What is the user/account and image/repository name for the image `company/nginx`?**


* [ ] image=company, user=nginx
* [ ] image=company, user=company
* [ ] image=nginx, user=nginx
* [ ]  image=nginx, user=company

**Correct answer:**
* [x]  image=nginx, user=company

**23. What are the features of docker trusted registry (DTR)?**


* [ ] Built-in Access Control
* [ ] Image and Job Management
* [ ] Security Scanning
* [ ] Auto scaling applications
* [ ] Image Signing

**Correct answer:**
* [x] Built-in Access Control
* [x] Image and Job Management
* [x] Security Scanning
* [x] Image Signing

**24. Using `RUN apt-get update && apt-get install -y` ensures your Dockerfile installs the latest package versions everytime an image is built. This technique is known as …..**


* [ ] Fast build
* [ ] Cache busting
* [ ] Version pinning
* [ ] Build-context

**Correct answer:**
* [x] Cache busting

**25. Which image is used to deploy the Docker Trusted Registry?**


* [ ] dtr
* [ ] docker/dtr
* [ ] ucp
* [ ] docker/ucp

**Correct answer:**
* [x] docker/dtr

**26. When a container is created using the image built with the following Dockerfile, what is the command used to RUN the application inside it**


* [ ] <code>pip install flask</code>
* [ ] <code>docker run app.py</code>
* [ ] <code>app.py</code>
* [ ] <code>python app.py</code>

**Correct answer:**
* [x] <code>python app.py</code>

**Code**: 
FROM python:3.6
RUN pip install flask
COPY . /opt/
EXPOSE 8080
WORKDIR /opt
ENTRYPOINT ["python", "app.py"]

**27. Which of the following are the major components of Docker Engine - Enterprise?**


* [ ] A server which is a type of long-running program called a daemon process (the dockerd command).
* [ ] A REST API which specifies interfaces that programs can use to talk to the daemon and instruct it what to do.
* [ ] A command line interface (CLI) client (the docker command).

**Correct answer:**
* [x] A server which is a type of long-running program called a daemon process (the dockerd command).
* [x] A REST API which specifies interfaces that programs can use to talk to the daemon and instruct it what to do.
* [x] A command line interface (CLI) client (the docker command).

**28. Where is the log of the webapp container, with id `78373635`, stored on the Docker Host?**


* [ ] <code>/var/lib/docker/containers/78373635/78373635.json</code>
* [ ] <code>/var/log/docker/78373635.json</code>
* [ ] <code>/etc/docker/78373635.json</code>
* [ ] <code>/var/lib/docker/tmp/78373635/78373635.json</code>

**Correct answer:**
* [x] <code>/var/lib/docker/containers/78373635/78373635.json</code>

**29. What are the prerequisites for restoring swarm?**


* [ ] You must restore the backup on the same Docker Engine version.
* [ ] You must use the same IP as the node from which you made the backup.
* [ ] If auto-lock was enabled on the old Swarm, the unlock key is required to perform the restore.

**Correct answer:**
* [x] You must restore the backup on the same Docker Engine version.
* [x] You must use the same IP as the node from which you made the backup.
* [x] If auto-lock was enabled on the old Swarm, the unlock key is required to perform the restore.

**30. Which of the statements best describes `Roles` in the Access control model?**


* [ ] Roles define what operations can be done by whom.
* [ ] A role is a group of teams that share a specific set of permissions.
* [ ] Most organizations use multiple roles to fine-tune the appropriate access.

**Correct answer:**
* [x] Roles define what operations can be done by whom.
* [x] Most organizations use multiple roles to fine-tune the appropriate access.

**Explaination**: A role is a set of permitted operations against a type of resource, like a container or volume, which is assigned to a user or a team with a grant.

**31. Which of the following steps are required on each manager node to restore data to a new swarm?**


* [ ] Shut down the Docker Engine on the node you select for the restore
* [ ] Remove the /var/lib/docker directory on the new Swarm if it exists.
* [ ] Remove the contents of the /var/lib/docker/swarm directory on the new Swarm if it exists.
* [ ] Restore the /var/lib/docker/swarm directory with the contents of the backup
* [ ] Start Docker on the new node. Unlock the swarm if necessary
* [ ] Re-initialize the swarm so that the node does not attempt to connect to nodes that were part of the old swarm, and presumably no longer exist.

**Correct answer:**
* [x] Shut down the Docker Engine on the node you select for the restore
* [x] Remove the contents of the /var/lib/docker/swarm directory on the new Swarm if it exists.
* [x] Restore the /var/lib/docker/swarm directory with the contents of the backup
* [x] Start Docker on the new node. Unlock the swarm if necessary
* [x] Re-initialize the swarm so that the node does not attempt to connect to nodes that were part of the old swarm, and presumably no longer exist.

**32. Which component is responsible to serve the UCP components such as the web ui, the authentication api,metrics server, proxy and data stores used by UCP in the form of containers?**


* [ ] UCP Agent
* [ ] Docker Enterprise Edition
* [ ] Docker Community Edition
* [ ] Docker Trusted Registry (DTR)

**Correct answer:**
* [x] UCP Agent

**33. Which environment variable will be used to connect a remote docker server?**


* [ ] DOCKER_REMOTE
* [ ] DOCKER_HOST
* [ ] DOCKER_CONFIG
* [ ] DOCKER_SERVICE

**Correct answer:**
* [x] DOCKER_HOST

**34. What is the command to perform a backup of DTR node?**


* [ ] Run the <code>docker/dtr backup <dtr-cli-backup></code> command
* [ ] Run the <code>docker/dtr-backup <dtr-cli-backup></code> command
* [ ] Run the <code>docker/backup-dtr <dtr-cli-backup></code> command
* [ ] Run the <code>docker/backup dtr <dtr-cli-backup></code> command

**Correct answer:**
* [x] Run the <code>docker/dtr backup <dtr-cli-backup></code> command

**35. What is a linux feature that allows isolation of containers from the Docker host?**


* [ ] Control Groups (CGroups)
* [ ] Namespaces
* [ ] Kernel Capabilities
* [ ] LXC

**Correct answer:**
* [x] Namespaces

**36. By default, all containers get the same share of CPU cycles. How to modify the shares?**


* [ ] <code>docker container run --cpu-shares=512 webapp</code>
* [ ] </code>docker container run --cpuset-cups=512 webapp</code>
* [ ] <code>docker container run --cpu-quota=512 webapp</code>
* [ ] <code>docker container run --cpus=512 webapp</code>

**Correct answer:**
* [x] <code>docker container run --cpu-shares=512 webapp</code>

**37. What component is responsible for managing CPU resources and allocating the time of the CPU between different processes?**


* [ ] Allocater
* [ ] CFS
* [ ] Controller
* [ ] Allocater, Controller

**Correct answer:**
* [x] CFS

**38. The built-in DNS server in Docker always runs at IP address …**


* [ ] 127.0.0.11
* [ ] 127.0.0.1
* [ ] 172.17.0.3
* [ ] 172.17.0.1

**Correct answer:**
* [x] 127.0.0.11

**39. What is the command to create an `overlay` network driver called `my-overlay`?**


* [ ] <code>docker network create my-overlay</code>
* [ ] <code>docker create network my-overlay</code>
* [ ] <code>docker network create -d overlay my-overlay</code>
* [ ] <code>docker network create overlay my-overlay</code>

**Correct answer:**
* [x] <code>docker network create -d overlay my-overlay</code>

**40. When you initialize a Docker Swarm cluster it creates a new network of type overlay which is an internal private network that spans across all the nodes participating in the swarm cluster.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**41. If the service type is NodePort, then Kubernetes will allocate a port on every worker node.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**42. What is the default range of ports that Kubernetes uses for NodePort if one is not specified?**


* [ ] 32767-64000
* [ ] 30000-32767
* [ ] 32000-3276
* [ ] 80-8080

**Correct answer:**
* [x] 30000-32767

**43. Which among the following statements are true without any change made to the default behaviour of network policies in the namespace?**


* [ ] As soon as a network policy is associated with a POD traffic between all PODs in the namespace is denied
* [ ] As soon as a network policy is associated with a POD all ingress and egress traffic to that POD are denied except allowed by the network policy
* [ ] As soon as a network policy is associated with a POD all ingress and egress traffic to that POD are allowed except for the the ones blocked by the network policy

**Correct answer:**
* [x] As soon as a network policy is associated with a POD all ingress and egress traffic to that POD are denied except allowed by the network policy

**44. The communication between the swarm related services on different nodes in the swarm cluster are not secured by default.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**Explaination**: The communication between the nodes in the swarm cluster are secured by authentication and encryption using TLS/SSL certificates

**45. In which service does the DTR image scanning occur?**


* [ ] A service known as the dtr-jobrunner container
* [ ] A service known as the dtr-registry container
* [ ] A service known as the dtr-api container
* [ ] A service known as the dtr-runner container

**Correct answer:**
* [x] A service known as the dtr-jobrunner container

**Explaination**: When you install DTR on a node, the <code>dtr-jobrunner</code> started and runs cleanup jobs in the background

**46. Which of the statements best describe "Resource sets" in Access Control Model?**


* [ ] To control user access, cluster resources are grouped into Docker Swarm collections or Kubernetes namespaces.
* [ ] Together, collections and namespaces are named resource sets.
* [ ] A group of teams that share a specific set of permissions

**Correct answer:**
* [x] To control user access, cluster resources are grouped into Docker Swarm collections or Kubernetes namespaces.
* [x] Together, collections and namespaces are named resource sets.

**47. Which of the following is a common workflow for RBAC in Docker EE is**


* [ ] Create users, teams and organization
* [ ] Create custom roles with set of permissions
* [ ] Combine resources sets using collection
* [ ] Create a grant that combines subjects, roles and resource sets.

**Correct answer:**
* [x] Create users, teams and organization
* [x] Create custom roles with set of permissions
* [x] Combine resources sets using collection
* [x] Create a grant that combines subjects, roles and resource sets.

**48. UCP has its own built-in authentication mechanism and integrates with LDAP and AD services.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**49. A client bundle is a group of certificates downloadable directly from the Docker Trusted Registry (DTR) user interface within the admin section for “My Profile”**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**50. overlay2, aufs, and devicemapper all operate at the file level rather than the block level.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**Explaination**: devicemapper operate at the block level rather than the file level


**51. The volumes are mounted as “readonly” by default inside the container if no options are specified.**


* [ ] True
* [ ] False 

**Correct answer:**
* [x] False 

**Explaination**: The volumes are mounted as “readwrite” by default…

**52. Which among the below is a correct command to start a container named `webapp` with the volume `vol3`, mounted to the destination directory `/opt` in `readonly` mode?**


* [ ] <code>docker run -d --name webapp --mount source=vol3,target=/opt,readonly httpd</code>
* [ ] <code>docker run -d --name webapp -v vol3:/opt:ro httpd</code>
* [ ] <code>docker run -d --name webapp -v vol3:/opt:readonly httpd</code>
* [ ] <code>docker run -d --name webapp --volume vol3:/opt:ro httpd</code>
* [ ] <code>docker run -d --name webapp --mount source=vol3,target=/opt,ro httpd</code>

**Correct answer:**
* [x] <code>docker run -d --name webapp --mount source=vol3,target=/opt,readonly httpd</code>
* [x] <code>docker run -d --name webapp -v vol3:/opt:ro httpd</code>
* [x] <code>docker run -d --name webapp --volume vol3:/opt:ro httpd</code>

**53. Which statements best describe <code>emptyDir</code> volume type?**


* [ ] An <code>emptyDir</code> volume is first created when a Pod is assigned to a node, and still exists after a pod termination.
* [ ] An <code>emptyDir</code> volume is first created when a Pod is assigned to a node, and exists as long as that Pod is running on that node. 
* [ ] The <code>emptyDir</code> volume is initially empty
* [ ] When a Pod is removed from a node for any reason, the data in the <code>emptyDir</code> is deleted permanently

**Correct answer:**
* [x] An <code>emptyDir</code> volume is first created when a Pod is assigned to a node, and exists as long as that Pod is running on that node. 
* [x] The <code>emptyDir</code> volume is initially empty
* [x] When a Pod is removed from a node for any reason, the data in the <code>emptyDir</code> is deleted permanently

**54. What is the command to delete the persistent volumes?**


* [ ] <code>kubectl delete pv PV-NAME</code>
* [ ] <code>kubectl del pv PV-NAME</code>
* [ ] <code>kubectl rm pv PV-NAME</code>
* [ ] <code>kubectl erase pv PV-NAME</code>

**Correct answer:**
* [x] <code>kubectl delete pv PV-NAME</code>

**55. What is the sequence of operations to be followed while configuring a storage class for an application?**


* [ ] Create a storage class with a provisioner, create a persistent volume with definition using the storage class, create a PVC and then use the PVC in the volumes section in the pod definition file
* [ ] Create a storage class with a provisioner, create a PVC with the storage class, and then use the PVC in the volumes section in the pod definition file
* [ ] Create a storage class, and use it directly in the volumes section in the pod definition file

**Correct answer:**
* [x] Create a storage class with a provisioner, create a PVC with the storage class, and then use the PVC in the volumes section in the pod definition file

---


## Mock Exam 4

**1. After restarting the docker service and trying to run docker service ls, you get an error "Error response from daemon: Swarm is encrypted and needs to be unlocked before it can be used. How can you solve this error?**


* [ ] <code>docker swarm leave</code>
* [ ] <code>docker swarm update</code>
* [ ] <code>docker swarm lock</code>
* [ ] <code>docker swarm unlock</code>

**Correct answer:**
* [x] <code>docker swarm unlock</code>

**2. How to get the Os field alone of the httpd image?**


* [ ] <code>docker image inspect httpd -f '{{.Os}}'</code>
* [ ] <code>docker image ls | grep Os</code>
* [ ] <code>docker image history | grep Os</code>
* [ ] <code>docker image inspect httpd -f '{{.OperatingSystem}}'</code>

**Correct answer:**
* [x] <code>docker image inspect httpd -f '{{.Os}}'</code>

**3. Which command can be used to stop (only and not delete) the whole stack of containers created by compose file?**


* [ ] <code>docker-compose down</code>
* [ ] <code>docker-compose stop</code>
* [ ] <code>docker-compose destroy</code>
* [ ] <code>docker-compose halt</code>

**Correct answer:**
* [x] <code>docker-compose stop</code>

**4. You are required to deploy an agent of splunk on all nodes in the swarm cluster to monitor the health of the nodes and gather logs. What is the best approach to achieve this?**


* [ ] Deploy the agent as a docker container on each node in the cluster. Use a cron job to set this up.
* [ ] Deploy the agent as a global service in the swarm cluster
* [ ] Deploy the agent as a replicated service with the replica count equal to the number of worker nodes in the swarm cluster

**Correct answer:**
* [x] Deploy the agent as a global service in the swarm cluster

**5. What component is responsible for creating tasks in swarm**


* [ ] scheduler
* [ ] dispatcher
* [ ] orchestrator
* [ ] allocater

**Correct answer:**
* [x] orchestrator

**6. Which command can be used to deploy the STACKDEMO  stack from a compose file? Select the all right answers**


* [ ] <code>docker stack deploy --compose-file docker-compose.yml STACKDEMO</code>
* [ ] <code>cat docker-compose.yml | docker stack deploy --compose-file - STACKDEMO</code>
* [ ] <code>docker stack services --compose-file docker-compose.yml STACKDEMO</code>
* [ ] <code>docker stack ps --compose-file docker-compose.yml STACKDEMO</code>

**Correct answer:**
* [x] <code>docker stack deploy --compose-file docker-compose.yml STACKDEMO</code>
* [x] <code>cat docker-compose.yml | docker stack deploy --compose-file - STACKDEMO</code>

**7. The services only under the same network will get resolved with their names so all of the micro-service component should be under the same network so that they can resolve each other.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**8. Create a replicated service webapp with 2 replicas.**


* [ ] <code>docker service create --replicas=2 webapp</code>
* [ ] <code>docker service create --mode=replicated --replicas=2 webapp</code>
* [ ] <code>docker service create --mode=global --replicas=2 webapp</code>
* [ ] <code>docker service create --replicas=2 webapp.</code>

**Correct answer:**
* [x] <code>docker service create --replicas=2 webapp</code>
* [x] <code>docker service create --mode=replicated --replicas=2 webapp</code>

**9. What is the command to run 3 instances of httpd on a swarm cluster?**


* [ ] <code>docker swarm  service create --instances=3 httpd</code
* [ ] <code>docker swarm  service create --replicas=3 httpd</code>
* [ ] <code>docker service create --instances=3 httpd</code>
* [ ] <code>docker service create --replicas=3 httpd</code>

**Correct answer:**
* [x] <code>docker service create --replicas=3 httpd</code>

**10. Assume that you have 3 managers in your cluster, what will happen if 2 managers fail at the same time?**


* [ ] The services hosted on the available worker nodes will continue to run.
* [ ] The services hosted on the available worker nodes will stop running.
* [ ] New services/workers can be created or added.
* [ ] New services/workers can’t be created or added.

**Correct answer:**
* [x] The services hosted on the available worker nodes will continue to run.
* [x] New services/workers can’t be created or added.

**11. Map TCP port 80 in the container to port 8080 on the Docker host for connections to host IP 192.168.1.10.**


* [ ] <code>-p 192.168.1.10:8080:80</code>
* [ ] <code>-p 192.168.1.10:80:8080</code>
* [ ] <code>-p 192.168.1.10:8080:80/tcp</code>
* [ ] <code>-p 192.168.1.10:8080:8080</code>

**Correct answer:**
* [x] <code>-p 192.168.1.10:8080:80</code>
* [x] <code>-p 192.168.1.10:8080:80/tcp</code>

**12. what is the default Secret type if omitted from a Secret configuration file?**


* [ ] <code>kubernetes.io/tls</code>
* [ ] <code>kubernetes.io/ssh-auth</code>
* [ ] <code>Opaque</code>
* [ ] <code>kubernetes.io/dockercfg</code>

**Correct answer:**
* [x] <code>Opaque</code>

**13. Which of the following statements about kubernetes deployments are correct?**


* [ ] You describe a desired state in a Deployment, and the Deployment Controller changes the actual state to the desired state at a controlled rate.
* [ ]  You can define Deployments to create new ReplicaSets, or to remove existing Deployments and adopt all their resources with new Deployments.
* [ ] You may manually update the ReplicaSets owned by a Deployment.
* [ ] You should not manually update the ReplicaSets owned by a Deployment.

**Correct answer:**
* [x] You describe a desired state in a Deployment, and the Deployment Controller changes the actual state to the desired state at a controlled rate.
* [x]  You can define Deployments to create new ReplicaSets, or to remove existing Deployments and adopt all their resources with new Deployments.
* [x] You should not manually update the ReplicaSets owned by a Deployment.

**14. What command would you use to create a Deployment?**


* [ ] <code>kubectl get deployments</code>
* [ ] <code>kubectl get nodes</code>
* [ ] <code>kubectl create</code>
* [ ] <code>kubectl run</code>

**Correct answer:**
* [x] <code>kubectl create</code>

**Explaination**: kubectl create is the command that will create a new deployment

- Option A is incorrect because this get command will only list the existing Deployments
- Option B is incorrect because this get command will only list the available Nodes where you can deploy applications
- Option D is incorrect because this command will create a pod in kubernetes


**15. Which statements best describe configmaps?**


* [ ] ConfigMap is an API object mainly used to store confidential data in key-value pairs.
* [ ] ConfigMap is an API object mainly used to store non-confidential data in key-value pairs.
* [ ] Pods can consume ConfigMaps as environment variables, command-line arguments, or as configuration files in a volume.
* [ ] ConfigMap provides secrecy or encryption

**Correct answer:**
* [x] ConfigMap is an API object mainly used to store non-confidential data in key-value pairs.
* [x] Pods can consume ConfigMaps as environment variables, command-line arguments, or as configuration files in a volume.

**Explaination**: A ConfigMap allows you to decouple environment-specific configuration from your container images, so that your applications are easily portable.

**16. What is the command to create a secret using the "kubectl create secret" command?**


* [ ] <code>kubectl create secret test-secret --from-literal='username=my-app' --from-literal='password=39528$vdg7Jb'</code>
* [ ] <code>kubectl create secret opaque test-secret --from-literal='username=my-app' --from-literal='password=39528$vdg7Jb'</code>
* [ ] <code>kubectl create secret credentials test-secret --from-literal='username=my-app' --from-literal='password=39528$vdg7Jb'</code>
* [ ] <code>kubectl create secret generic test-secret --from-literal='username=my-app' --from-literal='password=39528$vdg7Jb'</code>

**Correct answer:**
* [x] <code>kubectl create secret generic test-secret --from-literal='username=my-app' --from-literal='password=39528$vdg7Jb'</code>

**17. The health check on the web service is configured to run at an interval of every 30 seconds. What would happen if the web server takes 45 seconds to boot up the first time?**


* [ ] The web server container will be killed and restarted after 30 seconds
* [ ] The health checks only start after 2 minutes, so the web server has sufficient time to boot up
* [ ] The health checks runs every 5 seconds and will mark the container as failed after 5 attempts
* [ ] The web service will go into an infinite loop

**Correct answer:**
* [x] The health checks only start after 2 minutes, so the web server has sufficient time to boot up

**Code**: 

version: 3
services:
  redis:
    image: "redis:alpine"
    deploy:
      replicas: 3
  db:
    image: postgres:9.4
    deploy:
      replicas: 1
      placement:
        constraints:
          - "node.role==manager"
  web:
    image: webapp
    deploy:
      replicas: 5
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 120s


**18. Regarding the following YAML , What should we do to correct the syntax errors?**


* [ ] We need to use <code>apiVersion</code> as <code>v1</code> but not <code>v1/apps</code>
* [ ] <code>kind</code> should be <code>Pod</code> but not <code>Pods</code>
* [ ] <code>containers<code> should be <code>container</code>
* [ ] <code>labels</code> keyword should be inline with <code>name</code> under <code>metadata</code>

**Correct answer:**
* [x] We need to use <code>apiVersion</code> as <code>v1</code> but not <code>v1/apps</code>
* [x] <code>kind</code> should be <code>Pod</code> but not <code>Pods</code>
* [x] <code>labels</code> keyword should be inline with <code>name</code> under <code>metadata</code>

**Code**: 
apiVersion: v1/apps
  kind: Pods
  metadata:
    name: apache 
     labels:
      app: myapp
  spec:
   containers: 
   - name: apache
     image: httpd

**19. If CMD is used to provide default arguments for the ENTRYPOINT instruction, both the CMD and ENTRYPOINT instructions should be specified.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**20. Build an image using a context build under path /tmp/docker and name it webapp**


* [ ] <code>docker build  /tmp/docker</code
* [ ] <code>docker build /tmp/docker -t webapp</code>
* [ ] <code>docker build webapp -t /tmp/docker</code>
* [ ] <code>docker pull -it /tmp/docker bash</code>

**Correct answer:**
* [x] <code>docker build /tmp/docker -t webapp</code>

**21. How do you identify if a Docker file is configured to use multi-stage builds?**


* [ ] The Dockerfile has the tag multi-stage at the to
* [ ] The Dockerfile has multiple FROM instructions
* [ ] The Dockerfile has multiple RUN instructions
* [ ] The Dockerfile is built from the scratch imag

**Correct answer:**
* [x] The Dockerfile has multiple FROM instructions

**22. What is the command to remove all unused images on the Docker host?**


* [ ] <code>docker image prune -a</code>
* [ ] <code>docker image rm -a</code>
* [ ] <code>docker image delete -a</code>
* [ ] <code>docker rm image  -a</code>

**Correct answer:**
* [x] <code>docker image prune -a</code>

**23. Which is the recommended approach to install packages following the best practices in Dockerfile?**


* [ ] <code>"RUN apt-get update && apt-get install -y git httpd"</code>
* [ ] <code>"RUN apt-get update && apt-get install -y  \               git \               httpd"</code>
* [ ] <code> "RUN apt-get update \           RUN apt-get install -y git \           RUN apt-get install -y httpd"</code>

**Correct answer:**
* [x] <code>"RUN apt-get update && apt-get install -y  \               git \               httpd"</code>

**Explaination**: this technique called cache busting which ensures your Dockerfile installs the latest package versions with no further coding or manual intervention

**24. Display all layers of httpd image along with the size on each layer**


* [ ] <code>docker image layers httpd</code>
* [ ] <code>docker image history httpd</code>
* [ ] <code>docker image inspect httpd</code>
* [ ] <code>docker images history httpd</code>

**Correct answer:**
* [x] <code>docker image history httpd</code>

**25. Docker Trusted Registry (DTR) is a containerized application that runs on a Docker Universal Control Plane cluster.**


* [ ] True 
* [ ] False

**Correct answer:**
* [x] True 

**26. ... is one of the system requirements for UCP**


* [ ] The nodes must be Linux Kernel version 3.10 or higher
* [ ] Each node must be configured with a static ip address
* [ ] User namespaces should not be configured on any node
* [ ] all nodes must have Docker Engine Enterprise installed

**Correct answer:**
* [x] The nodes must be Linux Kernel version 3.10 or higher
* [x] Each node must be configured with a static ip address
* [x] User namespaces should not be configured on any node
* [x] all nodes must have Docker Engine Enterprise installed

**27. What is the command to push the image to a docker private registry?**


* [ ] <code>docker push <docker-registry-address>/username/docke-repo-name</code>
* [ ] <code>docker push <docker-registry-address>/docker-repo-name</code>
* [ ] <code>docker upload <docker-registry-address>/username/docker-repo-name</code>
* [ ] <code>docker upload <docker-registry-address>/docker-repo-name</code>

**Correct answer:**
* [x] <code>docker push <docker-registry-address>/username/docke-repo-name</code>

**28. What is the default logging driver?**


* [ ] json-file
* [ ] syslog
* [ ] journald
* [ ] splunk

**Correct answer:**
* [x] json-file

**29. When you run the docker image inspect ubuntu command it gives the error “No such image”. Why is that?**


* [ ] Must run the command docker inspect ubuntu/ubuntu
* [ ] Image Ubuntu does not have the latest tag
* [ ] Must authenticate to docker hub first before running this command
* [ ] Must run the command docker image history ubuntu

**Correct answer:**
* [x] Image Ubuntu does not have the latest tag

**30. What is a linux feature that allows restriction of CPU and memory resources on docker containers?**


* [ ] Control Groups (CGroups)
* [ ] Namespaces
* [ ] Kernel Capabilities
* [ ] LXC

**Correct answer:**
* [x] Control Groups (CGroups)

**31. Which of the following are the steps that are required to set up UCP?**


* [ ] Make sure Docker EE is up and running and Pull the UCP image from the registry
* [ ] Set the Admin Username and Password for UCP Console
* [ ] Login to the Browser and provide the downloaded Docker EE License
* [ ] Add more managers and workers as per requirement

**Correct answer:**
* [x] Make sure Docker EE is up and running and Pull the UCP image from the registry
* [x] Set the Admin Username and Password for UCP Console
* [x] Login to the Browser and provide the downloaded Docker EE License
* [x] Add more managers and workers as per requirement

**32. Which of the following flags are required to install DTR as a docker container?**


* [ ] --dtr-external-url
* [ ] --ucp-node
* [ ] --ucp-username
* [ ] --ucp-url

**Correct answer:**
* [x] --dtr-external-url
* [x] --ucp-node
* [x] --ucp-username
* [x] --ucp-url

**33. To restore an existing UCP installation from a backup, you need to uninstall UCP from the swarm by using the uninstall-ucp command.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**34. The ... network disables all networking. Usually used in conjunction with a custom network driver**


* [ ] host
* [ ] bridge
* [ ] overlay
* [ ] none

**Correct answer:**
* [x] none

**35. Limit a container to only use the first CPU or core. Select the right command**


* [ ] <code>docker container run --cpuset-shares=1 webapp</code>
* [ ] <code>docker container run --cpus=0 webapp</code>
* [ ] <code>docker container run --cpuset-cpus=0 webapp</code>

**Correct answer:**
* [x] <code>docker container run --cpuset-cpus=0 webapp</code>

**Explaination**: The first CPU is numbered 0. A valid value might be 0-3 (to use the first, second, third, and fourth CPU) or 1,3 (to use the second and fourth CPU).

**36. If you use the … network mode for a container, that container’s network stack is not isolated from the Docker host (the container shares the host’s networking namespace), and the container does not get its own IP-address allocated**


* [ ] host
* [ ] bridge
* [ ] overlay
* [ ] NAT

**Correct answer:**
* [x] host

**37. Macvlan network driver assigns a MAC address to each container’s virtual network interface, making it appear to be a physical network interface directly connected to the physical network.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**38. Assume that you have 1 CPU, which of the following commands guarantees the container at most 50% of the CPU every second?**


* [ ] <code>docker run -it --cpu-shares=512 ubuntu /bin/bash</code>
* [ ] <code>docker container run --cpuset-cups=.5 webapp</code>
* [ ] <code>docker run -it --cpus=".5" ubuntu /bin/bash</code>
* [ ] <code>docker run -it --cpus=".5" --cpuset-cups=1 ubuntu /bin/bash</code>

**Correct answer:**
* [x] <code>docker run -it --cpus=".5" ubuntu /bin/bash</code>

**39. Docker requires an external DNS server to be configured during installation to help the containers resolve each other using the container name.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**Explaination**: Docker will by default check for a DNS server defined in /etc/resolv.conf in the host OS.


**40. Which command is used to get the stream logs of the webapp container so that you can view the logs live?**


* [ ] <code>docker container log webapp</code>
* [ ] <code>docker container log -f webapp</code>
* [ ] <code>docker container logs webapp</code>
* [ ] <code>docker container logs -f webapp</code>

**Correct answer:**
* [x] <code>docker container logs -f webapp</code>

**41. What is the default traffic flow configuration between pods in a kubernetes cluster?**


* [ ] All traffic is allowed between different pods in the cluster
* [ ] All traffic is denied between different pods in the cluster
* [ ] Traffic between different pods must be explicitly allowed using rules

**Correct answer:**
* [x] All traffic is allowed between different pods in the cluster

**42. By default all swarm service management traffic is encrypted using ... algorithm**


* [ ] TKIP
* [ ] DES
* [ ] AES
* [ ] RSA

**Correct answer:**
* [x] AES

**43. Which of the statements best describe "Grants" in the Access Control Model?**


* [ ] Grants define which users can access what resources in what way
* [ ] A grant is made up of a role and a resource set.
* [ ] A grant is made up of a subject, a role, and a resource set.
* [ ] Grants are effectively Access Control Lists (ACLs) which provide comprehensive access policies for an entire organization when grouped together.

**Correct answer:**
* [x] Grants are effectively Access Control Lists (ACLs) which provide comprehensive access policies for an entire organization when grouped together.
* [x] A grant is made up of a subject, a role, and a resource set.

**44. Which of the following statements are true about deploying workload via CLI on UCP Cluster?**


* [ ] With CLI you may use the docker command line interface to interact with the UCP cluster.
* [ ] CLI access doesn't require authentication to the UCP Cluster.
* [ ] CLI access requires authentication to the UCP Cluster.
* [ ]  Download the certificate from UCP Console and copy this over to the server from where you’d like to access and extract it to a path.
* [ ] The docker host and docker_cert_path environment variables are automatically set by the application.

**Correct answer:**
* [x] With CLI you may use the docker command line interface to interact with the UCP cluster.
* [x] CLI access requires authentication to the UCP Cluster.
* [x] The docker host and docker_cert_path environment variables are automatically set by the application.

**45. Once the image scan is complete, a report shows all the vulnerabilities detected categorized as __________.**


* [ ] Major
* [ ] Minor
* [ ] Critical
* [ ] Warning

**Correct answer:**
* [x] Major
* [x] Minor
* [x] Critical

**46. Using ... in Docker EE we can control who can access and make changes to your cluster and applications**


* [ ] DTR
* [ ] UCP
* [ ] Client bundle
* [ ] RBAC

**Correct answer:**
* [x] RBAC

**47. DTR offers a deeply integrated vulnerability scanner that analyzes container images by only manual user request.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**Explaination**: DTR offers a deeply integrated vulnerability scanner that analyzes container images, either by manual user request or automatically whenever an image is uploaded to the registry.


**48. To authorize access to cluster resources across your organization, which of the following high-level steps UCP administrators might take?**


* [ ] Add and configure subjects (users, teams, and service accounts).
* [ ] Define custom roles (or use defaults) by adding permitted operations per type of resource.
* [ ] Group cluster resources into resource sets of Swarm collections or Kubernetes namespaces.
* [ ] Create grants by combining subject + role + resource set

**Correct answer:**
* [x] Add and configure subjects (users, teams, and service accounts).
* [x] Define custom roles (or use defaults) by adding permitted operations per type of resource.
* [x] Group cluster resources into resource sets of Swarm collections or Kubernetes namespaces.
* [x] Create grants by combining subject + role + resource set

**49. What is the default path file to do any storage driver customization?**


* [ ] <code>/var/lib/docker/daemon.json</code>
* [ ] <code>/var/log/docker/daemon.json</code>
* [ ] <code>/etc/docker/daemon.json</code>
* [ ] <code>/home/docker/daemon.json</code>

**Correct answer:**
* [x] <code>/etc/docker/daemon.json</code>

**50. ... driver stores every image and container on its own virtual device. These devices are thin-provisioned copy-on-write snapshot devices**


* [ ] AUFS
* [ ] overlay2
* [ ] Device Mapper
* [ ] OverlayFS

**Correct answer:**
* [x] Device Mapper

**51. By default all files inside an image are in a writable layer.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**52. What is the command to remove unused volumes**


* [ ] <code>docker container rm my-vol</code>
* [ ] <code>docker volume rm my-vol</code>
* [ ] <code>docker volume prune</code>
* [ ] <code>docker volume rm --all</code>

**Correct answer:**
* [x] <code>docker volume prune</code>

**53. What is the status of a volume after it is created but not yet bound to a claim?**


* [ ] Available
* [ ] Bound 
* [ ] Released
* [ ] Failed 

**Correct answer:**
* [x] Available

**54. Which of the following is the etcd command line tool?**


* [ ] etcd
* [ ] etcdctl
* [ ] kubectl

**Correct answer:**
* [x] etcdctl

**Explaination**: The default client that comes with ETCD is the etcdctl client. You can use it to store and retrieve key-value pairs.

- Option A is incorrect because etcd is a service
- Option C is incorrect because kubectl is the kubernetes command-line tool.
- Option D is incorrect because kubeadm is a tool which performs the actions necessary to get a minimum viable cluster up and running. 


**55. What are the types of volumes that kubernetes supports?**


* [ ] hostPath
* [ ] configMap
* [ ] emptyDir
* [ ] local

**Correct answer:**
* [x] hostPath
* [x] configMap
* [x] emptyDir
* [x] local

---


## Python PCEP Literals

**1. …. is one of the literal types in Python.**


* [ ] None of the above
* [ ] Boolean
* [ ] String
* [ ] Numeric

**Correct answer:**
* [x] Boolean
* [x] String
* [x] Numeric

**2. What is the data type of <code>print(type(45.50))</code>?**


* [ ] integer
* [ ] float
* [ ] string
* [ ] boolean

**Correct answer:**
* [x] float

**3. What is the numerical value for boolean <code>True</code>?**


* [ ] 0
* [ ] 1

**Correct answer:**
* [x] 1

**4. Strings can be enclosed in double or single quotes.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**5. What is the data type of <code>print(type(10))</code>?**


* [ ] integer
* [ ] float
* [ ] string
* [ ] boolean

**Correct answer:**
* [x] integer

**6. What is the data type of <code>print(type(1_00_0000_000))</code>?**


* [ ] integer
* [ ] string
* [ ] boolean
* [ ] None of the above

**Correct answer:**
* [x] integer

**7. …. can be created by writing a text(a group of Characters ) surrounded by the single(") or double(" ")quotes.**


* [ ] String literals
* [ ] Numeric literals
* [ ] Boolean literals
* [ ] Literal Collections

**Correct answer:**
* [x] String literals

---


## Python PCEP Print Function

**1. ….  is a block of code that only runs when it is called and used to cause an effect or evaluate a value.**


* [ ] Variable
* [ ] Function
* [ ] Method
* [ ] Tuple

**Correct answer:**
* [x] Function

**2. The …. function prints the specified message to the screen, or another standard output device.**


* [ ] print()
* [ ] return()
* [ ] vars()
* [ ] input()

**Correct answer:**
* [x] print()

**3. What is the output of the following python code?**


* [ ] Hello
* [ ] Hello future python!
* [ ] python!
* [ ] Hello python!

**Correct answer:**
* [x] Hello future python!

**Code**: 
 print("Hello" + " " "future" + " " "python!")

**4. What is the special character that is used for a new line?**


* [ ] \t
* [ ] \r
* [ ] \n
* [ ] \d

**Correct answer:**
* [x] \n

**5. What is the output of the following python code?**


* [ ] 1234#&
* [ ] 1#2#3#4&
* [ ] 1&2&3&4#
* [ ] 1234sep='#'end='&'

**Correct answer:**
* [x] 1#2#3#4&

**Code**: 
print(1, 2, 3, 4, sep='#', end='&')

**6. The output of the following code will be:**


* [ ] My age is 25
* [ ] 25
* [ ] TypeError
* [ ] My age is + 25

**Correct answer:**
* [x] TypeError

**Code**: 
print('My age is ' + 25)

**Explaination**: TypeError: can only concatenate str (not "int") to str

**7. Is it possible to pass multiple arguments to a function?**


* [ ] Yes
* [ ] No

**Correct answer:**
* [x] Yes

---


## Python PCEP Operators

**1. What is the output of the following python code?**


* [ ] 8.0
* [ ] 0.8
* [ ] 8
* [ ] 8.5

**Correct answer:**
* [x] 8

**Code**: 
print(2 ** 3)

**2. What is the output of the following python code?**


* [ ] -3
* [ ] 12
* [ ] 10
* [ ] 20

**Correct answer:**
* [x] 10

**Code**: 
print(2*(2+3))

**3. What is the output of the following python code?**


* [ ] 1.5
* [ ] 1.0
* [ ] -1.5
* [ ] -2.0

**Correct answer:**
* [x] 1.0

**Code**: 
print(6. // 4)

**4. What is the output of the following python code?**


* [ ] 5.0
* [ ] 0.5
* [ ] 5
* [ ] 5.5

**Correct answer:**
* [x] 5.0

**Code**: 
print(10 / 2)

**Explaination**: Hint: division operator always returns a float

**5. What is the output of the following python code?**


* [ ] 8.0
* [ ] 0.8
* [ ] 8
* [ ] 8.5

**Correct answer:**
* [x] 8.0

**Code**: 
print(2 ** 3.)

**Explaination**: Hint: If one of the values is a floating-point number, the result will be a floating-point number as well.

**6. Fill in the missing operators:**


* [ ] / and *
* [ ] //  and *
* [ ] % and /
* [ ] * and /

**Correct answer:**
* [x] / and *

**Code**: 
20 ... 5 ... 4 = 16.0 

**7. What is the output of the following python code?**


* [ ] -311029.0
* [ ] 29.0
* [ ] 50
* [ ] -29.0

**Correct answer:**
* [x] -29.0

**Code**: 
print(10 - 6 ** 2 / 9 * 10 + 1) 

**8. What is the output of the following python code?**


* [ ] 0
* [ ] 1
* [ ] 2
* [ ] 2.25

**Correct answer:**
* [x] 1

**Code**: 
print(9 % 4) 

**9. What does the following print?**


* [ ] 4
* [ ] 5.0
* [ ] 4.5
* [ ] 7
* [ ] None of the above

**Correct answer:**
* [x] 5.0

**Code**: 
x = 10 / 4
y = 5 / 2.0
print (x + y)

**10. What is the output of the following python code?**


* [ ] 4
* [ ] 3
* [ ] 3.25
* [ ] 4.25
* [ ] None of the above

**Correct answer:**
* [x] 4.25

**Code**: 
print(13 / 4 + 13 % 4)

**11. What is the value of the expression <code> 100 / 50 </code>?**


* [ ] 2
* [ ] 2.0
* [ ] 4
* [ ] 4.0

**Correct answer:**
* [x] 2.0

**12. What is the output of <code> print(2 * 3 ** 3 * 4)</code>?**


* [ ] 216
* [ ] 864
* [ ] “2 * 3 ** 3 * 4”
* [ ] 144

**Correct answer:**
* [x] 216

---


## Python PCEP Variables

**1. In Python, a variable must be declared before it is assigned a value:**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**Explaination**:  Hint: Variables need not be declared or defined in advance in Python. To create a variable, you just assign it a value.

**2. Which of the following statements assigns the value 50 to the variable x in Python:**


* [ ] <code> x == 50 </code>
* [ ] <code> x : 50 </code>
* [ ] <code> x = 50 </code>
* [ ] <code> x >> 50 </code>

**Correct answer:**
* [x] <code> x = 50 </code>

**3. What is the output of the following python code?**


* [ ] 8
* [ ] 16
* [ ] 10
* [ ] 12

**Correct answer:**
* [x] 16

**Code**: 
amount = 4
cost = 2
cost += 2
print(amount * cost)


**4. <code>True</code> is what type of variable?**


* [ ] float
* [ ] string
* [ ] boolean
* [ ] integer

**Correct answer:**
* [x] boolean

**5. Which of the following is a valid variable name in Python?**


* [ ] do it
* [ ] do+1
* [ ] 1do
* [ ] All of the above
* [ ] None of the above

**Correct answer:**
* [x] None of the above

**6. Which of the following is a keyword in Python?**


* [ ] int
* [ ] float
* [ ] return
* [ ] All of the above
* [ ] finally

**Correct answer:**
* [x] All of the above

**7. Which of the following variable names are valid?**


* [ ] <code> not = "Don't do that!" </code>
* [ ] <code>name = "Lydia"</code>
* [ ] <code>2timesage = 44</code>
* [ ] <code>Function = "function"</code>

**Correct answer:**
* [x] <code>name = "Lydia"</code>
* [x] <code>Function = "function"</code>

**8. Which of the following is correct regarding variables in Python?**


* [ ] Variable names in Python cannot start with a number. However, it can contain the number in any other position of the variable name.
* [ ] Variable names can start with an underscore.
* [ ] Data type of variable names should not be declared
* [ ] All of the above

**Correct answer:**
* [x] All of the above

**9. Which of the following are valid Python variable names?**


* [ ] Age
* [ ] 1st_student
* [ ] class
* [ ] _st

**Correct answer:**
* [x] Age
* [x] _st

**10. What would get printed to the console?**


* [ ] 22
* [ ] 22.0
* [ ] 88
* [ ] 55.0

**Correct answer:**
* [x] 55.0

**Code**: 
age = 22
AGE = 44

age /= 2

print(age + AGE)


**11. What is the output of the following python code?**


* [ ] 5
* [ ] Jack
* [ ] 5Jack
* [ ] Error

**Correct answer:**
* [x] Jack

**Code**: 
y = 5
y = "Jack"
print(y)


---


## Python PCEP Comments

**1. What is the output of the following python code?**


* [ ] Sally#employee name+#123
* [ ] Sally#employee name#123
* [ ] Sally+#123
* [ ] Sally#123

**Correct answer:**
* [x] Sally#123

**Code**: 
name = "Sally"# employee name
 
data = "#123" 
print (name+data)


**2. A comment in Python starts with the hash character(#) and extends to the end of the physical line.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**3. What is the output of the following python code?**


* [ ] 18
* [ ] 7
* [ ] 5+6+7
* [ ] NameError

**Correct answer:**
* [x] NameError

**Code**: 
#x = 5
#y = 6
z = 7
print(x+y+z)


**4. You can use Python comments inline, on independent lines, or on multiple lines to include larger documentation.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**5. What is the output of the following python code?**


* [ ] Hello, jack!
* [ ] Hello, jack,Sally!
* [ ] Hello, Sally!
* [ ] Hello, jack!,Sally!

**Correct answer:**
* [x] Hello, Sally!

**Code**: 
#print("Hello, jack!")
print("Hello, Sally!")


**6. Which character is used in Python to make a single line comment?**


* [ ] \/
* [ ] \//
* [ ] \#
* [ ] \!

**Correct answer:**
* [x] \#

**7. Comments can be used to ….**


* [ ] explain Python code
* [ ] make the code more readable
* [ ] comment out code that you don’t want to execute
* [ ] None of the Above

**Correct answer:**
* [x] explain Python code
* [x] make the code more readable
* [x] comment out code that you don’t want to execute

**8. What is the output of the following python code?**


* [ ] line1
* [ ] line1       <br>line2       <br>line3
* [ ] line1       <br>line2       <br>#line3
* [ ] line1       <br>#line3

**Correct answer:**
* [x] line1       <br>#line3

**Code**: 
print("line1")
#print("line2")
print("#line3")

---


## Python PCEP Input

**1. What is the output of the following python code if we enter 25 as input?**


* [ ] 25
* [ ]  My age is: 25
* [ ] input("My age is: " )
* [ ] Error 

**Correct answer:**
* [x] 25

**Code**: 
age =input("My age is: " )
print (age)


**2. In Python3, Whatever you enter as input, the <code>input() </code>function converts it into a string.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**3. The <code>input()</code> method returns string value. So, if we want to perform arithmetic operations, we need to cast the value first.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**4. What is the output of the following python code if we enter “Hello Python” as input?**


* [ ] Hello Python#&
* [ ] Hello Python
* [ ] Hello#Python&
* [ ] Hello Python&

**Correct answer:**
* [x] Hello Python&

**Code**: 
inputString = input('Enter a string: ')
print(inputString, sep='#', end='&')


**5. What will be printed when the following executes?**


* [ ] 5.5
* [ ] 6
* [ ] 5
* [ ] 6.5

**Correct answer:**
* [x] 5

**Code**: 
print(int(15.5)-10)

**6. What is the input function?**


* [ ] A function that allows us to ask the user to enter some data.
* [ ] A function used to display numbers and text on the screen.
* [ ] A piece of data that is shown on the screen.
* [ ] To calculate something.

**Correct answer:**
* [x] A function that allows us to ask the user to enter some data.

---


## Python PCEP String Methods

**1. What is the output of the following python code if we enter 5 as input?**


* [ ] 15
* [ ] NumNumNum
* [ ] 555
* [ ] Error 

**Correct answer:**
* [x] 555

**Code**: 
Num = input("Enter a Number: ") 
print (Num * 3 )


**2. What is the output of the following python code if we enter 5 as input?**


* [ ] 15
* [ ] NumNumNum
* [ ] 555
* [ ] Error

**Correct answer:**
* [x] 15

**Code**: 
Num = input("Enter a Number: ")
Num = int(Num) 
print ( Num * 3 )


**3. Which operator you can use to perform string concatenation?**


* [ ] \/
* [ ] \*
* [ ] \+
* [ ] \-

**Correct answer:**
* [x] \+

**4. Which method should you use in order to convert the input into a string correctly:**


* [ ] str
* [ ] int
* [ ] float
* [ ] bin 

**Correct answer:**
* [x] str

**Code**: 
year_of_birth = int(input("In what year were you born? "))

print("You were born in " + ...(year_of_birth))


**5. What is the output of the following python code if we enter “HelloPython” as input?**


* [ ] HelloPython
* [ ] Enter a string: HelloPython*2
* [ ] HelloPythonHelloPython
* [ ] HelloPython*2

**Correct answer:**
* [x] HelloPythonHelloPython

**Code**: 
inputString = input('Enter a string: ')
print(inputString*2)

**6. What is the output of the following python code?**


* [ ] 5Sally
* [ ] 9
* [ ] Error
* [ ] 5+Sally

**Correct answer:**
* [x] 5Sally

**Code**: 
x = 5
y = "Sally"
print(str(x) + y)

**7. All string methods return new values and the original string values will be changed with the new values.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**Explaination**: All string methods returns new values. They do not change the original string.

**8. What will be printed when the following executes?**


* [ ] ha2
* [ ] haha
* [ ] ha*2
* [ ] ha       ha

**Correct answer:**
* [x] haha

**Code**: 
print("ha"*2)

---


## Python PCEP Comparison Operators

**1. What is the output of the following code?**


* [ ] 8
* [ ] 12
* [ ] 10
* [ ] SyntaxError

**Correct answer:**
* [x] SyntaxError

**Code**: 
y = 20
x = y += 3
print(x)


**2. What would get printed?**


* [ ] False<br>True
* [ ] True<br>False
* [ ] False<br>False
* [ ] True<br>True

**Correct answer:**
* [x] False<br>True

**Code**: 
min_score = 13
score = 13

print(score > min_score)
print(score <= min_score)


**3. What is the output when the following executes?**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**Code**: 
print(2 < 4)

**4. What is the output of the following python code?**


* [ ] True
* [ ] False
* [ ] 7
* [ ] 6

**Correct answer:**
* [x] True

**Code**: 
x = 6
y = 7
print(x != y)


**5. What is the output when the following executes?**


* [ ] python
* [ ] False
* [ ] True
* [ ] Error

**Correct answer:**
* [x] True

**Code**: 
'python'>'Python'

---


## Python PCEP Conditional Statements

**1. What is the output of the following python code?**


* [ ] TRUE
* [ ] TRUE <br> FALSE
* [ ] FALSE <br> TRUE
* [ ] TRUE <br> FALSE <br> TRUE

**Correct answer:**
* [x] FALSE <br> TRUE

**Code**: 
if 4 + 5 == 10:
    print("TRUE")
else:
    print("FALSE")
print("TRUE")


**2. What is the output of the following python code?**


* [ ] True
* [ ] b is greater than a
* [ ] False
* [ ] IndentationError

**Correct answer:**
* [x] IndentationError

**Code**: 
a = 5
b = 10
if b > a:
print("b is greater than a")

**3. Which of the following is not a boolean expression?**


* [ ] True
* [ ] 3 == 4
* [ ] 3 + 4
* [ ] 3 + 4 == 7

**Correct answer:**
* [x] 3 + 4

**4. What is the output of the following python code?**


* [ ] This is always printed
* [ ] The negative number -10 is not valid here.
* [ ] The negative number -10 is not valid here. <br> This is always printed

**Correct answer:**
* [x] The negative number -10 is not valid here. <br> This is always printed

**Code**: 
x = -10
if x < 0:
    print("The negative number ",  x, " is not valid here.")
print("This is always printed")


**5. What is the output of the following python code?**


* [ ] a and b are equal
* [ ] b is greater than a
* [ ] a is greater than b
* [ ] a and b are equal,       b is greater than a

**Correct answer:**
* [x] b is greater than a

**Code**: 
a = 5
b = 10
if b < a:
  print("a is greater than b")
elif a == b:
  b = 5
  print("a and b are equal")
else:
  print("b is greater than a")


**6. Given the nested if-else below, what will be the value x when the code executed successfully.**


* [ ] 0
* [ ] 2
* [ ] 3
* [ ] 4

**Correct answer:**
* [x] 4

**Code**: 
x = 0
a = 6
b = 6
if a > 0:
    if b < 0: 
        x = x + 6 
    elif a > 6:
        x = x + 5
    else:
        x = x + 4
else:
    x = x + 3

print(x)


**7. Which one of the following if statements will not execute successfully ?**


* [ ] 1st if statement
* [ ] 2nd if statement
* [ ] 3rd if statement
* [ ] 4th if statement

**Correct answer:**
* [x] 2nd if statement
* [x] 3rd if statement

**Code**: 
# 1st if statement
if True: print('hello')

# 2nd if statement
if (5,10):
print('hello')

# 3rd if statement
if (yes):
  print('hello')

# 4th if statement
if (5,10): print('hello')

**Explaination**: 2nd statement: IndentationError: expected an indented block, 3rd statement: NameError: name 'yes' is not defined.

NOTE: The condition in if statement is (5,10), which is a tuple containing the values 5 and 10. In Python, non-empty containers like tuples are considered "truthy," meaning they are treated as true in a boolean context.


**8. What does the following Python program display?**


* [ ] Am I here?
* [ ] Or here?
* [ ] Am I here?       <br>Or here?
* [ ] Or here?       <br>Or over here?

**Correct answer:**
* [x] Or here?       <br>Or over here?

**Code**: 
x = 3
if ( x == 0 ):
  print("Am I here?")
elif ( x == 3 ):
  print("Or here?")
print("Or over here?")


**9. What keyword would you use to add an alternative condition to an if statement?**


* [ ] else if
* [ ] elseif
* [ ] elif
* [ ] None of the above

**Correct answer:**
* [x] elif

**10. Which statement will check if a is equal to b?**


* [ ] <code>if a = b:</code>
* [ ] <code>if a == b:</code>
* [ ] <code>if a === c:</code>
* [ ] <code>if (a == b):</code>

**Correct answer:**
* [x] <code>if a == b:</code>
* [x] <code>if (a == b):</code>

**Explaination**: In Python, you can also use parentheses around the condition in an if statement, and it will not affect the functionality. The parentheses can be used for readability or to clarify the order of operations in more complex conditions. So both the statements "if (a == b):" and "if a == b:" are correct.

---


## Python PCEP Loops - While

**1. What will be the output of the following Python code?**


* [ ] error
* [ ] 2<br>4
* [ ] 2<br>3
* [ ] 2<br>4<br>6<br>8<br>10

**Correct answer:**
* [x] 2<br>4

**Code**: 
i = 2
while True:
    if i%3 == 0:
        break
    print(i)
    i += 2


**2. What will be the output of the following Python code?**


* [ ] error
* [ ] 5<br>6<br>7<br>8
* [ ] 5<br>6
* [ ] 5<br>6<br>7<br>8<br>9<br>10

**Correct answer:**
* [x] 5<br>6<br>7<br>8

**Code**: 
i = 5
while True:
    if i % 0o11 == 0:
        break
    print(i)
    i += 1


**3. What is the value of x ...**


* [ ] 49
* [ ] 50
* [ ] None of the above, this is an infinite loop
* [ ] 51

**Correct answer:**
* [x] 50

**Code**: 
 x = 0
while (x < 50):
  x+=2

print(x)


**Explaination**: In the "while" loop as per statement "x" is increased by 2. Like 2, 4, 6 ... 48 50. 
Value 50 is not satisfying the given condition so it's exited from the loop and the final value we got for "x" is 50.


**4. What is the output when this code executes?**


* [ ] 6
* [ ] 1
* [ ] 4
* [ ] 5
* [ ] None of the above

**Correct answer:**
* [x] 6

**Code**: 
x = 1
while ( x <= 5 ):
  x += 1
print(x)


**5. What will be the output of the following Python code?**


* [ ] error
* [ ] None of the above
* [ ] 1<br>2<br>3<br>4<br>5<br>6
* [ ] 1<br>2<br>3<br>4<br>5<br>6<br>7

**Correct answer:**
* [x] 1<br>2<br>3<br>4<br>5<br>6

**Code**: 
i = 1
while True:
    if i % 0o7 == 0:
        break
    print(i)
    i += 1


**Explaination**: Hint: Control exits the loop when i becomes 7.

**6. What keyword would you use to add an alternative condition to an if statement?**


* [ ] else if
* [ ] elseif
* [ ] elif
* [ ] None of the above

**Correct answer:**
* [x] elif

**7. Which statement will check if a is equal to b?**


* [ ] <code>if a = b:</code>
* [ ] <code>if a == b:</code>
* [ ] <code>if a === c:</code>
* [ ] <code>if (a == b):</code>

**Correct answer:**
* [x] <code>if a == b:</code>
* [x] <code>if (a == b):</code>

**Explaination**: In Python, you can also use parentheses around the condition in an if statement, and it will not affect the functionality. The parentheses can be used for readability or to clarify the order of operations in more complex conditions. So both the statements "if (a == b):" and "if a == b:" are correct.

**8. What will be the output of the following Python code?**


* [ ] error
* [ ] None of the above
* [ ] 1<br>2
* [ ] 1<br>2<br>3

**Correct answer:**
* [x] 1<br>2

**Code**: 
i = 1
while True:
    if i%3 == 0:
        break
    print(i)
    i += 1


**9. What does the following Python program display?**


* [ ] 2
* [ ] 5
* [ ] 19
* [ ] 32

**Correct answer:**
* [x] 32

**Code**: 
x = 1
while ( x < 20 ):
 x = x * 2
print(x)


**10. What does the following code produce as output?**


* [ ] 12
* [ ] 6
* [ ] 3
* [ ] 1

**Correct answer:**
* [x] 6

**Code**: 
i = 1
x = 3
sum = 0
while ( i <= x ):
 sum += i
 i += 1
print(sum)


---


## Python PCEP Loops - For

**1. What will be the output of the following Python code?**


* [ ] error
* [ ] a<br>b<br>c<br>d
* [ ] A<br>B<br>C<br>D
* [ ] a<br>B<br>C<br>D

**Correct answer:**
* [x] a<br>b<br>c<br>d

**Code**: 
x = 'abcd'
for i in x:
    print(i)
    x.upper()


**2. What will be the output of the following Python code?**


* [ ] error
* [ ] a<br>b<br>c<br>d
* [ ] A<br>B<br>C<br>D
* [ ] a<br>B<br>C<br>D

**Correct answer:**
* [x] A<br>B<br>C<br>D

**Code**: 
x = 'abcd'
for i in x:
    print(i.upper())


**Explaination**: Hint: The instance of the string returned by upper() is being printed.

**3. What will be the output of the following Python code?**


* [ ] a b c d
* [ ] 0 1 2 3
* [ ] error
* [ ] a<br>b<br>c<br>d
* [ ] none of the above

**Correct answer:**
* [x] error

**Code**: 
x = 'abcd'
for i in range(x):
    print(i)


**Explaination**: Hint: range(str) is not allowed.

**4. We want to iterate over the values from 0 to 10, and print their values. However, we want to skip all the values that are even. How can we achieve this?**


* [ ] if num % 2 == 0: break;
* [ ] if num % 2 == 0: continue;
* [ ] if num % 2 != 0:      print(num);
* [ ] if num % 2 == 0:      return

**Correct answer:**
* [x] if num % 2 != 0:      print(num);
* [x] if num % 2 == 0: continue;

**Code**: 
for num in range(0, 11):
	#your answer should be here 
print(num)


**5. What will be the output of the following Python code?**


* [ ] error
* [ ] num<br>num<br>num<br>num
* [ ] 0<br>1<br>2<br>3
* [ ] 5<br>6<br>7<br>8<br>9<br>10

**Correct answer:**
* [x] 5<br>6<br>7<br>8<br>9<br>10

**Code**: 
x = 'abcd'
for num in range(5, 11):
  print(num)


**6. What will be the output of the following Python code snippet?**


* [ ] a b c d
* [ ] 0 1 2 3
* [ ] error
* [ ] none of the mentioned

**Correct answer:**
* [x] error

**Code**: 
x = 'abcd'
for i in range(len(x)):
    i.upper()
print (x)


**Explaination**: Hint: Objects of type int have no attribute upper().

**7. What will be the output of the following Python code?**


* [ ] error
* [ ] 1<br>2<br>3<br>4
* [ ] a<br>b<br>c<br>d
* [ ] 0<br>1<br>2<br>3

**Correct answer:**
* [x] 0<br>1<br>2<br>3

**Code**: 
x = 'abcd'
for i in range(len(x)):
    print(i)


**8. What will be the output of the following Python code?**


* [ ] hello  hello  hello  hello
* [ ] 0  1  2  3
* [ ] error
* [ ] a  b  c  d
* [ ] hello<br>hello<br>hello<br>hello

**Correct answer:**
* [x] hello<br>hello<br>hello<br>hello

**Code**: 
x = 'abcd'
for i in range(len(x)):
    print("hello")


**9. What will be the output of the following Python code?**


* [ ] i i i i
* [ ]  Error!
* [ ] 2 0 2 1
* [ ] 0 1 2 3

**Correct answer:**
* [x]  Error!

**Code**: 
x = 2021
for i in x:
  print(i)


**Explaination**: Hint: Objects of type int are not iterable instead a list, dictionary or a tuple should be used.

---


## Python PCEP Lists

**1. What will be the output of below Python code?**


* [ ] 5
* [ ] 4
* [ ] 6
* [ ] none of the above

**Correct answer:**
* [x] 6

**Code**: 
numbers = [1, 2, 3, 4, 5]
numbers[4] = 6
print(numbers[4])


**2. What will be the output of below Python code?**


* [ ] 5
* [ ] 1
* [ ] 2
* [ ] 4

**Correct answer:**
* [x] 5

**Code**: 
list1 = [1, 2, 3, 4, 5]
print(list1[4])


**3. Which of the following would give an error?**


* [ ] <code>list1=[] ++</code>
* [ ] <code>list1=[]</code>
* [ ] <code>list1=[] + 2</code>
* [ ] <code>list1=["USA","Canada","India"]</code>

**Correct answer:**
* [x] <code>list1=[] + 2</code>
* [x] <code>list1=[] ++</code>

**Explaination**: can only concatenate list (not "int") to list.



**4. What will be the output of below Python code?**


* [ ] [10, 13]
* [ ] [10, 12, 14]
* [ ] [11, 12, 13, 14]
* [ ] [10, 11, 12, 13]

**Correct answer:**
* [x] [10, 13]

**Code**: 
list1 = [10, 11, 12, 13, 14]
print(list1[::3])


**5. What will be the output of below Python code?**


* [ ] [1, 2, 10, 4, 5]
* [ ] [1, 2, 3, 4, 10]
* [ ] [1, 2, 3, 4, 5]
* [ ] [10, 2, 3, 4, 5]

**Correct answer:**
* [x] [10, 2, 3, 4, 5]

**Code**: 
list1 = [1, 2, 3, 4, 5]
list1[0] = 10
print(list1)


**6. Which of the following will reverse <code>list1=[2,5,3,1]</code>?**


* [ ] <code>list1[::-1]</code>
* [ ] <code>list1[::2]</code>
* [ ] <code>list1[::1]</code>
* [ ] <code>list1[2:4]</code>

**Correct answer:**
* [x] <code>list1[::-1]</code>

---


## Python PCEP Lists Methods

**1. What will be the output of below Python code?**


* [ ] [24, 46, 56, 72]
* [ ] [24, 46, 72, 56]
* [ ] [56, 72, 24, 46]
* [ ] [24, 46, 56]

**Correct answer:**
* [x] [24, 46, 56, 72]

**Code**: 
ages = [56, 72, 24, 46]
ages.sort()
print(ages)


**2. What will be the output of below Python code?**


* [ ] ['UK', 8, 'India', 'Canada']
* [ ] ['UK', 'India', 'Canada', 8]
* [ ] ['UK', 1, 'India', 'Canada']
* [ ] ['UK',1, 8, 'India', 'Canada']

**Correct answer:**
* [x] ['UK', 8, 'India', 'Canada']

**Code**: 
list1=['UK','India','Canada']

list1.insert(1,8)

print(list1)


**Explaination**: Hint: with insert method, we can insert a new item in between values so item 8 will be inserted into index 1

**3. What will be the output of following Python code?**


* [ ] Go
* [ ] Java
* [ ] C
* [ ] Rust

**Correct answer:**
* [x] C

**Code**: 
list1=["Go","Java","C","Rust"]
print(min(list1))


**4. What will be the output of below Python code?**


* [ ] [10, 20, 30, 40, 50]
* [ ] [10, 20, 30, 40, 50, 60]
* [ ] [60, 10, 20, 30, 40, 50]
* [ ] [10, 20, 30, 40, 60]

**Correct answer:**
* [x] [10, 20, 30, 40, 50, 60]

**Code**: 
list1 = [10, 20, 30, 40, 50]
list1.append(60)
print(list1)


**Explaination**: Hint: with append method, we can append a new item at the end of the list

**5. What will be the output of below Python code?**


* [ ] [1, 3, 4, 4]
* [ ] [4, 4, 3, 1]
* [ ] [1, 4, 4, 3]
* [ ] [4, 3, 4, 1]

**Correct answer:**
* [x] [1, 3, 4, 4]

**Code**: 
num = [4, 4, 3, 1]
num.sort()
print(num)


**6. What is the <code>len(list1)</code> for below value?**


* [ ] 5
* [ ] 4
* [ ] None
* [ ] Error

**Correct answer:**
* [x] 5

**Code**: 
list1=['h', 'e', 'l', 'l', 'o']

**7. What will be the output of following Python code?**


* [ ] Go
* [ ] Java
* [ ] C
* [ ] Python

**Correct answer:**
* [x] Python

**Code**: 
list1=["Go","Java","C","Python"]
print(max(list1))


**8. What will be the output of below Python code?**


* [ ] [4, 4, 3]
* [ ] [4, 4, 1]
* [ ] [4, 3, 1]
* [ ] [4, 4, 3, 1]

**Correct answer:**
* [x] [4, 4, 1]

**Code**: 
list1 = [4, 4, 3, 1]
list1.pop(2)
print(list1)


---


## Python PCEP Iterating Lists

**1. What will be printed by the following code when it executes?**


* [ ] 2
* [ ] 11
* [ ] 12
* [ ] 19

**Correct answer:**
* [x] 19

**Code**: 
sum = 0
values = [2,9,1,7]
for number in values:
    sum += number

print(sum)


**2. <code>for i in [9, 1, 5, 6]:</code>, how many times a loop runs ?**


* [ ] 1
* [ ] 2
* [ ] 3
* [ ] 4

**Correct answer:**
* [x] 4

**3. What will be printed by the following code when it executes?**


* [ ] 2
* [ ] 11
* [ ] 12
* [ ] 19

**Correct answer:**
* [x] 19

**Code**: 
sum = 0
values = [2,9,1,7]
for number in values:
    sum = sum + number

print(sum)


**4. Which of the following statements won’t be printed when this Python code is run?**


* [ ] Letter : K
* [ ] Letter : u
* [ ] Letter : e
* [ ] Letter : d

**Correct answer:**
* [x] Letter : u

**Code**: 
for letter in 'KodeKloud':
    if letter == 'u':
        continue
    print('Letter : ' + letter)


**5. <code>for i in [1, 1, 7, 0, 6]:</code>, how many times a loop run ?**


* [ ] 1
* [ ] 3
* [ ] 2
* [ ] 5

**Correct answer:**
* [x] 5

**6. How many asterisks will be printed when the following code executes?**


* [ ] 20
* [ ] 16
* [ ] 5
* [ ] 4

**Correct answer:**
* [x] 16

**Code**: 
for x in [0, 2, 1, 3]:
    for y in [0, 4, 1, 2]:
            print('*')


**7. How many asterisks will be printed when the following code executes?**


* [ ] 5
* [ ] 4
* [ ] 20
* [ ] 16

**Correct answer:**
* [x] 16

**Code**: 
for x in [0, 1, 1, 3]:
    for y in [0, 2, 1, 2]:
            print('*')


**8. Which of the following statements won’t be printed when this Python code is run?**


* [ ] Letter : K
* [ ] Letter : o
* [ ] Letter : e
* [ ] Letter : d

**Correct answer:**
* [x] Letter : e

**Code**: 
for letter in 'KodeKloud':
    if letter == 'e':
        continue
    print('Letter : ' + letter)


---


## Python PCEP Understanding Lists

**1. Which of the following would give an error?**


* [ ] list1=[]
* [ ] list1=[]*2
* [ ] list1=["USA","Canada","India"]
* [ ] None of the above

**Correct answer:**
* [x] None of the above

**2. What will be the output of below Python code?**


* [ ] [10, 11, 12, 13, 14, 15]
* [ ] [10, 11, 12, 15, 13, 14]
* [ ] [15, 10, 11, 12, 13, 14]
* [ ] [10, 11, 12, 13, 14]

**Correct answer:**
* [x] [10, 11, 12, 13, 14, 15]

**Code**: 
list1 = [10, 11, 12, 13, 14]
list1.append(15)
print(list1)


**3. What will be the output of below Python code?**


* [ ] 10
* [ ] 11
* [ ] 12
* [ ] 14

**Correct answer:**
* [x] 10

**Code**: 
list1 = [10, 11, 12, 13, 14]
print(list1[0])


**4. What will be the output of below Python code?**


* [ ] [1, 2, 3, 2, 5]
* [ ] [8, 9, 10]
* [ ] [4, 5, 6, 7]
* [ ] [10, 11, 12, 13]

**Correct answer:**
* [x] [8, 9, 10]

**Code**: 
list1 = [[1,2,3,2,5],[4,5,6,7],[8,9,10]]
for i in list1:
      if len(i)==3:
        print(i)


**5. What will be the output of below Python code?**


* [ ] [1, 7, 0, 4]
* [ ] [1, 4, 0]
* [ ] [1, 7, 4, 0]
* [ ] [4,0,7,1]

**Correct answer:**
* [x] [1, 7, 0, 4]

**Code**: 
list1=[4,0,7,1]
print(list1[::-1])


**6. What will be the output of below Python code?**


* [ ] [1, 2, 3, 2, 5]
* [ ] [8, 9, 10]
* [ ] [4, 5, 6, 7]
* [ ] [10, 11, 12, 13]

**Correct answer:**
* [x] [4, 5, 6, 7]

**Code**: 
list1 = [[1,2,3,2,5],[4,5,6,7],[8,9,10]]
for i in list1:
      if len(i)==4:
        print(i)


**7. What will be the output of below Python code?**


* [ ] ['C', 'D', 'E']
* [ ] ['B', 'C', 'D', 'E']
* [ ] ['B', 'A', 'C', 'D', 'E']
* [ ] ['A', 'B', 'C', 'D', 'E']

**Correct answer:**
* [x] ['B', 'C', 'D', 'E']

**Code**: 
letters = ["A", "B", "C", "D", "E"]
print(letters[1:])


**8. What will be the output of below Python code?**


* [ ] [1, 2, 3, 4]
* [ ] [0, 1, 2, 3]
* [ ] 0 1       <br>1 2       <br>2 3       <br>3 4
* [ ] 1 0       <br>2 1       <br>3 2       <br>4 3

**Correct answer:**
* [x] 0 1       <br>1 2       <br>2 3       <br>3 4

**Code**: 
list1 = [1, 2, 3, 4]
for i, j in enumerate(list1):
     print(i, j)


**9. What will be the output of below Python code?**


* [ ] [0, 1, 2, 3]
* [ ] [1, 2, 3, 4]
* [ ] 0       <br>1       <br>2       <br>3
* [ ] 1       <br>2       <br>3       <br>4
* [ ] 0 1<br>1 2<br>2 3 <br> 3 4

**Correct answer:**
* [x] 0 1<br>1 2<br>2 3 <br> 3 4

**Code**: 
list1 = [1, 2, 3, 4]
for index, j in enumerate(list1):
     print(index, j)


**10. What will be the output of below Python code?**


* [ ] [10, 11, 12, 13, 14]
* [ ] [10, 12, 14]
* [ ] [11, 12, 13, 14]
* [ ] [10, 11, 12, 13]

**Correct answer:**
* [x] [10, 11, 12, 13, 14]

**Code**: 
list1 = [10, 11, 12, 13, 14]
print(list1[::1])


---


## Python PCEP Slicing Lists

**1. What will be the output of below Python code?**


* [ ] 0
* [ ] 1
* [ ] 9
* [ ] 4

**Correct answer:**
* [x] 4

**Code**: 
my_list = [0, 1, 2, 3, 4]
print(my_list[-1])


**2. What will be the output of below Python code?**


* [ ] [0, 1, 2, 3, 4]
* [ ] [0, 1, 2]
* [ ] [2, 3]
* [ ] [0, 1, 2, 3]

**Correct answer:**
* [x] [2, 3]

**Code**: 
my_list = [0, 1, 2, 3, 4]
print(my_list[2:4])


**3. What will be the output of below Python code?**


* [ ] [1, 66, "python"]
* [ ] ['python', [11, 55, 'cat']]
* [ ] [[11, 55, "cat"], [ ]]
* [ ] [2.22, True]

**Correct answer:**
* [x] ['python', [11, 55, 'cat']]

**Code**: 
list1 = [1, 66, "python", [11, 55, "cat"], [ ], 2.22, True]
print(list1[2:4])


**4. What will be the output of below Python code?**


* [ ] [4, 3, 2, 1, 0]
* [ ] [0, 2, 4, 1, 3]
* [ ] [0, 3, 4]
* [ ] [0, 1, 2, 3]

**Correct answer:**
* [x] [4, 3, 2, 1, 0]

**Code**: 
my_list = [0, 1, 2, 3, 4]
print(my_list[::-1])


**5. What will be the output of below Python code?**


* [ ] [1, 66, "python"]
* [ ] ['python', [11, 55, 'cat']]
* [ ] [[11, 55, "cat"], [ ]]
* [ ] [1, 66, 'python', [11, 55, 'cat']]

**Correct answer:**
* [x] [1, 66, 'python', [11, 55, 'cat']]

**Code**: 
list1 = [1, 66, "python", [11, 55, "cat"], [ ], 2.22, True]
print(list1[0:4])


**6. What will be the output of below Python code?**


* [ ] [0, 2, 4, 3]
* [ ] [0, 2, 4]
* [ ] [0, 3, 4]
* [ ] [0, 1, 2, 3]

**Correct answer:**
* [x] [0, 2, 4]

**Code**: 
my_list = [0, 1, 2, 3, 4]
print(my_list[::2])


**7. What will be the output of below Python code?**


* [ ] [0, 2]
* [ ] [0, 2, 4]
* [ ] [0, 3]
* [ ] [0, 1, 2, 3]

**Correct answer:**
* [x] [0, 3]

**Code**: 
my_list = [0, 1, 2, 3, 4]
print(my_list[::3])


**8. What will be the output of below Python code?**


* [ ] Error!
* [ ] ['python', [11, 55, 'cat']]
* [ ] [[11, 55, "cat"], [ ]]
* [ ] [1, 66, 'python', [11, 55, 'cat']]

**Correct answer:**
* [x] Error!

**Code**: 
list1 = [1, 66, "python", [11, 55, "cat"], [ ], 2.22, True]
print(list1.upper())


**Explaination**: ‘list' object has no attribute 'upper'

**9. What will be the output of below Python code?**


* [ ] [2, 3, 4, 'python']
* [ ] [2, 3, 4]
* [ ] [1, 2, 3, 4, 'python']
* [ ] [0, 1, 2, 3, 4, 'python']

**Correct answer:**
* [x] [2, 3, 4, 'python']

**Code**: 
my_list = [0, 1, 2, 3, 4]
my_list.append("python")
print(my_list[2:])


**10. What will be the output of below Python code?**


* [ ] [2, 3, 4, 'python']
* [ ] [2, 3, 4]
* [ ] [1, 2, 3, 4, 'python']
* [ ] [0, 1, 2, 3, 4, 'python']

**Correct answer:**
* [x] [1, 2, 3, 4, 'python']

**Code**: 
my_list = [0, 1, 2, 3, 4]
my_list.append("python")
b = my_list[1:]
print(b)


---


## Python PCEP Finding in Lists

**1. What is the output of the following code:**


* [ ] False
* [ ] True
* [ ] None

**Correct answer:**
* [x] True

**Code**: 
(4, 6) not in [(4, 7), (5, 6), "hello"]

**2. Choose the correct answer if the following list contains the element 'A'. Check if you get "True" in the output.**


* [ ] 'A' in Li
* [ ] A in Li
* [ ] ‘A’ not in Li

**Correct answer:**
* [x] 'A' in Li

**Code**: 
Li = ['A','C','b', 1, 3, 4]

**3. What will be the output of below Python code?**


* [ ] 4
* [ ] 3
* [ ] 2
* [ ] 1

**Correct answer:**
* [x] 2

**Code**: 
my_list = [0, 1, 2, 3, 4]
print(my_list.index(2))


**4. What will be the output of the following Python code?**


* [ ] [0, [8, 9], 4, 1, 2]
* [ ] [0, 4, 1, 2]
* [ ] [0, 3, 8, 9]
* [ ] [[8, 9], 3, 4, 1, 2]

**Correct answer:**
* [x] [0, 3, 8, 9]

**Code**: 
list1 = [0, 3, 4, 1, 2]
list1[2:5]=[8,9]
print(list1)


**5. What will be the output of below Python code?**


* [ ] ["USA", "India", "Canada"]
* [ ] ["USA", "Canada", "India"]
* [ ] ['Canada', 'USA', 'India']
* [ ] ['Canada', 'India', 'USA']

**Correct answer:**
* [x] ['Canada', 'USA', 'India']

**Code**: 
countries = ["USA", "Canada", "India"]
countries[0], countries[1] = countries[1], countries[0]
print(countries)


**6. What will be the output of the following Python code?**


* [ ] [0, [8, 9], 4, 1, 2]
* [ ] [0, 4, 1, 2]
* [ ] [4, 1, 2]
* [ ] [[8, 9], 3, 4, 1, 2]

**Correct answer:**
* [x] [0, [8, 9], 4, 1, 2]

**Code**: 
list1 = [0, 3, 4, 1, 2]
list1[1]=[8,9]
print(list1)


**7. What will be the output of the following Python code?**


* [ ] [0, 3, [1, 2], 1, 2]
* [ ] [0, 4, 1, 2]
* [ ] [3, 4, 1, 2]
* [ ] [0, 3, 1, 2, 2]

**Correct answer:**
* [x] [0, 3, 1, 2, 2]

**Code**: 
list1 = [0, 3, 4, 1, 2]
list1[2:4]=[1,2]
print(list1)


**8. What will be the output of below Python code?**


* [ ] 4
* [ ] 3
* [ ] 2
* [ ] 1

**Correct answer:**
* [x] 3

**Code**: 
my_list = [0, 3, 4, 1, 2]
print(my_list.index(1))


**9. What will be the output of the following Python code?**


* [ ] [9, 4, 6, 1, 2]
* [ ] [3, 4, 6, 1, 2]
* [ ] [3, 4, 1, 2]
* [ ] [3, 9, 6, 1, 2]

**Correct answer:**
* [x] [9, 4, 6, 1, 2]

**Code**: 
list1=[3,4,6,1,2]
list2=list1
list1[0]=9
print(list2)


**10. What will be the output of the following Python code?**


* [ ] [9, 4, 6, 1, 2]
* [ ] [3, 4, 6, 1, 2]
* [ ] [3, 4, 1, 2]
* [ ] [3, 9, 6, 1, 2]

**Correct answer:**
* [x] [3, 9, 6, 1, 2]

**Code**: 
list1=[3,4,6,1,2]
list2=list1
list1[1]=9
print(list2)


---


## Python PCEP Nested Lists - 2D

**1. What will be the output of the following Python code?**


* [ ] 4
* [ ] 2
* [ ] 0
* [ ] 1

**Correct answer:**
* [x] 0

**Code**: 
matrix = [[0, 1, 2], [0, 1, 2], [0, 1, 2]]

matrix2 = []

for submatrix in matrix:
  for val in submatrix:
    matrix2.append(val)

print(matrix2[0])


**2. What will be the output of the following Python code?**


* [ ] 4
* [ ] 2
* [ ] 0
* [ ] 1

**Correct answer:**
* [x] 2

**Code**: 
matrix = [[0, 1, 2], [0, 1, 2], [0, 1, 2]]

matrix2 = []

for submatrix in matrix:
  for val in submatrix:
    matrix2.append(val)

print(matrix2[2])


**3. What will be the output of the following Python code?**


* [ ] 4
* [ ] 2
* [ ] 0
* [ ] 1

**Correct answer:**
* [x] 2

**Code**: 
matrix = [[j for j in range(3)] for i in range(3)] 
print(matrix[1][2])


**4. What will be the output of the following Python code?**


* [ ] ['Egypt', 'USA', 'India', 'Dubai', 'Spain']
* [ ] ['Egypt', 'America', 'India', 'Dubai', 'London']
* [ ] ['Egypt', 'USA', 'France', 'Dubai', 'Spain']
* [ ] ['France', 'USA', 'India', 'England', 'Spain']

**Correct answer:**
* [x] ['Egypt', 'USA', 'India', 'Dubai', 'Spain']

**Code**: 
countries = [['Egypt', 'USA', 'India'],
       ['Dubai', 'America', 'Spain'], 
       ['London', 'England', 'France']]
countries2  = [country for sublist in countries for country in 
                       sublist if len(country) < 6]
print(countries2)


**5. What will be the output of the following Python code?**


* [ ] 4
* [ ] 2
* [ ] 0
* [ ] 1

**Correct answer:**
* [x] 1

**Code**: 
matrix = [[j for j in range(4)] for i in range(4)] 
print(matrix[3][1])


**6. What will be the output of the following Python code?**


* [ ] ['India']
* [ ] ['Egypt']
* [ ] ['Dubai']
* [ ] ['USA']

**Correct answer:**
* [x] ['USA']

**Code**: 
countries = [['Egypt', 'USA', 'India'], ['Dubai', 'America', 'Spain'], ['London', 'England', 'France']]
countries2  = [country for sublist in countries for country in sublist if len(country) < 4]
print(countries2)


**7. What will be the output of the following Python code?**


* [ ] 1
* [ ] 0
* [ ] 2
* [ ] 3

**Correct answer:**
* [x] 3

**Code**: 
a = []
for i in range(5):
    a.append([])
    for j in range(5):
        a[i].append(j)

print(a[2][3])


**8. What will be the output of the following Python code?**


* [ ] 4
* [ ] 2
* [ ] 0
* [ ] 1

**Correct answer:**
* [x] 1

**Code**: 
matrix = [[j for j in range(3)] for i in range(3)] 
print(matrix[2][1])


**9. What will be the output of the following Python code?**


* [ ] 1
* [ ] 0
* [ ] 2
* [ ] 3

**Correct answer:**
* [x] 3

**Code**: 
a = []
for i in range(5):
    a.append([])
    for j in range(5):
        a[i].append(j)

print(a[3][3])


**10. What will be the output of the following Python code?**


* [ ] [[0, 1], [0, 1]]
* [ ] [[0, 2], [0, 2]]
* [ ] [[1, 2], [1, 2]]
* [ ] [[1, 1], [1, 1]]

**Correct answer:**
* [x] [[0, 1], [0, 1]]

**Code**: 
a = []
for i in range(2):
    a.append([])
    for j in range(2):
        a[i].append(j)

print(a)


**11. Choose the correct answer to define a list “Num” which contains numbers from 1-9 with 3 elements only in the row.**


* [ ] <code> Num =[ [1,2,3] , [4,5,6,7] , [8,9] ]</code>
* [ ] <code> Num =[ [1,2,3] , [4,5,6,] , [7,8,9] ]</code>
* [ ] <code> Num =[ [1,2,3,4] , [4,5,6,7] , [8,9,6] ]</code>
* [ ] <code> Num =[ [1,2,3,4] , [4,5,6] , [7,8,9] ]</code>

**Correct answer:**
* [x] <code> Num =[ [1,2,3] , [4,5,6,] , [7,8,9] ]</code>

---


## Python PCEP Nested Lists - 3D

**1. Choose the correct answer to get the “Red” color from the following list:**


* [ ] <code>print( Colors[0][2] )</code>
* [ ] <code>print( Colors[2][0][2] )</code>
* [ ] <code>print( Colors[2][1][2] )</code>
* [ ] <code>print( Colors[2][0][1] )</code>

**Correct answer:**
* [x] <code>print( Colors[2][0][2] )</code>

**Code**: 
Colors= [ [['Blue','Green','White','Black']], [['Green','Blue','White','Yellow']] , [['White','Blue','Red','Green']] ]


**2. What will be the output of the following Python code?**


* [ ] 2
* [ ] [0, 1, 2]
* [ ] 0
* [ ] 1

**Correct answer:**
* [x] [0, 1, 2]

**Code**: 
matrix = [[[0, 1, 2], [0, 1, 2], [0, 1, 2]], [[0, 1, 2], [0, 1, 2], [0, 1, 2]], [[0, 1, 2], [0, 1, 2], [0, 1, 2]]]

matrix2 = []

for submatrix in matrix:
  for val in submatrix:
    matrix2.append(val)

print(matrix2[2])


**3. What will be the output of the following Python code?**


* [ ] [0, 1, 2]
* [ ] 2
* [ ] 0
* [ ] 1

**Correct answer:**
* [x] 1

**Code**: 
matrix = [[[k for k in range(3)] for j in range(3)] for i in range(3)]
print(matrix[1][1][1])


**4. What will be the output of the following Python code?**


* [ ] [0, 1, 2]
* [ ] 2
* [ ] 0
* [ ] 1

**Correct answer:**
* [x] 1

**Code**: 
matrix = [[[k for k in range(3)] for j in range(3)] for i in range(3)]
print(matrix[0][0][1])


**5. What will be the output of the following Python code?**


* [ ] [0, 1, 2]
* [ ] 2
* [ ] 0
* [ ] 1

**Correct answer:**
* [x] [0, 1, 2]

**Code**: 
matrix = [[[k for k in range(3)] for j in range(3)] for i in range(3)]
print(matrix[1][2])


**6. What will be the output of the following Python code?**


* [ ] 4
* [ ] 2
* [ ] 0
* [ ] 1

**Correct answer:**
* [x] 0

**Code**: 
matrix = [[[0, 1, 2], [0, 1, 2], [0, 1, 2]], [[0, 1, 2], [0, 1, 2], [0, 1, 2]], [[0, 1, 2], [0, 1, 2], [0, 1, 2]]]

matrix2 = []

for submatrix in matrix:
  for val in submatrix:
    matrix2.append(val)

print(matrix2[2][0])


**7. Choose the correct code to get the third element in the second row,   Regarding the following list :**


* [ ] <code>print(Colors[2][3])</code>
* [ ] <code>print(Colors[1][3])</code>
* [ ] <code>print(Colors[1][2])</code>
* [ ] <code>print(Colors[2][2])</code>

**Correct answer:**
* [x] <code>print(Colors[1][2])</code>

**Code**: 
Colors = [ ['Red', 'Green', 'White', 'Black'], ['Green', 'Blue', 'White', 'Yellow'] ,['White', 'Blue', 'Green', 'Red'] ]

**8. What will be the output of the following Python code?**


* [ ] 1
* [ ] 2
* [ ] 0
* [ ] [0, 1, 2]

**Correct answer:**
* [x] [0, 1, 2]

**Code**: 
matrix = [[[k for k in range(3)] for j in range(3)] for i in range(3)]
print(matrix[2][1])


**9. What will be the output of the following Python code?**


* [ ] 4
* [ ] 2
* [ ] 0
* [ ] 1

**Correct answer:**
* [x] 2

**Code**: 
matrix = [[[0, 1, 2], [0, 1, 2], [0, 1, 2]], [[0, 1, 2], [0, 1, 2], [0, 1, 2]], [[0, 1, 2], [0, 1, 2], [0, 1, 2]]]

matrix2 = []

for submatrix in matrix:
  for val in submatrix:
    matrix2.append(val)

print(matrix2[2][2])


**10. What will be the output of the following Python code?**


* [ ] 4
* [ ] 2
* [ ] 0
* [ ] 1

**Correct answer:**
* [x] 0

**Code**: 
matrix = [[[0, 1, 2], [0, 1, 2], [0, 1, 2]], [[0, 1, 2], [0, 1, 2], [0, 1, 2]], [[0, 1, 2], [0, 1, 2], [0, 1, 2]]]
print(matrix[0][0][0])


---


## Python PCEP Functions

**1. Method is called by its name, but it is associated with an object.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**2. What is the output of the following snippet:**


* [ ] The tallest student is Jackson
* [ ] The tallest student is Ella
* [ ] Error
* [ ] Jackson

**Correct answer:**
* [x] The tallest student is Jackson

**Code**: 
def my_function(*students):
  print("The tallest student is " + students[2])

my_function("James", "Ella", "Jackson")


**Explaination**: If the number of arguments is unknown, we can add a * before the parameter name.

**3. Choose the correct answer which defines a function to get numeric input from the user:**


* [ ] <code>def input_num(): return input(num)</code>
* [ ] <code>def input_num(): return input()</code>
* [ ] <code>def input_num(): int(input())</code>
* [ ] <code>def input_num(): return int(input())</code>

**Correct answer:**
* [x] <code>def input_num(): return int(input())</code>

**4. The output of the following snippet of the code will be:**


* [ ] 0
* [ ] 1
* [ ] 2
* [ ] 3

**Correct answer:**
* [x] 1

**Code**: 
a = 0
def add_one(a):
	return a+1

result = add_one(a)
print(result)


**5. Regarding the definition of the function and the sample input, Choose the correct value of the output.**


* [ ] 50
* [ ] None
* [ ] Error
* [ ] 15

**Correct answer:**
* [x] Error

**Code**: 
Function :  def multi_func(num1,num2):
                      return num1 *num2 
Sample input: print ( multi_func(5 , num1= 10) )

**Explaination**: The correct input should be <code>print ( multi_func(5 , 10) ) </code> 
Or
<code>print ( multi_func(num1=5 , num2=10) ) </code>


**6. What is the error in the following snippet code:**


* [ ] The multi_func() function doesn’t have any parameters
* [ ] The result is not defined as a global variable
* [ ] The function return None
* [ ] print() is not a built-in function

**Correct answer:**
* [x] The result is not defined as a global variable

**Code**: 
def multi_func():
  result = int(input()) * 5
  return result     

print(result)

**7. The function can have only one parameter. If any data (parameters) are passed, they are passed explicitly.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**Explaination**: The function can have different parameters or may not have any at all. If any data (parameters) are passed, they are passed explicitly.

**8. Define a function that gets the user input and multiply it by a number we passed to the function.**


* [ ] <code>def multi_num(num):  return int(input () ) * num</code>
* [ ] <code>def multi_num():  return int(input (num) ) * 6</code>
* [ ] <code>def multi_num(6):  return int(input () ) * num</code>
* [ ] <code>def multi_num():  return int(input () ) * num</code>

**Correct answer:**
* [x] <code>def multi_num(num):  return int(input () ) * num</code>

**Code**: 
Sample input:  6
Sample output: 

print( multi_num(5) )
30


**9. What will be the result of calling the print_info function with the arguments 'john' and 19?**


* [ ] john 19
* [ ] john 18
* [ ] name 19
* [ ] name age

**Correct answer:**
* [x] john 19

**Code**: 
def print_info(name, age=18):
    print(name, age)

print_info('john', 19)


---


## Python PCEP Arguments

**1. Define a function that gets the user input and add to it a number we passed to the function.**


* [ ] <code>def add_num(num):  return int(input () ) + num</code>
* [ ] <code>def add_num():  return int(input (num) ) + 6</code>
* [ ] <code>def add_num(6):  return int(input () ) + num</code>
* [ ] <code>def add_num():  return int(input () ) + num</code>

**Correct answer:**
* [x] <code>def add_num(num):  return int(input () ) + num</code>

**Code**: 
Sample input:  6
Sample output: 
print( add_num(5) )
11


**2. The output of the following snippet of the code will be:**


* [ ] [5,4,1]
* [ ] [7,4,1]
* [ ] [7,4,5]
* [ ] [7,5,4]

**Correct answer:**
* [x] [7,4,5]

**Code**: 
nums= [7,4,1]
def change_third_item(list):
	list[2] = 5

change_third_item(nums)
print(nums)


**3. Regarding the definition of the function and the sample input, Choose the correct value of the output.**


* [ ] 15
* [ ] None
* [ ] Error
* [ ] 10

**Correct answer:**
* [x] 10

**Code**: 
Function :  def add_func(num1,num2):
            	  	return num1 + num2 
		
Sample input: print ( add_func(5 , 5) )


**4. Regarding the definition of the function and the sample input, Choose the correct value of the output.**


* [ ] 15
* [ ] None
* [ ] Error
* [ ] 50

**Correct answer:**
* [x] Error

**Code**: 
Function :  def add_func(num1,num2):
                   	return num1 + num2
 
Sample input: print ( add_func(5 , num1= 10) )


**5. The output of the following snippet of the code will be:**


* [ ] 0
* [ ] 1
* [ ] 6
* [ ] 3

**Correct answer:**
* [x] 6

**Code**: 
a = 0
def add_three(a):
	return a+3

result = add_three(3)
print(result)


**6. The output of the following snippet of the code will be:**


* [ ] john 19
* [ ] john 18
* [ ] name 19
* [ ] name age

**Correct answer:**
* [x] john 18

**Code**: 
def print_name_age(name, age=19):
    print(name, age)

print_name_age('john', 18)


**7. What is the output of the following snippet:**


* [ ] The tallest student is john
* [ ] The tallest student is Ella
* [ ] Error
* [ ] mark

**Correct answer:**
* [x] The tallest student is john

**Code**: 
def my_function(*friends):
  print("The tallest student is " + friends[0])

my_function("john", "Ella", "mark")


**8. The output of the following snippet of the code will be:**


* [ ] 0
* [ ] 1
* [ ] 2
* [ ] 3

**Correct answer:**
* [x] 3

**Code**: 
a = 0
def add_three(a):
	return a+3

result = add_three(a)
print(result)


**9. What will be the result of calling the fullname_func function with the argument 'John'?**


* [ ] John
* [ ] john Mark
* [ ] John Mark
* [ ] Mark

**Correct answer:**
* [x] John Mark

**Code**: 
def fullname_func(fname):
  print(fname + " Mark")

fullname_func("John")


**10. What will be the result of calling the fullname_func function with the arguments 'John' and 'Mark'?**


* [ ] John
* [ ] john Mark
* [ ] John Mark
* [ ] Mark

**Correct answer:**
* [x] John Mark

**Code**: 
def fullname_func(fname, lname):
  print(fname + " " + lname)

fullname_func("John", "Mark")


---


## Python PCEP Return Statement

**1. The output of the following snippet of the code will be:**


* [ ] 9
* [ ] 5
* [ ] 8
* [ ] 6

**Correct answer:**
* [x] 6

**Code**: 
def my_function(x):
  return 10 - x

print(my_function(4))


**2. The output of the following snippet of the code will be:**


* [ ] Hello, World
* [ ] None
* [ ] Error
* [ ] return_greeting

**Correct answer:**
* [x] Hello, World

**Code**: 
def return_greeting():
  return "Hello, World"

print(return_greeting())


**3. The output of the following snippet of the code will be:**


* [ ] 9
* [ ] 5
* [ ] 8
* [ ] 3

**Correct answer:**
* [x] 8

**Code**: 
def my_function(x):
  return 5 + x

print(my_function(3))


**4. What will the following function return?**


* [ ] None
* [ ] The value of x + y
* [ ] The value of x
* [ ] The value of y

**Correct answer:**
* [x] The value of x + y

**Code**: 
def add(x, y):
  return x+y


**5. The output of the following snippet of the code will be:**


* [ ] The result is False
* [ ] The result is True
* [ ] The result is bool
* [ ] The result is 0

**Correct answer:**
* [x] The result is True

**Code**: 
def is_true(a): 
  return bool(a) 

result = is_true(3<6) 
print("The result is", result)


**6. The output of the following snippet of the code will be:**


* [ ] The result of  4  is  16
* [ ] The result of  2  is  4
* [ ] The result of  num  is result
* [ ] The result of  4  is  2

**Correct answer:**
* [x] The result of  2  is  4

**Code**: 
def square(i):
    j = i * i
    return j

num = 2
result = square(num)
print("The result of ", num, " is ", result)


**7. The output of the following snippet of the code will be:**


* [ ] 5
* [ ] 5.0
* [ ] 8
* [ ] 6.0

**Correct answer:**
* [x] 5.0

**Code**: 
def my_function(x):
  return 10 / x

print(my_function(2))


**8. The output of the following snippet of the code will be:**


* [ ] 9
* [ ] 4
* [ ] 12
* [ ] 8

**Correct answer:**
* [x] 9

**Code**: 
def square(i):
    j = i * i
    return j

print(square(3))


**9. The output of the following snippet of the code will be:**


* [ ] The result is False
* [ ] The result is True
* [ ] The result is bool
* [ ] The result is 0

**Correct answer:**
* [x] The result is False

**Code**: 
def is_true(a): 
  return bool(a) 

result = is_true(6<3) 
print("The result is", result)


---


## Python PCEP List as Argument

**1. What is the output of the following snippet:**


* [ ] 6.5
* [ ] 6.25
* [ ] 2.5
* [ ] 5.5

**Correct answer:**
* [x] 6.5

**Code**: 
def mean_func(list1):
    return sum(list1) / len(list1)

print(mean_func([5, 6, 7, 8]))


**2. What is the output of the following snippet:**


* [ ] 1 3 4
* [ ] 2 3 4
* [ ] 1 2 3
* [ ] 2 1 4

**Correct answer:**
* [x] 2 3 4

**Code**: 
def my_function(numbers):
  for i in numbers:
    print(i+1, end=' ')

numbers = [1, 2, 3] 
my_function(numbers)


**3. What is the output of the following snippet:**


* [ ] john emmy mark
* [ ] john mark emmy
* [ ] john mark
* [ ] emmy mark

**Correct answer:**
* [x] john mark emmy

**Code**: 
def my_function(names):
  for i in names:
    print(i, end=' ')

names = ["john", "mark", "emmy"]
my_function(names)


**4. What is the output of the following snippet:**


* [ ] 3.5
* [ ] 3.25
* [ ] 2.5
* [ ] 3

**Correct answer:**
* [x] 3.25

**Code**: 
def mean_func(list1):
    return sum(list1) / len(list1)

print(mean_func([5, 2, 2, 4]))


**5. What is the output of the following snippet:**


* [ ] [1, 2, 3, 4, 5, 6]
* [ ] [1, 2, 3]
* [ ] [1, 3, 5]
* [ ] [4, 5, 6]

**Correct answer:**
* [x] [1, 3, 5]

**Code**: 
def get_odd_func(numbers):
    odd_numbers = [num for num in numbers if num % 2]
    return odd_numbers

print(get_odd_func([1, 2, 3, 4, 5, 6]))


**6. What is the output of the following snippet:**


* [ ] [7, 5, 9]
* [ ] [7, 4, 5]
* [ ] [4, 6, 8, 12]
* [ ] [5, 6, 8, 12]

**Correct answer:**
* [x] [7, 5, 9]

**Code**: 
def get_odd_func(numbers):
    odd_numbers = [num for num in numbers if num % 2]
    return odd_numbers

print(get_odd_func([7, 4, 5, 6, 9, 8, 12]))


**7. What is the output of the following snippet:**


* [ ] 1 2 3
* [ ] 2 3 4
* [ ] 12 14 16
* [ ] 1 12 14

**Correct answer:**
* [x] 12 14 16

**Code**: 
def my_function(numbers):
  for i in numbers:
    print(i*2+10, end=' ')

numbers = [1, 2, 3]
my_function(numbers)


**8. What is the output of the following snippet:**


* [ ] [2, 4, 6]
* [ ] [1, 2, 3]
* [ ] [1, 3, 5]
* [ ] [4, 5, 6]

**Correct answer:**
* [x] [2, 4, 6]

**Code**: 
def get_even_func(numbers):
    even_numbers = [num for num in numbers if not num % 2]
    return even_numbers

get_even_func([1, 2, 3, 4, 5, 6])


**9. What is the output of the following snippet:**


* [ ] [1, 2,3]
* [ ] [1,2,3[1,2,3]]
* [ ] [1, 2, 3][1,2,3]
* [ ] [1, 2, 3, 1, 2, 3]

**Correct answer:**
* [x] [1, 2, 3, 1, 2, 3]

**Code**: 
def double_list(numbers):
  return 2 * numbers

numbers = [1, 2, 3]
print(double_list(numbers))


---


## Python PCEP Scopes

**1. What is the output of the following snippet:**


* [ ] 20
* [ ] Error
* [ ] 0
* [ ] False

**Correct answer:**
* [x] Error

**Code**: 
def myfunc():
  a = 20

myfunc()
print(a)


**Explaination**: Hint: A variable created inside a function belongs to the local scope of that function, and can only be used inside that function.

**2. What is the output of the following snippet:**


* [ ] 20  20
* [ ] Error
* [ ] 30   20
* [ ] False

**Correct answer:**
* [x] 30   20

**Code**: 
x = 20
def my_function():
  x = 30
  print(x, end=' ')

my_function()
print(x, end=' ')


**3. What is the output of the following snippet:**


* [ ] Error
* [ ] 20
* [ ] False
* [ ] 20 20

**Correct answer:**
* [x] 20 20

**Code**: 
x = 20
def my_function():
  print(x, end=' ')

my_function()
print(x, end=' ')


**Explaination**: Hint: A variable created outside of a function is global and can be used by anyone.

**4. What is the output of the following snippet:**


* [ ] 0
* [ ] Error
* [ ] 20
* [ ] False

**Correct answer:**
* [x] 20

**Code**: 
def my_function():
  def my_inner_function():
    x = 20
    print(x)
  my_inner_function()

my_function()


**5. What is the output of the following snippet:**


* [ ] 20
* [ ] Error
* [ ] 0
* [ ] False

**Correct answer:**
* [x] 20

**Code**: 
def my_function():
  x = 20
  def my_inner_function():
    print(x)
  my_inner_function()
my_function()

**Explaination**: Hint: The local variable can be accessed from a function within the function.

**6. What is the output of the following snippet:**


* [ ] 20
* [ ] Error
* [ ] 0
* [ ] False

**Correct answer:**
* [x] 20

**Code**: 
def myfunc():
  a = 20
  print(a)

myfunc()


**7. What is the output of the following snippet:**


* [ ] 0
* [ ] Error
* [ ] 20
* [ ] False

**Correct answer:**
* [x] Error

**Code**: 
def my_function():
  def my_inner_function():
    x = 20
  print(x)
  my_inner_function()

my_function()


**8. What is the output of the following snippet:**


* [ ] 20   20
* [ ] Error
* [ ] 30   20
* [ ] False

**Correct answer:**
* [x] 30   20

**Code**: 
x = 20
def my_function():
  x = 30
  print(x, end=' ')

my_function()
print(x, end=' ')


**9. What is the output of the following snippet:**


* [ ] 20
* [ ] Error
* [ ] 30
* [ ] False

**Correct answer:**
* [x] 30

**Code**: 
def my_function():
  global x
  x = 30

my_function()
print(x)


**10. What is the output of the following snippet:**


* [ ] 20
* [ ] Error
* [ ] 30
* [ ] False

**Correct answer:**
* [x] 20

**Code**: 
x = 30
def my_function():
  global x
  x = 20

my_function()
print(x)


---


## Python PCEP Arguments Explained

**1. What is the output of the following snippet:**


* [ ] ('Hello', 'World!')
* [ ] Hello, World!
* [ ] Hello
* [ ] Error

**Correct answer:**
* [x] Hello

**Code**: 
def my_function(*argv):
  print(argv[0])

my_function('Hello', 'World!')


**2. What is the output of the following snippet:**


* [ ] Error
* [ ] Welcome to Python!
* [ ] First argument: Welcome to Python!
* [ ] First argument: Welcome<br>Next argument: to<br>Next argument: Python!

**Correct answer:**
* [x] First argument: Welcome<br>Next argument: to<br>Next argument: Python!

**Code**: 
def my_function(arg1, *argv): 
    print ("First argument:", arg1) 
    for arg in argv: 
        print("Next argument:", arg) 

my_function('Welcome', 'to', 'Python!')


**3. What is the output of the following snippet:**


* [ ] Hello       World!
* [ ] Hello
* [ ] World!
* [ ] Hello<br>World!

**Correct answer:**
* [x] Hello<br>World!

**Code**: 
def my_function(*argv):  
    for arg in argv:  
        print(arg) 

my_function('Hello', 'World!')


**Explaination**: Hint: The syntax is to use the symbol * to take in a variable number of arguments

**4. What is the output of the following snippet:**


* [ ] 5
* [ ] 2
* [ ] Error
* [ ] 3

**Correct answer:**
* [x] 5

**Code**: 
def sum(a,b):
    return a+b

print(sum(2,3))


**5. What is the output of the following snippet:**


* [ ] The older friend is 13 years
* [ ] The older friend is 11 years
* [ ] Error
* [ ] The older friend is 12 years

**Correct answer:**
* [x] The older friend is 13 years

**Code**: 
def my_function(*ages):
  print("The older friend is " + ages[0] + " years")

my_function("13", "12", "11")


**6. What is the output of the following snippet:**


* [ ] ('Hello', 'World!')
* [ ] Hello, World!
* [ ] ['Hello', 'World!']
* [ ] Error

**Correct answer:**
* [x] ('Hello', 'World!')

**Code**: 
def my_function(*argv):
  print(argv)

my_function('Hello', 'World!')


**7. What is the output of the following snippet:**


* [ ] 6
* [ ] 2
* [ ] Error
* [ ] 3

**Correct answer:**
* [x] Error

**Code**: 
def sum(*args):
    for arg in args:
        result += arg
    return result 

print(sum(2,3,1))


**Explaination**: Hint: UnboundLocalError: local variable 'result' referenced before assignment

**8. What is the output of the following snippet:**


* [ ] 5.0
* [ ] 4.0
* [ ] Error
* [ ] 2

**Correct answer:**
* [x] 4.0

**Code**: 
def division(a,b):
    return a/b

division(8,2)


---


## Python PCEP Tuples

**1. Write a Python program to create an empty tuple:**


* [ ] x = tuple(3)
* [ ] x = ()
* [ ] x = (2,3)
* [ ] x = (0)

**Correct answer:**
* [x] x = ()

**2. …. is one of the data types in Python that used to store collections of data.**


* [ ] List
* [ ] Set
* [ ] Dictionary
* [ ] Tuple

**Correct answer:**
* [x] List
* [x] Set
* [x] Dictionary
* [x] Tuple

**3. What is the output of the following command if the tuple1 has values (1,2,3,4,5) :**


* [ ] (1,2,3,4,6)
* [ ] (6,)
* [ ] (1,2,3,4,5,6)
* [ ] AttributeError

**Correct answer:**
* [x] AttributeError

**Code**: 
print(tuple1.append(6))

**4. Write a Python program to create a tuple with different data types:**


* [ ] x = ("john", True, 2.2, 2)
* [ ] x = ("tuple", "john", "list")
* [ ] x = (2.1, 3.2, 1.3)
* [ ] x = (1, 2, 3)

**Correct answer:**
* [x] x = ("john", True, 2.2, 2)

**Code**: 
print(x)

**5. Which of the following is used to create a tuple called tuple1 which contains numbers from 1 to 5?**


* [ ] tuple1 = (1,2,3,4,5)
* [ ] tuple1 = 1,2,3,4,5
* [ ] tuple1 = [1,2,3,4,5]
* [ ] tuple1 = 1 2 3 4 5

**Correct answer:**
* [x] tuple1 = (1,2,3,4,5)
* [x] tuple1 = 1,2,3,4,5

**6. A ….  is a collection of items that are ordered, unchangeable, and allow duplicate values.**


* [ ] List
* [ ] Set
* [ ] Dictionary
* [ ] Tuple

**Correct answer:**
* [x] Tuple

**7. What is the output of the following command if the tuple1 has values (0,1,2,3,4,5) :**


* [ ] (0, 1, 2, 3)
* [ ] (1,2,3,4)
* [ ] (0,1,2,3,4,5)
* [ ] (0,1,2,3,4)

**Correct answer:**
* [x] (0, 1, 2, 3)

**Code**: 
print(tuple1[0:4])

**8. What is the output of the following snippet:**


* [ ] Error
* [ ] (3)
* [ ] 3
* [ ] tuple(3)

**Correct answer:**
* [x] Error

**Code**: 
x = tuple(3)
print(x)


**9. What is the output of the following snippet:**


* [ ] (50, 40, 30, 20, 10)
* [ ] (40, 20)
* [ ] (10, 30, 50)
* [ ] (10, 20, 30, 40, 50)

**Correct answer:**
* [x] (50, 40, 30, 20, 10)

**Code**: 
a = (10, 20, 30, 40, 50)
a = a[::-1]
print(a)


**10. Access value 30 from the following tuple:**


* [ ] a[30]
* [ ] a[0][0]
* [ ] a[1][1]
* [ ] a[1][0]

**Correct answer:**
* [x] a[1][1]

**Code**: 
a = (10, [20, 30], 40, 50)

**11. Access value 30 from the following tuple:**


* [ ] a[30]
* [ ] a[0]
* [ ] a[1]
* [ ] a[2]

**Correct answer:**
* [x] a[2]

**Code**: 
a = (10, 20, 30, 40, 50)

---


## Python PCEP Dictionaries

**1. A ….  is a collection that is ordered, changeable, and does not allow duplicates.**


* [ ] List
* [ ] Set
* [ ] Dictionary
* [ ] Tuple

**Correct answer:**
* [x] Dictionary

**2. Dictionaries are used to store data values in key-value pairs.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**3. …. is one of the dictionary built-in methods.**


* [ ] dictionary.update()
* [ ] dictionary.values()
* [ ] dictionary.items()
* [ ] dictionary.keys()

**Correct answer:**
* [x] dictionary.update()
* [x] dictionary.values()
* [x] dictionary.items()
* [x] dictionary.keys()

**4. What is the output of the following snippet code:**


* [ ] {'brand': 'apple', 'ram': '3', 'year': 2020}
* [ ] {'brand': 'apple', 'ram': '3', 'year': 2021}
* [ ] {'brand': 'apple', 'ram': '3', 'year': 2020, 'year': 2021}
* [ ] {'brand': 'apple', 'ram': '3'}

**Correct answer:**
* [x] {'brand': 'apple', 'ram': '3', 'year': 2021}

**Code**: 
testdict = {
  "brand": "apple",
  "ram": "3",
  "year": 2020,
  "year": 2021
}

print(testdict)


**5. What is the output of the following snippet code:**


* [ ] {'brand': 'Samsung', 'ram': '3', 'Os': 'Android', 'year': 2020}
* [ ] {'brand': 'Samsung', 'brand': 'oppo','ram': '3', 'Os': 'Android', 'year': 2020}
* [ ] {'brand': 'oppo', 'ram': '3', 'Os': 'Android', 'year': 2020}
* [ ] { 'ram': '3', 'Os': 'Android', 'year': 2020, 'brand': 'oppo' }

**Correct answer:**
* [x] {'brand': 'oppo', 'ram': '3', 'Os': 'Android', 'year': 2020}

**Code**: 
testdict = {
  "brand": "Samsung",
  "ram": "3",
  "Os": "Android",
  "year": 2020
}

testdict.update({'brand':'oppo' })
print(testdict)


**6. Which of the following method is used to delete a <code>brand</code>'s key and its value from the following dictionary:**


* [ ] del.testdict[0]
* [ ] del testdict['brand':'oppo']
* [ ] del.testdict['brand']
* [ ] del testdict['brand']

**Correct answer:**
* [x] del testdict['brand']

**Code**: 
testdict = {'brand': 'oppo', 'ram': '3', 'Os': 'Android', 'year': 2020}

**7. What is the output of the following snippet code:**


* [ ] dict_items([('brand', 'Samsung'), ('ram', '3'), ('Os', 'Android'), ('year', 2020)])
* [ ] dict_keys(['brand', 'ram', 'Os', 'year'])
* [ ] dict_keys(['Samsung', '3', 'Android', 2020])
* [ ] (['brand', 'ram', 'Os', 'year'])

**Correct answer:**
* [x] dict_keys(['brand', 'ram', 'Os', 'year'])

**Code**: 
testdict = {
  "brand": "Samsung",
  "ram": "3",
  "Os": "Android",
  "year": 2020
}

print(testdict.keys())


**8. What is the output of the following snippet code:**


* [ ] dict_items([('brand', 'Samsung'), ('ram', '3'), ('Os', 'Android'), ('year', 2020)])
* [ ] dict_items([('brand': 'Samsung'), ('ram': '3'), ('Os': 'Android'), ('year': 2020)])
* [ ] dict_keys(['brand', 'ram', 'Os', 'year'])
* [ ] dict_values([('brand', 'Samsung'), ('ram', '3'), ('Os', 'Android'), ('year', 2020)])

**Correct answer:**
* [x] dict_items([('brand', 'Samsung'), ('ram', '3'), ('Os', 'Android'), ('year', 2020)])

**Code**: 
testdict = {
  "brand": "Samsung",
  "ram": "3",
  "Os": "Android",
  "year": 2020
}

print(testdict.items())


**9. …. is one of the dictionary built-in methods that used to delete all items from the dictionary.**


* [ ] dictionary.prune()
* [ ] dictionary.clear()
* [ ] dictionary.delete()
* [ ] dictionary.remove()

**Correct answer:**
* [x] dictionary.clear()

**10. …. is one of the dictionary built-in methods that used to delete the last item from the dictionary.**


* [ ] dictionary.delitem()
* [ ] dictionary.clear()
* [ ] dictionary.pop()
* [ ] dictionary.popitem()

**Correct answer:**
* [x] dictionary.popitem()

**11. A dictionary is a collection that is ..., ... and ...**


* [ ] Ordered, changeable, does not allow duplicates
* [ ] Unordered, changeable, does not allow duplicates
* [ ] Ordered, immutable, does allow duplicates
* [ ] Ordered, immutable, does not allow duplicates

**Correct answer:**
* [x] Ordered, changeable, does not allow duplicates

---


## Python PCEP Python Internals

**1. Python is ........**


* [ ] interpreted language
* [ ] compiled language

**Correct answer:**
* [x] interpreted language

**2. Which of the following translates and executes program code line by line rather than the whole program in one step?**


* [ ] Interpreter
* [ ] Translator
* [ ] Assembler
* [ ] Compiler

**Correct answer:**
* [x] Interpreter

**3. In Cython, the Code written in Python is converted to .........**


* [ ] Java language
* [ ] C language
* [ ] Ruby language
* [ ] PHP language

**Correct answer:**
* [x] C language

**4. What do you call a program that can directly execute instructions from a programming language ?**


* [ ] Translator
* [ ] Assembler
* [ ] Compiler
* [ ] Interpreter

**Correct answer:**
* [x] Interpreter

**Explaination**: Hint: Interpreter is a program that can execute high-level language programs “directly,” without first being translated into machine language.

**5. Which of the following isn’t a characteristic of High level languages?**


* [ ] platform independent
* [ ] machine code
* [ ] interactive execution
* [ ] user-friendly

**Correct answer:**
* [x] machine code

**6. Jython is designed to run on which platform?**


* [ ] Java
* [ ] C
* [ ] Ruby
* [ ] PHP

**Correct answer:**
* [x] Java

**7. cython can run on .......**


* [ ] Windows
* [ ] MacOS
* [ ] Linux
* [ ] All the above

**Correct answer:**
* [x] All the above

---


## Python PCEP Mock Exam 1

**1. In Python, a variable must be declared before it is assigned a value:**


* [ ] False
* [ ] True

**Correct answer:**
* [x] False

**Explaination**: Hint: Variables need not be declared or defined in advance in Python. To create a variable, you just assign it a value.

**2. …. is one of the literal types in Python.**


* [ ] None of the above
* [ ] Boolean
* [ ] String
* [ ] Numeric

**Correct answer:**
* [x] Boolean
* [x] String
* [x] Numeric

**3. The …. function prints the specified message to the screen, or another standard output device.**


* [ ] print()
* [ ] return()
* [ ] vars()
* [ ] input()

**Correct answer:**
* [x] print()

**4. …. is a string method used to convert string into a lower case.**


* [ ] casefold()
* [ ] islower()
* [ ] lower()
* [ ] tolower()

**Correct answer:**
* [x] casefold()
* [x] lower()

**5. Which method should you use in order to convert the input into a string correctly:**


* [ ] str
* [ ] int
* [ ] float
* [ ] bin

**Correct answer:**
* [x] str

**Code**: 
year_of_birth = int(input("In what year were you born? "))

print("You were born in " + ...(year_of_birth))


**6. Which of the following statements assigns the value 50 to the variable x in Python:**


* [ ] <code> x == 50 </code>
* [ ] <code> x : 50 </code>
* [ ] <code> x = 50 </code>
* [ ] <code> x >> 50 </code>

**Correct answer:**
* [x] <code> x = 50 </code>

**7. What is the output of the following python code?**


* [ ] Sally#employee name+#123
* [ ] Sally#employee name#123
* [ ] Sally+#123
* [ ] Sally#123

**Correct answer:**
* [x] Sally#123

**Code**: 
name = "Sally"# employee name
 
data = "#123" 
print (name+data)


**8. Fill out the missing operators:**


* [ ] \/ and *
* [ ] \//  and *
* [ ] \% and /
* [ ] \* and /

**Correct answer:**
* [x] \/ and *

**Code**: 
20 ... 5 ... 4 = 16.0

**9. Given the nested if-else below, what will be the value x when the code executed successfully.**


* [ ] 0
* [ ] 2
* [ ] 3
* [ ] 4

**Correct answer:**
* [x] 4

**Code**: 
x = 0
a = 6
b = 6
if a > 0:
    if b < 0: 
        x = x + 6 
    elif a > 6:
        x = x + 5
    else:
        x = x + 4
else:
    x = x + 3

print(x)


**10. What will be the output of the following Python code?**


* [ ] error
* [ ] 5<br>6<br>7<br>8
* [ ] 5<br>6
* [ ] 5<br>6<br>7<br>8<br>9<br>10

**Correct answer:**
* [x] 5<br>6<br>7<br>8

**Code**: 
i = 5
while True:
    if i%0o11 == 0:
        break
    print(i)
    i += 1


**11. What will be the output of below Python code?**


* [ ] 5
* [ ] 1
* [ ] 2
* [ ] 4

**Correct answer:**
* [x] 5

**Code**: 
list1 = [1, 2, 3, 4, 5]
for i in list1:
    if i==5:
        print(i)


**12. What is the output of the following nested loop:**


* [ ] SyntaxError: invalid syntax
* [ ] 5<br>6<br>7<br>8
* [ ] 7<br>8
* [ ] 8<br>7<br>6<br>5

**Correct answer:**
* [x] 5<br>6<br>7<br>8

**Code**: 
for num in range(5, 9):
   for i in range(2, num):
       if num%i == 1:
          print(num)
          break


**13. What does the following code produce as output?**


* [ ] 12
* [ ] 6
* [ ] 3
* [ ] 1

**Correct answer:**
* [x] 6

**Code**: 
i = 1
x = 3
sum = 0
while ( i <= x ):
 sum += i
 i += 1
print(sum)


**14. What would get printed:**


* [ ] False<br>True
* [ ] True<br>True
* [ ] True<br>False
* [ ] False<br>False

**Correct answer:**
* [x] False<br>True

**Code**: 
min_score = 13
score = 13

print(score > min_score)
print(score <= min_score)


**15. What will be the output of the below Python code?**


* [ ] ['UK', 8, 'India', 'Canada']
* [ ] ['UK', 'India', 'Canada', 8]
* [ ] ['UK', 1, 'India', 'Canada']
* [ ] ['UK',1, 8, 'India', 'Canada']

**Correct answer:**
* [x] ['UK', 8, 'India', 'Canada']

**Code**: 
list1=['UK','India','Canada']

list1.insert(1,8)

print(list1)


**16. <code>for i in [9, 1, 5, 6]:</code>, how many times a loop runs ?**


* [ ] 1
* [ ] 2
* [ ] 3
* [ ] 4

**Correct answer:**
* [x] 4

**17. What will be the output of below Python code?**


* [ ] [2, 3, 4, 'python']
* [ ] [2, 3, 4]
* [ ] [1, 2, 3, 4, 'python']
* [ ] [0, 1, 2, 3, 4, 'python']

**Correct answer:**
* [x] [2, 3, 4, 'python']

**Code**: 
my_list = [0, 1, 2, 3, 4]
my_list.append("python")
print(my_list[2:])


**18. What will be the output of below Python code?**


* [ ] 10
* [ ] 11
* [ ] 12
* [ ] 14

**Correct answer:**
* [x] 10

**Code**: 
list1 = [10, 11, 12, 13, 14]
print(list1[0])


**19. What will be the output of below Python code?**


* [ ] ["USA", "India", "Canada"]
* [ ] ["USA", "Canada", "India"]
* [ ] ['Canada', 'USA', 'India']
* [ ] ['Canada', 'India', 'USA']

**Correct answer:**
* [x] ['Canada', 'USA', 'India']

**Code**: 
countries = ["USA", "Canada", "India"]
countries[0], countries[1] = countries[1], countries[0]
print(countries)


**20. What will be the output of the following Python code?**


* [ ] 4
* [ ] 2
* [ ] 0
* [ ] 1

**Correct answer:**
* [x] 2

**Code**: 
matrix = [[[0, 1, 2], [0, 1, 2], [0, 1, 2]], [[0, 1, 2], [0, 1, 2], [0, 1, 2]], [[0, 1, 2], [0, 1, 2], [0, 1, 2]]]

matrix2 = []

for submatrix in matrix:
  for val in submatrix:
    matrix2.append(val)

print(matrix2[2][2])


**21. The function can have only one parameter. If any data (parameters) are passed, they are passed explicitly.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**22. What will be the output of the following Python code?**


* [ ] 4
* [ ] 2
* [ ] 0
* [ ] 1

**Correct answer:**
* [x] 2

**Code**: 
matrix = [[j for j in range(3)] for i in range(3)] 
print(matrix[1][2])


**23. What will be the result of calling the fullname_func function with the arguments 'John' and 'Mark'?**


* [ ] John
* [ ] john Mark
* [ ] John Mark
* [ ] Mark

**Correct answer:**
* [x] John Mark

**Code**: 
def fullname_func(fname, lname):
  print(fname + " " + lname)

fullname_func("John", "Mark")


**24. The output of the following snippet of the code will be:**


* [ ] 9
* [ ] 5
* [ ] 8
* [ ] 6

**Correct answer:**
* [x] 6

**Code**: 
def my_function(x):
  return 10 - x

print(my_function(4))


**25. What is the output of the following snippet:**


* [ ] [2, 4, 6]
* [ ] [1, 2, 3]
* [ ] [1, 3, 5]
* [ ] [4, 5, 6]

**Correct answer:**
* [x] [2, 4, 6]

**Code**: 
def get_even_func(numbers):
    even_numbers = [num for num in numbers if not num % 2]
    return even_numbers

get_even_func([1, 2, 3, 4, 5, 6])


**26. What is the output of the following snippet:**


* [ ] (50, 40, 30, 20, 10)
* [ ] (40, 20)
* [ ] (10, 30, 50)
* [ ] (10, 20, 30, 40, 50)

**Correct answer:**
* [x] (50, 40, 30, 20, 10)

**Code**: 
a = (10, 20, 30, 40, 50)
a = a[::-1]
print(a)


**27. What is the output of the following snippet:**


* [ ] 20  20
* [ ] Error
* [ ] 30   20
* [ ] False

**Correct answer:**
* [x] 30   20

**Code**: 
x = 20
def my_function():
  x = 30
  print(x, end=' ')

my_function()
print(x, end=' ')


**28. Write a Python program to create an empty tuple:**


* [ ] x = tuple(3)
* [ ] x = ()
* [ ] x = (2,3)
* [ ] x = (0)

**Correct answer:**
* [x] x = ()

**29. Which of the following method is used to delete a <code>brand</code> key and its value from the following dictionary:**


* [ ] del.testdict[0]
* [ ] del testdict['brand':'oppo']
* [ ] del.testdict['brand']
* [ ] del testdict['brand']

**Correct answer:**
* [x] del testdict['brand']

**Code**: 
testdict = {'brand': 'oppo', 'ram': '3', 'Os': 'Android', 'year': 2020}

**30. A dictionary is a collection that is ..., ... and ...**


* [ ] Ordered, changeable, does not allow duplicates
* [ ] Unordered, changeable, does not allow duplicates
* [ ] Ordered, immutable, does allow duplicates
* [ ] Ordered, immutable, does not allow duplicates

**Correct answer:**
* [x] Ordered, changeable, does not allow duplicates

---


## Python PCEP Operators 1

**1. What will be the output of the following Python code?**


* [ ] True
* [ ] You are not hungry
* [ ] False
* [ ] You are hungry

**Correct answer:**
* [x] You are hungry

**Code**: 
is_hungry = True
if(not is_hungry):
  print("You are not hungry")
else:
  print("You are hungry")


**2. What will be the output of the following Python code?**


* [ ] True
* [ ] You are not hungry
* [ ] False
* [ ] You are hungry

**Correct answer:**
* [x] You are not hungry

**Code**: 
is_hungry = False
if(not is_hungry):
  print("You are not hungry")
else:
  print("You are hungry")


**3. Which of the following will evaluate to true?**


* [ ] True AND False
* [ ] False or True
* [ ] False AND (True or False)
* [ ] False AND (False or True)

**Correct answer:**
* [x] False or True

**4. What is the output of the following python code?**


* [ ] True
* [ ] False
* [ ] 12
* [ ] 6

**Correct answer:**
* [x] True

**Code**: 
x = 6
print(x > 4 and x < 12)


**5. Which python operator means 'bigger than or equal to'?**


* [ ] \>=
* [ ] \>
* [ ] \<
* [ ] \<=

**Correct answer:**
* [x] \>=

**6. What is the output of the following python code?**


* [ ] True
* [ ] False
* [ ] 7
* [ ] 6

**Correct answer:**
* [x] False

**Code**: 
x = 6
y = 7
print(x == y)


**7. Which python operator means 'less than or equal to'?**


* [ ] \>=
* [ ] \>
* [ ] \<
* [ ] \<=

**Correct answer:**
* [x] \<=

**8. What is the output of the following python code?**


* [ ] True
* [ ] False
* [ ] 12
* [ ] 6

**Correct answer:**
* [x] True

**Code**: 
x = 6
print(x > 7 or x < 12)


---


## Python PCEP Bitwise Operators

**1. Bitwise shift operators (<<, >>) has higher precedence than Bitwise AND(&) operator.**


* [ ] False
* [ ] True

**Correct answer:**
* [x] True

**2. Bitwise  _________  gives 1 if either of the bits is 1 and 0 when both of the bits are 1.**


* [ ] OR
* [ ] AND
* [ ] XOR
* [ ] NOT

**Correct answer:**
* [x] XOR

**3. What will be the output of the following Python code snippet if x=2?**


* [ ] 32
* [ ] 16
* [ ] 8
* [ ] 4

**Correct answer:**
* [x] 8

**Code**: 
x << 2

**4. Bitwise  _________  gives 1 if either of the bits is 1 and 0 when both of the bits are 0.**


* [ ] OR
* [ ] AND
* [ ] XOR
* [ ] NOT

**Correct answer:**
* [x] OR

**5. What will be the output of the following Python expression?**


* [ ] 0 0 0 0 0 0 1 0
* [ ] 1001
* [ ] 0 0 0 0 1 0 0 1
* [ ] 9

**Correct answer:**
* [x] 1001

**Code**: 
int(1001)

**6. Bitwise  _________  gives 1 if both of the bits is 1 and 0 when either of the bits are 0.**


* [ ] OR
* [ ] AND
* [ ] XOR
* [ ] NOT

**Correct answer:**
* [x] AND

**7. What will be the output of the following Python code snippet if x=2 ?**


* [ ] 32
* [ ] 16
* [ ] 8
* [ ] 4

**Correct answer:**
* [x] 32

**Code**: 
x << 4

**8. What will be the output of the following Python expression?**


* [ ] 14
* [ ] 8
* [ ] 12
* [ ] 2

**Correct answer:**
* [x] 14

**Code**: 
5^11

**9. What will be the output of the following Python expression?**


* [ ] 14
* [ ] 33
* [ ] 44
* [ ] 25

**Correct answer:**
* [x] 44

**Code**: 
print(22 << 1)

**10. Which of the following represents the bitwise XOR operator?**


* [ ] &
* [ ] ^
* [ ] |
* [ ] !

**Correct answer:**
* [x] ^

**11. What will be the output of the following Python expression?**


* [ ] 201
* [ ] -201
* [ ] 200
* [ ] -200

**Correct answer:**
* [x] -201

**Code**: 
~200

**12. Which operator is used by the or() function?**


* [ ] |
* [ ] /
* [ ] //
* [ ] ||

**Correct answer:**
* [x] |

**13. What will be the output of the following Python code?**


* [ ] a & b = 21
* [ ] a & b = 4
* [ ] a & b = 6
* [ ] a & b = 12

**Correct answer:**
* [x] a & b = 4

**Code**: 
a = 20
b = 5
print("a & b =", a & b)


**14. What will be the output of the following Python code?**


* [ ] a | b = 21
* [ ] a | b = 8
* [ ] a | b = 6
* [ ] a | b = 12

**Correct answer:**
* [x] a | b = 21

**Code**: 
a = 20
b = 5
print("a | b =", a | b)


---


## Python PCEP Mock Exam 2

**1. What is the numerical value for boolean <code>True</code>?**


* [ ] 0
* [ ] 1

**Correct answer:**
* [x] 1

**2. What is the output of the following python code?**


* [ ] 1.5
* [ ] 1.0
* [ ] -1.5
* [ ] -2.0

**Correct answer:**
* [x] 1.0

**Code**: 
print(6. // 4)

**3. You have to import the <code>print()</code> function in the code so that you can access it.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**4. Which of the following is correct regarding variables in Python?**


* [ ] Variable names in Python cannot start with a number. However, it can contain the number in any other position of the variable name.
* [ ] Variable names can start with an underscore.
* [ ] Data type of variable names should not be declared
* [ ] All of the above

**Correct answer:**
* [x] All of the above

**5. What would get printed:**


* [ ] True<br>True
* [ ] True<br>False
* [ ] False<br>True
* [ ] False<br>False

**Correct answer:**
* [x] False<br>True

**Code**: 
min_score = 13
score = 13

print(score > min_score)
print(score <= min_score)

**6. What will be the output of the following Python code?**


* [ ] error
* [ ] none of the above
* [ ] 1<br>2
* [ ] 1<br>2<br>3

**Correct answer:**
* [x] 1<br>2

**Code**: 
i = 1
while True:
    if i%3 == 0:
        break
    print(i)
    i += 1


**7. In Python3, Whatever you enter as input, the <code>input() </code>function converts it into a string**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**8. To have a multi-line comment in Python, we use triple single quotes at the beginning and at the end of the comment.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**9. What is the <code>len(list1)</code> of the following snippet:**


* [ ] 5
* [ ] 4
* [ ] None
* [ ] Error

**Correct answer:**
* [x] 5

**Code**: 
list1 = ['h', 'e', 'l', 'l', 'o']

**10. What will be the output of below Python code?**


* [ ] Error!
* [ ] ['python', [11, 55, 'cat']]
* [ ] [[11, 55, "cat"], [ ]]
* [ ] [1, 66, 'python', [11, 55, 'cat']]

**Correct answer:**
* [x] Error!

**Code**: 
list1 = [1, 66, "python", [11, 55, "cat"], [ ], 2.22, True]
print(list1.upper())


**11. We want to iterate over the values from 0 to 10, and print their values. However, we want to skip all the values that are even. How can we achieve this?**


* [ ] if num % 2 == 0: break;
* [ ] if num % 2 == 0: continue;
* [ ] if num % 2 != 0:      print(num);
* [ ] if num % 2 == 0:      return

**Correct answer:**
* [x] if num % 2 != 0:      print(num);

**Code**: 
for num in range(0, 11):
	#your answer should be here 
print(num)


**12. What will be the output of below Python code?**


* [ ] [1, 2, 10, 4, 5]
* [ ] [1, 2, 3, 4, 10]
* [ ] [1, 2, 3, 4, 5]
* [ ] [10, 2, 3, 4, 5]

**Correct answer:**
* [x] [10, 2, 3, 4, 5]

**Code**: 
list1 = [1, 2, 3, 4, 5]
list1[0] = 10
print(list1)


**13. What will be the output of below Python code?**


* [ ] [1, 2, 3, 2, 5]
* [ ] [8, 9, 10]
* [ ] [4, 5, 6, 7]
* [ ] [10, 11, 12, 13]

**Correct answer:**
* [x] [8, 9, 10]

**Code**: 
list1 = [[1,2,3,2,5],[4,5,6,7],[8,9,10]]
for i in list1:
      if len(i)==3:
        print(i)


**14. What will be the output of the following Python code?**


* [ ] 2
* [ ] [0, 1, 2]
* [ ] 0
* [ ] 1

**Correct answer:**
* [x] [0, 1, 2]

**Code**: 
matrix = [[[0, 1, 2], [0, 1, 2], [0, 1, 2]], [[0, 1, 2], [0, 1, 2], [0, 1, 2]], [[0, 1, 2], [0, 1, 2], [0, 1, 2]]]

matrix2 = []

for submatrix in matrix:
  for val in submatrix:
    matrix2.append(val)

print(matrix2[2])


**15. What will be the output of the following Python code?**


* [ ] 4
* [ ] 2
* [ ] 0
* [ ] 1

**Correct answer:**
* [x] 1

**Code**: 
matrix = [[j for j in range(4)] for i in range(4)] 
print(matrix[3][1])


**16. What is the output of the following code:**


* [ ] False
* [ ] True

**Correct answer:**
* [x] True

**Code**: 
(4, 6) not in [(4, 7), (5, 6), "hello"]

**17. Choose the correct answer which defines a function to get numeric input from the user.**


* [ ] <code>def input_num():  return input () </code>
* [ ] <code>def input_num():  return int(input () )</code>
* [ ] <code>def input_num():  int(input () )</code>
* [ ] <code>def input_num():  return input (num)</code>

**Correct answer:**
* [x] <code>def input_num():  return int(input () )</code>

**18. What is the output of the following snippet:**


* [ ] 0
* [ ] Error
* [ ] 20
* [ ] False

**Correct answer:**
* [x] Error

**Code**: 
def my_function():
  def my_inner_function():
    x = 20
  print(x)
  my_inner_function()

my_function()


**19. What is the output of the following snippet:**


* [ ] [7, 5, 9]
* [ ] [7, 4, 5]
* [ ] [4, 6, 8, 12]
* [ ] [5, 6, 8, 12]

**Correct answer:**
* [x] [7, 5, 9]

**Code**: 
def get_odd_func(numbers):
    odd_numbers = [num for num in numbers if num % 2]
    return odd_numbers

print(get_odd_func([7, 4, 5, 6, 9, 8, 12]))


**20. What is the output of the following command if the <code>tuple1</code> has values (1,2,3,4,5) :**


* [ ] (1,2,3,4,6)
* [ ] (6,)
* [ ] (1,2,3,4,5,6)
* [ ] AttributeError

**Correct answer:**
* [x] AttributeError

**Code**: 
print(tuple1.append(6))

**21. The output of the following snippet of the code will be:**


* [ ] 0
* [ ] 1
* [ ] 2
* [ ] 3

**Correct answer:**
* [x] 3

**Code**: 
a = 0
def add_three(a):
	return a+3

result = add_three(a)
print(result)


**22. The output of the following snippet of the code will be:**


* [ ] 9
* [ ] 4
* [ ] 12
* [ ] 8

**Correct answer:**
* [x] 9

**Code**: 
def square(i):
    j = i * i
    return j

print(square(3))


**23. Access value 30 from the following tuple:**


* [ ] a[30]
* [ ] a[0][0]
* [ ] a[1][1]
* [ ] a[1][0]

**Correct answer:**
* [x] a[1][1]

**Code**: 
a = (10, [20, 30], 40, 50)

**24. What is the output of the following snippet code:**


* [ ] dict_items([('brand', 'Samsung'), ('ram', '3'), ('Os', 'Android'), ('year', 2020)])
* [ ] dict_keys(['brand', 'ram', 'Os', 'year'])
* [ ] dict_keys(['Samsung', '3', 'Android', 2020])
* [ ] (['brand', 'ram', 'Os', 'year'])

**Correct answer:**
* [x] dict_keys(['brand', 'ram', 'Os', 'year'])

**Code**: 
testdict = {
  "brand": "Samsung",
  "ram": "3",
  "Os": "Android",
  "year": 2020
}

print(testdict.keys())


**25. What is the output of the following snippet:**


* [ ] The older friend is 13 years
* [ ] The older friend is 11 years
* [ ] Error
* [ ] The older friend is 12 years

**Correct answer:**
* [x] The older friend is 12 years

**Code**: 
def my_function(*ages):
  print("The older friend is " + ages[1] + " years")

my_function("13", "12", "11")


**26. Which of the following isn’t a characteristic of High level languages?**


* [ ] platform independent
* [ ] machine code
* [ ] interactive execution
* [ ] user-friendly

**Correct answer:**
* [x] machine code

**27. What is the output of the following snippet:**


* [ ] Error
* [ ] (3)
* [ ] 3
* [ ] tuple(3)

**Correct answer:**
* [x] Error

**Code**: 
x = tuple(3)
print(x)


---


## Python PCEP Mock Exam 3

**1. You have to import the <code>print()</code> function in the code so that you can access it.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**2. What is the data type of <code>print(type(100_25))</code>?**


* [ ] integer
* [ ] float
* [ ] string
* [ ] boolean
* [ ] none of the above

**Correct answer:**
* [x] integer

**3. Which of the following variable names are valid?**


* [ ] <code> not = "Don't do that!" </code>
* [ ] <code>name = "Kodekloud"</code>
* [ ] <code>2timesage = 44</code>
* [ ] <code>Function = "function"</code>

**Correct answer:**
* [x] <code>name = "Kodekloud"</code>
* [x] <code>Function = "function"</code>

**4. What is the output of the following python code?**


* [ ] TRUE
* [ ] FALSE<br>TRUE
* [ ] TRUE<br>FALSE
* [ ] TRUE<br>FALSE<br>TRUE

**Correct answer:**
* [x] FALSE<br>TRUE

**Code**: 
if 4 + 5 == 10:
    print("TRUE")
else:
    print("FALSE")
print("TRUE")


**5. Comments can be used to ….**


* [ ] explain Python code
* [ ] make the code more readable
* [ ] comment out code that you don’t want to execute
* [ ] None of the Above

**Correct answer:**
* [x] explain Python code
* [x] make the code more readable
* [x] comment out code that you don’t want to execute

**6. What is the output of the following python code?**


* [ ] 5.0
* [ ] 0.5
* [ ] 5
* [ ] 5.5

**Correct answer:**
* [x] 5.0

**Code**: 
print(10 / 2)

**7. What is the output of <code> print(2 * 3 ** 3 * 4)</code>?**


* [ ] 216
* [ ] 864
* [ ] “2 * 3 ** 3 * 4”
* [ ] 144

**Correct answer:**
* [x] 216

**8. The input() method returns string value. So, if we want to perform arithmetic operations, we need to cast the value first.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**9. What will be the output of the following Python code?**


* [ ] error
* [ ] A<br>B<br>C<br>D
* [ ] a<br>B<br>C<br>D
* [ ] a<br>b<br>c<br>d

**Correct answer:**
* [x] A<br>B<br>C<br>D

**Code**: 
x = 'abcd'
for i in x:
    print(i.upper())


**10. What will be the output of the following Python code?**


* [ ] error
* [ ] none of the above
* [ ] 1<br>2
* [ ] 1<br>2<br>3

**Correct answer:**
* [x] 1<br>2

**Code**: 
i = 1
while True:
    if i%3 == 0:
        break
    print(i)
    i += 1


**11. What will be the output of below Python code?**


* [ ] [1, 2, 3, 4, 11, 10]
* [ ] [10, 2, 3, 4, 5]
* [ ] [1, 2, 3, 4, 10, 11]
* [ ] [1, 2, 3, 4, 10]

**Correct answer:**
* [x] [1, 2, 3, 4, 10, 11]

**Code**: 
list1 = [1, 2, 3, 4, 5]
list1[4] = 10
list1.append(11)
print(list1)


**12. What will be the output of below Python code?**


* [ ] [24, 46, 56, 72]
* [ ] [24, 46, 72, 56]
* [ ] [56, 72, 24, 46]
* [ ] [24, 46, 56]

**Correct answer:**
* [x] [24, 46, 56, 72]

**Code**: 
ages = [56, 72, 24, 46]
ages.sort()
print(ages)


**13. What will be the output of below Python code?**


* [ ] [1, 2, 3, 4]
* [ ] [0, 1, 2, 3]
* [ ] 0 1<br>1 2<br>2 3<br>3 4
* [ ] 1 0<br>2 1<br>3 2<br>4 3

**Correct answer:**
* [x] 0 1<br>1 2<br>2 3<br>3 4

**Code**: 
list1 = [1, 2, 3, 4]
for i, j in enumerate(list1):
     print(i, j)


**14. What will be the output of below Python code?**


* [ ] [5]
* [ ] [4, 5]
* [ ] [3, 4, 5]
* [ ] [2, 3, 4, 5]

**Correct answer:**
* [x] [5]

**Code**: 
numbers = [1, 2, 3, 4, 5]
print(numbers[4:])


**15. Which of the following statements won’t be printed when this Python code is run?**


* [ ] Letter : K
* [ ] Letter : o
* [ ] Letter : e
* [ ] Letter : d

**Correct answer:**
* [x] Letter : e

**Code**: 
for letter in 'KodeKloud':
    if letter == 'e':
        continue
    print('Letter : ' + letter)


**16. What will be the output of below Python code?**


* [ ] [1, 66, "python"]
* [ ] ['python', [11, 55, 'cat']]
* [ ] [[11, 55, "cat"], [ ]]
* [ ] [1, 66, 'python', [11, 55, 'cat']]

**Correct answer:**
* [x] [1, 66, 'python', [11, 55, 'cat']]

**Code**: 
list1 = [1, 66, "python", [11, 55, "cat"], [ ], 2.22, True]
print(list1[0:4])


**17. What will be the output of below Python code?**


* [ ] 4
* [ ] 3
* [ ] 2
* [ ] 1

**Correct answer:**
* [x] 2

**Code**: 
my_list = [0, 1, 2, 3, 4]
print(my_list.index(2))


**18. What will be the output of the following Python code?**


* [ ] 2
* [ ] 0
* [ ] 1
* [ ] Error

**Correct answer:**
* [x] 2

**Code**: 
matrix = [[0, 1, 2], [0, 1, 2], [0, 1, 2]]

matrix2 = []

for submatrix in matrix:
  for val in submatrix:
    matrix2.append(val)

print(matrix2[2])

**19. What will be the output of the following Python code?**


* [ ] [0, 1, 2]
* [ ] 2
* [ ] 0
* [ ] 1

**Correct answer:**
* [x] 1

**Code**: 
matrix = [[[k for k in range(3)] for j in range(3)] for i in range(3)]
print(matrix[1][1][1])


**20. Define a function that gets the user input and multiply it by a number we passed to the function.**


* [ ] <code>def multi_num(num):  return int(input () ) * num</code>
* [ ] <code>def multi_num():  return int(input (num) ) * 6</code>
* [ ] <code>def multi_num(6):  return int(input () ) * num</code>
* [ ] <code>def multi_num():  return int(input () ) * num</code>

**Correct answer:**
* [x] <code>def multi_num(num):  return int(input () ) * num</code>

**Code**: 
Sample input:  6
Sample output: 
print( multi_num(5) )
30


**21. What will be the output of the following Python code?**


* [ ] 4
* [ ] 2
* [ ] 0
* [ ] 1- [ ] 4

**Correct answer:**
* [x] 0

**Code**: 
matrix = [[[0, 1, 2], [0, 1, 2], [0, 1, 2]], [[0, 1, 2], [0, 1, 2], [0, 1, 2]], [[0, 1, 2], [0, 1, 2], [0, 1, 2]]]
print(matrix[0][0][0])


**22. What is the output of the following snippet:**


* [ ] The tallest student is john
* [ ] The tallest student is Ella
* [ ] Error
* [ ] mark

**Correct answer:**
* [x] The tallest student is Ella

**Code**: 
def my_function(*friends):
  print("The tallest student is " + friends[1])

my_function("john", "Ella", "mark")


**23. What will the following function return?**


* [ ] None
* [ ] The value of x + y
* [ ] The value of x
* [ ] The value of y

**Correct answer:**
* [x] The value of x + y

**Code**: 
def add(x, y):
  return x+y


**24. What is the output of the following snippet:**


* [ ] 3.5
* [ ] 3.25
* [ ] 2.5
* [ ] 3

**Correct answer:**
* [x] 3.25

**Code**: 
def mean_func(list1):
    return sum(list1) / len(list1)

print(mean_func([5, 2, 2, 4]))


**25. What is the output of the following snippet:**


* [ ] 20
* [ ] Error
* [ ] 0
* [ ] False

**Correct answer:**
* [x] 20

**Code**: 
def my_function():
  x = 20
  def my_inner_function():
    print(x)
  my_inner_function()
my_function()


**26. What is the output of the following command if the tuple1 has values (0,1,2,3,4,5) :**


* [ ] (0, 1, 2, 3)
* [ ] (1,2,3,4)
* [ ] (0,1,2,3,4,5)
* [ ] (0,1,2,3,4)

**Correct answer:**
* [x] (0, 1, 2, 3)

**Code**: 
print(tuple1[0:4])

**27. cython can run on .......**


* [ ] Windows
* [ ] MacOS
* [ ] Linux
* [ ] All the above

**Correct answer:**
* [x] All the above

**28. Write a Python program to create a tuple with different data types:**


* [ ] x = ("john", True, 2.2, 2)
* [ ] x = ("tuple", "john", "list")
* [ ] x = (2.1, 3.2, 1.3)
* [ ] x = (1, 2, 3)

**Correct answer:**
* [x] x = ("john", True, 2.2, 2)

**Code**: 
print(x)

**29. …. is one of the dictionary built-in methods.**


* [ ] dictionary.update()
* [ ] dictionary.values()
* [ ] dictionary.items()
* [ ] dictionary.keys()

**Correct answer:**
* [x] dictionary.update()
* [x] dictionary.values()
* [x] dictionary.items()
* [x] dictionary.keys()

---


## Python PCEP Mock Exam 4

**1. What is the output of the following python code if we enter 5 as input?**


* [ ] 15
* [ ] NumNumNum
* [ ] 555
* [ ] Error

**Correct answer:**
* [x] 555

**Code**: 
Num = input("Enter a Number: ") 
print (Num * 3 )


**2. What is the output of the following python code?**


* [ ] Hello, jack!
* [ ] Hello, jack,Sally!
* [ ] Hello, Sally!
* [ ] Hello, jack!,Sally!

**Correct answer:**
* [x] Hello, Sally!

**Code**: 
#print("Hello, jack!")
print("Hello, Sally!")


**3. A ….  is a collection of items that is both unordered, unindexed and used to store multiple items in a single variable.**


* [ ] List
* [ ] Set
* [ ] Dictionary
* [ ] Tuple

**Correct answer:**
* [x] Set

**4. Is it possible to pass multiple arguments to a function?**


* [ ] Yes
* [ ] No

**Correct answer:**
* [x] Yes

**5. In Python, a string is:**


* [ ] An immutable sequence of characters, delimited by quotes
* [ ] A mutable sequence of characters, delimited by quotes
* [ ] An immutable sequence of characters, optionally delimited by quotes
* [ ] A mutable sequence of characters, optionally delimited by quotes

**Correct answer:**
* [x] An immutable sequence of characters, delimited by quotes

**6. Which of the following is a valid variable name in Python?**


* [ ] do it
* [ ] do+1
* [ ] 1do
* [ ] All of the above
* [ ] None of the above

**Correct answer:**
* [x] None of the above

**7. What is the output when this code executes?**


* [ ] 6
* [ ] 1
* [ ] 4
* [ ] 5

**Correct answer:**
* [x] 6

**Code**: 
x = 1
while ( x <= 5 ):
  x += 1
print(x)


**8. What does the following Python program display?**


* [ ] Am I here?
* [ ] Or here?
* [ ] Am I here?<br>Or here?
* [ ] Or here?<br>Or over here?

**Correct answer:**
* [x] Or here?<br>Or over here?

**Code**: 
x = 3
if ( x == 0 ):
  print("Am I here?")
elif ( x == 3 ):
  print("Or here?")
print("Or over here?")


**9. What will be the output of the following Python code?**


* [ ] error
* [ ] 0<br>1<br>2<br>3
* [ ] 1<br>2<br>3<br>4
* [ ] a<br>b<br>c<br>d

**Correct answer:**
* [x] 0<br>1<br>2<br>3

**Code**: 
x = 'abcd'
for i in range(len(x)):
    print(i)


**10. What will be the output of below Python code?**


* [ ] [4, 4, 3]
* [ ] [4, 4, 1]
* [ ] [4, 3, 1]
* [ ] [4, 4, 3, 1]

**Correct answer:**
* [x] [4, 4, 1]

**Code**: 
list1 = [4, 4, 3, 1]
list1.pop(2)
print(list1)


**11. What will be the output of following Python code?**


* [ ] Go
* [ ] Java
* [ ] C
* [ ] Python

**Correct answer:**
* [x] Python

**Code**: 
list1=["Go","Java","C","Python"]
print(max(list1))


**12. Which of the following will reverse <code>list1=[2,5,3,1]</code>?**


* [ ] <code>list1[::-1]</code>
* [ ] <code>list1[::2]</code>
* [ ] <code>list1[::1]</code>
* [ ] <code>list1[2:4]</code>

**Correct answer:**
* [x] <code>list1[::-1]</code>

**13. What will be the output of below Python code?**


* [ ] [10, 11, 12, 13, 14]
* [ ] [10, 12, 14]
* [ ] [11, 12, 13, 14]
* [ ] [10, 11, 12, 13]

**Correct answer:**
* [x] [10, 11, 12, 13, 14]

**Code**: 
list1 = [10, 11, 12, 13, 14]
print(list1[::1])


**14. How many asterisks will be printed when the following code executes?**


* [ ] 20
* [ ] 16
* [ ] 5
* [ ] 4

**Correct answer:**
* [x] 16

**Code**: 
for x in [0, 2, 1, 3]:
    for y in [0, 4, 1, 2]:
            print('*')


**15. What will be the output of below Python code?**


* [ ] [1, 66, "python"]
* [ ] ['python', [11, 55, 'cat']]
* [ ] [[11, 55, "cat"], [ ]]
* [ ] [2.22, True]

**Correct answer:**
* [x] ['python', [11, 55, 'cat']]

**Code**: 
list1 = [1, 66, "python", [11, 55, "cat"], [ ], 2.22, True]
print(list1[2:4])


**16. What will be the output of the following Python code?**


* [ ] [0, [8, 9], 4, 1, 2]
* [ ] [0, 4, 1, 2]
* [ ] [4, 1, 2]
* [ ] [[8, 9], 3, 4, 1, 2]

**Correct answer:**
* [x] [0, [8, 9], 4, 1, 2]

**Code**: 
list1 = [0, 3, 4, 1, 2]
list1[1]=[8,9]
print(list1)


**17. What will be the output of the following Python code?**


* [ ] [0, [8, 9], 4, 1, 2]
* [ ] [0, 4, 1, 2]
* [ ] [0, 3, 8, 9]
* [ ] [[8, 9], 3, 4, 1, 2]

**Correct answer:**
* [x] [0, 3, 8, 9]

**Code**: 
list1 = [0, 3, 4, 1, 2]
list1[2:5]=[8,9]
print(list1)


**18. What will be the output of the following Python code?**


* [ ] ['Egypt', 'USA', 'India', 'Dubai', 'Spain']
* [ ]  ['Egypt', 'America', 'India', 'Dubai', 'London']
* [ ] ['Egypt', 'USA', 'France', 'Dubai', 'Spain']
* [ ] ['France', 'USA', 'India', 'England', 'Spain']

**Correct answer:**
* [x] ['Egypt', 'USA', 'India', 'Dubai', 'Spain']

**Code**: 
countries = [['Egypt', 'USA', 'India'], ['Dubai', 'America', 'Spain'], ['London', 'England', 'France']]
countries2  = [country for sublist in countries for country in sublist if len(country) < 6]
print(countries2)


**19. What will be the output of the following Python code?**


* [ ] 4
* [ ] 2
* [ ] 0
* [ ] 1

**Correct answer:**
* [x] 0

**Code**: 
matrix = [[0, 1, 2], [0, 1, 2], [0, 1, 2]]

matrix2 = []

for submatrix in matrix:
  for val in submatrix:
    matrix2.append(val)

print(matrix2[0])

**20. What is the error in the following snippet code:**


* [ ] The multi_func() function doesn’t have any parameters
* [ ] The result is not defined as a global variable
* [ ] The function return None
* [ ] print() is not a built-in function

**Correct answer:**
* [x] The result is not defined as a global variable

**Code**: 
def multi_func():
  result = int(input()) * 5
  return result     

print(result)

**21. What will be the output of the following Python code?**


* [ ] [0, 1, 2]
* [ ] 2
* [ ] 0
* [ ] 1

**Correct answer:**
* [x] [0, 1, 2]

**Code**: 
matrix = [[[k for k in range(3)] for j in range(3)] for i in range(3)]
print(matrix[1][2])


**22. Regarding the definition of the function and the sample input, Choose the correct value of the output.**


* [ ] 15
* [ ] None
* [ ] Error
* [ ] 50

**Correct answer:**
* [x] Error

**Code**: 
Function :  def add_func(num1,num2):
                   	return num1 + num2

Sample input: print ( add_func(5 , num1= 10) )


**23. The output of the following snippet of the code will be:**


* [ ] The result is False
* [ ] The result is True
* [ ] The result is bool
* [ ] The result is 0

**Correct answer:**
* [x] The result is True

**Code**: 
def is_true(a): 
  return bool(a) 

result = is_true(3<6) 
print("The result is", result)


**24. What is the output of the following snippet:**


* [ ] john emmy mark
* [ ] john mark emmy
* [ ] john mark
* [ ] emmy mark

**Correct answer:**
* [x] john mark emmy

**Code**: 
def my_function(names):
  for i in names:
    print(i, end=' ')

names = ["john", "mark", "emmy"]
my_function(names)


**25. What is the output of the following snippet:**


* [ ] 20
* [ ] Error
* [ ] 0
* [ ] False

**Correct answer:**
* [x] 20

**Code**: 
def myfunc():
  a = 20
  print(a)

myfunc()


**26. Which of the following is used to create a tuple called <code>tuple1</code> which contains numbers from 1 to 5:**


* [ ] tuple1 = (1,2,3,4,5)
* [ ] tuple1 = 1,2,3,4,5
* [ ] tuple1 = [1,2,3,4,5]
* [ ] tuple1 = 1 2 3 4 5

**Correct answer:**
* [x] tuple1 = (1,2,3,4,5)
* [x] tuple1 = 1,2,3,4,5

**27. What is the output of the following snippet code:**


* [ ] {'brand': 'apple', 'ram': '3', 'year': 2020}
* [ ] {'brand': 'apple', 'ram': '3', 'year': 2021}
* [ ] {'brand': 'apple', 'ram': '3', 'year': 2020, 'year': 2021}
* [ ] {'brand': 'apple', 'ram': '3'}

**Correct answer:**
* [x] {'brand': 'apple', 'ram': '3', 'year': 2021}

**Code**: 
testdict = {
  "brand": "apple",
  "ram": "3",
  "year": 2020,
  "year": 2021
}

print(testdict)


**28. What is the output of the following snippet:**


* [ ] Error
* [ ] Welcome to Python!
* [ ] First argument: Welcome to Python!
* [ ] First argument: Welcome<br>Next argument: to<br>Next argument: Python!

**Correct answer:**
* [x] First argument: Welcome<br>Next argument: to<br>Next argument: Python!

**Code**: 
def my_function(arg1, *argv): 
    print ("First argument:", arg1) 
    for arg in argv: 
        print("Next argument:", arg) 

my_function('Welcome', 'to', 'Python!')


**29. …. is one of the dictionary built-in methods that used to delete all items from the dictionary.**


* [ ] dictionary.prune()
* [ ] dictionary.clear()
* [ ] dictionary.delete()
* [ ] dictionary.remove()

**Correct answer:**
* [x] dictionary.clear()

---


## Python PCEP Mock Exam 5

**1. What is the output of the following python code if we enter “HelloPython” as input?**


* [ ] HelloPython
* [ ] Enter a string: HelloPython*2
* [ ] HelloPythonHelloPython
* [ ] HelloPython*2

**Correct answer:**
* [x] HelloPythonHelloPython

**Code**: 
inputString = input('Enter a string: ')
print(inputString*2)


**2. What is the output of the following python code?**


* [ ] 8.0
* [ ] 0.8
* [ ] 8
* [ ] 8.5

**Correct answer:**
* [x] 8

**Code**: 
print(2 ** 3)

**3. What is the output of the following python code?**


* [ ] line1       line2       line3
* [ ] line1
* [ ] line1<br>#line3
* [ ] line1<br>line2<br>#line3

**Correct answer:**
* [x] line1<br>#line3

**Code**: 
print("line1")
#print("line2")
print("#line3")


**4. The output of the following code will be:**


* [ ] My age is 25
* [ ] 25
* [ ] TypeError
* [ ] My age is + 25

**Correct answer:**
* [x] TypeError

**Code**: 
print('My age is ' + 25)

**5. Which operator you can use to perform string concatenation?**


* [ ] \/
* [ ] \*
* [ ] \+
* [ ] \-

**Correct answer:**
* [x] \+

**6. What is the output of the following python code?**


* [ ] 8
* [ ] 12
* [ ] 10
* [ ] SyntaxError

**Correct answer:**
* [x] SyntaxError

**Code**: 
y = 20
x = y += 3
print(x)


**7. <code>True</code> is what type of variable?**


* [ ] float
* [ ] string
* [ ] boolean
* [ ] integer

**Correct answer:**
* [x] boolean

**8. What is the output of the following snippet code:**


* [ ] 0o11 0x12b 6
* [ ] 9 299 6
* [ ] 11 12 6
* [ ] Error

**Correct answer:**
* [x] 9 299 6

**Code**: 
x = 0o11
y = 0x12b
z = 6
print(x, y, z)


**9. Which one of the following if statements will not execute successfully?**


* [ ] <code>if (5,10):<br>&nbsp;&nbsp;print('hello')</code>
* [ ] <code>if (yes):<br>&nbsp;&nbsp;print('hello')</code>
* [ ] <code> if True:<br>&nbsp;&nbsp;print('hello')</code>
* [ ] <code>if (5,10):<br>print('hello')</code>

**Correct answer:**
* [x] <code>if (yes):<br>&nbsp;&nbsp;print('hello')</code>
* [x] <code>if (5,10):<br>print('hello')</code>

**Explaination**: <code>if(yes): <br>&nbsp;&nbsp;print('hello')</code>

The above code will give IndentationError: expected an indented block,

<code>if(5,10):<be>print('hello') </code>
The above code will give NameError: name 'yes' is not defined

**10. What is the value of x?**


* [ ] 49
* [ ] 50
* [ ] None of the above, this is an infinite loop
* [ ] 51

**Correct answer:**
* [x] 50

**Code**: 
x = 0
while (x < 50):
  x+=2

print(x)


**Explaination**: In the "while" loop as per statement "x" is increased by 2. Like 2, 4, 6 ... 48 50. 
Value 50 is not satisfying the given condition so it's exited from the loop and the final value we got for "x" is 50.

**11. What will be the output of the following python code?**


* [ ] abcd
* [ ] ABCD
* [ ] error
* [ ] none of the mentioned

**Correct answer:**
* [x] abcd

**Code**: 
x = 'abcd'
for i in range(len(x)):
    x[i].upper()
print(x)


**12. Which of the following would give an error?**


* [ ] <code>list1=[] ++</code>
* [ ] <code>list1=[]</code>
* [ ] <code>list1=[] + 2</code>
* [ ] <code>list1=["USA","Canada","India"]</code>

**Correct answer:**
* [x] <code>list1=[] + 2</code>
* [x] <code>list1=[] ++</code>

**13. What will be the output of below Python code?**


* [ ] 0
* [ ] 1
* [ ] 9
* [ ] 4

**Correct answer:**
* [x] 4

**Code**: 
my_list = [0, 1, 2, 3, 4]
print(my_list[-1])


**14. What will be printed by the following code when it executes?**


* [ ] 2
* [ ] 11
* [ ] 12
* [ ] 19

**Correct answer:**
* [x] 19

**Code**: 
sum = 0
values = [2,9,1,7]
for number in values:
    sum = sum + number

print(sum)


**15. What will be the output of below Python code?**


* [ ] 25
* [ ] Error!
* [ ] 20
* [ ] 21

**Correct answer:**
* [x] 25

**Code**: 
list1=[7,8,1,3,9]
list1.remove(3)
print(sum(list1))


**16. What will be the output of below python code?**


* [ ] [0, 1, 2, 3, 4]
* [ ] [0, 1, 2]
* [ ] [2, 3]
* [ ] [0, 1, 2, 3]

**Correct answer:**
* [x] [2, 3]

**Code**: 
my_list = [0, 1, 2, 3, 4]
print(my_list[2:4])


**17. What will be the output of the following Python code?**


* [ ] [0, 3, [1, 2], 1, 2]
* [ ] [0, 4, 1, 2]
* [ ] [3, 4, 1, 2]
* [ ] [0, 3, 1, 2, 2]

**Correct answer:**
* [x] [0, 3, 1, 2, 2]

**Code**: 
list1 = [0, 3, 4, 1, 2]
list1[2:4]=[1,2]
print(list1)


**18. What will be the output of the following Python code?**


* [ ] [9, 4, 6, 1, 2]
* [ ] [3, 4, 6, 1, 2]
* [ ] [3, 4, 1, 2]
* [ ] [3, 9, 6, 1, 2]

**Correct answer:**
* [x] [9, 4, 6, 1, 2]

**Code**: 
list1=[3,4,6,1,2]
list2=list1
list1[0]=9
print(list2)


**19. What will be the output of the following Python code?**


* [ ] 1
* [ ] 0
* [ ] 2
* [ ] 3

**Correct answer:**
* [x] 3

**Code**: 
a = []
for i in range(5):
    a.append([])
    for j in range(5):
        a[i].append(j)

print(a[2][3])


**20. Choose the correct answer to define a list "Num", which contains numbers from 1-9 with 3 elements only in the row.**


* [ ] <code> Num =[ [1,2,3] , [4,5,6,7] , [8,9] ]</code>
* [ ] <code> Num =[ [1,2,3] , [4,5,6,] , [7,8,9] ]</code>
* [ ] <code> Num =[ [1,2,3,4] , [4,5,6,7] , [8,9,6] ]</code>
* [ ] <code> Num =[ [1,2,3,4] , [4,5,6] , [7,8,9] ]</code>

**Correct answer:**
* [x] <code> Num =[ [1,2,3] , [4,5,6,] , [7,8,9] ]</code>

**21. The output of the following snippet code will be:**


* [ ] Hello, World
* [ ] None
* [ ] Error
* [ ] return_greeting

**Correct answer:**
* [x] Hello, World

**Code**: 
def return_greeting():
  return "Hello, World"

print(return_greeting())


**22. The output of the following snippet of the code will be:**


* [ ] 0
* [ ] 1
* [ ] 2
* [ ] 3

**Correct answer:**
* [x] 1

**Code**: 
a = 0
def add_one(a):
	return a+1

result = add_one(a)
print(result)


**23. Choose the correct code to get the third element in the second row,   Regarding the following list :**


* [ ] <code>print(Colors[2][3])</code>
* [ ] <code>print(Colors[1][3])</code>
* [ ] <code>print(Colors[1][2])</code>
* [ ] <code>print(Colors[2][2])</code>

**Correct answer:**
* [x] <code>print(Colors[1][2])</code>

**Code**: 
Colors = [ ['Red', 'Green', 'White', 'Black'], ['Green', 'Blue', 'White', 'Yellow'] ,['White', 'Blue', 'Green', 'Red'] ]

**24. What is the output of the following snippet:**


* [ ] [1, 2,3]
* [ ] [1,2,3[1,2,3]]
* [ ] [1, 2, 3][1,2,3]
* [ ] [1, 2, 3, 1, 2, 3]

**Correct answer:**
* [x] [1, 2, 3, 1, 2, 3]

**Code**: 
def double_list(numbers):
  return 2 * numbers

numbers = [1, 2, 3]
print(double_list(numbers))


**25. What is the output of the following snippet:**


* [ ] 20
* [ ] Error
* [ ] 0
* [ ] False

**Correct answer:**
* [x] Error

**Code**: 
def myfunc():
  a = 20

myfunc()
print(a)


**26. …. is one of the data types in Python that used to store collections of data.**


* [ ] List
* [ ] Set
* [ ] Dictionary
* [ ] Tuple

**Correct answer:**
* [x] List
* [x] Set
* [x] Dictionary
* [x] Tuple

**27. Dictionaries are used to store data values in key-value pairs.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**28. What is the output of the following snippet:**


* [ ] 6
* [ ] 2
* [ ] Error
* [ ] 3

**Correct answer:**
* [x] 6

**Code**: 
def sum(*args):
    result = 0
    for arg in args:
        result += arg
    return result 

print(sum(2,3,1))


---


## tej-singh-test

**1. Which of the following statements are correct? Select all that apply.**


* [ ] statement1
* [ ] statement2
* [ ] statement3
* [ ] statement4

**Correct answer:**
* [x] statement1
* [x] statement2

**Explaination**: Good Testing <br> Good Testing

**2. We have the following VNets in Azure. For which scenarios can we establish peering? ![question-image](https://res.cloudinary.com/dezmljkdo/image/upload/v1669304447/az104/az104-vnet_smp2ym.png)**


* [ ] A, B, C and D
* [ ] A and B
* [ ] A and C
* [ ] A, C, and D

**Correct answer:**
* [x] A and C

**Explaination**: In scenario B, the address spaces are overlapping so we cannot establish peering. In scenario-D, the CIDR for VNet-a is /30; in Azure we can only create till /29. Since the network itself cannot be created, peering cannot be established.

**3. One of your Linux virtual machines has the following NSGs attached to it. ![question-image](https://res.cloudinary.com/dezmljkdo/image/upload/v1658341172/az104/az1042_xvcrzk.png) <br> You are not able to connect to the VM over SSH. How can you fix this?**


* [ ] Change the priority of rule 1 of subnet level NSG to 100
* [ ] Delete rule 1 from NIC level NSG
* [ ] Change the source of rule 1 of subnet level NSG
* [ ] Delete the NIC level NSG

**Correct answer:**
* [x] Delete rule 1 from NIC level NSG

**Explaination**: Delete rule 1 from the NIC level so NSG will allow SSH traffic.

**4. You have the following resources in Azure:<br> <li>Blob container – imgfiles </li> <li>File share – executables</li> <li>VM – VM-01</li> <li>Azure Database for MySQL – wordpress</li> <br> Which of the following can be backed up to a recovery services vault?**


* [ ] File share and VM
* [ ] Blob container, file share, VM, and Azure Database for MySQL
* [ ] File share, VM, and Azure Database for MySQL
* [ ] VM and Azure Database for MySQL

**Correct answer:**
* [x] File share and VM

**Explaination**: In the Recovery Service vault, we can only backup VMs and file shares. For the Blob container and Azure Database for MySQL, you need to use the Backup vault.

---


## Lab MCQ IAC and Terraform Basics

**1. Which of the following statements is true?**


* [ ] Ansible cannnot be used to provision resources on the cloud
* [ ] Ansible uses declarative approach
* [ ] Ansible uses procedural approach
* [ ] Ansible can only be used to as a configuration management tool

**Correct answer:**
* [x] Ansible uses procedural approach

**Explaination**: As explained in the lecture, Ansible makes use of a procedural approach. This means that an Ansible playbook should specifically contain all the steps needed achieve a task. 


**2. What does “IaC” stand for?**


* [ ] Infrastructure as Code
* [ ] Initialization as Code
* [ ] Code as Infrastructure
* [ ] None of the above

**Correct answer:**
* [x] Infrastructure as Code

**Explaination**: IaC stands for Infrastructure as Code.

**3. Which of the following statements is true?**


* [ ] Terraform can only be installed on Windows OS
* [ ] Terraform cannot be installed on Solaris or OpenBSD
* [ ] Terraform can only be installed on Linux Distributions
* [ ] Terraform cannot be installed on MacOS
* [ ] None of the Above

**Correct answer:**
* [x] None of the Above

**Explaination**: Terraform can be installed on all major Linux Distributions, Windows, MacOS, Solaris and OpenBSD. Hence the correct answer here is "None of the Above"

**Documentation Link**: https://www.terraform.io/downloads.html

**4. What allows Terraform to make use of a declarative approach?**


* [ ] Terraform uses multiple providers that supports all major cloud providers
* [ ] Terraform makes use of a state file
* [ ] Terraform uses JSON instead of YAML
* [ ] Terraform uses modules unlike Ansible

**Correct answer:**
* [x] Terraform makes use of a state file

**Explaination**: Terraform uses a state file that contains data related to every resource provisioned by terraform. This state file along with the configuration files gives a clear definition of what the infrastructure should look like at any given point in time.

**5. Select the file extension used by terraform configuration files.**


* [ ] .TF
* [ ] .YAML
* [ ] .TOML
* [ ] .DAT
* [ ] None of the Above

**Correct answer:**
* [x] .TF

**6. A Junior DevOps engineer has just joined your organization. He is exploring ways to automate the existing process of infrastructure provisioning and management using code. As his tech lead, what suggestions in general would you like to provide him?**


* [ ] Use any technique that would ensure no human interference and should be automating resources.
* [ ] Utilize bash scripting to codify infrastructure and use it to define, provision, configure, update and destroy infrastructure resources.
* [ ] Run a POC using a single tool and stick to it for all tasks
* [ ] Explore the requirements and utilize the correct IaC tools that check all the required boxes.
* [ ] Use any IaC tool out there as they are all the same.

**Correct answer:**
* [x] Explore the requirements and utilize the correct IaC tools that check all the required boxes.

**Explaination**: As we learnt in the lecture, there are several categories of IaC tools and several different tools available in the market within a category. For optimum results, we should first explore the requirements and chose the tool or tools that can meet all the requirements.

**7. What is Immutable Infrastructure?**


* [ ] Resources once deployed are not intended to be changed
* [ ] Resources cannot be migrated to another platform
* [ ] Any aspect of a resource can be updated in place anytime 
* [ ] Resources strictly provisioned by Terraform

**Correct answer:**
* [x] Resources once deployed are not intended to be changed

**Explaination**: Immutable infrastructure is another paradigm in which it ensures that resources are never modified after they have been deployed.
If a change is to be made, a new instance of that resource will be provisioned in place of the old one.

**8. Your company has recently been contracted to oversee a business transformation project for a large bank. This project requires a lot of the legacy banking applications to be migrated to several different cloud platforms - AWS, Azure, GCP to name a few. The solution architects of your team have decided to go with Terraform as their choice for the infrastructure provisioning. Why do you think they went with this choice?**


* [ ] Terraform is the ideal choice for Configuration Management as well as Provisioning
* [ ] They just went with their gut feeling
* [ ] Terraform is vendor agnostic and supports multiple providers which is a good fit for this project.
* [ ] Disagree with the choice -  should have used CloudFormation

**Correct answer:**
* [x] Terraform is vendor agnostic and supports multiple providers which is a good fit for this project.

**Explaination**: The support for multiple providers that helps in managing third party platforms (including AWS, Azure and GCP) using the api makes Terraform a logical choice as a provisioning tool.

**9. What should be the very first command that should be run after writing a new Terraform configuration or cloning an existing one from version control?**


* [ ] Terraform init
* [ ] Terraform plan 
* [ ] Terraform apply
* [ ] Terraform get

**Correct answer:**
* [x] Terraform init

**10. Observe the below code snippet and choose correct options:**


* [ ] "local_file" is a provider with resource name "games"
* [ ] The resource name "games" is a user-defined value
* [ ] "local_file" is the resource type and "local" is the provider
* [ ] This code creates the file "/root/favorite-games" with terraform apply

**Correct answer:**
* [x] This code creates the file "/root/favorite-games" with terraform apply
* [x] "local_file" is the resource type and "local" is the provider
* [x] The resource name "games" is a user-defined value

**Code**: 
resource "local_file" "games" {
  file     = "/root/favorite-games"
  content  = "FIFA 21"
}


**11. Which of the options can be used to run a `terraform apply` without confirmation?**


* [ ] -auto-approve
* [ ] -approve
* [ ] -auto-approve=yes
* [ ] -auto-approve=true

**Correct answer:**
* [x] -auto-approve

**Documentation Link**: https://www.terraform.io/docs/cli/commands/apply.html#auto-approve

**12. Choose the correct terraform command to display the blueprint of the infrastructure to be applied.**


* [ ] terraform init
* [ ] terraform apply
* [ ] terraform plan
* [ ] terraform show

**Correct answer:**
* [x] terraform plan

**13. What is the order of commands to be run in a core terraform workflow?**


* [ ] terraform validate -> terraform init -> terraform apply
* [ ] terraform init -> terraform plan -> terraform apply
* [ ] terraform init -> terraform validate -> terraform apply
* [ ] Order of commands don’t matter in a terraform workflow
* [ ] terraform init -> terraform fmt -> terraform apply

**Correct answer:**
* [x] terraform init -> terraform plan -> terraform apply

**14. After testing a sample terraform code, the user now wants to get rid of everything that was provisioned. Which of the following does he need to execute in order to accomplish this?**


* [ ] terraform refresh
* [ ] terraform state rm 
* [ ] terraform destroy
* [ ] terraform workspace delete

**Correct answer:**
* [x] terraform destroy

**15. What does the "terraform show" command use to provide details of the Infrastructure?**


* [ ] Configuration Files
* [ ] State File
* [ ] Provider Plugins
* [ ] Terraform Workspace

**Correct answer:**
* [x] State File

**Explaination**: The "terraform show" command inspects the state file and displays the resource details

**16. You have been working on a terraform configuration file. When you run terraform plan, you see an output as shown below that states that the resource "local_file.pet must be replaced". What is the cause for this?**


* [ ] file_content was changed
* [ ] directory_permissions was changed
* [ ] file_permissions was changed
* [ ] terraform init was not run
* [ ] file_permission was changed

**Correct answer:**
* [x] file_permission was changed

**Code**: 
# local_file.pet must be replaced
-/+ resource "local_file" "pet" {
        content              = "We love pets!"
        directory_permission = "0777"
      ~ file_permission      = "0777" -> "0700" # forces replacement
        filename             = "/root/pet.txt"
      ~ id                   = "5f8fb950ac60f7f23ef968097cda0a1fd3c11bdf" -> (known after apply)
    }


**17. Upon running "terraform init" within a configuration directory, you see an output as shown below. What is the provider and version that was initialized?**


* [ ] local v1.4.0
* [ ] random v1.2
* [ ] aws v2.1
* [ ] local v1.0.0

**Correct answer:**
* [x] local v1.4.0

**Code**: 
Initializing the backend...

Initializing provider plugins...
- Finding latest version of hashicorp/local...
- Installing hashicorp/local v1.4.0...
- Installed hashicorp/local v1.4.0 (signed by HashiCorp)

The following providers do not have any version constraints in configuration,
so the latest version was installed.

To prevent automatic upgrades to new major versions that may contain breaking
changes, we recommend adding version constraints in a required_providers block
in your configuration, with the constraint strings suggested below.

* hashicorp/local: version = "~> 1.4.0"

Terraform has been successfully initialized!


**18. A simple terraform configuration file is given below. What is the name of the resource that will be created?**


* [ ] pet
* [ ] local_file
* [ ] pets.txt
* [ ] local

**Correct answer:**
* [x] pet

**Code**: 
resource "local_file" "pet" {​
  filename = "/root/pets.txt"​
  content = "We love pets!"​ ​
}


**Explaination**: The name of the resource is "pet" which is a local_file type resource.

**19. What is the resource type used by this sample configuration file?**


* [ ] google_compute_instance
* [ ] my-testvm
* [ ] test-vm
* [ ] google

**Correct answer:**
* [x] google_compute_instance

**Code**: 
resource "google_compute_instance" "my-testvm" {
  count        = 3
  name         = my-testvm${count.index + 1}"
  machine_type = var.instance_type
  zone         = var.zone
  tags = ["testvm"]
  }


**20. Which of the following is an "argument" used in the sample terraform configuration given below?**


* [ ] ami
* [ ] t2.micro
* [ ] ami-0edab43b6fa892279
* [ ] webserver
* [ ] aws_instance

**Correct answer:**
* [x] ami

**Code**: 
resource "aws_instance" "webserver" {
  ami           = "ami-0edab43b6fa892279"
  instance_type = "t2.micro"
}


---


##  Lab MCQ Providers

**1. Where can we declare the version of the provider that is required by a terraform configuration? Choose the most appropriate answer.**


* [ ] Under the providers block
* [ ] As a variable
* [ ] As an provider alias
* [ ] Under the required_providers block 
* [ ] While running terraform init 

**Correct answer:**
* [x] Under the required_providers block 

**Explaination**: The required providers for a configuration can be declared inside the  required_providers block which is nested inside the terraform block.

Example:

terraform {
  required_providers {
    mycloud = {
      source  = "mycorp/mycloud"
      version = "~> 1.0"
    }
  }
}

**Documentation Link**: https://www.terraform.io/docs/language/providers/requirements.html#requiring-providers

**2. Choose the easiest way to list the versions of all installed plugins in terraform along with terraform versions.**


* [ ] Run terraform version command
* [ ] Search the contents of the .terraform directory for plugin versions
* [ ] There is no easy way to accomplish this
* [ ] All of the above

**Correct answer:**
* [x] Run terraform version command

**Explaination**: The terraform version command provides the version of terraform as well as the version of the provider plugins that are downloaded in the configuration directory. 

For example, the below command shows that the version of aws provider is v3.69.0 and that of local provider is  v2.1.0

iac-server $ terraform version
Terraform v0.13.3
+ provider registry.terraform.io/hashicorp/aws v3.69.0
+ provider registry.terraform.io/hashicorp/local v2.1.0

**3. Your team has deployed an EKS cluster in the AWS cloud using terraform. To the existing configuration, you have added a new resource block for the "kubernetes_deployment" type resource. When you run terraform apply,  you see an error that states - “Failed to instantiate provider”. What could be the reson for this error?**


* [ ] Restart the machine and it should work
* [ ] There is no provider with the name kubernetes
* [ ] A terraform plan was not run to generate the execution plan
* [ ] EKS cluster was not provisioned correctly
* [ ] The aws module was not initialized for the configuration
* [ ] The kubernetes provider was not initialized for the configuration

**Correct answer:**
* [x] The kubernetes provider was not initialized for the configuration

**Explaination**: Since the EKS cluster was already provisioned and the error was displayed only after adding the resource block for the "kubernetes_deployment",  it would appear that a "terraform init" command was not run to download the provider plugin for the kubernetes provider.

**4. Whenever the target APIs change or when new functionality is added, the provider maintainers may update new versions for a provider. This may lead to unexpected infrastructure changes. What is the best approach to overcome this?**


* [ ] Never touch what you don’t understand
* [ ] Use required_providers block to clearly define the provider version you want to use
* [ ] API changes does not affect the provider usage within terraform
* [ ] There would be no issue as terraform always downloads the latest version of the provider

**Correct answer:**
* [x] Use required_providers block to clearly define the provider version you want to use

**Explaination**: The functionality of a provider plugin may vary drastically from one version to another. 
Our terraform configuration may not work as expected when using a version different than the one it was written in. As a best practice, always declare the exact version of the provider we want to use within the required_providers block.


**5. Observe the below code and determine the providers used.**


* [ ] Local_file and random_pet
* [ ] Pet_name and my-pet
* [ ] Local and random
* [ ] Local_file, random_pet, pet_name, my-pet

**Correct answer:**
* [x] Local and random

**Code**: 
resource "local_file" "pet_name" {
            content = "We love pets!"
            filename = "/root/pets.txt"
}
resource "random_pet" "my-pet" {
              prefix = "Mrs"
              separator = "."
              length = "1"
}


**6. The terraform providers command shows information about the provider requirements of the configuration in the current working directory. True or False?**


* [ ] True 
* [ ] False

**Correct answer:**
* [x] True 

**Explaination**: True. The "terraform providers" command displays the providers needed by the configuration. 

For example:

iac-server $ terraform providers

Providers required by configuration:
.
├── provider[registry.terraform.io/hashicorp/local]
└── provider[registry.terraform.io/hashicorp/aws]



**Documentation Link**: https://www.terraform.io/docs/cli/commands/providers.html

**7. Select the reasons why we may need to specify the provider's argument?**


* [ ] It’s just a practice we need to blindly follow
* [ ] No specific reason
* [ ] To use multiple configurations of the same provider
* [ ] To change the default Provider Configurations
* [ ] To use multiple provider plugins in the same configuration

**Correct answer:**
* [x] To use multiple configurations of the same provider
* [x] To change the default Provider Configurations

**Explaination**: There are two reasons to use a provider argument in the configuration. 

1. To override the default provider configuration.  For example, the default configuration may be to deploy resources in the "us-east-1" region. If the requirement is to deploy resources in a different region, we can use the provider argument to override the default.

2. In some cases, a configuration may need to use multiple versions of the same provider. For example -  a resource that deploys to the "us-east-1" and another resource within the same configuration that deploys to the "us-west-2" region. 

**Documentation Link**: https://www.terraform.io/docs/language/meta-arguments/module-providers.html#when-to-specify-providers

---


## MCQ Version Constraints aliases

**1. Which of the following are valid when using version constraints with a provider?**


* [ ] Use of comparison operators
* [ ] All of the above
* [ ] Combining comparison operations to use a specific version within a range
* [ ] Using the ~> operator
* [ ] Using pessimistic operator
* [ ] Using the "!="  operator 

**Correct answer:**
* [x] All of the above

**Documentation Link**: https://www.terraform.io/docs/language/expressions/version-constraints.html

**2. Which block is used to configure settings related to Terraform itself?**


* [ ] terraform
* [ ] source
* [ ] providers
* [ ] resource

**Correct answer:**
* [x] terraform

**Documentation Link**: https://www.terraform.io/docs/language/settings/index.html

**3. Providers use a ______-based architecture that is available for most infrastructure platforms within the public Terraform registry.**


* [ ] plugin
* [ ] module
* [ ] platform
* [ ] software
* [ ] infrastructure

**Correct answer:**
* [x] plugin

**Documentation Link**: https://www.terraform.io/docs/extend/how-terraform-works.html

**4. A version constraint is a string literal containing one or more conditions, which are separated by commas. True or False?**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**Documentation Link**: https://www.terraform.io/docs/language/expressions/version-constraints.html#version-constraint-syntax

**5. Which among the following is not a valid operator in terraform version constraints:**


* [ ] <code>=</code>
* [ ] <code>!=</code>
* [ ] <code><=</code>
* [ ] <code>>=</code>
* [ ] <code>==</code>

**Correct answer:**
* [x] <code>==</code>

**Documentation Link**: https://www.terraform.io/docs/language/expressions/version-constraints.html#version-constraint-syntax

**6. Which "terraform command"  from the following downloads the latest version of the provider plugins?**


* [ ] terraform plan
* [ ] terraform init
* [ ] terraform apply
* [ ] terraform pull

**Correct answer:**
* [x] terraform init

**Documentation Link**: https://www.terraform.io/docs/cli/commands/init.html

**7. Where can we make use of version constraints?**


* [ ] a. Modules
* [ ] b. Provider requirements
* [ ] c. The required_version setting in the terraform block
* [ ] d. All of the above

**Correct answer:**
* [x] d. All of the above

**Explaination**: Version constraints can be used anywhere terraform allows us to specify versions. Most commonly they can be set at:

1. Within the provider version configuration (Inside the required_providers block nested inside the terraform block)
2. The "required_version" argument which is used to set the version of Terraform to use.
3. Within modules. This is where we specify the version of module to be used. 

**Documentation Link**: https://www.terraform.io/docs/language/expressions/version-constraints.html

**8. Your team assigned you the task of developing a terraform configuration to provision a bunch of services on GCP. You did everything to the point but forgot to mention the provider's version in the terraform block. What default behavior would you expect from terraform:**


* [ ] a. Terraform init will fail
* [ ] b. Terraform will download and use the latest version of providers used in the configuration
* [ ] c. Terraform init will succeed but an apply will fail because of unsupported provider versions
* [ ] d. None of the above
* [ ] e. All of the above

**Correct answer:**
* [x] b. Terraform will download and use the latest version of providers used in the configuration

**Explaination**: Terraform will download the latest version for all the providers used within the configuration. The version downloaded may or may not work well with the configuration developed. 

---


## Lab MCQ Variables

**1. A variable block is given below. Inspect it and choose the valid options.**


* [ ] Invalid. We cannot use "providers" as a variable name
* [ ] Valid. The "default" argument is optional
* [ ] Invalid. "default" argument is not used
* [ ] Invalid. Incorrect "type" used

**Correct answer:**
* [x] Invalid. We cannot use "providers" as a variable name

**Code**: 
variable "providers" {
  type = string
}

**Explaination**: We can use any name for a variable except for: source,  version, providers, count, for_each, lifecycle, depends_on and locals. 

We have used the variable name as "providers". This is not a valid identifier

**Documentation Link**: https://www.terraform.io/docs/language/values/variables.html#declaring-an-input-variable

**2. Inspect the below code and choose expected behavior when you run a terraform plan or apply:**


* [ ] user will be prompted to enter a value for the variable "is_this_correct"
* [ ] Terraform plan or apply will work as is
* [ ] Error response as the "default" argument is not used
* [ ] Error as the variable block is incorrect

**Correct answer:**
* [x] user will be prompted to enter a value for the variable "is_this_correct"

**Code**: 
variable "is_this_correct" { }


**Explaination**: The given variable block is valid. However, since we have not supplied the default value for the variable, the user will be prompted to enter it when running terraform plan or apply. 

**3. The label after the variable keyword should be unique among all variables.**


* [ ] a. Should be unique among the variables in the same module
* [ ] b.You can create just two variables of the same label
* [ ] Both the statements (a) and (b) are true
* [ ] None of the above

**Correct answer:**
* [x] a. Should be unique among the variables in the same module

**Explaination**: A variable name or a label must be unique within the same module or configuration. 

**Documentation Link**: https://www.terraform.io/docs/language/values/variables.html#declaring-an-input-variable

**4. If both the "type" and the "default" argument are specified inside the variable block, the given default value must be convertible to the specified type. True or False?**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**Explaination**: For example consider the following variable block:

variable "max_number" {
  type      = number
  default =  "100"
}

Here, the type is a "number" however, the default value is expressed as a string since it is enclosed in double-quotes. In this case, terraform will convert the value to a number by default.

However, the following variable declaration is invalid:

variable "istrue" {
    type = bool
    default = 1
}

Here the default value is a number but the type is "boolean". Terraform cannot convert a number to a boolean.   

**5. Each output value exported by a module must be declared using an ______ block.**


* [ ] output 
* [ ] input
* [ ] variable
* [ ] resource
* [ ] data

**Correct answer:**
* [x] output 

**Documentation Link**: https://www.terraform.io/docs/language/values/outputs.html#declaring-an-output-value

**6. Which keyword is reserved for declaring variables in the terraform configuration files?**


* [ ] variable
* [ ] var
* [ ] Use the syntax var.<variable_name>
* [ ] variable block does not need a keyword
* [ ] user-defined keyword

**Correct answer:**
* [x] variable

**Explaination**: The variable block begins with the "variable" keyword followed by a user defined name/label for the variable. 

**Documentation Link**: https://www.terraform.io/docs/language/values/variables.html#declaring-an-input-variable

**7. We just created an environment variable named “TF_VAR_content=foo-3” and ran the following command: `terraform apply -var "content=foo-4"`  .Determine the content of file foo.txt**


* [ ] foo-4
* [ ] foo-3
* [ ] foo
* [ ] foo-1

**Correct answer:**
* [x] foo-4

**Code**: 
resource "local_file" "foo" {
  content  = var.content
  filename = “/random/foo.txt”
}

variable "content" {
  type        = string
  description = "Content of the file to be created"

  validation {
    condition     = substr(var.content, 0, 4) == "foo-"
    error_message = "The content value must be a valid word starting \"foo-\"."
  }
}


**Explaination**: The variables passed with the -var or -var-file command line flags have the highest priority and will take precedence over environment variables. As such, the file will be created with "foo-4" as the content.

**Documentation Link**: https://www.terraform.io/docs/language/values/variables.html#variable-definition-precedence

**8. What is the expected outcome of a 'terraform apply -var "content=random-string"' using the below configuration?**


* [ ] File “foo.txt” with content “random-string” will be created.
* [ ] It will throw an error as we have a custom validation rule before the variable declaration.
* [ ] Wrong declaration of variable
* [ ] Operation will fail as the variable used does not match the validation rule

**Correct answer:**
* [x] Operation will fail as the variable used does not match the validation rule

**Code**: 
resource "local_file" "foo" {
  content  = var.content
  filename = “/random/foo.txt”
}

variable "content" {
  type        = string
  description = "Content of the file to be created"

  validation {
    condition     = substr(var.content, 0, 4) == "foo-"
    error_message = "The content value must be a valid word starting \"foo-\"."
  }
}


**Explaination**: The validation rule used with the variable called "content" expects the value to start with "foo=". Since this does not match the value we supplied with the -var flag (random-string),  the operation will fail. 

Sample output:

-------------------------------------------------------------------------------------------------
iac-server $ terraform apply -var "content=random-string"

Error: Invalid value for variable

  on variables.tf line 6:
   6: variable "content" {

The content value must be a valid word starting "foo-".

This was checked by the validation rule at variables.tf:10,3-13.
-------------------------------------------------------------------------------------------------

**9. Which of the following statements are true regarding output variables?**


* [ ] Running terraform plan will not render outputs
* [ ] None of the above
* [ ] Running the "terraform apply" will render the output variables defined
* [ ] Running the "terraform plan" will render the output variables defined
* [ ] Running the "terraform apply" will not render the output variables defined
* [ ] Running the "terraform output" will render the output variables defined

**Correct answer:**
* [x] Running terraform plan will not render outputs
* [x] Running the "terraform apply" will render the output variables defined
* [x] Running the "terraform output" will render the output variables defined

**Documentation Link**: https://www.terraform.io/docs/language/values/outputs.html#declaring-an-output-value

**10. Select the optional arguments that are available for the output block.**


* [ ] description
* [ ] sensitive
* [ ] depends_on
* [ ] All of the above

**Correct answer:**
* [x] All of the above

**Documentation Link**: https://www.terraform.io/docs/language/values/outputs.html#optional-arguments

---


## Lab MCQ Resource Attributes and Deps

**1. Which option best describes the meaning of interpolation syntax?**


* [ ] A way to reference variables, attributes of resources, and call functions
* [ ] A way to declare values to variables
* [ ] A way to provide runtime options with terraform operations
* [ ] None of the above

**Correct answer:**
* [x] A way to reference variables, attributes of resources, and call functions

**Explaination**: Interpolation syntax allows us to reference variables, resource attributes and even make use of built-in functions in terraform.

**Documentation Link**: https://www.terraform.io/docs/language/expressions/strings.html#interpolation

**2. Which among the following best explains the need of the dependency concept in terraform?**


* [ ] Terraform functions error-free even without the notion of dependency
* [ ] Allows resources to be created in the correct order
* [ ] Allows resources to be destroyed in the correct order
* [ ] Allows resources to be created  and destroyed in the correct order

**Correct answer:**
* [x] Allows resources to be created  and destroyed in the correct order

**3. Inspect the below code block and determine the resource attribute that creates a dependency between the given resources.**


* [ ] aws_subnet.cidr_block
* [ ] aws_vpc.cidr_block
* [ ] aws_vpc.backend-vpc.id
* [ ] aws_vpc.backend_vpc.cidr_block
* [ ] aws_subnet.private-subnet1.cidr_block

**Correct answer:**
* [x] aws_vpc.backend-vpc.id

**Code**: 
resource "aws_vpc" "backend-vpc" {
  cidr_block = "10.0.0.0/16"
  tags = {
    Name = "backend-vpc"
  }
}
resource "aws_subnet" "private-subnet1" {
  vpc_id     = aws_vpc.backend-vpc.id
  cidr_block = "10.0.2.0/24"
  tags = {
    Name = "private-subnet1"
  }
}

**Explaination**: The aws_subnet type resource called private-subnet1 makes use of the resource attribute "aws_vpc.backend-vpc.id". 

**4. Which meta-argument is used when Terraform cannot infer dependencies between different parts of your infrastructure?**


* [ ] depend_on
* [ ] depends_on
* [ ] depends-on
* [ ] depend-on
* [ ] resource attribute reference

**Correct answer:**
* [x] depends_on

**Documentation Link**: https://www.terraform.io/docs/language/meta-arguments/depends_on.html

**5. What is the generic way to reference attributes within the terraform expression?**


* [ ] None of the above
* [ ] RESOURCE_TYPE.ATTRIBUTE.NAME
* [ ] RESOURCE_TYPE.NAME.ATTRIBUTE
* [ ] RESOURCE_TYPE.NAME

**Correct answer:**
* [x] RESOURCE_TYPE.NAME.ATTRIBUTE

**Documentation Link**: https://www.terraform.io/docs/language/expressions/references.html#resources

**6. Terraform resources and data sources make all of their arguments available as readable attributes, and also typically export additional read-only attributes. True or False?**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**Explaination**: Both resources and datasources export arguments  as readable attributes.

**Documentation Link**: https://www.terraform.io/docs/language/expressions/references.html#references-to-resource-attributes

---


##  Lab MCQ Datasources

**1. We have a local file resource with certain content. Once this resource is provisioned, the file is created in the `/root` directory and the information about this file is also stored in the Terraform state file. Now let's create a new file using a simple shell script in the same directory `/root`. Quite evidently, this file is outside the control and management of Terraform at this point in time. How would you include the second file in your Terraform configuration?**


* [ ] By creating a resource type object inside the main.tf file.
* [ ] By creating a data type object inside the main.tf file
* [ ] Terraform automatically syncs the files under the same directory
* [ ] Terraform doesn't provide such functionality

**Correct answer:**
* [x] By creating a data type object inside the main.tf file

**2. Which of the following statements is true for the terraform data block?**


* [ ] Reads data provisioned using other tools, such as Puppet, CloudFormation , SaltStack, Ansible etc
* [ ] Reads data provisioned using ad-hoc scripts as well as and manually provisioned infrastructure
* [ ] All of the above
* [ ] Reads resources created by terraform by other configuration directories.

**Correct answer:**
* [x] All of the above

**3. Why does terraform incorporate the concept of data sources?**


* [ ] It allows Terraform to read attributes from resources which are provisioned within its control
* [ ] It allows Terraform to read attributes from resources which are provisioned outside its control.
* [ ] It allows Terraform to import attributes from resources provisioned using ad-hoc scripts only
* [ ] All of the above

**Correct answer:**
* [x] It allows Terraform to read attributes from resources which are provisioned outside its control.

**4. Choose the suitable options that best describe the data source block**


* [ ] It only creates the Infrastructure objects
* [ ] It only updates the Infrastructure objects
* [ ] It only destroys the Infrastructure objects
* [ ] It only reads the Infrastructure objects

**Correct answer:**
* [x] It only reads the Infrastructure objects

**5. The data read from a data source is available under the "______ object" in Terraform.**


* [ ] data
* [ ] resource
* [ ] terraform
* [ ] local

**Correct answer:**
* [x] data

**6. The behavior of __________ data sources is the same as all other data sources, but their result data exists only temporarily during a Terraform operation, and is re-calculated each time a new plan is created.**


* [ ] remote-only
* [ ] local-only
* [ ] Both
* [ ] None

**Correct answer:**
* [x] local-only

**7. Each data source in turn belongs to a provider.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**8. Data resources have the different dependency resolution behavior as defined for managed resources**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**9. A data source can be created using the data block.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**10. When a module has multiple configurations for the same provider, which meta-argument can you use to specify the configuration?**


* [ ] provider
* [ ] providers
* [ ] specific_provider
* [ ] None

**Correct answer:**
* [x] providers

**11. What is the form of the reference expression used for the data block?**


* [ ] data.[NAME].[TYPE].[ATTRIBUTE]
* [ ] data.[TYPE].[NAME]
* [ ] data.[TYPE].[NAME].[ATTRIBUTE]
* [ ] [TYPE].[NAME].[ATTRIBUTE] 

**Correct answer:**
* [x] data.[TYPE].[NAME].[ATTRIBUTE]

**12. Choose the meta-argument which is not supported by the data block.**


* [ ] depends_on
* [ ] count
* [ ] for_each
* [ ] provider
* [ ] lifecycle

**Correct answer:**
* [x] lifecycle

---


## Lab MCQ Terraform State

**1. Unlike the configuration files, why it's not a good idea to store the state file in a version control system?**


* [ ] It would mostly contain sensitive information pertaining to our infrastructure
* [ ] Version control systems such as GitHub do not support state locking
* [ ] Very difficult to maintain the integrity of the state file
* [ ] All of the above.

**Correct answer:**
* [x] All of the above.

**2. What purpose does the `terraform.tfstate` file serve?**


* [ ] map real world resources to your configuration
* [ ] keep track of metadata
* [ ] to improve performance for large infrastructures
* [ ] All of the above

**Correct answer:**
* [x] All of the above

**3. How does Terraform protect itself where concurrent operations are run against the same configuration?**


* [ ] Users need to take precautions
* [ ] Terraform does it automatically with no additional configuration
* [ ] It uses the concept of state-locking for all backends
* [ ] Terraform will lock your state for all operations that could write state, if supported by your backend

**Correct answer:**
* [x] Terraform will lock your state for all operations that could write state, if supported by your backend

**4. Areas of Terraform's behavior that are not determined by the backend:**


* [ ] Where state is stored
* [ ] Where operations are performed
* [ ] Both
* [ ] None

**Correct answer:**
* [x] None

**5. Name the file that is created by default when you run the `terraform apply` command for the first time**


* [ ] terraform.state
* [ ] state.tf
* [ ] terraform.tf
* [ ] terraform.tfstate

**Correct answer:**
* [x] terraform.tfstate

**6. What steps are needed in order to change the backend in terraform? For instance : local to remote**


* [ ] Adding a terraform block only, would be enough
* [ ] Add a backend block within a terraform block
* [ ] Specifying the backend block within the terraform block is followed by reinitialization of the backend using `terraform init`.
* [ ] Any one of the options is enough.

**Correct answer:**
* [x] Specifying the backend block within the terraform block is followed by reinitialization of the backend using `terraform init`.

**7. Which option can be used to disable the state locking for most commands?**


* [ ] -state-lock
* [ ] -lock=yes
* [ ] -lock
* [ ] -locked

**Correct answer:**
* [x] -lock

**8. You wanted to play with terraform to check what it has to offer. After a while you remembered that you didn’t specify any configuration for the backend. What default behaviour is expected here of terraform?**


* [ ] Terraform will use a local backend, which requires no configuration
* [ ] Terraform will use a remote backend, which requires no configuration.
* [ ] Terraform will randomly use a backend from a pool of local or remote ones
* [ ] None of the above

**Correct answer:**
* [x] Terraform will use a local backend, which requires no configuration

**9. A requirement has come up which requires you to inspect the state file of terraform configuration. Your terraform script is already configured to work with the remote backend. Which of the following commands would you use to view a specific field in the state file**


* [ ] terraform state mv
* [ ] terraform state list
* [ ] terraform state show
* [ ] terraform state pull

**Correct answer:**
* [x] terraform state pull

**10. Choose the appropriate option which lists all the resources recorded within the Terraform state file**


* [ ] state mv
* [ ] state list
* [ ] state show
* [ ] state pull

**Correct answer:**
* [x] state list

**11. What are the steps required to remove a resource from the management of terraform?**


* [ ] Use `terraform rm` command
* [ ] Use the `terraform state rm` command followed by manual removal of corresponding resources from the configuration file as well
* [ ] Remove the corresponding resource from the configuration file
* [ ] All are true under certain case-scenarios
* [ ] None of the options are correct

**Correct answer:**
* [x] Use the `terraform state rm` command followed by manual removal of corresponding resources from the configuration file as well

**12. Which of the following is not the valid sub-command of `terraform state` command?**


* [ ] state mv
* [ ] state list
* [ ] state show
* [ ] state pull
* [ ] state rm
* [ ] state replace

**Correct answer:**
* [x] state replace

**13. Which of the following `terraform state` sub-commands list detailed information about a resource from the state file?**


* [ ] mv
* [ ] list
* [ ] show
* [ ] pull

**Correct answer:**
* [x] show

---


## Lab MCQ Terraform Commands

**1. Which command can be used to create a visual representation of our terraform resources?**


* [ ] terraform view
* [ ] terraform console
* [ ] terraform map
* [ ] terraform graph

**Correct answer:**
* [x] terraform graph

**2. Your friend was working on a terraform project but he is unable to execute the `terraform  apply` command successfully. As having experience in automation he asks you for help. On further investigation he reported that he already initialized the directory and also ran `terraform validate` command. Even `terraform plan` also ran with no errors. What could be at fault here?**


* [ ] This is because the `terraform validate` command only carries out a general verification of the configuration.
* [ ] There could be some issue with the value that arguments expect for a specific resource.
* [ ] He just found a new bug in terraform
* [ ] All the statements are true except the bug one.

**Correct answer:**
* [x] All the statements are true except the bug one.

**3. `terraform plan` and `terraform apply` both refresh the state before their execution. Which option could be used to disable this default behaviour?**


* [ ] -refresh=disable
* [ ] -refresh=no
* [ ] -refresh=false
* [ ] -no-refresh=true

**Correct answer:**
* [x] -refresh=false

**4. Two teams are working on a single terraform project that concerns the provisioning of various services on the AWS cloud platform. Your team just handed over the remaining last bits of the project that needs to be cross-checked.  While going through the configuration files you want to look if they consist of any syntax discrepancies and fix some indentation issues that are hampering the readability of files. Select from the following that can solve the issue at hand:**


* [ ] `terraform validate` to check for syntax errors and `terraform fmt` to convert code to canonical format.
* [ ] `terraform check` to check for syntax errors and `terraform format` to convert code for better readability.
* [ ] `terraform plan` will validate any errors as well as format the code before creating the execution plan.
* [ ] It’s better to go through each line and check yourself than to trust terraform binaries.

**Correct answer:**
* [x] `terraform validate` to check for syntax errors and `terraform fmt` to convert code to canonical format.

**5. The `terraform providers ______` command can automatically populate a directory that will be used as a local filesystem mirror in the provider installation configuration.**


* [ ] mirror
* [ ] populate
* [ ] download
* [ ] pull

**Correct answer:**
* [x] mirror

**6. Is it necessary to initialize the terraform configuration directory prior to execution of `terraform validate`.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**7. Choose the valid subcommand for `terraform providers` :**


* [ ] terraform providers mirror
* [ ] terraform providers lock
* [ ] terraform providers schema
* [ ] All the above are correct

**Correct answer:**
* [x] All the above are correct

**8. You are working in collaboration with your team on some project involving automation using terraform. One of the team members modified a resource on the real cloud infrastructure on the azure platform. You want those resources to be reflected in your local machine. Which of the following would you go with:**


* [ ] `terraform refresh` as it modifies the state as well as the configuration files in the local configuration directory.
* [ ] `terraform refresh` as it only modifies the state file on your local machine.
* [ ] It is best to ask for the changes made by the developer directly.
* [ ] There is no need to do anything as the state files are synced at regular intervals of 15 minute.

**Correct answer:**
* [x] `terraform refresh` as it only modifies the state file on your local machine.

**9. Every terraform command listed is useful for inspecting infrastructure:- `output`, `graph`, `show`, `state list`, `state show`**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**10. The terraform graph command is used to generate a visual representation in which format?**


* [ ] YAML 
* [ ] JSON
* [ ] TOML 
* [ ] DOT

**Correct answer:**
* [x] DOT

**11. Applying a Terraform configuration will:**


* [ ] Create resources that exist in the configuration but are not associated with a real infrastructure object in the state
* [ ] Destroy resources that exist in the state but no longer exist in the configuration.
* [ ] Update in-place resources whose arguments have changed
* [ ] Destroy and re-create resources whose arguments have changed but which cannot be updated in-place due to remote API limitations
* [ ] All of the above

**Correct answer:**
* [x] All of the above

---


## Lab MCQ Terraform Taint and Logging

**1. Which environment variable should be used to export the logs to a specific path?**


* [ ] TF_LOG
* [ ] var.TF_LOG
* [ ] VAR_TF_LOG
* [ ] TF_LOG_PATH

**Correct answer:**
* [x] TF_LOG_PATH

**2. Can you export the debug logs from `terraform` only by setting the `TF_LOG_PATH` environment variable?**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**3. Which Log Level provides the most details when you run `terraform` commands?**


* [ ] LOG_LEVEL=5
* [ ] --v=5
* [ ] WARN
* [ ] ERROR
* [ ] TRACE

**Correct answer:**
* [x] TRACE

**4. Your team is working collaboratively on a project that uses terraform scripts heavily. In your team one team member was not familiar with terraform, so he was making the required changes manually, using the GUI console of that specific cloud provider on the resources provisioned using terraform. Since these unmanaged changes are hampering the efficiency of the team , you want to revert these changes. How would you go about doing this?**


* [ ] Manually remove these resources using the same GUI console used to provision it
* [ ] Use `terraform destroy` and then `terraform apply` commands in this specific order
* [ ] You will taint the resources that you want removed on the next `terraform apply`.
* [ ] All of the above

**Correct answer:**
* [x] Use `terraform destroy` and then `terraform apply` commands in this specific order
* [x] You will taint the resources that you want removed on the next `terraform apply`.

**5. The terraform ______command informs Terraform that a particular object has become degraded or damaged.**


* [ ] taint
* [ ] mark
* [ ] hold
* [ ] None of the above

**Correct answer:**
* [x] taint

**6. How would you achieve the force replacement of a particular object even though there are no configuration changes? Choose the most appropriate option among the following:**


* [ ] There is no function in the terraform
* [ ] Using option `-replace` with `terraform apply` command
* [ ] Using `terraform taint` is the only way possible
* [ ] Usage of `terraform apply -replace=”<resource_name>” is preferred over `terraform taint`
* [ ] None of the above

**Correct answer:**
* [x] Usage of `terraform apply -replace=”<resource_name>” is preferred over `terraform taint`

**Documentation Link**: https://www.terraform.io/docs/cli/commands/taint.html

**7. Choose the invalid log level values for the TF_LOG environment variable.**


* [ ] DEBUG
* [ ] INFO
* [ ] WARN 
* [ ] ERROR
* [ ] JSON
* [ ] None of the above

**Correct answer:**
* [x] None of the above

**Explaination**: You can set TF_LOG to one of the log levels (in order of decreasing verbosity) TRACE, DEBUG, INFO, WARN or ERROR to change the verbosity of the logs. Also if we set TF_LOG to JSON, it output logs at the TRACE level or higher, and uses a parseable JSON encoding as the formatting.



**8. Which environment variable is exported to set a different log level in the terraform?**


* [ ] TF_LOG
* [ ] var.TF_LOG
* [ ] VAR_TF_LOG
* [ ] TF_LOG_PATH

**Correct answer:**
* [x] TF_LOG

---


## Lab MCQ import and workspaces

**1. When we start off and create a configuration in `terraform`, what is the workspace that is created, to begin with?**


* [ ] local
* [ ] remote
* [ ] default
* [ ] cloud

**Correct answer:**
* [x] default

**2. Choose the statement among the following that terraform workspace deals with:**


* [ ] Maintaining separate cache of plugins and modules for each working directory
* [ ] Maintaining multiple directories can waste bandwidth and disk space.
* [ ] Update your configuration code from version control separately for each directory
* [ ] Reinitialize each directory separately when changing the configuration
* [ ] All of the above

**Correct answer:**
* [x] All of the above

**3. You can use  ________ to manage multiple non-overlapping groups of resources with the same configuration.**


* [ ] workspaces 
* [ ] Modules
* [ ] Multiple State files
* [ ] None of the options

**Correct answer:**
* [x] workspaces 

**4. Every initialized working directory has at least one workspace.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**5. For a given working directory, how many workspaces can be selected at a time?**


* [ ] One
* [ ] Two
* [ ] Three
* [ ] Depends on user preference

**Correct answer:**
* [x] One

**6. We can delete the `default` terraform workspace.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**7. Select the invalid subcommand for `terraform workspace` among the following:**


* [ ] list
* [ ] new
* [ ] rename
* [ ] show
* [ ] delete
* [ ] select

**Correct answer:**
* [x] rename

**8. Within your Terraform configuration, how can you include the name of the current workspace.**


* [ ] Using ${workspace} interpolation sequence
* [ ] Using ${workspace.workspace_name} interpolation sequence
* [ ] Using ${terraform.workspace} interpolation sequence
* [ ] None of the above

**Correct answer:**
* [x] Using ${terraform.workspace} interpolation sequence

**9. A developer working on a complex set of infrastructure changes needs to freely experiment with changes without affecting the default workspace. How can he achieve that?**


* [ ] Create a new temporary workspace
* [ ] Create a new terraform configuration directory with the same configuration files
* [ ] No option but to use the default workspace
* [ ] Using temporary workspace is preferred over duplication of configuration files.

**Correct answer:**
* [x] Using temporary workspace is preferred over duplication of configuration files.

**10. The example below will import an AWS instance into the aws_instance resource named bar into a module named foo.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**Code**: 
terraform import module.foo.aws_instance.bar i-abcd1234

**11. For local state, Terraform stores the workspace states in a directory called :**


* [ ] terraform.tfstate
* [ ] terraform.tfstate.d
* [ ] tfstate.d
* [ ] None of the above

**Correct answer:**
* [x] terraform.tfstate.d

**12. Command used to import existing resources into Terraform**


* [ ] terraform pull
* [ ] terraform state pull
* [ ] terraform import
* [ ] terraform imports

**Correct answer:**
* [x] terraform import

**13. What is the usage pattern of the `terraform import` command?**


* [ ] terraform import [options] ADDRESS ID
* [ ] terraform import [options] ID ADDRESS
* [ ] terraform import [options] ADDRESS
* [ ] Any one of the above statement could be used

**Correct answer:**
* [x] terraform import [options] ADDRESS ID

**14. What ways do you have to bring resources created by other methods into terraform?**


* [ ] Use `data sources`
* [ ] Use `terraform import`
* [ ] Updating terraform configuration only
* [ ] All of the above

**Correct answer:**
* [x] Use `data sources`
* [x] Use `terraform import`

**15. Select the statements valid for data sources and the `terraform import` command?**


* [ ] Data sources help us to make use of the attributes of the data source that is not managed by Terraform at this stage.
* [ ] Terraform import brings a resource completely in the management and control of Terraform
* [ ] Data sources and `terraform import` command both satisfy the same goal
* [ ] None of the above

**Correct answer:**
* [x] Data sources help us to make use of the attributes of the data source that is not managed by Terraform at this stage.
* [x] Terraform import brings a resource completely in the management and control of Terraform

**16. Choose the correct options from the following:**


* [ ] Terraform import updates the configuration files as well as updates the state file with the details of the infrastructure being imported
* [ ] Terraform import does not update the configuration files at all, only updates the state file with the details of the infrastructure being imported.
* [ ] Terraform import only updates the configuration files and doesn't touch the state file at all.
* [ ] None of the above

**Correct answer:**
* [x] Terraform import does not update the configuration files at all, only updates the state file with the details of the infrastructure being imported.

**17. You intend to import two resources to your terraform configuration. You executed only the `terraform import` command until now and it worked. Will the `terraform apply` work if executed now?**


* [ ] It will throw an error
* [ ] Works like a charm
* [ ] We haven’t updated the resource with correct argument values yet
* [ ] None of the options

**Correct answer:**
* [x] It will throw an error
* [x] We haven’t updated the resource with correct argument values yet

---


## Lab MCQ Terraform Modules

**1. Select the sources that terraform can load modules from.**


* [ ] Local relative paths
* [ ] Remote repositories
* [ ] Both
* [ ] None of the above

**Correct answer:**
* [x] Both

**2. Modules can be used to create lightweight abstractions, so that you can describe your infrastructure in terms of its architecture, rather than directly in terms of physical objects.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**3. What is a root module?**


* [ ] The terraform configuration directory under `/root/` is called root module.
* [ ] The current terraform configuration directory consisting of `.tf` files forms the root module.
* [ ] Both 
* [ ] None of the above

**Correct answer:**
* [x] The current terraform configuration directory consisting of `.tf` files forms the root module.

**4. Which module can call other modules and connect them together by passing output values from one as input values to another.**


* [ ] root
* [ ] child
* [ ] default
* [ ] provider

**Correct answer:**
* [x] root

**5. Modules can also call other modules using a _________ block**


* [ ] modules
* [ ] module
* [ ] Only the root module can call other modules
* [ ] None of the above

**Correct answer:**
* [x] module

**6. When we introduce _______ blocks, our configuration becomes hierarchical rather than flat.**


* [ ] module
* [ ] root module
* [ ] child module
* [ ] None of the above

**Correct answer:**
* [x] module

**7. What do we call this flat type of module usage:**


* [ ] module composition
* [ ] dependency inversion
* [ ] module gathering
* [ ] None of the above

**Correct answer:**
* [x] module composition

**Code**: 
module "network" {
  source = "./modules/aws-network"
  ...
}

module "consul_cluster" {
  source = "./modules/aws-consul-cluster"
  ...
}


**8. What are some of the problems that terraform `module` addresses?**


* [ ] Complex configuration files
* [ ] Duplicate code
* [ ] Increased risk. (like updating the resource)
* [ ] Limitation on reusability
* [ ] All of the above

**Correct answer:**
* [x] All of the above

**9. Select the types of terraform modules based on the credibility tier.**


* [ ] Verified
* [ ] Non-verified
* [ ] Official 
* [ ] Community

**Correct answer:**
* [x] Verified
* [x] Community
* [x] Official 

**10. Choose the ways available to download the module in your current terraform configuration directory.**


* [ ] terraform init
* [ ] terraform get
* [ ] terraform pull
* [ ] terraform module pull

**Correct answer:**
* [x] terraform init
* [x] terraform get

**11. Not specifying any version leads to what kind of behaviour of terraform.**


* [ ] It downloads the latest version available for that module.
* [ ] Throws an error
* [ ] We can specify the version along with the source argument. No extra arguments are needed
* [ ] None of the above

**Correct answer:**
* [x] It downloads the latest version available for that module.

---


##  Lab MCQ  Loops

**1. Resources using  the for_each appear as a ______ of objects when used in expressions.**


* [ ] map

**Correct answer:**
* [x] map

**2. Which of the following combinations of meta-arguments are correct.**


* [ ] depends_on, lifecycles, count, for_each
* [ ] depend_on, lifecycles, count, for_each
* [ ] depend_on, lifecycle, count, for-each
* [ ] depends_on, lifecycle, count, for_each

**Correct answer:**
* [x] depends_on, lifecycle, count, for_each

**3. What is the difference between for and for_each meta-argument**


* [ ] for provisions similar resources in module and resource blocks whereas for_each creates a list or map by iterating over a collection, such as another list or map.
* [ ] for_each provisions similar resources in module and resource blocks whereas for creates a list or map by iterating over a collection, such as another list or map.
* [ ] for_each and for both provide the same functionality in terraform
* [ ] There is no meta-argument named for in terraform.

**Correct answer:**
* [x] for_each provisions similar resources in module and resource blocks whereas for creates a list or map by iterating over a collection, such as another list or map.

**4. A given resource or module block cannot use both `count` and `for_each` simultaneously.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**5. Choose the correct option that best represents the meaning of count keyword:**


* [ ] Create resources in the format of a list, each identified by its index
* [ ] Create resources in the format of a map.
* [ ] Both are true under certain conditions
* [ ] None of the options

**Correct answer:**
* [x] Create resources in the format of a list, each identified by its index

**6. What are the characteristics of infrastructure objects created using for_each meta-argument when the configuration is applied:**


* [ ] Each is separately created
* [ ] Each is separately updated
* [ ] Each is separately destroyed
* [ ] All of the above

**Correct answer:**
* [x] All of the above

**7. Choose the meta-arguments specific to loops in terraform**


* [ ] for 
* [ ] for_each
* [ ] count
* [ ] All of the above

**Correct answer:**
* [x] All of the above

**8. The for_each meta-argument accepts:**


* [ ] map 
* [ ] set of strings
* [ ] list of strings
* [ ] Only alphanumeric characters

**Correct answer:**
* [x] map 
* [x] set of strings

**9. Which built-in function can be used in the count meta-argument to dynamically determine the size of the variable.**


* [ ] length()
* [ ] sizeof()
* [ ] len()
* [ ] size()

**Correct answer:**
* [x] length()

**10. In blocks where for_each is set, an additional _____ object is available in expressions, so you can modify the configuration of each instance.**


* [ ] each
* [ ] item
* [ ] value
* [ ] key

**Correct answer:**
* [x] each

**11. for_each value must be known before Terraform performs any remote resource actions.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

---


## Lab MCQ Provisioners

**1. Provisioners should only be considered as a Last Resort.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**2. Expressions in provisioner blocks cannot refer to their parent resource by name. They can use special _______ objects.**


* [ ] self
* [ ] Only the name of the resource without specifying its type
* [ ] There is no such special object
* [ ] None of the above

**Correct answer:**
* [x] self

**3. Fill the blank with the suitable choice:**


* [ ] destroy
* [ ] create
* [ ] apply
* [ ] plan 

**Correct answer:**
* [x] destroy

**Code**: 
resource "aws_instance" "web" {
  # ...

  provisioner "local-exec" {
    when    = ________
    command = "echo 'Destroy-time provisioner'"
  }
}


**4. By default, provisioners that fail will also cause the Terraform apply itself to fail, which argument can we use to change this behaviour?**


* [ ] on_failure
* [ ] fail
* [ ] failure_status
* [ ] None of the above

**Correct answer:**
* [x] on_failure

**5. Which statement best explains terraform provisioners?**


* [ ] used to model specific actions only on the local machine
* [ ] used to model specific actions only on the remote machine
* [ ] used to model specific actions on the local machine or on a remote machine
* [ ] None of the above

**Correct answer:**
* [x] used to model specific actions on the local machine or on a remote machine

**6. Which of the following provisioners does not need a `connection` block defined?**


* [ ] local-exec
* [ ] remote-exec
* [ ] Both
* [ ] None

**Correct answer:**
* [x] local-exec

**7. What are the possible consequences of making heavy usage of provisioners within your  terraform script?**


* [ ] Add a considerable amount of complexity and uncertainty to Terraform usage
* [ ] Terraform cannot model the actions of provisioners as part of a plan
* [ ] Use of provisioners requires coordinating many more details than Terraform usage usually requires
* [ ] All of the options

**Correct answer:**
* [x] All of the options

**8. Where should you add a `provisioner` block?**


* [ ] Nested block inside the resource block
* [ ] Outside the resource block
* [ ] Nested block inside the provider block
* [ ] Inside the terraform block

**Correct answer:**
* [x] Nested block inside the resource block

**9. What are the valid values available for argument `on_failure`:**


* [ ] continue
* [ ] true
* [ ] fail
* [ ] false

**Correct answer:**
* [x] continue
* [x] fail

**10. Which keyword is used for provisioner utilization within terraform script?**


* [ ] provision
* [ ] provisioner
* [ ] provider
* [ ] terraform

**Correct answer:**
* [x] provisioner

**11. Select the types of provisioners available:**


* [ ] local-exec
* [ ] remote-exec
* [ ] file
* [ ] all of the above

**Correct answer:**
* [x] all of the above

**12. What happens when provisioners fail to execute successfully?**


* [ ] Terraform will taint the resource that will be replaced on the next run
* [ ] Resources will be created but without the changes mentioned in the provisioner block
* [ ] It will show an error message showing the user to taint the resource manually
* [ ] None of the above

**Correct answer:**
* [x] Terraform will taint the resource that will be replaced on the next run

**13. How can you invoke the provisioner at the time when resources get destroyed?**


* [ ] using `when = destroy` within the `provisioner` block
* [ ] Provisioners are invoked by default when resources are destroyed
* [ ] using `invoke = destroy` within the `provisioner` block
* [ ] None of the above

**Correct answer:**
* [x] using `when = destroy` within the `provisioner` block

---


## Lab MCQ Lifecycle Rules

**1. Configuration _____  is the possibility that each of the servers vary from one another maybe in software, or configuration, or operating system etc.**


* [ ] Drift
* [ ] shift
* [ ] change
* [ ] None of the above

**Correct answer:**
* [x] Drift

**2. What are the common issues that come with configuration drift?**


* [ ] Make it difficult to plan and carry out subsequent updates.
* [ ] Troubleshooting issues would also be a difficult task
* [ ] All of the above
* [ ] Can leave the infrastructure in a complex state

**Correct answer:**
* [x] All of the above

**3. Which of the following statements is FALSE about Immutable infrastructure?**


* [ ] It doesn’t let us to execute in-place updates of the resources anymore
* [ ] It ensures updating this way will not lead to failures
* [ ] It takes care that infrastructure is in a simple and easy-to-understand state
* [ ] All are true

**Correct answer:**
* [x] It ensures updating this way will not lead to failures

**4. Select the available arguments that are available for the lifecycle meta-argument**


* [ ] create_before_destroy
* [ ] ignore_change
* [ ] prevent_destroy
* [ ] ignore_changes
* [ ] destroy_after_create

**Correct answer:**
* [x] create_before_destroy
* [x] prevent_destroy
* [x] ignore_changes

**5. Chose the option that  best describes “in-place updates”:**


* [ ] The underlying infrastructure remains the same
* [ ] Software and configuration of operating system changes as part of the update
* [ ] Both 
* [ ] Neither

**Correct answer:**
* [x] Both 

**6. Which argument of the lifecycle meta-argument supports a list as a value ?**


* [ ] create_before_destroy
* [ ] ignore_changes 
* [ ] prevent_destroy 
* [ ] All the options

**Correct answer:**
* [x] ignore_changes 

**Explaination**: ignore_changes (list of attribute names) - By default, Terraform detects any difference in the current settings of a real infrastructure object and plans to update the remote object to match configuration.

**7. Choose the possible scenario that the lifecycle block of the terraform resource satisfies**


* [ ] When you want the updated version of the resource to be created first before the older one is deleted.
* [ ] When you don’t want the resource to be deleted at all.
* [ ] Neither
* [ ] Both

**Correct answer:**
* [x] Both

---


## Lab MCQ Functions

**1. Which terraform command can be used to experiment with the behavior of Terraform's built-in functions?**


* [ ] terraform console
* [ ] terraform validate
* [ ] terraform check
* [ ] terraform terminal

**Correct answer:**
* [x] terraform console

**Explaination**: This command provides an `interactive command-line console` for evaluating and experimenting with expressions. This is useful for testing interpolations before using them in configurations, and for interacting with any values currently saved in state.

**2. General syntax for function calls is :**


* [ ] Function name followed by comma-separated arguments in parentheses.
* [ ] Function name followed by space-separated arguments in parentheses
* [ ] Both
* [ ] None

**Correct answer:**
* [x] Function name followed by comma-separated arguments in parentheses.

**3. We cannot use the `file` or `templatefile` function to read files that our configuration might generate dynamically on disk as part of the plan or apply steps.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**Documentation Link**: https://www.terraform.io/language/expressions/function-calls#when-terraform-calls-functions

**4. If the arguments for passing to a function are available in the form of list or tuple value, how would you expand that inside the function?**


* [ ] Provide the list value as an argument and follow it with the “...” symbol
* [ ] Provide the list value as an argument and follow it with the keyword “expand”.
* [ ] Terraform doesn’t support expanding arguments this way
* [ ] None of the above

**Correct answer:**
* [x] Provide the list value as an argument and follow it with the “...” symbol

**5. Choose the built-in function that returns the closest whole number that is greater than or equal to the given value:**


* [ ] ceil()
* [ ] floor()
* [ ] low()
* [ ] high()

**Correct answer:**
* [x] ceil()

**6. The Terraform language supports user-defined functions**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**7. Choose the built-in function that produces a list by dividing a given string at all occurrences of a given separator:**


* [ ] split(separator, string)
* [ ] trim(separator, string)
* [ ] format(separator, string)
* [ ] None of the above

**Correct answer:**
* [x] split(separator, string)

**8. Passing an object containing a sensitive input variable to the keys() function will result in a list that is _________ .**


* [ ] Sensitive
* [ ] Non-sensitive
* [ ] Random output
* [ ] None of the above.

**Correct answer:**
* [x] Sensitive

**9. Select the correct options that produce a string by concatenating together all elements of a given list of strings with the given delimiter:**


* [ ] join(separator, list)
* [ ] attach(separator, list)
* [ ] concat(separator, list)
* [ ] None of the above

**Correct answer:**
* [x] join(separator, list)

**10. Which of the following function is no longer available in terraform:**


* [ ] map()
* [ ] tomap()
* [ ] zipmap()
* [ ] None of the above

**Correct answer:**
* [x] map()

**Documentation Link**: https://www.terraform.io/language/functions/map#map-function

**11. Which of the following can be used to determine the length of a given list, map, or string.**


* [ ] length()
* [ ] len()
* [ ] size()
* [ ] count()

**Correct answer:**
* [x] length()

---


## Lab MCQ Dynamic Blocks

**1. Select the options that does not support dynamic block:**


* [ ] provisioner
* [ ] provider
* [ ] dynamic
* [ ] data
* [ ] resource
* [ ] locals

**Correct answer:**
* [x] locals

**Explaination**: A dynamic block can only generate arguments that belong to the resource type, data source, provider or provisioner being configured. It is not possible to generate meta-argument blocks such as lifecycle and provisioner blocks , since Terraform must process these before it is safe to evaluate expressions.

**2. Which nested argument defines the body of each generated block under dynamic block.**


* [ ] body
* [ ] content 
* [ ] data
* [ ] None of the above

**Correct answer:**
* [x] content 

**3. You can dynamically construct repeatable nested blocks using which special block type:**


* [ ] dynamic
* [ ] generate
* [ ] local
* [ ] None of the above

**Correct answer:**
* [x] dynamic

**Explaination**: You can dynamically construct repeatable nested blocks like setting using a special "dynamic block" type, which is supported inside "resource", "data", "provider", and "provisioner" blocks.

**4. Can we use `dynamic` blocks to generate `meta-argument` blocks such as `lifecycle` and `provisioner` blocks?**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**Explaination**: A dynamic block can only generate arguments that belong to the resource type, data source, provider, or provisioner being configured. It is not possible to generate meta-argument blocks such as lifecycle and provisioner blocks, since Terraform must process these before it is safe to evaluate expressions.

**5. Is it possible to declare the `dynamic` block inside another `dynamic` block?**


* [ ] Yes
* [ ] No

**Correct answer:**
* [x] Yes

**Explaination**: Some providers define resource types that include multiple levels of blocks nested inside one another. You can generate these nested structures dynamically when necessary by nesting dynamic blocks in the content portion of other dynamic blocks.

**Documentation Link**: https://developer.hashicorp.com/terraform/language/expressions/dynamic-blocks#multi-level-nested-block-structures

**6. Select the best practices around using dynamic block.**


* [ ] Use them when you need to hide details in order to build a clean user interface for a reusable module.
* [ ] Overuse of dynamic blocks can make configuration hard to read and maintain
* [ ] Always write nested blocks out literally where possible.
* [ ] All of the options

**Correct answer:**
* [x] All of the options

**Documentation Link**: https://www.terraform.io/language/expressions/dynamic-blocks#best-practices-for-dynamic-blocks

---


## Lab MCQ Terraform Cloud

**1. Terraform Cloud is an application that helps teams use Terraform together.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**2. Which of the following is true for terraform cloud?**


* [ ] It manages Terraform runs in a consistent and reliable environment
* [ ] It includes easy access to shared state and secret data, access controls for approving changes to infrastructure
* [ ] It provides a private registry for sharing Terraform modules
* [ ] It has detailed policy controls for governing the contents of Terraform configurations
* [ ] All of the above

**Correct answer:**
* [x] All of the above

**Documentation Link**: https://learn.hashicorp.com/tutorials/terraform/cloud-sign-up?in=terraform/cloud-get-started#what-is-terraform-cloud

**3. Remote Terraform execution is sometimes referred to as "____________".**


* [ ] remote operations
* [ ] remote workflows
* [ ] remote pipelines
* [ ] None of the above

**Correct answer:**
* [x] remote operations

**4. By default terraform cloud runs terraform on:**


* [ ] its own cloud infrastructure
* [ ] on your own isolated, private, or on-premises infrastructure
* [ ] You need to specify each time you run terraform commands
* [ ] None of the above

**Correct answer:**
* [x] its own cloud infrastructure

**5. What is `Sentinel`?**


* [ ] Policy as code framework for HashiCorp Enterprise Products
* [ ] Infrastructure as code framework for Terraform
* [ ] Both 
* [ ] None of the above

**Correct answer:**
* [x] Policy as code framework for HashiCorp Enterprise Products

**6. What concept is used by `terraform cloud` to manage infrastructure collections instead of directories?**


* [ ] Workspaces 
* [ ] Different accounts
* [ ] Different repositories
* [ ] None of the above

**Correct answer:**
* [x] Workspaces 

**7. Which of the following options doesn’t involve when using Sentinel with Terraform Cloud:**


* [ ] Defining the policies
* [ ] Managing policies for organizations
* [ ] Enforcing policy checks on runs
* [ ] Mocking Sentinel Terraform data
* [ ] None of the above.

**Correct answer:**
* [x] None of the above.

**8. Terraform Enterprise and Terraform Cloud are the same application**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**9. Is `Terraform Cloud workspaces` same as `Terraform CLI Workspaces`.**


* [ ] Yes
* [ ] No

**Correct answer:**
* [x] No

**10. What are the advantages of using `Terraform Cloud's private registry`:**


* [ ] Helps you share `Terraform providers` and `Terraform modules` across your organization
* [ ] Includes support for versioning, a searchable list of available providers and modules
* [ ] Includes support for a `configuration designer` to help you build new workspaces faster
* [ ] All of the above

**Correct answer:**
* [x] All of the above

**11. Which feature is used to manage how members of your organization can use modules from the  terraform private registry?**


* [ ] Using IAM policies
* [ ] Using sentinel policies 
* [ ] Terraform provides no such feature
* [ ] None of the above

**Correct answer:**
* [x] Using sentinel policies 

---


## Python PCAP Mock Exam 1

**1. What does the pyc extension for Python files mean?<br><br> A. compiled python code<br><br> B. python interpreted code<br><br> C. files generated each time we run our python files<br><br> D. files generated when we import a module for the first time**


* [ ] A, D
* [ ] B, C
* [ ] A, C
* [ ] B, D

**Correct answer:**
* [x] A, D

**2. "from all.foo import bar" will import:**


* [ ] entity bar from module all from package foo
* [ ] entity bar from module foo from package all
* [ ] entity foo from module bar from package all
* [ ] entity foo from module all from package bar

**Correct answer:**
* [x] entity bar from module foo from package all

**3. What is the output of `pip list`?**


* [ ] a list of the installed local packages
* [ ] a list of available pip commands
* [ ] a list of packages available online
* [ ] a list of recently installed packages

**Correct answer:**
* [x] a list of the installed local packages

**4. Which of the following variables outputs the name of a python module?**


* [ ] \_\_name\_\_
* [ ] \_\_main\_\_
* [ ] \_\_modulename\_\_
* [ ] \_\_main\_name\_\_

**Correct answer:**
* [x] \_\_name\_\_

**5. What is true of the following statements with regard to the platform module?<br><br> A. system returns OS name<br><br> B. version returns OS version<br><br> C. processor returns the number of processes running on our OS<br><br> D. version returns python version**


* [ ] A, B
* [ ] C, D
* [ ] A, D
* [ ] B, C

**Correct answer:**
* [x] A, B

**6. What is inside the  \_\_pycache\_\_ directory?**


* [ ] pyc files created after we import a module
* [ ] pyc files created when we run our code
* [ ] pyc files created by the \_\_init\_\_ directory of a package
* [ ] pyc files created when we run our python files

**Correct answer:**
* [x] pyc files created after we import a module

**7. What is the output of the following snippet?**


* [ ] 0\*\*0\*\*
* [ ] 0\*\*1\*\*, or 1\*\*0\*\*
* [ ] 1\*\*1\*\*
* [ ] 0\*\*1\*\*, or 1\*\*0\*\*, or 1\*\*1\*\*, or 0\*\*0\*\*

**Correct answer:**
* [x] 0\*\*1\*\*, or 1\*\*0\*\*, or 1\*\*1\*\*, or 0\*\*0\*\*

**Code**: 
from random import randint

for i in range(2):
          print(randint(0, 1), end='**')

**8. How can we list all properties of a module?**


* [ ] dir()
* [ ] list
* [ ] \_\_dir\_\_
* [ ] dict

**Correct answer:**
* [x] dir()

**9. What is the output of the following code snippet?**


* [ ] 1
* [ ] 0
* [ ] Error
* [ ] False

**Correct answer:**
* [x] 1

**Code**: 
import math

res = math.pi != math.pow(2, 4)
print(int(res))

**10. What is the correct term in the world of exceptions?**


* [ ] an exception is raised
* [ ] an exception is thrown
* [ ] an exception is handled

**Correct answer:**
* [x] an exception is raised

**11. What is the output of the following code snippet:**


* [ ] d
* [ ] f
* [ ] b
* [ ] 5

**Correct answer:**
* [x] f

**Code**: 
print(chr(ord('a') + 5))

**12. In case we have multiple exception branches in our code, which one is going to be executed?**


* [ ] As many as required in handling the various errors
* [ ] The first matching exception branch
* [ ] The last matching exception branch

**Correct answer:**
* [x] The first matching exception branch

**13. What is the output of the following code snippet:**


* [ ] False
* [ ] True

**Correct answer:**
* [x] False

**Code**: 
print("abcd" > "abcde")

**14. What is an alternative name to PyPi?**


* [ ] Cheese Shop
* [ ] Spec and Eggs
* [ ] Monty Python
* [ ] Python shop

**Correct answer:**
* [x] Cheese Shop

**15. The except branch without a name … :**


* [ ] should be the first branch of a try/except block
* [ ] should be the last branch of a try/except block
* [ ] no specific order is required for all the exception branches

**Correct answer:**
* [x] should be the last branch of a try/except block

**16. What is true of the following statements?<br><br> A. isalnum() is a function<br><br> B. isalnum() is a method<br><br> C. isalnum() returns True if a string contains alphanumeric values<br><br> D. isalnum() returns True if a string contains only numbers**


* [ ] B, C
* [ ] B, D
* [ ] A, C
* [ ] A, D

**Correct answer:**
* [x] B, C

**17. What is the output of the following code snippet:**


* [ ] 1
* [ ] 3
* [ ] 2
* [ ] 0

**Correct answer:**
* [x] 1

**Code**: 
print(ord('f') - ord('e'))

**18. Select the options which will return an error:**


* [ ] 'python'.rfind('')
* [ ] 'python'.sort()
* [ ] 'python'.index('')

**Correct answer:**
* [x] 'python'.sort()

**19. What is the output of the following code snippet?**


* [ ] did something go wrong?
* [ ] random takes at least one argument
* [ ] exiting the code
* [ ] None of these

**Correct answer:**
* [x] None of these

**Code**: 
import random

try:
    random.random()
except TypeError:
    print("random takes at least one argument")
else:
    print("did something go wrong?")
finally:
    print("exiting the code")

**20. What is the output of the following string method?**


* [ ] 'error' : error :
* [ ] ('error',) : error :
* [ ] error : error :
* [ ] error, : error :

**Correct answer:**
* [x] ('error',) : error :

**Code**: 
try:
    raise Exception("error")
except Exception as e:
    print(e.args, e.__str__(), sep=' : ' ,end=' : ')

**21. What is the outcome of the following code?**


* [ ] 3 (1, 2, 3)
* [ ] TypeError: object of type 'Exception' has no len()
* [ ] (1, 2, 3)
* [ ] 3, 1, 2, 3

**Correct answer:**
* [x] 3 (1, 2, 3)

**Code**: 
try:
    raise Exception(1,2,3)
except Exception as e:
    print(len(e.args), e)

**22. What is the result of the following code?**


* [ ] it will raise an error
* [ ] alpha
* [ ] beta

**Correct answer:**
* [x] alpha

**Code**: 
class Alpha:
    def __str__(self):
        return 'alpha'


class Beta:
    def __str__(self):
        return 'beta'


class C(Alpha, Beta):
    def str(self):
        pass


o = C()
print(o)

**23. What is the output of the following code?**


* [ ] False
* [ ] True
* [ ] it will raise an exception

**Correct answer:**
* [x] it will raise an exception

**Code**: 
class A:
    def __init__(self):
        pass

a = A(1)

**24. What is the output of the following string method?**


* [ ] SyntaxError
* [ ] True
* [ ] False
* [ ] TypeError

**Correct answer:**
* [x] True

**Code**: 
("¾").isnumeric()

**25. What is the result of the following code?**


* [ ] 3
* [ ] 2
* [ ] 1
* [ ] code is erroneous

**Correct answer:**
* [x] 3

**Code**: 
class A:
    def __init__(self, a = 1):
        self.a = a
    def swap(self, a):
        self.a *= a
        return a


a = A(2)
print(a.swap(a.a + 1))

**26. What is the result of the following code?**


* [ ] ee
* [ ] e
* [ ] an empty line

**Correct answer:**
* [x] e

**Code**: 
class E(Exception):
    def __init__(self,msg):
        Exception.__init__(self,msg * 2)
        self.args = (msg,)


try:
    raise E('e')
except E as e:
    print(e)
except Exception as e:
    print(e)

**27. What is the result of the following code? <br>a.<br>  A__init__(self, b)  <br><br>b.<br>  A__init__(self)  <br><br>c.<br>  super().__init__(b)  <br><br>d.<br>  super().__init__(a)<br>**


* [ ] A, C
* [ ] B, D
* [ ] A, D
* [ ] B, C

**Correct answer:**
* [x] A, C

**Code**: 
class A:
    def __init__(self, a):
        self.a = a


class B(A):
    def __init__(self, b):
        # Put the correct line here
        self.b = b



**28. What is the result of the following code?**


* [ ] k
* [ ] h
* [ ] it will raise an error

**Correct answer:**
* [x] h

**Code**: 
class A:
    def __str__(self):
        return 'k'


class B(A):
    def __str__(self):
        return 'h'


class C(B):
    pass


c = C()
print(c)

**29. What is the result of the following code?**


* [ ] the code is erroneous
* [ ] 3
* [ ] 2
* [ ] ValueError

**Correct answer:**
* [x] 3

**Code**: 
class A:
    A = 0
    def __init__(self,v = 0):
        self.Y = v
        A.A += v


a = A()
b = A(1)
c = A(2)
print(c.A)

**30. What is the result of the following code?**


* [ ] a generator object
* [ ] the code will raise an exception
* [ ] 0 1 2
* [ ] [0, 1, 2]

**Correct answer:**
* [x] 0 1 2

**Code**: 
def fun():
    for num in range(3):
        yield num


for i in fun():
    print(i, end=" ")

**31. What is the result of the following code?**


* [ ] ception, unhandled exception
* [ ] ceptionextion
* [ ] tionextion
* [ ] tionex

**Correct answer:**
* [x] ceptionextion

**Code**: 
def f(n):
    try:
        x = n / n
    except:
        print("ex",end='')
    else:
        print("cep",end='')
    finally:
        print("tion",end='')

f(1)
f(0)

**32. Which of the following statements are true?<br><br> <b>A.</b><br> the map() function takes only two arguments<br><br> <b>B.</b><br> the map() function may take more than two arguments<br><br> <b>C.</b><br> the filter() function returns an iterator<br><br> <b>D.</b><br> both map() and filter() functions' arguments order are first "iterable" then the "function"**


* [ ] B, D
* [ ] A, C
* [ ] B, C

**Correct answer:**
* [x] B, C

**33. What is the result of the following code?**


* [ ] 3 5
* [ ] 1, 7, 9
* [ ] 5 3
* [ ] [1. 7, 9]

**Correct answer:**
* [x] 3 5

**Code**: 
numbers = (1, 3, 5, 7, 9)

def filter_nums(num):
    nums = (0, 5, 17, 3)
    if num in nums:
        return True
    else:
        return False


filtered = filter(filter_nums, numbers)
for num in filtered:
    print(num, end=" ")

**34. Which is the correct option to complete the following code?<br><br>\# output should be<br> (5, 7)**


* [ ] o = tuple(filter(lambda x: x==1 and x==3, tup))
* [ ] o = list(filter(lambda x: x>1 and x>3, tup))
* [ ] o = tuple(filter(lambda x: x-1 and x-3, tup))
* [ ] o = list(filter(lambda x: x-1 and x-3, tup))

**Correct answer:**
* [x] o = tuple(filter(lambda x: x-1 and x-3, tup))

**Code**: 
tup = (1, 3, 5, 7)

**35. What is the result of the following code?**


* [ ] b'\x00\x00'
* [ ] 2
* [ ] bytearray(b'\x00\x00')

**Correct answer:**
* [x] bytearray(b'\x00\x00')

**Code**: 
b = bytearray(2)
print(b)

**36. Assuming the following code is executed, which of the following commands would delete `exercises`?**


* [ ] os.rmdir("./python/*")
* [ ] os.rmdir("./exercises")
* [ ] os.removedirs("./python/exercises")
* [ ] None of these

**Correct answer:**
* [x] None of these

**Code**: 
import os

os.mkdir('python')
os.chdir('python')
os.mkdir('exercises')
os.chdir('exercises')
f = open('f', 'w')
f.close()
os.chdir('../')

**37. How can we read a bytes file in Python?**


* [ ] with the function read(bytearray)
* [ ] with the function readlines(bytearray)
* [ ] with the function readinto(bytearray)
* [ ] with the function readbytes()

**Correct answer:**
* [x] with the function readinto(bytearray)

**38. How can we execute command line code from within Python?**


* [ ] os.system("ls -a")
* [ ] os.machine("ls -a")
* [ ] os.path("ls -a")
* [ ] os("ls -a")

**Correct answer:**
* [x] os.system("ls -a")

**Code**: 
import os

**39. Which option can replace the list comprehension and return the same output?**


* [ ] foo = filter(lambda x: x % 2 == 0 [x for x in range(5)])
* [ ] foo = map(lambda x: x % 2 == 0 [x for x in range(5)])
* [ ] foo = map(lambda x: x % 2 == 0, [x for x in range(5)])
* [ ] foo = filter(lambda x: x % 2 == 0, [x for x in range(5)])

**Correct answer:**
* [x] foo = filter(lambda x: x % 2 == 0, [x for x in range(5)])

**Code**: 
foo = (x for x in range(5) if x % 2 == 0)
for i in foo:
    print(i)

**40. Assuming the below code has been executed:<br><br>How should we call the closure function html() so that the output is:<br> `<b>Python</b>`**


* [ ] <div style="text-align: left"> html('<<bb>b>') </div><br><div style="text-align: left"> html('python') </div>
* [ ] <div style="text-align: left"> html('<<bb>b>') </div><br><div style="text-align: left"> h = tag('python') </div>
* [ ] <div style="text-align: left"> h = html('<<bb>b>') </div><br><div style="text-align: left"> h('python') </div>
* [ ] <div style="text-align: left"> h = html('<<bb>b>', '<</b>/b>') </div><br><div style="text-align: left"> h('python') </div>

**Correct answer:**
* [x] <div style="text-align: left"> h = html('<<bb>b>') </div><br><div style="text-align: left"> h('python') </div>

**Code**: 
def html(t):
    t2 = t[0] + '/' + t[1:]

    def bold(str):
        return t + str.title() + t2
    return bold

---


## Python PCAP Mock Exam 2

**1. How is the python dedicated repository for libraries called?**


* [ ] PyPi
* [ ] PyCode
* [ ] pyRepo
* [ ] PyLab

**Correct answer:**
* [x] PyPi

**2. What is true of the following statements?**


* [ ] modules can contain other modules
* [ ] modules can contain packages
* [ ] packages can contain modules

**Correct answer:**
* [x] packages can contain modules

**3. How do we mark a module property as private?<br><br> A. with prefix \_\_<br><br> B. with prefix \_<br><br> C. \#<br><br> D. triple quotes**


* [ ] A, B
* [ ] A, C
* [ ] D, B

**Correct answer:**
* [x] A, B

**4. How can we import hypot from math?**


* [ ] from math import hypot
* [ ] import hypot from math
* [ ] import math.hypot

**Correct answer:**
* [x] from math import hypot

**5. What is a namespace in Python?**


* [ ] a space in which a collection of names exist
* [ ] a space with a name
* [ ] a collection of variables

**Correct answer:**
* [x] a space in which a collection of names exist

**6. pip means**


* [ ] python installation of packages
* [ ] py installation process
* [ ] pip install packages

**Correct answer:**
* [x] pip install packages

**7. What is the output of the following code snippet?**


* [ ] it would return one of the numbers in [0, 5, 10]
* [ ] it would return one of the numbers in [0, 5, 10, 15]
* [ ] it would return one of the numbers in [5, 10, 15]

**Correct answer:**
* [x] it would return one of the numbers in [0, 5, 10]

**Code**: 
from random import randrange

randrange(0, 15, 5)

**8. Which of the following options return True?**


* [ ] math.trunc(5.5) > math.floor(5.5)
* [ ] math.ceil(0.2) == math.floor(1.2)
* [ ] math.factorial(2) == math.floor(1.8)
* [ ] math.hypot(3,4) != math.sqrt(25)

**Correct answer:**
* [x] math.ceil(0.2) == math.floor(1.2)

**9. What is true about the \_\_init\_\_ directory in Python projects?<br><br> A. It contains info about a python package<br><br> B. It contains code that needs to run upon the import of a package's modules<br><br> C. It is added as a distinguisher between a package and a module directory<br><br> D. It may be empty**


* [ ] A, B
* [ ] B, C
* [ ] B, C, D
* [ ] A, B, C

**Correct answer:**
* [x] B, C, D

**10. Which of the options can complete the following import when we want to call the function fun of module mod?**


* [ ] mod.fun
* [ ] mod().fun()
* [ ] mod.fun()
* [ ] mod fun()

**Correct answer:**
* [x] mod.fun()

**Code**: 
import mod

**11. Which of the following statements are correct?<br><br> A. We can only have one except branch in our try/except block<br><br> B. We can have more than one except branch in our try/except block<br><br> C. We can have multiple name exceptions branches as long as they have unique names<br><br> D. We can have multiple name exceptions handling and they may have unique names**


* [ ] A, D
* [ ] B, D
* [ ] B, C
* [ ] A, C

**Correct answer:**
* [x] B, C

**12. Where should we place the code that handles an exception?**


* [ ] inside the 'except as Exception' branch
* [ ] inside the 'try:' branch
* [ ] inside the 'else:' branch after a try/except block
* [ ] inside the 'finally:' branch after a try/except block

**Correct answer:**
* [x] inside the 'except as Exception' branch

**13. The expression:**


* [ ] is True
* [ ] is False
* [ ] returns an error

**Correct answer:**
* [x] is True

**Code**: 
'aBc' > 'ABC'

**14. ASCII stands for:**


* [ ] <div style="text-align: left"> American Standard Code for Information Interchange </div>
* [ ] <div style="text-align: left"> American Studies and Code for Information Interchanged </div>
* [ ] <div style="text-align: left"> American Standard Computer for Information Intelligence </div>
* [ ] <div style="text-align: left"> Asynchronous Standard Computer Information Intelligence </div>

**Correct answer:**
* [x] <div style="text-align: left"> American Standard Code for Information Interchange </div>

**15. What is the output of the following code snippet?**


* [ ] Am error
* [ ] Arithmetics error
* [ ] Zero error
* [ ] TypeError

**Correct answer:**
* [x] Arithmetics error

**Code**: 
try:
    print(1/0)
except ArithmeticError:
    print("Arithmetics error")
except ZeroDivisionError:
    prin("Zero error")
except Exception:
    print("An error")

**16. Which of the following exceptions are specific and are not parents to other subclass exceptions?<br><br> A. IndexError<br><br> B. ImportError<br><br> C. ValueError<br><br> D. AttributeError**


* [ ] B, C
* [ ] A, D
* [ ] A, B
* [ ] B, D

**Correct answer:**
* [x] A, D

**17. What is the output of:**


* [ ] True
* [ ] False
* [ ] 0
* [ ] 1

**Correct answer:**
* [x] False

**Code**: 
'Rose' > 'Roses'

**18. What is the output of the following code snippet?**


* [ ] APNUF
* [ ] PNUF
* [ ] FUNPPNUF
* [ ] ANUF

**Correct answer:**
* [x] APNUF

**Code**: 
string = 'FUNPARK'[:5:]
string = string[-1] + string[-2::-1]
print(string)

**19. What is the output of the following code snippet?**


* [ ] 0
* [ ] 1
* [ ] 3
* [ ] 33

**Correct answer:**
* [x] 0

**Code**: 
x = str(1 // 3)
dummy = ''
for i in x:
    x = x + dummy

print(x[-1])

**20. What is the purpose of the function `super()` in the subclass?**


* [ ] <div style="text-align: left"> it changes the inheritance chain </div>
* [ ] <div style="text-align: left"> we can access the parent's class properties and methods when building the subclass </div>
* [ ] <div style="text-align: left"> it is a convention to make our subclass inheritance more clear </div>

**Correct answer:**
* [x] <div style="text-align: left"> we can access the parent's class properties and methods when building the subclass </div>

**21. How can we check whether a class E is a subclass of A?**


* [ ] issubclass(E, A)
* [ ] isinstance(E, A)
* [ ] hassubclass(A, E)

**Correct answer:**
* [x] issubclass(E, A)

**22. What is the output of the following code?**


* [ ] <div style="text-align: left"> test exception </div><br><div style="text-align: left"> RuntimeError No active exception to reraise </div><br><div style="text-align: left"> AttributeError: 'RuntimeError' object has no attribute 'message' </div>
* [ ] <div style="text-align: left"> test exception </div><br><div style="text-align: left"> error custom exception custom message </div>
* [ ] <div style="text-align: left"> test exception </div><br><div style="text-align: left"> error custom exception custom message </div><br><div style="text-align: left"> Done </div>
* [ ] <div style="text-align: left"> test exception </div><br><div style="text-align: left"> Unhanddled exception </div>

**Correct answer:**
* [x] <div style="text-align: left"> test exception </div><br><div style="text-align: left"> RuntimeError No active exception to reraise </div><br><div style="text-align: left"> AttributeError: 'RuntimeError' object has no attribute 'message' </div>

**Code**: 
class E(Exception):
    def __init__(self, message):
        self.message = message

    def __str__(self):
        return 'custom exception'

try:
    print('test exception')
    raise
except BaseException as e:
    print('error', e, e.message)
else:
    print('Done')

**23. Which of the following options return `18` if the code below is executed:<br><br>class Rectangle():<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;def __init__(self, length, width):<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.length = length<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.width = width<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;def surface(self):<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return self.length * self.width**


* [ ] A, D
* [ ] A, B
* [ ] B, C

**Correct answer:**
* [x] A, B

**Code**: 
  ---
| A. |
 ---
class Cube(Rectangle):
    def density(self, height):
        surface = super().surface()
        return surface * height

c = Cube(3, 2)
c.density(3)

 ---
| B. |
 ---
class Cube(Rectangle):
    def __init__(self, height, length, width):
        self.height = height
        super().__init__(length, width)

    def density(self):
        surface = super().surface()
        return surface * self.height

c = Cube(3, 2)
c.density(3)

 ---
| C. |
 ---
class Cube(Rectangle):
    def __init__(self, height):
        self.height = height

    def density(self):
        surface = Rectangle.surface()
        return surface * self. height

c = Cube(3, 2)
c.density(3)
 
 ---
| D. |
 ---
class Cube(Rectangle):
    def __init__(self, height):
        self.height = height

    def density(self):
        surface = super.surface()
        return surface * self. height

c = Cube(3, 2)
c.density(3)

**24. A variable that is part of an object is called:**


* [ ] an instance variable
* [ ] an object variable
* [ ] a class variable

**Correct answer:**
* [x] an instance variable

**25. What we call a stack in data structure, we can also call:**


* [ ] LIFO
* [ ] FILO
* [ ] There is no such thing

**Correct answer:**
* [x] LIFO

**26. What is the result of the following code?**


* [ ] True
* [ ] False
* [ ] it will raise an error

**Correct answer:**
* [x] True

**Code**: 
class A:
    pass

class B(A):
    pass

class C(B):
    pass

class D(C, B):
    pass

print(issubclass(C,A))

**27. What is the result of the following code?**


* [ ] <div style="text-align: left"> test </div><br><div style="text-align: left"> error custom exception my message </div>
* [ ] <div style="text-align: left"> test </div><br><div style="text-align: left"> error custom exception my message </div><br><div style="text-align: left"> done </div>
* [ ] <div style="text-align: left"> test </div><br><div style="text-align: left"> error E my message </div>

**Correct answer:**
* [x] <div style="text-align: left"> test </div><br><div style="text-align: left"> error custom exception my message </div>

**Code**: 
class E(Exception):
    def __init__(self, message):
        self.message = message

    def __str__(self):
        return 'custom exception'

try:
    print('test')
    raise E('my message')
except BaseException as e:
    print('error', e, e.message)
else:
    print('Done')

**28. Which of the following code snippets won't raise an unhandled exception?**


* [ ] A
* [ ] B
* [ ] C
* [ ] They will all raise an unhandled exception

**Correct answer:**
* [x] B

**Code**: 
  ---
| A. |
 ---

try:
    x = y + 1
except NameError, SystemError:
    x = y + 1

 ---
| B. |
 ---
try:
    x = y + 1
except (NameError, SystemError):
    y = 1
 ---
| C. |
 ---
try:
    x = y + 1
except (NameError, SystemError):
    y = y + 1

**29. Assuming the following code is executed, which of the following options return True:<br><br>A.  'val' in a.\_\_dict\_\_<br><br> B.  'Var' in a.\_\_dict\_\_<br><br> C.  'var' in A.\_\_dict\_\_<br><br> D.  'Var' in A.\_\_dict\_\_**


* [ ] A, D
* [ ] B, C

**Correct answer:**
* [x] A, D

**Code**: 
class A:
    Var = 1
    def __init__(self, val)
        self.val = value = 2

a = A()

**30. Which of the following statements are true?<br><br> A. the map() function creates a copy of its iterable to apply the function argument<br><br> B. A list comprehension can become a generator when we replace the square brackets with parentheses<br><br> C. lambda functions cannot take more than certain number of arguments<br><br> D. the yield keyword is interchangeable with the return keyword**


* [ ] B, D
* [ ] A, C
* [ ] B, A

**Correct answer:**
* [x] B, A

**31. Assuming the code below has been executed, select the answers that won't raise a MRO error:-<br><br>A. class Multi(A, B): pass<br><br> B. class Multi2(C, A): pass<br><br> C. class Multi3(D, B): pass<br><br> D. class Multi4(B, C): pass**


* [ ] B, C
* [ ] C, D
* [ ] A, C
* [ ] A, B

**Correct answer:**
* [x] B, C

**Code**: 
class A:
    pass

class B(A):
    pass

class C(B):
    pass

class D(C, B):
    pass

**32. What is the result of the following code if "bar" exists?**


* [ ] EEXIST, short for FileExistsError
* [ ] DEXIST, short for DirectoryExistsError
* [ ] the bar directory will be overwritten
* [ ] python will prompt for user's confirmation

**Correct answer:**
* [x] EEXIST, short for FileExistsError

**Code**: 
try:
    os.mkdir("foo/bar")
except Exception as e:
    print(e.message)

**33. What is the result of the following code?**


* [ ] obr
* [ ] fb
* [ ] fbfbfbfbfbfb
* [ ] obrobrobrobrobrobr

**Correct answer:**
* [x] fb

**Code**: 
def iter():
    s = 'foobar'
    for letter in s[::3]:
        yield letter


for i in iter():
    print(i, end='')

**34. Which option can fill in the missing line?<br><br>\# result should be (1, 4, 27)**


* [ ] bar = tuple(map(lambda a: a**a, my_list))
* [ ] bar = list(map(lambda a: a**a, my_list))
* [ ] bar = tuple(map(lambda a: a*a, my_list))

**Correct answer:**
* [x] bar = tuple(map(lambda a: a**a, my_list))

**Code**: 
my_list = [1, 2, 3]
# Add line of code here
print(bar)

**35. Which of the following statements are true?<br><br> A. lambda functions are anonymous functions<br><br> B. lambda functions are like other python functions<br><br> C. lambda functions can evaluate only one expression<br><br> D. lambda functions can take only up to two arguments**


* [ ] A, C
* [ ] A, D
* [ ] B, C
* [ ] C, D

**Correct answer:**
* [x] A, C

**36. What is the result of the following code?**


* [ ] ['.', 'file.txt', 'tmp']
* [ ] ['file.txt', 'tmp']
* [ ] ['tmp', 'file.txt']

**Correct answer:**
* [x] ['file.txt', 'tmp']

**Code**: 
import os

os.mkdir("foo")
os.chdir("foo")
os.mkdir("tmp")
f = open("file.txt", "w")
f.close()
os.listdir()

**37. How can we open a non-human readable file in Python?**


* [ ] A
* [ ] B
* [ ] C

**Correct answer:**
* [x] A

**Code**: 
 ---
| A. |
 ---
f = open("bytes.jpeg", "rb")
bf = bytearray(f.read(100))

 ---
| B. |
 ---
f = open("bytes.jpeg", "rb")
bf = bytearray(f)

 ---
| C. |
 ---
f = open("bytes.jpeg", "r")
bf = bytearray(f.read())

**38. What is the result of the following code?**


* [ ] \***
* [ ] \**
* [ ] \*
* [ ] \****

**Correct answer:**
* [x] \***

**Code**: 
def foobar(p):
    def bar():
        return '*' * p
    return bar


foo = foobar(1)
bar = foobar(2)
print(foo() + bar())

**39. What is the result of the following code?**


* [ ] ++++++
* [ ] ++++
* [ ] ++
* [ ] it will throw an error

**Correct answer:**
* [x] ++++++

**Code**: 
def my_fun(n):
    a = '+'
    for i in range(n):
        a *= 2
        yield a


for i in my_fun(2):
    print(i, end='')

**40. Assuming the following code is executed, which of the following statements are correct?<br><br>A.   opens the file file.txt in write mode<br><br> B. deletes the file contents if the file file.txt already exists<br><br> C. leaves the file contents unchanged if the file file.txt already exists<br><br> D. creates the file file.txt if it does not exist<br><br> E. raises the FileNotFoundError exception if the file does not exist**


* [ ] A, C, E
* [ ] A, B, E
* [ ] A, B, D

**Correct answer:**
* [x] A, B, D

**Code**: 
f = open("file.txt", "w")
f.close()

**Explaination**: f = open("file.txt", "w")
f.close()

---


## Terraform Mock  Exam 1

**1. What is Immutable Infrastructure?**


* [ ] Resources once deployed are not intended to be changed
* [ ] Resources cannot be migrated to another platform
* [ ] Any aspect of a resource can be updated in place anytime 
* [ ] Resources strictly provisioned by Terraform

**Correct answer:**
* [x] Resources once deployed are not intended to be changed

**Explaination**: Immutable infrastructure is another paradigm in which it ensures that resources are never modified after they have been deployed.
If a change is to be made, a new instance of that resource will be provisioned in place of the old one.

**2. Select the file extension used by terraform configuration files.**


* [ ] .TF
* [ ] .YAML
* [ ] .TOML
* [ ] .DAT
* [ ] None of the Above

**Correct answer:**
* [x] .TF

**3. What does “IaC” stand for?**


* [ ] Infrastructure as Code
* [ ] Initialization as Code
* [ ] Code as Infrastructure
* [ ] None of the above

**Correct answer:**
* [x] Infrastructure as Code

**Explaination**: IaC stands for Infrastructure as Code.

**4. A simple terraform configuration file is given below. What is the name of the resource that will be created?**


* [ ] pet
* [ ] local_file
* [ ] pets.txt
* [ ] local

**Correct answer:**
* [x] pet

**Code**: 
resource "local_file" "pet" {​
  filename = "/root/pets.txt"​
  content = "We love pets!"​ ​
}


**Explaination**: The name of the resource is "pet" which is a local_file type resource.

**5. Which "terraform command"  from the following downloads the latest version of the provider plugins?**


* [ ] terraform plan
* [ ] terraform init
* [ ] terraform apply
* [ ] terraform pull

**Correct answer:**
* [x] terraform init

**Documentation Link**: https://www.terraform.io/docs/cli/commands/init.html

**6. Choose the correct terraform command to display the blueprint of the infrastructure to be applied.**


* [ ] terraform init
* [ ] terraform apply
* [ ] terraform plan
* [ ] terraform show

**Correct answer:**
* [x] terraform plan

**7. Observe the below code and determine the providers used.**


* [ ] Local_file and random_pet
* [ ] Pet_name and my-pet
* [ ] Local and random
* [ ] Local_file, random_pet, pet_name, my-pet

**Correct answer:**
* [x] Local and random

**Code**: 
resource "local_file" "pet_name" {
            content = "We love pets!"
            filename = "/root/pets.txt"
}
resource "random_pet" "my-pet" {
              prefix = "Mrs"
              separator = "."
              length = "1"
}


**8. Whenever the target APIs change or when new functionality is added, the provider maintainers may update new versions for a provider. This may lead to unexpected infrastructure changes. What is the best approach to overcome this?**


* [ ] Never touch what you don’t understand
* [ ] Use required_providers block to clearly define the provider version you want to use
* [ ] API changes does not affect the provider usage within terraform
* [ ] There would be no issue as terraform always downloads the latest version of the provider

**Correct answer:**
* [x] Use required_providers block to clearly define the provider version you want to use

**Explaination**: The functionality of a provider plugin may vary drastically from one version to another. 
Our terraform configuration may not work as expected when using a version different than the one it was written in. As a best practice, always declare the exact version of the provider we want to use within the required_providers block.


**9. Where can we make use of version constraints?**


* [ ] a. Modules
* [ ] b. Provider requirements
* [ ] c. The required_version setting in the terraform block
* [ ] d. All of the above

**Correct answer:**
* [x] d. All of the above

**Explaination**: Version constraints can be used anywhere terraform allows us to specify versions. Most commonly they can be set at:

1. Within the provider version configuration (Inside the required_providers block nested inside the terraform block)
2. The "required_version" argument which is used to set the version of Terraform to use.
3. Within modules. This is where we specify the version of module to be used. 

**Documentation Link**: https://www.terraform.io/docs/language/expressions/version-constraints.html

**10. Which keyword is reserved for declaring variables in the terraform configuration files?**


* [ ] variable
* [ ] var
* [ ] Use the syntax var.<variable_name>
* [ ] variable block does not need a keyword
* [ ] user-defined keyword

**Correct answer:**
* [x] variable

**Explaination**: The variable block begins with the "variable" keyword followed by a user defined name/label for the variable. 

**Documentation Link**: https://www.terraform.io/docs/language/values/variables.html#declaring-an-input-variable

**11. The label after the variable keyword should be unique among all variables.**


* [ ] a. Should be unique among the variables in the same module
* [ ] b.You can create just two variables of the same label
* [ ] Both the statements (a) and (b) are true
* [ ] None of the above

**Correct answer:**
* [x] a. Should be unique among the variables in the same module

**Explaination**: A variable name or a label must be unique within the same module or configuration. 

**Documentation Link**: https://www.terraform.io/docs/language/values/variables.html#declaring-an-input-variable

**12. Your team assigned you the task of developing a terraform configuration to provision a bunch of services on GCP. You did everything to the point but forgot to mention the provider's version in the terraform block. What default behavior would you expect from terraform:**


* [ ] a. Terraform init will fail
* [ ] b. Terraform will download and use the latest version of providers used in the configuration
* [ ] c. Terraform init will succeed but an apply will fail because of unsupported provider versions
* [ ] d. None of the above
* [ ] e. All of the above

**Correct answer:**
* [x] b. Terraform will download and use the latest version of providers used in the configuration

**Explaination**: Terraform will download the latest version for all the providers used within the configuration. The version downloaded may or may not work well with the configuration developed. 

**13. A variable block is given below. Inspect it and choose the valid options.**


* [ ] Invalid. We cannot use "providers" as a variable name
* [ ] Valid. The "default" argument is optional
* [ ] Invalid. "default" argument is not used
* [ ] Invalid. Incorrect "type" used

**Correct answer:**
* [x] Invalid. We cannot use "providers" as a variable name

**Code**: 
variable "providers" {
  type = string
}

**Explaination**: We can use any name for a variable except for: source,  version, providers, count, for_each, lifecycle, depends_on and locals. 

We have used the variable name as "providers". This is not a valid identifier

**Documentation Link**: https://www.terraform.io/docs/language/values/variables.html#declaring-an-input-variable

**14. Inspect the below code block and determine the resource attribute that creates a dependency between the given resources.**


* [ ] aws_subnet.cidr_block
* [ ] aws_vpc.cidr_block
* [ ] aws_vpc.backend-vpc.id
* [ ] aws_vpc.backend_vpc.cidr_block
* [ ] aws_subnet.private-subnet1.cidr_block

**Correct answer:**
* [x] aws_vpc.backend-vpc.id

**Code**: 
resource "aws_vpc" "backend-vpc" {
  cidr_block = "10.0.0.0/16"
  tags = {
    Name = "backend-vpc"
  }
}
resource "aws_subnet" "private-subnet1" {
  vpc_id     = aws_vpc.backend-vpc.id
  cidr_block = "10.0.2.0/24"
  tags = {
    Name = "private-subnet1"
  }
}

**Explaination**: The aws_subnet type resource called private-subnet1 makes use of the resource attribute "aws_vpc.backend-vpc.id". 

**15. What is the generic way to reference attributes within the terraform expression?**


* [ ] None of the above
* [ ] RESOURCE_TYPE.ATTRIBUTE.NAME
* [ ] RESOURCE_TYPE.NAME.ATTRIBUTE
* [ ] RESOURCE_TYPE.NAME

**Correct answer:**
* [x] RESOURCE_TYPE.NAME.ATTRIBUTE

**Documentation Link**: https://www.terraform.io/docs/language/expressions/references.html#resources

**16. We just created an environment variable named “TF_VAR_content=foo-3” and ran the following command: `terraform apply -var "content=foo-4"`  .Determine the content of file foo.txt**


* [ ] foo-4
* [ ] foo-3
* [ ] foo
* [ ] foo-1

**Correct answer:**
* [x] foo-4

**Code**: 
resource "local_file" "foo" {
  content  = var.content
  filename = “/random/foo.txt”
}

variable "content" {
  type        = string
  description = "Content of the file to be created"

  validation {
    condition     = substr(var.content, 0, 4) == "foo-"
    error_message = "The content value must be a valid word starting \"foo-\"."
  }
}


**Explaination**: The variables passed with the -var or -var-file command line flags have the highest priority and will take precedence over environment variables. As such, the file will be created with "foo-4" as the content.

**Documentation Link**: https://www.terraform.io/docs/language/values/variables.html#variable-definition-precedence

**17. Each output value exported by a module must be declared using an ______ block.**


* [ ] output 
* [ ] input
* [ ] variable
* [ ] resource
* [ ] data

**Correct answer:**
* [x] output 

**Documentation Link**: https://www.terraform.io/docs/language/values/outputs.html#declaring-an-output-value

**18. Select the optional arguments that are available for the output block.**


* [ ] description
* [ ] sensitive
* [ ] depends_on
* [ ] All of the above

**Correct answer:**
* [x] All of the above

**Documentation Link**: https://www.terraform.io/docs/language/values/outputs.html#optional-arguments

**19. We have a local file resource with certain content. Once this resource is provisioned, the file is created in the `/root` directory and the information about this file is also stored in the Terraform state file. Now let's create a new file using a simple shell script in the same directory `/root`. Quite evidently, this file is outside the control and management of Terraform at this point in time. How would you include the second file in your Terraform configuration?**


* [ ] By creating a resource type object inside the main.tf file.
* [ ] By creating a data type object inside the main.tf file
* [ ] Terraform automatically syncs the files under the same directory
* [ ] Terraform doesn't provide such functionality

**Correct answer:**
* [x] By creating a data type object inside the main.tf file

**20. Choose the meta-argument which is not supported by the data block.**


* [ ] depends_on
* [ ] count
* [ ] for_each
* [ ] provider
* [ ] lifecycle

**Correct answer:**
* [x] lifecycle

**21. What steps are needed in order to change the backend in terraform? For instance : local to remote**


* [ ] Adding a terraform block only, would be enough
* [ ] Add a backend block within a terraform block
* [ ] Specifying the backend block within the terraform block is followed by reinitialization of the backend using `terraform init`.
* [ ] Any one of the options is enough.

**Correct answer:**
* [x] Specifying the backend block within the terraform block is followed by reinitialization of the backend using `terraform init`.

**22. Name the file that is created by default when you run the `terraform apply` command for the first time**


* [ ] terraform.state
* [ ] state.tf
* [ ] terraform.tf
* [ ] terraform.tfstate

**Correct answer:**
* [x] terraform.tfstate

**23. Which of the following is not the valid sub-command of `terraform state` command?**


* [ ] state mv
* [ ] state list
* [ ] state show
* [ ] state pull
* [ ] state rm
* [ ] state replace

**Correct answer:**
* [x] state replace

**24. You wanted to play with terraform to check what it has to offer. After a while you remembered that you didn’t specify any configuration for the backend. What default behaviour is expected here of terraform?**


* [ ] Terraform will use a local backend, which requires no configuration
* [ ] Terraform will use a remote backend, which requires no configuration.
* [ ] Terraform will randomly use a backend from a pool of local or remote ones
* [ ] None of the above

**Correct answer:**
* [x] Terraform will use a local backend, which requires no configuration

**25. Which command can be used to create a visual representation of our terraform resources?**


* [ ] terraform view
* [ ] terraform console
* [ ] terraform map
* [ ] terraform graph

**Correct answer:**
* [x] terraform graph

**26. `terraform plan` and `terraform apply` both refresh the state before their execution. Which option could be used to disable this default behaviour?**


* [ ] -refresh=disable
* [ ] -refresh=no
* [ ] -refresh=false
* [ ] -no-refresh=true

**Correct answer:**
* [x] -refresh=false

**27. Every terraform command listed is useful for inspecting infrastructure:- `output`, `graph`, `show`, `state list`, `state show`**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**28. Can you export the debug logs from `terraform` only by setting the `TF_LOG_PATH` environment variable?**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**29. Every initialized working directory has at least one workspace.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**30. Which environment variable is exported to set a different log level in the terraform?**


* [ ] TF_LOG
* [ ] var.TF_LOG
* [ ] VAR_TF_LOG
* [ ] TF_LOG_PATH

**Correct answer:**
* [x] TF_LOG

**31. We can delete the `default` terraform workspace.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**32. For local state, Terraform stores the workspace states in a directory called :**


* [ ] terraform.tfstate
* [ ] terraform.tfstate.d
* [ ] tfstate.d
* [ ] None of the above

**Correct answer:**
* [x] terraform.tfstate.d

**33. You intend to import two resources to your terraform configuration. You executed only the `terraform import` command until now and it worked. Will the `terraform apply` work if executed now?**


* [ ] It will throw an error
* [ ] Works like a charm
* [ ] We haven’t updated the resource with correct argument values yet
* [ ] None of the options

**Correct answer:**
* [x] It will throw an error
* [x] We haven’t updated the resource with correct argument values yet

**34. Modules can also call other modules using a _________ block**


* [ ] modules
* [ ] module
* [ ] Only the root module can call other modules
* [ ] None of the above

**Correct answer:**
* [x] module

**35. Select the types of terraform modules based on the credibility tier.**


* [ ] Verified
* [ ] Non-verified
* [ ] Official 
* [ ] Community

**Correct answer:**
* [x] Verified
* [x] Community
* [x] Official 

**36. A given resource or module block cannot use both `count` and `for_each` simultaneously.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**37. The for_each meta-argument accepts:**


* [ ] map 
* [ ] set of strings
* [ ] list of strings
* [ ] Only alphanumeric characters

**Correct answer:**
* [x] map 
* [x] set of strings

**38. Which statement best explains terraform provisioners?**


* [ ] used to model specific actions only on the local machine
* [ ] used to model specific actions only on the remote machine
* [ ] used to model specific actions on the local machine or on a remote machine
* [ ] None of the above

**Correct answer:**
* [x] used to model specific actions on the local machine or on a remote machine

**39. Where should you add a `provisioner` block?**


* [ ] Nested block inside the resource block
* [ ] Outside the resource block
* [ ] Nested block inside the provider block
* [ ] Inside the terraform block

**Correct answer:**
* [x] Nested block inside the resource block

**40. Fill the blank with the suitable choice:**


* [ ] destroy
* [ ] create
* [ ] apply
* [ ] plan 

**Correct answer:**
* [x] destroy

**Code**: 
resource "aws_instance" "web" {
  # ...

  provisioner "local-exec" {
    when    = ________
    command = "echo 'Destroy-time provisioner'"
  }
}


**41. Select the types of provisioners available:**


* [ ] local-exec
* [ ] remote-exec
* [ ] file
* [ ] all of the above

**Correct answer:**
* [x] all of the above

**42. Chose the option that  best describes “in-place updates”:**


* [ ] The underlying infrastructure remains the same
* [ ] Software and configuration of operating system changes as part of the update
* [ ] Both 
* [ ] Neither

**Correct answer:**
* [x] Both 

**43. What happens when provisioners fail to execute successfully?**


* [ ] Terraform will taint the resource that will be replaced on the next run
* [ ] Resources will be created but without the changes mentioned in the provisioner block
* [ ] It will show an error message showing the user to taint the resource manually
* [ ] None of the above

**Correct answer:**
* [x] Terraform will taint the resource that will be replaced on the next run

**44. Which argument of the lifecycle meta-argument supports a list as a value ?**


* [ ] create_before_destroy
* [ ] ignore_changes 
* [ ] prevent_destroy 
* [ ] All the options

**Correct answer:**
* [x] ignore_changes 

**Explaination**: ignore_changes (list of attribute names) - By default, Terraform detects any difference in the current settings of a real infrastructure object and plans to update the remote object to match configuration.

**45. Select the available arguments that are available for the lifecycle meta-argument**


* [ ] create_before_destroy
* [ ] ignore_change
* [ ] prevent_destroy
* [ ] ignore_changes
* [ ] destroy_after_create

**Correct answer:**
* [x] create_before_destroy
* [x] prevent_destroy
* [x] ignore_changes

**46. The Terraform language supports user-defined functions**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**47. General syntax for function calls is :**


* [ ] Function name followed by comma-separated arguments in parentheses.
* [ ] Function name followed by space-separated arguments in parentheses
* [ ] Both
* [ ] None

**Correct answer:**
* [x] Function name followed by comma-separated arguments in parentheses.

**48. Inspect the following code and select what will happen if we run a terraform init for this configuration:**


* [ ] Valid code. Terraform init will work
* [ ] Invalid default value for variable
* [ ] Invalid default value but terraform will convert the value to the type specified
* [ ] None of the above

**Correct answer:**
* [x] Invalid default value for variable

**Code**: 
variable "is_true" {
    type = bool
    default = 1
}

**Explaination**: The default value for this variable is a number and the type is set to a boolean. Terraform init will fail with an error as shown below:

------------------------------------------------------------------------------------------------------------------------
The Terraform configuration must be valid before initialization so that
Terraform can determine which modules and providers need to be installed.

Error: Invalid default value for variable

  on variables.tf line 20, in variable "istrue":
  20:     default = 1

This default value is not compatible with the variable's type constraint: bool
required.
------------------------------------------------------------------------------------------------------------------------

**49. Passing an object containing a sensitive input variable to the keys() function will result in a list that is _________ .**


* [ ] Sensitive
* [ ] Non-sensitive
* [ ] Random output
* [ ] None of the above.

**Correct answer:**
* [x] Sensitive

**50. Which of the following can be used to determine the length of a given list, map, or string.**


* [ ] length()
* [ ] len()
* [ ] size()
* [ ] count()

**Correct answer:**
* [x] length()

**51. Select the options that does not support dynamic block:**


* [ ] provisioner
* [ ] provider
* [ ] dynamic
* [ ] data
* [ ] resource
* [ ] locals

**Correct answer:**
* [x] locals

**Explaination**: A dynamic block can only generate arguments that belong to the resource type, data source, provider or provisioner being configured. It is not possible to generate meta-argument blocks such as lifecycle and provisioner blocks , since Terraform must process these before it is safe to evaluate expressions.

**52. Can we use `dynamic` blocks to generate `meta-argument` blocks such as `lifecycle` and `provisioner` blocks?**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**Explaination**: A dynamic block can only generate arguments that belong to the resource type, data source, provider, or provisioner being configured. It is not possible to generate meta-argument blocks such as lifecycle and provisioner blocks, since Terraform must process these before it is safe to evaluate expressions.

**53. By default terraform cloud runs terraform on:**


* [ ] its own cloud infrastructure
* [ ] on your own isolated, private, or on-premises infrastructure
* [ ] You need to specify each time you run terraform commands
* [ ] None of the above

**Correct answer:**
* [x] its own cloud infrastructure

**54. What is `Sentinel`?**


* [ ] Policy as code framework for HashiCorp Enterprise Products
* [ ] Infrastructure as code framework for Terraform
* [ ] Both 
* [ ] None of the above

**Correct answer:**
* [x] Policy as code framework for HashiCorp Enterprise Products

**55. Select the workflows that `Terraform cloud` utilizes to manage Terraform runs:**


* [ ] UI/VCS-driven run workflow
* [ ] API-driven run workflow
* [ ] CLI-driven run workflow
* [ ] None of the above

**Correct answer:**
* [x] UI/VCS-driven run workflow
* [x] API-driven run workflow
* [x] CLI-driven run workflow

**56. Is `Terraform Cloud workspaces` same as `Terraform CLI Workspaces`.**


* [ ] Yes
* [ ] No

**Correct answer:**
* [x] No

**57. You were working with different terraform scripts which are provisioning various sets of resources , you need to look up for some additional details related to one specific resource from the state file. Which `terraform command` will help you achieve this?**


* [ ] terraform state list ADDRESS
* [ ] terraform state show ADDRESS
* [ ] terraform show ADDRESS
* [ ] terraform get ADDRESS

**Correct answer:**
* [x] terraform state show ADDRESS

**58. `terraform import` command updates the configuration files as well as the state file, with the details of the infrastructure being imported.**


* [ ] True 
* [ ] False

**Correct answer:**
* [x] False

**59. In the UI and VCS workflow, every workspace is associated with a specific branch of a VCS repo of Terraform configurations.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**Explaination**: In the UI and VCS workflow, every workspace is associated with a specific branch of a VCS repo of Terraform configurations. Terraform Cloud registers webhooks with your VCS provider when you create a workspace, then automatically queues a Terraform run whenever new commits are merged to that branch of workspace's linked repository.

**60. “alias” and “version” are the `meta-arguments` which are available for all provider blocks?**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

---


## Linux Basics

**1. In the command “echo -n hello”, what is “-n”**


* [ ] command
* [ ] argument
* [ ] option

**Correct answer:**
* [x] option

**2. Which command would you use to find out the type of a command?**


* [ ] type
* [ ] file
* [ ] echo
* [ ] cd
* [ ] pwd

**Correct answer:**
* [x] type

**3. Which directory contains the user home directories by default?**


* [ ]  home
* [ ] /home
* [ ] /root
* [ ] home-directory

**Correct answer:**
* [x] /home

**4. Which symbol represents a user’s home directory in Linux?**


* [ ] $
* [ ]  ~
* [ ] :
* [ ] /

**Correct answer:**
* [x]  ~

**5. In the command “echo -n hello”, what is “echo”**


* [ ] argument
* [ ] switch
* [ ] option
* [ ] flag
* [ ] command

**Correct answer:**
* [x] command

---


## Terraform Mock Exam 2

**1. What does “IaC” stand for?**


* [ ] Infrastructure as Code
* [ ] Initialization as Code
* [ ] Code as Infrastructure
* [ ] None of the above

**Correct answer:**
* [x] Infrastructure as Code

**Explaination**: IaC stands for Infrastructure as Code.

**2. Name the file that is created by default when you run the `terraform apply` command for the first time**


* [ ] terraform.state
* [ ] state.tf
* [ ] terraform.tf
* [ ] terraform.tfstate

**Correct answer:**
* [x] terraform.tfstate

**3. Every initialized working directory has at least one workspace.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**4. Select the reasons why we may need to specify the provider's argument?**


* [ ] It’s just a practice we need to blindly follow
* [ ] No specific reason
* [ ] To use multiple configurations of the same provider
* [ ] To change the default Provider Configurations
* [ ] To use multiple provider plugins in the same configuration

**Correct answer:**
* [x] To use multiple configurations of the same provider
* [x] To change the default Provider Configurations

**Explaination**: There are two reasons to use a provider argument in the configuration. 

1. To override the default provider configuration.  For example, the default configuration may be to deploy resources in the "us-east-1" region. If the requirement is to deploy resources in a different region, we can use the provider argument to override the default.

2. In some cases, a configuration may need to use multiple versions of the same provider. For example -  a resource that deploys to the "us-east-1" and another resource within the same configuration that deploys to the "us-west-2" region. 

**Documentation Link**: https://www.terraform.io/docs/language/meta-arguments/module-providers.html#when-to-specify-providers

**5. What are the possible consequences of making heavy usage of provisioners within your  terraform script?**


* [ ] Add a considerable amount of complexity and uncertainty to Terraform usage
* [ ] Terraform cannot model the actions of provisioners as part of a plan
* [ ] Use of provisioners requires coordinating many more details than Terraform usage usually requires
* [ ] All of the options

**Correct answer:**
* [x] All of the options

**6. When we introduce _______ blocks, our configuration becomes hierarchical rather than flat.**


* [ ] module
* [ ] root module
* [ ] child module
* [ ] None of the above

**Correct answer:**
* [x] module

**7. What are the common issues that come with configuration drift?**


* [ ] Make it difficult to plan and carry out subsequent updates.
* [ ] Troubleshooting issues would also be a difficult task
* [ ] All of the above
* [ ] Can leave the infrastructure in a complex state

**Correct answer:**
* [x] All of the above

**8. Which option best describes the meaning of interpolation syntax?**


* [ ] A way to reference variables, attributes of resources, and call functions
* [ ] A way to declare values to variables
* [ ] A way to provide runtime options with terraform operations
* [ ] None of the above

**Correct answer:**
* [x] A way to reference variables, attributes of resources, and call functions

**Explaination**: Interpolation syntax allows us to reference variables, resource attributes and even make use of built-in functions in terraform.

**Documentation Link**: https://www.terraform.io/docs/language/expressions/strings.html#interpolation

**9. Which argument of the lifecycle meta-argument supports a list as a value ?**


* [ ] create_before_destroy
* [ ] ignore_changes 
* [ ] prevent_destroy 
* [ ] All the options

**Correct answer:**
* [x] ignore_changes 

**Explaination**: ignore_changes (list of attribute names) - By default, Terraform detects any difference in the current settings of a real infrastructure object and plans to update the remote object to match configuration.

**10. Which terraform command can be used to experiment with the behavior of Terraform's built-in functions?**


* [ ] terraform console
* [ ] terraform validate
* [ ] terraform check
* [ ] terraform terminal

**Correct answer:**
* [x] terraform console

**Explaination**: This command provides an `interactive command-line console` for evaluating and experimenting with expressions. This is useful for testing interpolations before using them in configurations, and for interacting with any values currently saved in state.

**11. If the arguments for passing to a function are available in the form of list or tuple value, how would you expand that inside the function?**


* [ ] Provide the list value as an argument and follow it with the “...” symbol
* [ ] Provide the list value as an argument and follow it with the keyword “expand”.
* [ ] Terraform doesn’t support expanding arguments this way
* [ ] None of the above

**Correct answer:**
* [x] Provide the list value as an argument and follow it with the “...” symbol

**12. You can dynamically construct repeatable nested blocks using which special block type:**


* [ ] dynamic
* [ ] generate
* [ ] local
* [ ] None of the above

**Correct answer:**
* [x] dynamic

**Explaination**: You can dynamically construct repeatable nested blocks like setting using a special "dynamic block" type, which is supported inside "resource", "data", "provider", and "provisioner" blocks.

**13. Which of the following function is no longer available in terraform:**


* [ ] map()
* [ ] tomap()
* [ ] zipmap()
* [ ] None of the above

**Correct answer:**
* [x] map()

**Documentation Link**: https://www.terraform.io/language/functions/map#map-function

**14. Which of the following is true for terraform cloud?**


* [ ] It manages Terraform runs in a consistent and reliable environment
* [ ] It includes easy access to shared state and secret data, access controls for approving changes to infrastructure
* [ ] It provides a private registry for sharing Terraform modules
* [ ] It has detailed policy controls for governing the contents of Terraform configurations
* [ ] All of the above

**Correct answer:**
* [x] All of the above

**Documentation Link**: https://learn.hashicorp.com/tutorials/terraform/cloud-sign-up?in=terraform/cloud-get-started#what-is-terraform-cloud

**15. Select the best practices around using dynamic block.**


* [ ] Use them when you need to hide details in order to build a clean user interface for a reusable module.
* [ ] Overuse of dynamic blocks can make configuration hard to read and maintain
* [ ] Always write nested blocks out literally where possible.
* [ ] All of the options

**Correct answer:**
* [x] All of the options

**Documentation Link**: https://www.terraform.io/language/expressions/dynamic-blocks#best-practices-for-dynamic-blocks

**16. Is it possible to declare the `dynamic` block inside another `dynamic` block?**


* [ ] Yes
* [ ] No

**Correct answer:**
* [x] Yes

**Explaination**: Some providers define resource types that include multiple levels of blocks nested inside one another. You can generate these nested structures dynamically when necessary by nesting dynamic blocks in the content portion of other dynamic blocks.

**Documentation Link**: https://developer.hashicorp.com/terraform/language/expressions/dynamic-blocks#multi-level-nested-block-structures

**17. What are the advantages of using `Terraform Cloud's private registry`:**


* [ ] Helps you share `Terraform providers` and `Terraform modules` across your organization
* [ ] Includes support for versioning, a searchable list of available providers and modules
* [ ] Includes support for a `configuration designer` to help you build new workspaces faster
* [ ] All of the above

**Correct answer:**
* [x] All of the above

**18. Select the workflows that `Terraform cloud` utilizes to manage Terraform runs:**


* [ ] UI/VCS-driven run workflow
* [ ] API-driven run workflow
* [ ] CLI-driven run workflow
* [ ] None of the above

**Correct answer:**
* [x] UI/VCS-driven run workflow
* [x] API-driven run workflow
* [x] CLI-driven run workflow

**19. In the UI and VCS workflow, every workspace is associated with a specific branch of a VCS repo of Terraform configurations.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**Explaination**: In the UI and VCS workflow, every workspace is associated with a specific branch of a VCS repo of Terraform configurations. Terraform Cloud registers webhooks with your VCS provider when you create a workspace, then automatically queues a Terraform run whenever new commits are merged to that branch of workspace's linked repository.

**20. A terraform configuration can consist of multiple files and directories.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**Explaination**: A Terraform module can use module calls to explicitly include other modules into the configuration. These child modules can come from local directories (nested in the parent module's directory, or anywhere else on disk), or from external sources like the Terraform Registry.



**Documentation Link**: https://www.terraform.io/language/files#directories-and-modules

**21. Terraform assumes an empty default configuration for any provider that is not explicitly configured.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**Explaination**: Unlike many other objects in the Terraform language, a provider block may be `omitted` if its contents would otherwise be `empty`. Terraform assumes an empty default configuration for any provider that is not explicitly configured.

**22. Which `provider` is used in the below code snippet:**


* [ ] local
* [ ] aws_instance
* [ ] aws
* [ ] aws.west

**Correct answer:**
* [x] aws.west

**Code**: 
resource "aws_instance" "foo" {
  provider = aws.west

  # ...
}


**Explaination**: To use an alternate provider configuration for a resource or data source, set its provider meta-argument to a <PROVIDER NAME>.<ALIAS>

**Documentation Link**: https://www.terraform.io/language/providers/configuration#selecting-alternate-provider-configurations

**23. The Terraform language is ___________ :**


* [ ] Declarative
* [ ] Imperative 
* [ ] Both 
* [ ] None of the options

**Correct answer:**
* [x] Declarative

**Explaination**: Terraform directly describes the end state of the system without defining the steps to reach there. It works at a high level of abstraction to describe what services and resources should be created and defined

**24. How does terraform allow you to review changes before Terraform creates, updates, or destroys infrastructure?**


* [ ] By generating an execution plan and asking for approval before implementing infrastructure changes.
* [ ] By creating a graph to help in visualizing what is to be implemented.
* [ ] Users need to cross-check the files in the current configuration directory themselves.
* [ ] Terraform doesn't provide review capability.

**Correct answer:**
* [x] By generating an execution plan and asking for approval before implementing infrastructure changes.

**25. What will happen if Terraform isn't able to obtain acceptable versions of `external dependencies`, or if it doesn't have an acceptable `version of itself`?**


* [ ] It will attempt to download the newest version that meets the applicable constraints.
* [ ] It won't proceed with any plans, applies, or state manipulation actions
* [ ] It will ignore such errors and proceed without warning
* [ ] None of the options

**Correct answer:**
* [x] It won't proceed with any plans, applies, or state manipulation actions

**Explaination**: Both the root module and any child module can constrain the acceptable versions of Terraform and any providers they use. Terraform considers these constraints equal, and will only proceed if all of them can be met.

**26. What is the default behaviour of Terraform when it doesn't have an acceptable version of a required `plugin` or `module`?**


* [ ] It will attempt to download the newest version that meets the applicable constraints.
* [ ] It won't proceed with any plans, applies, or state manipulation actions
* [ ] It will attempt to create that plugin or module itself
* [ ] None of the options

**Correct answer:**
* [x] It will attempt to download the newest version that meets the applicable constraints.

**27. The `default` argument within the `variable` block should satisfy which of the following conditions:**


* [ ]  `default` argument requires a `literal` value
* [ ] It cannot reference other objects in the configuration.
* [ ] If present, the variable is considered to be optional.
* [ ] `default` value will be used if no value is set when calling the module or running Terraform.
* [ ] All of the options

**Correct answer:**
* [x] All of the options

**Documentation Link**: https://www.terraform.io/language/values/variables#default-values

**28. Select the correct options about the `local values` among the following:**


* [ ] Local values are created by a `locals` block
* [ ] Local values are created by a `local` block
* [ ] Local values are referenced as attributes on an object named `local`
* [ ] Local values are referenced as attributes on an object named `locals`
* [ ] Terraform supports local values creation with both `local` as well as `locals` keyword

**Correct answer:**
* [x] Local values are created by a `locals` block
* [x] Local values are referenced as attributes on an object named `local`

**Explaination**: Local values are created by a `locals block (plural)`, but you reference them as attributes on an object named `local (singular)`. Make sure to leave off the "s" when referencing a local value!

**29. Select the most appropriate options among the following:**


* [ ] Resources declared using `data` block are known as `data resources`.
* [ ] Resources declared by a resource block are known as `managed resources`.
* [ ] Managed resources are often referred to just as `resources` when the meaning is clear from context.
* [ ] data resources cause Terraform to create, update, and delete infrastructure objects.
* [ ] Managed resources cause Terraform only to read objects

**Correct answer:**
* [x] Resources declared using `data` block are known as `data resources`.
* [x] Resources declared by a resource block are known as `managed resources`.
* [x] Managed resources are often referred to just as `resources` when the meaning is clear from context.

**30. Choose the correct option for referencing local values from the below code snippet:**


* [ ] local.common_tags
* [ ] locals.common_tags
* [ ] common_tags.locals
* [ ] common_tags.local

**Correct answer:**
* [x] local.common_tags

**Code**: 
locals {
  # Common tags to be assigned to all resources
  common_tags = {
    Service = local.service_name
    Owner   = local.owner
  }
}

resource "aws_instance" "example" {
  # ...

  tags = ____________
}


**Explaination**: Local values are created by a locals block (plural), but you reference them as attributes on an object named local (singular). Make sure to leave off the "s" when referencing a local value!



**31. What are the options that best correspond to the definition of `local-only` data sources:**


* [ ] It is the same as all other data sources, but their result data exists only temporarily during a Terraform operation.
* [ ] Local-only data sources are exactly the same as normal data sources.
* [ ] It is re-calculated each time a new plan is created.
* [ ] Local-only data sources behave similarly to local values declared using `locals` block.

**Correct answer:**
* [x] It is the same as all other data sources, but their result data exists only temporarily during a Terraform operation.
* [x] It is re-calculated each time a new plan is created.

**Explaination**: The behavior of local-only data sources is the same as all other data sources, but their result data exists only temporarily during a Terraform operation, and is re-calculated each time a new plan is created.

**Documentation Link**: https://www.terraform.io/language/data-sources#local-only-data-sources

**32. Local values can be helpful to avoid repeating the same values or expressions multiple times in a configuration.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**Explaination**: Local values can be helpful to avoid repeating the same values or expressions multiple times in a configuration, but if overused they can also make a configuration hard to read by future maintainers by hiding the actual values used.

**33. Terraform recommends treating the state itself as sensitive data if you manage any sensitive data with Terraform (like database passwords, user passwords, or private keys etc).**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**Documentation Link**: https://www.terraform.io/language/state/sensitive-data#sensitive-data-in-state

**34. Terraform expects a _________ mapping between configured resource instances and remote objects.**


* [ ] One-to-one
* [ ] One-to-many
* [ ] Many-to-many
* [ ] Many-to-one

**Correct answer:**
* [x] One-to-one

**Explaination**: Terraform expects that each remote object is bound to only one resource instance, which is normally guaranteed by Terraform being responsible for creating the objects and recording their identities in the state.

**Documentation Link**: https://www.terraform.io/language/state/purpose#mapping-to-the-real-world

**35. You recently joined an organization with the capacity of devops-engineer. Your team has been working on various projects and you were assigned one that involves heavy usage of terraform configuration files. You wanted to extract the list of resources that have been provisioned till date using this terraform configuration. What terraform command would you make use of to achieve this?**


* [ ] terraform state list
* [ ] terraform state show
* [ ] terraform list
* [ ] terraform show

**Correct answer:**
* [x] terraform state list

**Explaination**: The command will list all resources in the state file matching the given addresses (if any). If no addresses are given, all resources are listed.



**36. Select the appropriate option to correct the below code snippet:**


* [ ] Variable declaration is not done correctly. The `var` keyword should be used instead of `variable`.
* [ ] Invalid argument `types` used.
* [ ] `default` argument is not used . It will throw an error as it is not `optional`.
* [ ] There is no issue with the given code.

**Correct answer:**
* [x] Invalid argument `types` used.

**Code**: 
variable "image_id" {
  types        = string
  description = "The id of the machine image (AMI) to use for the server."
}


**Documentation Link**: https://www.terraform.io/language/values/variables#declaring-an-input-variable

**37. Provisioners can `only` be used to model specific actions on a remote machine in order to prepare servers or other infrastructure objects for service.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**Explaination**: Provisioners can be used to model specific actions on the "local machine" or on a "remote machine" in order to prepare servers or other infrastructure objects for service.

**38. What steps are needed to `enable state-locking` to protect state file from concurrent operations against the same terraform configuration?**


* [ ] We can enable state-locking using `-lock=true`
* [ ] Terraform enables it automatically for all backends
* [ ] Terraform enables it automatically if your backends have support for it.
* [ ] None of the options are true

**Correct answer:**
* [x] Terraform enables it automatically if your backends have support for it.

**Explaination**: If supported by your backend, Terraform will lock your state for all operations that could write state. This prevents others from acquiring the lock and potentially corrupting your state.

**Documentation Link**: https://www.terraform.io/language/state/locking#state-locking

**39. Which variable block argument prevents terraform from showing sensitive data in `plan` or `apply` output?**


* [ ] sensitive_content = true
* [ ] sensitive = true
* [ ] No such argument exists
* [ ] private_content = true

**Correct answer:**
* [x] sensitive = true

**Explaination**: Setting a variable as "sensitive" prevents Terraform from showing its value in the "plan" or "apply" output, when you use that variable elsewhere in your configuration.

**40. Which environment variable needs to be defined in terraform for the persistence of the logged output?**


* [ ] TF_LOG_PATH
* [ ] LOG_PATH
* [ ] TF_LOG
* [ ] None of the options

**Correct answer:**
* [x] TF_LOG_PATH

**Explaination**: To persist logged output you can set TF_LOG_PATH in order to force the log to always be appended to a specific file when logging is enabled. Note that even when TF_LOG_PATH is set, TF_LOG must be set in order for any logging to be enabled.

**41. What is the significance of `-chdir` option in terraform?**


* [ ] It instructs Terraform to change its working directory to the given directory before running the given subcommand.
* [ ] It picks up the terraform binary from the directory specified by the `-chdir` option.
* [ ] No such option exists within terraform.
* [ ] `-chdir` option instructs terraform to download the provider plugin from the directory specified on the command line.

**Correct answer:**
* [x] It instructs Terraform to change its working directory to the given directory before running the given subcommand.

**Explaination**: The chdir option instructs Terraform to change its working directory to the given directory before running the given subcommand. This means that any files that Terraform would normally read or write in the current working directory will be read or written in the given directory instead.

**42. Logging can `not` be enabled separately for the provider plugins using the TF_LOG_PROVIDER environment variable.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**Explaination**: Logging can be enabled separately for terraform itself and the provider plugins using the TF_LOG_CORE or TF_LOG_PROVIDER environment variables. These take the same level arguments as TF_LOG, but only activate a subset of the logs.

**43. Which command applies a subset of the `Terraform language style conventions`, along with other minor adjustments for `readability`.**


* [ ] `terraform fmt`
* [ ] `terraform console`
* [ ] `terraform plan`
* [ ] `terraform validate`

**Correct answer:**
* [x] `terraform fmt`

**Explaination**: The "terraform fmt" command is used to rewrite Terraform configuration files to a canonical format and style. This command applies a subset of the Terraform language style conventions, along with other minor adjustments for readability.

**44. Choose the appropriate option among the following for the below command:**


* [ ] It will output the value of the variable NAME for the root module.
* [ ] It will display all the outputs for the root module.
* [ ] It will display all the outputs from the state file named `NAME` instead of `terraform.tfstate`.
* [ ] None of the options are correct.

**Correct answer:**
* [x] It will output the value of the variable NAME for the root module.

**Code**: 
terraform output [options] [NAME]

**45. Choose the suitable options for the `terraform plan` command.**


* [ ] Reads the current state of any already-existing remote objects to make sure that the Terraform state is up-to-date
* [ ] Compares the current configuration to the prior state and noting any differences
* [ ] Proposes a set of change actions that should, if applied, make the remote objects match the configuration
* [ ] All the options are correct.

**Correct answer:**
* [x] All the options are correct.

**Documentation Link**: https://www.terraform.io/cli/commands/plan#command-plan

**46. The __________ displays the current version of Terraform and of all installed plugins.**


* [ ] `terraform version`
* [ ] `terraform -version`
* [ ] `terraform version -all`
* [ ] None of the options are correct

**Correct answer:**
* [x] `terraform version`

**47. The lifecycle block and its contents are available for all `resource` blocks regardless of type.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**Explaination**: `lifecycle` is a nested block that can appear within a resource block. The lifecycle block and its contents are meta-arguments, available for all resource blocks regardless of type.

**Documentation Link**: https://www.terraform.io/language/meta-arguments/lifecycle#the-lifecycle-meta-argument

**48. Choose among the following that holds true for terraform `default` workspace.**


* [ ] It is the default workspace.
* [ ] This workspace cannot be deleted.
* [ ] Only `default` workspace uses the `terraform.tfstate` file.
* [ ] None of the options are correct.
* [ ] This workspace can be renamed

**Correct answer:**
* [x] It is the default workspace.
* [x] This workspace cannot be deleted.

**Explaination**: Terraform starts with a single workspace named "default". This workspace is special both because it is the default and also because it cannot ever be deleted. If you've never explicitly used workspaces, then you've only ever worked on the "default" workspace.



**Documentation Link**: https://www.terraform.io/language/state/workspaces#using-workspaces

**49. Choose the correct command which is used to list all existing workspaces.**


* [ ] `terraform workspace show`
* [ ] `terraform workspace list`
* [ ] `terraform workspace list -all`
* [ ] None of the options are correct.

**Correct answer:**
* [x] `terraform workspace list`

**50. ________ are the main way to `package` and `reuse` resource configurations with Terraform.**


* [ ] Modules
* [ ] Workspaces
* [ ] None of the options are correct.
* [ ] Both of the options are correct.

**Correct answer:**
* [x] Modules

**51. Is it possible for a Terraform configuration to have no modules and still works?**


* [ ] It will work with no errors.
* [ ] Terraform configuration needs at least one module .i.e. Root module.
* [ ] Terraform configuration needs at least one child module along with the root module.
* [ ] None of the options are correct.

**Correct answer:**
* [x] Terraform configuration needs at least one module .i.e. Root module.

**Explaination**: Every Terraform configuration has at least one module, known as its root module, which consists of the resources defined in the .tf files in the main working directory.



**Documentation Link**: https://www.terraform.io/language/modules/syntax#module-blocks

**52. The splat expression patterns shown below is not applicable for which among the following:**


* [ ] Lists
* [ ] Sets
* [ ] Tuples
* [ ] Maps
* [ ] None of the options

**Correct answer:**
* [x] Maps

**Code**: 
var.list[*].id

**Explaination**: The splat expression patterns shown above apply only to lists, sets, and tuples. To get a similar result with a map or object value you must use `for` expressions.



**53. Which among the following is similar to below code expressions:**


* [ ] var.list[*].id
* [ ] var.list(*).id
* [ ] var.*(list).id
* [ ] var.*.id
* [ ] None of the above

**Correct answer:**
* [x] var.list[*].id

**Code**: 
[for o in var.list : o.id]


**Explaination**: The special [*] symbol iterates over all of the elements of the list given to its left and accesses from each one the attribute name given on its right.

**54. Choose the suitable options among the following for the below code snippet:**


* [ ] Instructs Terraform to plan to replace the single resource instance with the given address.
* [ ] For Terraform v0.15.2 and later, terraform recommend using the `-replace option` with terraform apply instead of `taint`.
* [ ] Terraform recommends to use ` terraform taint` instead of `-replace option`.
* [ ] All of the options
* [ ] None of the options.

**Correct answer:**
* [x] Instructs Terraform to plan to replace the single resource instance with the given address.
* [x] For Terraform v0.15.2 and later, terraform recommend using the `-replace option` with terraform apply instead of `taint`.

**Code**: 
terraform apply -replace="aws_instance.example[0]"

**Explaination**: Terraform recommends the `-replace` option because the change will be reflected in the Terraform plan, letting you understand how it will affect your infrastructure before you take any externally-visible action. When you use `terraform taint`, other users could create a new plan against your tainted object before you can review the effects.



**55. Which among the following are the techniques that could be used to safely and securely manage secrets inside terraform?**


* [ ] Environment Variables
* [ ] Encrypted Files (e.g., KMS, PGP, SOPS)
* [ ] Secret Stores (e.g., Vault, AWS Secrets manager)
* [ ] Store Secrets in plain text
* [ ] Store Terraform state in a backend that supports encryption.

**Correct answer:**
* [x] Secret Stores (e.g., Vault, AWS Secrets manager)
* [x] Store Terraform state in a backend that supports encryption.
* [x] Encrypted Files (e.g., KMS, PGP, SOPS)
* [x] Environment Variables

**Documentation Link**: https://blog.gruntwork.io/a-comprehensive-guide-to-managing-secrets-in-your-terraform-code-1d586955ace1

**56. Select the appropriate options for which we can make use of version constraints:**


* [ ] Workspaces 
* [ ] Modules 
* [ ] Provider requirements
* [ ] Only provider requirements.
* [ ] The `required_version` setting in the terraform block

**Correct answer:**
* [x] Modules 
* [x] Provider requirements
* [x] The `required_version` setting in the terraform block

**Documentation Link**: https://www.terraform.io/language/expressions/version-constraints#version-constraints

**57. Choose the suitable option that could be used to access one of the module's output values:**


* [ ] module.[MODULE NAME].[OUTPUT NAME]
* [ ] module.[MODULE NAME].[VARIABLE NAME]
* [ ] module.[VARIABLE NAME].[MODULE NAME]
* [ ] module.[OUTPUT NAME].[MODULE NAME]

**Correct answer:**
* [x] module.[MODULE NAME].[OUTPUT NAME]

**Documentation Link**: https://www.terraform.io/language/expressions/references#child-module-outputs

**58. Considering provisioners are the best way to solve your problem, in what way could you make use of provisioner block.**


* [ ] Nested block inside the resource block
* [ ] Outside the resource block
* [ ] Nested block inside the provider block
* [ ] Inside the terraform block

**Correct answer:**
* [x] Nested block inside the resource block

**59. What is the type of the resource given in the below code snippet:**


* [ ] Managed resource
* [ ] Data resource
* [ ] Both options are correct
* [ ] None of the options are correct

**Correct answer:**
* [x] Data resource

**Code**: 
data "aws_ami" "amazon_linux" {
  most_recent = true
  owners      = ["amazon"]

  filter {
    name   = "name"
    values = ["amzn2-ami-hvm-*-x86_64-gp2"]
  }
}


**Explaination**: A data source is accessed via a special kind of resource known as a `data resource`, declared using a `data block`.

**60. Name the embedded `policy-as-code framework` integrated with the HashiCorp Enterprise products.**


* [ ] Sentinel
* [ ] Open Policy Agent (OPA)
* [ ] HashiCorp Boundary
* [ ] Checkov by Bridgecrew

**Correct answer:**
* [x] Sentinel

**Explaination**: Sentinel is an embedded policy-as-code framework integrated with the HashiCorp Enterprise products. It enables fine-grained, logic-based policy decisions, and can be extended to use information from external sources.

**Documentation Link**: https://www.terraform.io/cloud-docs/sentinel#sentinel-overview

---


## None

---


## sample-vijin

**1. Test image ![Sample Image](https://res.cloudinary.com/dezmljkdo/image/upload/v1652455651/kodekloud-challenges/archive4_nxqz5a.png)**


* [ ] a
* [ ] b
* [ ] c
* [ ] d

**Correct answer:**
* [x] a

---


## managing-azure-active-directory

**1. What is the maximum number of methods we can set for password reset?**


* [ ] 1
* [ ] 2
* [ ] 4
* [ ] 6

**Correct answer:**
* [x] 2

**Explaination**: Though we can make multiple methods available to the users, we can use one or two methods for a password reset, hence the maximum number is `2`.

**2. With Azure AD Free edition, we can have B2B collaboration.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**Explaination**: It is `True` as all editions of Azure AD support B2B collaboration.

**3. If you are deleting a user from Azure AD, how long will Azure AD will retain the user?**


* [ ] 10 days
* [ ] 15 days
* [ ] 30 days
* [ ] 60 days

**Correct answer:**
* [x] 30 days

**Explaination**: Azure AD will retain the deleted user for `30` days, during that time you can restore the user if needed.

**4. Which type of user account is owned by Abigail Richards? ![question-image](https://res.cloudinary.com/dezmljkdo/image/upload/v1655214383/az104/az1041_swnnph.png)**


* [ ] Member identity
* [ ] Cloud identity
* [ ] Guest identity
* [ ] Directory Synchronized identity

**Correct answer:**
* [x] Cloud identity

**Explaination**: Abigail is a cloud identity as the user type is Member and Directory synced flag is set to No. Guest identities will have the user type as Guest and Directory synchronized users will have the Directory synced flag set to Yes.

**5. You would like to add external users to your directory in bulk. Which of the following bulk operations will you use to accomplish this task?**


* [ ] Bulk export
* [ ] Bulk create
* [ ] Bulk add
* [ ] Bulk invite

**Correct answer:**
* [x] Bulk invite

**Explaination**: Since you are adding external users, you need to use the “Bulk invite” option. If you were adding cloud identities in bulk, then you need to use “Bulk create” option.

**6. Which feature in Azure AD can be used to enforce device management?**


* [ ] Azure Domain Join
* [ ] Azure AD Directory Services
* [ ] Azure AD Join
* [ ] Azure AD login

**Correct answer:**
* [x] Azure AD Join

**Explaination**: `Azure AD Join` can be used to join our devices to Azure AD using a Work or School account for device management.

**7. Which of the following statements is false? Select one.**


* [ ] Azure AD uses Kerberos.
* [ ] Azure AD is a managed service.
* [ ] Azure AD uses REST API calls for querying.
* [ ] Azure AD uses OAuth.

**Correct answer:**
* [x] Azure AD uses Kerberos.

**Explaination**: Kerberos is used by Active Directory Domain Services; Azure AD uses OAuth authorization.

**8. Your organization wants to use Identity Governance in Azure AD. Which Azure AD edition will you recommend for this?**


* [ ] Free
* [ ] Microsoft 365 Apps
* [ ] Premium P1
* [ ] Premium P2

**Correct answer:**
* [x] Premium P2

**Explaination**: Only Azure AD Premium P2 supports Identity Governance.

**9. Which protocol is used by Active Directory Domain services for querying objects?**


* [ ] HTTP
* [ ] HTTPS
* [ ] REST
* [ ] LDAP

**Correct answer:**
* [x] LDAP

**Explaination**: ADDS uses `LDAP` for querying, on the other hand, Azure AD uses HTTP/HTTPS.

**10. Which users will be part of the “Finance” group if the dynamic query for the group is user.department –eq "Finance" –and user.country –eq "US"? ![question-image](https://res.cloudinary.com/dezmljkdo/image/upload/v1656356062/az104/table1_qqyxc9.png)**


* [ ] 1 and 4
* [ ] 2, 4, and 5
* [ ] 3 and 5
* [ ] 3, 4, and 5

**Correct answer:**
* [x] 3 and 5

**Explaination**: Dynamic query will pick the users whose department is Finance and country is US.

---


## subscription-and-governance

**1. You need to group your subscription based on department, which of the following need to be used?**


* [ ] Management groups
* [ ] Subscription groups
* [ ] Resource groups
* [ ] Business groups

**Correct answer:**
* [x] Management groups

**Explaination**: Management groups offer a scope above subscriptions by which you will be able to group subscriptions together.

**2. What’s the maximum number of tags you can assign to a resource?**


* [ ] 20
* [ ] 40
* [ ] 50
* [ ] 60

**Correct answer:**
* [x] 50

**Explaination**: Maximum number of tags is 50.

**3. You are running production critical workloads in Azure, and you need to make sure that the resources are protected from accidental changes and deletion. Which of the following should you use?**


* [ ] Read-only lock
* [ ] Delete lock
* [ ] Tag
* [ ] Policy

**Correct answer:**
* [x] Read-only lock

**Explaination**: Read only lock is required as you are intending to avoid both accidental changes and deletion.

**4. Chris is the owner of the subscription, and he wants to assign a role to Alex. Alex is responsible for managing user access to Azure resources. The role should comply with the principle of least privilege. Which role should Chris assign to Alex?**


* [ ] Owner
* [ ] Administrator
* [ ] User access owner
* [ ] User access administrator

**Correct answer:**
* [x] User access administrator

**Explaination**: User Access Administrator can manage user access to Azure resources. Owner can also do the same, but the Owner role will make Alex over-privileged.

**5. Which service in Azure is used to manage access to Azure resources?**


* [ ] Azure Management groups
* [ ] RBAC
* [ ] Policy
* [ ] Tags

**Correct answer:**
* [x] RBAC

**Explaination**: RBAC is used to manage access to resources.

**6. You deployed a production webserver on Azure Ubuntu Linux Virtual Machine. Your manager is planning to perform cost cutting and asked you to optimize the cost of the VM. Which of the following methods can be used to save the cost?**


* [ ] Azure Visual Studio subscription
* [ ] Azure Dev/Test subscription
* [ ] Reserved Instances
* [ ] Azure Hybrid Benefit

**Correct answer:**
* [x] Reserved Instances

**Explaination**: Azure Visual Studio subscription and Azure Dev/Test subscription don’t have any SLA and are not recommended for production workloads. As Ubuntu doesn’t require any license, Azure Hybrid Benefit is not applicable. The only option is to buy a reserved instance.

**7. You need to make sure that the tags applied to the resource group is getting inherited to the underlying resources. How can we accomplish this?**


* [ ] No change is required, tags applied at higher scopes will be inherited to underlying resources.
* [ ] Azure Policy needs to be implemented to inherit the tags.
* [ ] Use inherit option in Azure to inherit if needed.
* [ ] You cannot assign tags to resource groups, tag resources individually.

**Correct answer:**
* [x] Azure Policy needs to be implemented to inherit the tags.

**Explaination**: Tags are not inherited by default; we need to use Azure Policy to make sure the tags are inherited. Alternatively, we can use scripting to copy the tags from resource group and apply to the resources, but it’s a lengthy process.

**8. Which of the following statements about Azure subscription is not true?**


* [ ] Logical boundary for our resources.
* [ ] Azure Subscription provides billing boundary.
* [ ] All resources will be mapped to a subscription.
* [ ] Only one subscription can be created inside an account

**Correct answer:**
* [x] Only one subscription can be created inside an account

**Explaination**: An account can have multiple subscriptions.

**9. Your organization is going for PCI: DSS compliance and there are several policies that your organization wants to enforce to their Azure environment. They are looking for an easier solution to assign multiple policies and evaluate them. What will you suggest?**


* [ ] Use Azure Policy Developer Console
* [ ] Use Azure Initiative
* [ ] Use Azure Policy bulk processing
* [ ] There is no way to assign and manage multiple policies.

**Correct answer:**
* [x] Use Azure Initiative

**Explaination**: Initiative can be used to chain policies, assign them, and evaluate the overall compliance.

**10. Which of the following statements about Azure Cost Management is false?**


* [ ] Cost Management can be used export Azure usage to storage account.
* [ ] We can create budgets and cost alerts in Cost Management.
* [ ] We can analyse AWS and GCP cost in Azure Cost Management.
* [ ] Cost Management offers API to download the usage programmatically.

**Correct answer:**
* [x] We can analyse AWS and GCP cost in Azure Cost Management.

**Explaination**: As of now, we can only analyze AWS cost in Azure Cost Management; GCP is not supported yet.

---


## implementing-virtual-networking

**1. Which of the following statements is false? Select all that apply.**


* [ ] Azure Firewall is a Layer 4 firewall-as-a-service solution
* [ ] Azure Firewall supports public IP
* [ ] Azure Firewall has built in threat intelligence
* [ ] Azure Firewall doesn’t require dedicated subnet

**Correct answer:**
* [x] Azure Firewall is a Layer 4 firewall-as-a-service solution
* [x] Azure Firewall doesn’t require dedicated subnet

**Explaination**: Azure Firewall is a Layer 7 firewall, and it requires a dedicated subnet.

**2. Which of the following statement is false?**


* [ ] Virtual Networks are representation of cloud network.
* [ ] Virtual Network is a global service in Azure used to connect to other virtual networks and on-premises environment.
* [ ] Virtual network can be divided to small sub networks called subnets.
* [ ] A subscription is required to create an Azure virtual network.

**Correct answer:**
* [x] Virtual Network is a global service in Azure used to connect to other virtual networks and on-premises environment.

**Explaination**: Virtual Network is not a global service, it’s a regional service.

**3. You are setting up a service endpoint between your VM-1 deployed in East US and storage account in West US. Which IP address will be used by the VM to connect to the storage account using service endpoint?**


* [ ] Public IP of the VM
* [ ] Private IP of the VM
* [ ] NAT IP of the VM
* [ ] Cross region is not possible with Service Endpoints

**Correct answer:**
* [x] Private IP of the VM

**Explaination**: The VM will be using its private IP to communicate with the storage account using a service endpoint.

**4. In Azure, without the need for deploying a NAT gateway, all VMs can communicate to the internet. What is the reason for this?**


* [ ] System route
* [ ] Built in NAT gateway
* [ ] VPN
* [ ] Built in internet gateway

**Correct answer:**
* [x] System route

**Explaination**: With the help of system routes, VMs can communicate to the internet without the need to deploy any additional infrastructure.

**5. You need to assign a static zone redundant IP address to one of your DNS servers to ensure that the IP address is not getting changed during planned reboot events. Which Public IP SKU should be selected for this IP?**


* [ ] Basic
* [ ] Premium
* [ ] Reserved
* [ ] Standard

**Correct answer:**
* [x] Standard

**Explaination**: Standard SKU supports zone redundancy and static IP allocation.

**6. Your organization is planning to host DNS in Azure by which VMs can resolve names of other VMs. Which of the following services should be used to create a DNS zone for name resolution in an Azure virtual network?**


* [ ] DNS zones
* [ ] Private DNS zones
* [ ] Virtual DNS
* [ ] Delegated DNS zone

**Correct answer:**
* [x] Private DNS zones

**Explaination**: Private DNS zones can be created to provide name resolution for services deployed in a virtual network. 

**7. You are setting up a hybrid environment. Your webservers are deployed in on-premises infrastructure, and you have a SQL Database deployed in Azure. You have already setup the VPN connection and the webserver can connect to resources in the Azure virtual network. Since Azure SQL Database is a PaaS solution and it’s not directly integrated to the virtual network, you cannot connect to it using a private IP. Your security asked you to find a way to connect to PaaS services via private IP. What is your recommendation?**


* [ ] Use Service Endpoint
* [ ] Use ExpressRoute
* [ ] Use Private Link
* [ ] Use Private DB Connection

**Correct answer:**
* [x] Use Private Link

**Explaination**: Private Link connection will create an interface for the SQL service in the virtual network and the on-premises resources can connect to it using the private IP.

**8. You have created a DNS zone in Azure called “kodekloudlabs.com”. You added records of your public-facing webservers to the DNS zone. When you try to resolve the DNS names from your Windows computer, they are not getting resolved. What could be the reason?**


* [ ] You need to create an on-premises DNS server to resolve the domain name
* [ ] You need to make sure that the requests are going to the Azure provided name servers for resolution
* [ ] You need to change the visibility of your Azure zone to public
* [ ] Try ipconfig /flushdns and retry querying the record

**Correct answer:**
* [x] You need to make sure that the requests are going to the Azure provided name servers for resolution

**Explaination**: For the name resolution to work, we need to make sure that the queries are pointed to the name servers provided by Azure.

**9. VM1 is deployed to subnet-1 in VNet-1. We have NSG-1 assigned to subnet-1 and NSG-2 assigned to NIC of VM1. You need to confirm if inbound RDP traffic will be allowed by looking at the following inbound rules: <br> ![question-image](https://res.cloudinary.com/dezmljkdo/image/upload/v1656356062/az104/table3_c4e1xa.png) <br> Select true if traffic is allowed, else select false.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**Explaination**: Inbound traffic will hit NSG-1 and because of rule 102 traffic will be allowed. Then the traffic will be evaluated by NSG-2, because of rule 104 traffic will be allowed to the VM.

**10. You are not able to connect to one of your VMs on port 22. Following is the NSG inbound rules attached to VM NIC: ![question-image](https://res.cloudinary.com/dezmljkdo/image/upload/v1656356062/az104/table2_clnmvy.png) <br>What should be done to establish SSH connectivity to the VM?**


* [ ] Change priority to 102 to 100
* [ ] Add a new rule with priority of 105 to allow SSH (22)
* [ ] Add a new rule with priority of 1 to allow SSH (22)
* [ ] Change Action of 101 to Allow

**Correct answer:**
* [x] Change priority to 102 to 100

**Explaination**: As the rule 101 is blocking port range 15-24, communication via 22 will be denied. Changing the action of this rule to “Allow” will allow all the ports mentioned in the destination port. As the priority value starts from 100, we cannot add a new rule with allow action. Rules 103 and 104 will be never evaluated as 101 includes the IPs and ports specified in 103 and 104. Based on options, the best choice is to change the priority of 102 to 100.

---


## chapter-7

**1. You are setting up Windows Server in Azure and would like to establish command line connectivity to the Windows VM. Which is the default port used for this?**


* [ ] 3389
* [ ] 22
* [ ] 21
* [ ] 5986

**Correct answer:**
* [x] 5986

**Explaination**: WinRM is used to take command line connectivity to Windows VMs, the default port used by WinRM is 5986.

**2. Currently, your organization is using jumpbox VMs to connect to the virtual machines that are deployed in the VNet. Your security team would like to eliminate these jumpbox VMs and replace them with a PaaS solution for better security. Which solution would you suggest?**


* [ ] Azure Bastion
* [ ] Azure Firewall
* [ ] Azure Load Balancer
* [ ] Azure Private Link

**Correct answer:**
* [x] Azure Bastion

**Explaination**: Azure Bastion is a PaaS solution that can replace jumpbox VMs and the need to manage the public IPs. Connectivity to VMs can be established directly from Azure Portal with the help of Bastion host.

**3. Which of the following statements about Azure Bastion is true? Select all that apply.**


* [ ] Azure Bastion can be deployed to any subnet where your VMs are deployed to connect to the VMs.
* [ ] Azure Bastion is PaaS solution.
* [ ] Azure Bastion can be used for Windows VMs only.
* [ ] Azure Bastion (standard tier) can automatically scale in and out based on the number of requests.

**Correct answer:**
* [x] Azure Bastion is PaaS solution.
* [x] Azure Bastion (standard tier) can automatically scale in and out based on the number of requests.

**Explaination**: Azure Bastion requires a dedicated subnet called AzureBastionSubnet, it cannot be deployed alongside with other VMs.
Also Azure Bastion can be used for both Linux and Windows VMs.

**4. Your organization needs to encrypt data-in-use due to the sensitivity of the data your organization is handling. Which of the following computing options should you use to achieve this?**


* [ ] Azure Disk Encryption
* [ ] Storage Service Encryption
* [ ] Confidential Computing
* [ ] Transparent Data Encryption

**Correct answer:**
* [x] Confidential Computing

**Explaination**: Confidential computing offers encryption for data-in-use and it’s ideal for organization that handle sensitive data.

**5. You are deploying DS2v2 VM in East US with a single disk, and your application is quite I/O intensive. Which disk tier will be ideal for your virtual machine?**


* [ ] Ultra SSD
* [ ] Super Fast SSD
* [ ] Premium SSD
* [ ] Standard SSD

**Correct answer:**
* [x] Premium SSD

**Explaination**: Premium SSDs are apt for applications which are I/O intensive. Though Ultra-disks has higher IOPS compared to Premium SSDs, they are not available for DSv2 family and cannot be used as an OS disk.

**6. What is the SLA offered by Microsoft if you are deploying two or more Virtual Machines across an availability set?**


* [ ] 99.00%
* [ ] 99.90%
* [ ] 99.95%
* [ ] 99.99%

**Correct answer:**
* [x] 99.95%

**Explaination**: Availability set offers 99.95% SLA.

**7. You are using Availability Set and your application team is insisting on creating the availability to 99.99%. What is the best way to increase SLA for your application?**


* [ ] Deploy more instances
* [ ] Use Availability Zones
* [ ] Increase the number of fault domains and update domains
* [ ] Roll back to single VMs and use Azure Load Balancer

**Correct answer:**
* [x] Use Availability Zones

**Explaination**: By leveraging Availability Zones, we can increase the SLA to 99.99% and we can save our VMs from datacenter failures.

**8. Currently, your organization's web application is running on single VMs and in Microsoft documentation, you found that there is a service called Virtual Machine Scale Set. What are the advantages of using Virtual Machine Scale Set compared to Virtual Machines? Select all that apply.**


* [ ] Virtual Machine Scale Set can automatically change the number of instances based on demand
* [ ] Virtual Machine scale set can be easily integrated with Azure Load Balancer or Application Gateway and there is no need to update the backend pool if the number of instances change.
* [ ] Marketplace and custom images can be scaled with Virtual Machine Scaleset.
* [ ] Cost optimization

**Correct answer:**
* [x] Virtual Machine Scale Set can automatically change the number of instances based on demand
* [x] Virtual Machine scale set can be easily integrated with Azure Load Balancer or Application Gateway and there is no need to update the backend pool if the number of instances change.
* [x] Marketplace and custom images can be scaled with Virtual Machine Scaleset.
* [x] Cost optimization

**Explaination**: All options are correct.

---


## load-balancing

**1. You created two App Services in East US region and are trying to load balance the request with the help of Azure Load Balancer. However, when you configure the backend pool for load balancer, you are not able to find the App Services. You confirmed that these are deployed in the same subscription, resource group and region as load balancer.**


* [ ] Make sure App Service is not in stopped state.
* [ ] Ensure you are using the Standard or Premium tier of App Service Plan
* [ ] Contact Microsoft support
* [ ] Azure Load Balancer only supports Virtual Machines and Virtual Machine Scale Set

**Correct answer:**
* [x] Azure Load Balancer only supports Virtual Machines and Virtual Machine Scale Set

**Explaination**: Azure Load Balancer only supports VMs and VMSS. For App Services, you can consider using other load balancing solutions.

**2. You are setting up a load balancer with the default session persistence settings. What are the factors that will be considered for routing the traffic to backend servers with the default session persistence settings?**


* [ ] Source IP, Destination IP, Protocol, Source Port, Destination Port
* [ ] Source, Destination IP, Source Port, Destination Port
* [ ] Source IP, Destination IP
* [ ] Source IP, Destination IP, Protocol

**Correct answer:**
* [x] Source IP, Destination IP, Protocol, Source Port, Destination Port

**Explaination**: By default, Azure Load Balancer uses 5-tuple hash which comprises of Source IP, Destination IP, Protocol, Source Port, and Destination Port.

**3. Azure Application Gateway is a Layer ____ load balancer.**


* [ ] 4
* [ ] 5
* [ ] 6
* [ ] 7

**Correct answer:**
* [x] 7

**Explaination**: Azure Application Gateway is a Layer-7 Load Balancer.

**4. Azure Load Balancer is a Layer ________ load balancer.**


* [ ] 3
* [ ] 4
* [ ] 5
* [ ] 7

**Correct answer:**
* [x] 4

**Explaination**: Azure Load Balancer is an L4 Load Balancer.

**5. While deploying Azure Load Balancer, you came to know there are multiple SKUs for Azure Load Balancer. Your application is a production application that requires 99.99% SLA. Which SKU would you choose?**


* [ ] Basic
* [ ] Standard
* [ ] Premium
* [ ] Production

**Correct answer:**
* [x] Standard

**Explaination**: Azure Load Balancer is offered in Basic and Standard SKU. Only the Standard SKU offers SLA.

**6. Which of the following features are supported by Azure Application Gateway? Select all that apply.**


* [ ] URL Redirect
* [ ] SSL termination
* [ ] HTTP header rewrite
* [ ] Custom error pages

**Correct answer:**
* [x] URL Redirect
* [x] SSL termination
* [x] HTTP header rewrite
* [x] Custom error pages

**Explaination**: All of the above features are supported by Azure Application Gateway.

**7. One of your applications behind Application Gateway was attacked by attackers and your security team found that they used cross-site scripting to attack the application. Which optional component of Application Gateway should be enabled to avoid these kinds of attacks?**


* [ ] Application Proxy
* [ ] DDOS Protection
* [ ] Web Application Firewall
* [ ] Azure Firewall

**Correct answer:**
* [x] Web Application Firewall

**Explaination**: WAF (Web Application Firewall) can be used to protect your web applications from cross site scripting and other known vulnerabilities.

**8. Which of the following statements about Azure Application Gateway is true? Select all that apply**


* [ ] Azure Application Gateway supports Azure Virtual Machines, Azure VMSS, App Services and App Service deployment slots as backend pools.
* [ ] Azure Application Gateway cannot be used to load balance requests to on-premises application.
* [ ] Behind a single Application Gateway, multiple sites can be hosted.
* [ ] We cannot create empty backend pools in Application Gateway.

**Correct answer:**
* [x] Azure Application Gateway supports Azure Virtual Machines, Azure VMSS, App Services and App Service deployment slots as backend pools.
* [x] Behind a single Application Gateway, multiple sites can be hosted.

**Explaination**: Azure Application Gateway can be used to load balance requests to on-premises or non-Azure web applications. We can create empty backend pools in Application Gateway and backend servers can be added later.

**9. Which of the following is considered as a DNS resolver in Azure?**


* [ ] Azure Load Balancer
* [ ] Azure Traffic Manager
* [ ] Azure Application Gateway
* [ ] Azure Front Door

**Correct answer:**
* [x] Azure Traffic Manager

**Explaination**: Azure Traffic Manager distributes traffic optimally to services across Azure regions and is a DNS resolver.

**10. If we take all the load balancing solutions in Azure, which of the following can be used for public facing non-Azure applications? Select all that apply.**


* [ ] Azure Load Balancer
* [ ] Azure Traffic Manager
* [ ] Azure Application Gateway
* [ ] Azure Front Door

**Correct answer:**
* [x] Azure Traffic Manager
* [x] Azure Application Gateway
* [x] Azure Front Door

**Explaination**: All load balancing solutions except Azure Load Balancer can be used with non-Azure applications.

---


## intersite-connectivity

**1. Which would be the cheapest method to establish private connectivity between two virtual networks in Azure? The estimated data transfer is 10 GB per month.**


* [ ] VNet-to-VNet connection
* [ ] ExpressRoute
* [ ] Site-to-Site connection
* [ ] Virtual Network Peering

**Correct answer:**
* [x] Virtual Network Peering

**Explaination**: Virtual Network Peering is cheap as we need to pay only for the ingress and egress data. In the case of gateways, there will be a fixed charge for gateways in addition to the egress cost.

**2. You would like to set up an architecture where you can use the gateway deployed in the peered network to send traffic to another destination, say, on-premises. Which feature should you enable while setting up peering?**


* [ ] Gateway transfer
* [ ] Gateway transit
* [ ] Route table
* [ ] Transit route

**Correct answer:**
* [x] Gateway transit

**Explaination**: Gateway transit needs to be enabled to route the traffic from a peered network through the gateway to another destination.

**3. What is the maximum number of peering a single virtual network can have?**


* [ ] 100
* [ ] 200
* [ ] 400
* [ ] 500

**Correct answer:**
* [x] 500

**Explaination**: You can have up to 500 peering per virtual network.

**4. Which of the following connections can be used as a failover path for ExpressRoute to on-premise site?**


* [ ] Point-to-Site
* [ ] Site-to-Site
* [ ] ExpressRoute failover
* [ ] Gateway transit

**Correct answer:**
* [x] Site-to-Site

**Explaination**: Site-to-site and ExpressRoute connections can co-exist and S2S can act as a failover path for ExpressRoute.

**5. XYZ Inc. has offices in NYC, LAX and DAL. In addition to deployments in these on-premises offices they have deployment in Azure as well. They need to set-up Site-to-Site connection from Azure to each of their offices. As of now, they only have one virtual network in Azure; how many Virtual Network Gateways and Local Network Gateways are required in Azure to set-up the connectivity?**


* [ ] 3 VPN Gateways and 3 Local Network Gateways
* [ ] 1 VPN Gateway and 3 Local Network Gateways
* [ ] 3 VPN Gateways and 1 Local Network Gateways
* [ ] 1 VPN Gateway and 1 Local Network Gateway

**Correct answer:**
* [x] 1 VPN Gateway and 3 Local Network Gateways

**Explaination**: You can have only one Virtual Network Gateway per virtual network, however, we can establish multiple connections to the single gateway. The number of connections is dependent on the SKU of the VPN Gateway. Since there are three offices, you need to have three local network gateways in Azure referencing each of the IP/FQDN of on-premises devices in-office.

**6. We have the following VNets in Azure. For which scenarios can we establish peering? ![question-image](https://res.cloudinary.com/dezmljkdo/image/upload/v1669304447/az104/az104-vnet_smp2ym.png)**


* [ ] A, B, C and D
* [ ] A and B
* [ ] A and C
* [ ] A, C, and D

**Correct answer:**
* [x] A and C

**Explaination**: In scenario B, the address spaces are overlapping so we cannot establish peering. In scenario-D, the CIDR for VNet-a is /30; in Azure we can only create till /29. Since the network itself cannot be created, peering cannot be established.

**7. There are three virtual networks in Azure, vnet-a is peered with vnet-b and vnet-b is peered with vnet-c. The resources deployed in vnet-a can communicate with resources in vnet-c.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**Explaination**: Virtual Network Peering is non-transitive.

---


## automating-deployment-and-configuration

**1. You need to pass a value during the ARM template deployment. How do we define this in the template?**


* [ ] Use parameters
* [ ] Store as a variable and modify as required
* [ ] Create a function which accepts the value
* [ ] Create a dynamic variable

**Correct answer:**
* [x] Use parameters

**Explaination**: With the help of parameters, we can provide values at the time of template execution.

**2. Which PowerShell command is used to deploy an ARM template?**


* [ ] New-AzTemplateDeployment
* [ ] New-AzGroupTemplateDeployment
* [ ] New-AzResourceGroupTemplate
* [ ] New-AzResourceGroupDeployment

**Correct answer:**
* [x] New-AzResourceGroupDeployment

**Explaination**: New-AzResourceGroupDeployment command can be used to deploy an ARM template using PowerShell.

**3. In your organization, there is a need to create a custom Linux image. This image should contain the Apache, PHP and MySQL installation. Also, the default files should be modified with your own HTML files. This custom image will be used with Virtual Machine Scale Set. Your Azure administrator already created a VM with the required software and files. What type of image should you create?**


* [ ] Snapshot
* [ ] Specialized image
* [ ] Backup image
* [ ] Generalized image

**Correct answer:**
* [x] Generalized image

**Explaination**: Generalized image needs to be created. VMs created from this image require a hostname, admin user, and other VM related setup. This image can be used to create VMs or VMSS.

**4. Which of the following connections can be used as a failover path for ExpressRoute to on-premise site?**


* [ ] Point-to-Site
* [ ] Site-to-Site
* [ ] ExpressRoute failover
* [ ] Gateway transit

**Correct answer:**
* [x] Site-to-Site

**Explaination**: Site-to-site and ExpressRoute connections can co-exist and S2S can act as a failover path for ExpressRoute.

**5. You would like to export all the resources in a resource group to an ARM template. Which of the following methods can be used. Select all that apply.**


* [ ] Use Deployments in Resource Group blade and export the templates
* [ ] Use Export template option in Resource Group blade
* [ ] Use az group export command in Azure CLI
* [ ] Use Export-AzResourceGroupTemplate command in Azure PowerShell

**Correct answer:**
* [x] Use Export template option in Resource Group blade
* [x] Use az group export command in Azure CLI

**Explaination**: We can use the export option in Azure Portal or use the az group export command in Azure CLI. If you want to export using Azure PowerShell, then you need to use the Export-AzResourceGroup command.

**6. Your Windows administrator would like to create multiple domain controllers and file servers using Azure VMs. They already have a PowerShell script for this, the script requires reboot and has complex installation. Which extension would you prefer to accomplish this task?**


* [ ] Custom Script Extension
* [ ] PowerShell Remoting Extension
* [ ] Configuration Management Extension
* [ ] Desired State Configuration Extension

**Correct answer:**
* [x] Desired State Configuration Extension

**Explaination**: DSC extension can be used for complex installations that require reboot.

**7. What is the maximum run window we have for Custom Script Extension before it hits timeout?**


* [ ] 1.5 hours
* [ ] 1 hour
* [ ] 30 minutes
* [ ] 15 minutes

**Correct answer:**
* [x] 1.5 hours

**Explaination**: CSE can run scripts up to 90 minutes, anything more that will be returned as a timed out operation.

---


## securing-storage

**1. You have successfully set up the storage account with GZRS redundancy. How many copies of data will be created by Azure for this redundancy?**


* [ ] 3
* [ ] 4
* [ ] 5
* [ ] 6

**Correct answer:**
* [x] 6

**Explaination**: GZRS will have six copies of data, three in the primary region and three in the secondary region.

**2. What type of encryption is used by Storage Service Encryption?**


* [ ] 128 bit RSA
* [ ] 128 bit AES
* [ ] 256 bit AES
* [ ] 512 bit AES

**Correct answer:**
* [x] 256 bit AES

**Explaination**: SSE uses 256-bit AES encryption, which is one of the strongest block ciphers.

**3. Which of the following statement is incorrect?**


* [ ] SSE allows you to use Microsoft-managed keys and Customer-managed keys
* [ ] SSE can be disabled for testing purposes from Azure Portal if required
* [ ] Customer-managed keys only supports encryption of files and blobs
* [ ] SSE automatically decrypts the data when you want to access it

**Correct answer:**
* [x] SSE can be disabled for testing purposes from Azure Portal if required

**Explaination**: SSE cannot be disabled.

**4. Your organization would like to set-up Geo-Zone-Redundant storage account. Which type of storage account do you recommend for this redundancy?**


* [ ] Premium Blob Storage
* [ ] StorageV1
* [ ] StorageV2
* [ ] Premium File Storage

**Correct answer:**
* [x] StorageV2

**Explaination**: General Purpose v2 or StorageV2 accounts are needed to use GZRS or RA-GZRS redundancy.

**5. In Azure Disk Encryption for Linux VMs, which tool is used for encryption?**


* [ ] BitLocker
* [ ] Crypt-Linux
* [ ] DM-Crypt
* [ ] Crypto

**Correct answer:**
* [x] DM-Crypt

**Explaination**: DM-Crypt is used for Linux VMs and BitLocker is used for Windows VMs.

**6. In your development environment, your developers require a storage account which offers at least 99.999999999999 (12 “9s”) of durability. Which type of redundancy should you choose? Make sure your choice is the cheapest one.**


* [ ] LRS
* [ ] ZRS
* [ ] GRS
* [ ] GZRS

**Correct answer:**
* [x] ZRS

**Explaination**: ZRS offers 99.999999999999% durability, while GRS and GZRS offers 99.(16 9’s) of durability. LRS being the cheapest option offers only 99.(“11 9s”) hence we cannot choose that. The right answer is ZRS.

**7. Your organization follows the principle of least privilege and uses custom RBAC roles to segregate the roles and responsibilities. You need to make sure that your storage admins are able to view, read, and copy the storage account keys. Which action should you add to your custom role?**


* [ ] Microsoft.Storage/storageAccounts/listkeys/action
* [ ] Microsoft.Storage/storageAccounts/keys/*
* [ ] Microsoft.Storage/storageAccounts/keys/action
* [ ] Microsoft.Storage/storageAccounts/listkeys

**Correct answer:**
* [x] Microsoft.Storage/storageAccounts/listkeys/action

**Explaination**: Users with Microsoft.Storage/storageAccounts/listkeys/action permission will be able to view, read, and copy the storage account keys.

**8. You are partnering with another organization to develop a C# application for your organization. They need a storage account to write logs from development and they need access to the storage account for 31 days. Due to organizational policies, you cannot set up guest accounts for these developers in your tenant. Which authorization method should you use?**


* [ ] Storage keys
* [ ] Azure AD + RBAC
* [ ] Storage Access Signature
* [ ] Just in time access

**Correct answer:**
* [x] Storage Access Signature

**Explaination**: With the help of SAS, we will be able to provide time bound access to our storage account.

**9. To which all-storage account services can we enable anonymous access?**


* [ ] Blobs
* [ ] Files
* [ ] Queues
* [ ] Tables

**Correct answer:**
* [x] Blobs

**Explaination**: We can set up anonymous access only for Blob service.

**10. As per your organizational policy, you need to rotate the storage account keys every 60 days. What all things you need to consider before making this change? Select all that apply.**


* [ ] All applications using the storage account keys will no longer be able to access the storage account
* [ ] There will be a minor downtime during the process and users accessing storage account using Azure AD will be affected.
* [ ] All SAS token generated with the keys will be revoked
* [ ] No impact on storage account

**Correct answer:**
* [x] All applications using the storage account keys will no longer be able to access the storage account
* [x] All SAS token generated with the keys will be revoked

**Explaination**: Rotating the storage account keys will revoke all SAS tokens and stop applications from accessing the storage account as they are still using old keys. You can consider storing the keys in Key Vault so that the application can obtain the keys from the vault. Users using Azure AD will not be impacted as storage account keys are not used in Azure AD authentication.

---


## administering-azure-blobs-and-azure-files

**1. Your storage administrator has set the public access level of a blob container to “Blob”. What does that mean?**


* [ ] Users will have anonymous read access to a single blob
* [ ] Users will have read access to all blobs in the storage account
* [ ] Users will have read access to blobs in the container
* [ ] Users will have read access to all containers and blobs

**Correct answer:**
* [x] Users will have read access to blobs in the container

**Explaination**: Setting the access level of a container to blob will allow anonymous read access to blobs in the container, however, they will not be able to list the blobs in the container. For this, you require anonymous read access at the container level.

**2. You started using blob access tier, however, the manual conversion of access tiers is not feasible considering the amount of data. You are looking for a solution by which you can automatically transition between the access tiers. What do you recommend?**


* [ ] Import/Export tool
* [ ] AzCopy
* [ ] Lifecycle management
* [ ] CORS

**Correct answer:**
* [x] Lifecycle management

**Explaination**: Lifecycle Management can be used to transition blobs automatically to the next access tier and eventually delete (if required) based on the last modified date.

**3. To which all-storage account services can we enable anonymous access?**


* [ ] Blobs
* [ ] Files
* [ ] Queues
* [ ] Tables

**Correct answer:**
* [x] Blobs

**Explaination**: We can set up anonymous access only for Blob service.

**4. One of your applications is writing a lot of data to an Azure Storage account. Only a part of it’s accessed regularly, rest of the data remain in the storage account. You should consider which feature of blob storage to optimize the cost of the storage?**


* [ ] Use Blob Scavenging option
* [ ] Use Blob Access Tiers
* [ ] Use Blob Access Policy
* [ ] Use Blob Clean up tool

**Correct answer:**
* [x] Use Blob Access Tiers

**Explaination**: With the help of access tier, you can transition data between hot, cool, and archive based on the frequency of access.

**5. Now that you are aware of Lifecycle Management, you started implementing it for all storage accounts from Azure Portal. One of your old storage accounts, which is a Premium General Purpose v1 storage account deployed in East US, cannot use this feature. What could be the reason?**


* [ ] GPv1 doesn’t support lifecycle management
* [ ] You need to have dedicated storage permission like Storage Blob Data Contributor to enable this feature
* [ ] By default, lifecycle management is disabled for GPv1, however, this can be enabled from storage account properties
* [ ] You cannot enable from Azure Portal, v1 account requires PowerShell or CLI to enable lifecycle management

**Correct answer:**
* [x] GPv1 doesn’t support lifecycle management

**Explaination**: GPv1 storage accounts don't support lifecycle management.

**6. Your storage administrator created a file share for you and when you mount it to your on-premises Windows server which is behind a firewall, you are not able to connect to the share. What should be done?**


* [ ] Put the Windows server in DMZ
* [ ] Open port 445 on the firewall
* [ ] Provide the Azure AD credentials to complete the setup
* [ ] Add Windows file server role to the server

**Correct answer:**
* [x] Open port 445 on the firewall

**Explaination**: Azure File share uses SMB and requires port 445 to be open.

**7. Which one of the following is not a use case of Azure Blob Storage?**


* [ ] Embed images or documents in webpages
* [ ] Stream video and audio directly to browser
* [ ] Store files for distributed access in websites
* [ ] Mount as a common storage for virtual machines

**Correct answer:**
* [x] Mount as a common storage for virtual machines

**Explaination**: Blob Storage cannot be mounted as a shared storage for virtual machines, for that, you need to use Azure File Storage.

---


## managing-storage

**1. You have a few GBs of data that needs to be copied to Azure Blobs everyday at 4:00 AM. Which tool do you recommend for this?**


* [ ] Import/Export tool
* [ ] Storage Explore
* [ ] Azure Portal
* [ ] AZCopy

**Correct answer:**
* [x] AZCopy

**Explaination**: Since we want to copy the data everyday at 4:00 AM, we are looking for more automation. We can write an AZCopy script and create a scheduled task or cron job to copy the data everyday at 4:00 AM. 

**2. Which tool is used to prepare the disks in the Import/Export tool?**


* [ ] AzImportExport tool
* [ ] WAImportExport tool
* [ ] PSImportExport tool
* [ ] ImportExport tool

**Correct answer:**
* [x] WAImportExport tool

**Explaination**: The WAImportExport tool is used to prepare the disks, copy the contents and generate the journal files.

---


## azure-app-service

**1. One of your production applications requires App Service Backup. The connected MySQL PaaS database is of size 8.3 GB. How can we back this up?**


* [ ] Use App Service Backup which supports backup of application and connected database
* [ ] Use Azure Site Recovery and setup backup for the database
* [ ] Use Azure Backup for Database
* [ ] Databases cannot be used with Azure App Services

**Correct answer:**
* [x] Use Azure Backup for Database

**Explaination**: The maximum backup database size supported by Azure App Service is 4 GB since the size is larger than that, you need to use Azure Backup for Database.

**2. Following the PaaS first approach, you started using Azure App Services. Before moving to production, you need to test your applications on Azure App Service. As per your development team, they require dedicated compute which is ideal for development and testing. What tier do you suggest? Make sure your suggestion is the cheapest option.**


* [ ] Free
* [ ] Shared
* [ ] Basic
* [ ] Standard

**Correct answer:**
* [x] Basic

**Explaination**: The Basic plan provides dedicated compute and is recommended for all development and testing purposes. Even though Free and Shared are recommended for development and they are cheap, they are using shared compute.

**3. You have a production application with the name kodekloud and the URL for the app is https://kodekloudsales.azurewebsites.net. You would like to set up a custom domain name sales.kodekloud.com. Which record type should you add to verify the ownership before you configure the alias?**


* [ ] CNAME
* [ ] SRV
* [ ] A
* [ ] TXT

**Correct answer:**
* [x] TXT

**Explaination**: You need to add the unique ID as the value of the TXT record in your domain to verify the domain ownership.

**4. You have a webapp with the URL: kodekloud-courses.azurewebsites.net. You created a deployment slot named "dev". What would be the URL to the slot?**


* [ ] dev-kodekloud-courses.azurewebsites.net
* [ ] kodekloud-courses-dev.azurewebsites.net
* [ ] kodekloud-courses.dev.azurewebsites.net
* [ ] dev.kodekloud-courses.azurewebsites.net

**Correct answer:**
* [x] kodekloud-courses-dev.azurewebsites.net

**Explaination**: The name of the slot will be added as a suffix to the production slot.

**5. Your production application is facing performance issues while too many users are accessing your application. Currently, your application is running on a Basic App Service Plan. As per your analysis, you require five instances to handle the load. Suggest a solution for this and make sure your choice is the cheapest one.**


* [ ] Enable autoscaling for Basic tier
* [ ] Upgrade to Isolated tier
* [ ] Upgrade to Premium tier
* [ ] Upgrade to Standard tier

**Correct answer:**
* [x] Upgrade to Standard tier

**Explaination**: Basic doesn’t offer auto scaling, you need to upgrade to Standard as it offers a maximum of five instances and is cheaper, considering other tiers that support autoscaling.

**6. Out of the following CI/CD methods, which are considered as manual deployment methods? Select all that apply.**


* [ ] Bitbucket
* [ ] OneDrive
* [ ] Dropbox
* [ ] External Git

**Correct answer:**
* [x] OneDrive
* [x] Dropbox
* [x] External Git

**Explaination**: All choices except Bitbucket are manual deployment methods. Bitbucket, Git, GitHub, and Azure Repos are considered as automated deployment options.

**7. Which of the following can be swapped during a deployment slot swap?**


* [ ] Connection strings
* [ ] Custom domain names
* [ ] TLS/SSL settings
* [ ] CORS

**Correct answer:**
* [x] Connection strings

**Explaination**: Connection strings can be swapped during a deployment slot swap, rest of the choices cannot be swapped.

**8. What factor determines the number of deployment slots you can have for an App Service?**


* [ ] Tier of the App Service Plan
* [ ] Operating System of the plan
* [ ] Runtime of the webapp
* [ ] Location of the webapp

**Correct answer:**
* [x] Tier of the App Service Plan

**Explaination**: App Service Plan tier decides the number of deployment slots you can have for a webapp. Deployment slot is supported only from Standard plan onwards.

---


## configuring-containers

**1. You are planning to deploy Azure Kubernetes Services. As per specifications, you would require a three node cluster. How the pricing for AKS is calculated for compute resources?**


* [ ] Pay for the number of nodes and Azure managed nodes
* [ ] Pay for AKS, based on the tier you will be charged. There is no additional compute cost
* [ ] Pay for the three nodes
* [ ] Pay for Azure managed node only

**Correct answer:**
* [x] Pay for the three nodes

**Explaination**: You need to pay only for the number of customer managed nodes, there is no cost for the Microsoft managed node.

**2. You need to open a specific port in your AKS cluster for internal communication. Which service should you use?**


* [ ] Ingress Controller
* [ ] ClusterIP
* [ ] NodePort
* [ ] LoadBalancer

**Correct answer:**
* [x] ClusterIP

**Explaination**: ClusterIP facilitates internal communication with other apps in your cluster. There is no external access.

**3. For creating a Kubernetes object, you need to write the manifest files. Which extensions are supported for the manifest file? Select all that apply.**


* [ ] YAML
* [ ] JSON
* [ ] XML
* [ ] ARM

**Correct answer:**
* [x] YAML
* [x] JSON

**Explaination**: Kubernetes supports YAML and JSON languages for writing manifests.

**4. Which of the following methods can be used to deploy a container group? Select all that apply.**


* [ ] Azure PowerShell
* [ ] YAML
* [ ] Azure Portal
* [ ] Azure Resource Manager templates

**Correct answer:**
* [x] YAML
* [x] Azure Resource Manager templates

**Explaination**: We can use YAML and ARM templates to deploy Container Groups. If your container groups include Azure resources like file share, then ARM template is the better option.

**5. While setting up the networking for AKS, you need to make sure each pod in your cluster get an IP address from the virtual network. Which network configuration should you select?**


* [ ] Kubenet
* [ ] Kube-proxy
* [ ] Azure CNI
* [ ] Kubelet

**Correct answer:**
* [x] Azure CNI

**Explaination**: Azure CNI networking plug-in allows cluster to use a new or existing VNet with customizable addresses. Application pods are connected directly to the VNet, which allows for native integration with VNet features.

**6. Which service is used for AKS bursting?**


* [ ] Azure Virtual Machines
* [ ] Azure Virtual Machines Scale Set
* [ ] Azure Container Registry
* [ ] Azure Container Instances

**Correct answer:**
* [x] Azure Container Instances

**Explaination**: Azure Container Instances are used to create virtual nodes during AKS bursting due to the low start-up time compared to virtual machines.

**7. What do we call the smallest unit of application instance which is a collection of one or more containers?**


* [ ] Pod
* [ ] Deployment
* [ ] Service
* [ ] Replica Set

**Correct answer:**
* [x] Pod

**Explaination**: Pod is the single instance of your application which is a collection of one or more containers.

**8. Which of the following statement is incorrect?**


* [ ] Azure Container Instances is a PaaS solution
* [ ] You can only have public IPs for ACI
* [ ] You can use Azure Files as persistent storage for ACI
* [ ] ACI can use images stored in Azure Container Registry

**Correct answer:**
* [x] You can only have public IPs for ACI

**Explaination**: Azure Container Instance can have public IPs as well as private IPs.

---


## implement-backup-and-recovery

**1. Which agent should you use to back-up on-premises files and folders?**


* [ ] MABS
* [ ] MARS
* [ ] waagent
* [ ] MMA

**Correct answer:**
* [x] MARS

**Explaination**: Microsoft Azure Recovery Services (MARS) is used to backup on-premises files and folders to Recovery Services Vault.

**2. What is the default redundancy for a newly created Recovery Services Vault?**


* [ ] LRS
* [ ] ZRS
* [ ] GRS
* [ ] GZRS

**Correct answer:**
* [x] GRS

**Explaination**: Recovery Service Vaults are created with the default redundancy as Geo-redundant storage.

**3. We have the following resources in Azure. Which of the following can be backed up to vault-01? ![question-image](https://res.cloudinary.com/dezmljkdo/image/upload/v1661187961/az104/azure5_kldth8.png)**


* [ ] db-vm only
* [ ] db-vm and share01
* [ ] share01 and web-vm
* [ ] db-vm, share01, and web-vm

**Correct answer:**
* [x] db-vm and share01

**Explaination**: Since the vault-01 is in East US, we will be able to select the resources that are there in East US only.

**4. You have the following resources in Azure:<br> <li>Blob container – imgfiles </li> <li>File share – executables</li> <li>VM – VM-01</li> <li>Azure Database for MySQL – wordpress</li> <br> Which of the following can be backed up to a recovery services vault?**


* [ ] File share and VM
* [ ] Blob container, file share, VM, and Azure Database for MySQL
* [ ] File share, VM, and Azure Database for MySQL
* [ ] VM and Azure Database for MySQL

**Correct answer:**
* [x] File share and VM

**Explaination**: In the Recovery Service vault, we can only backup VMs and file shares. For the Blob container and Azure Database for MySQL, you need to use the Backup vault.

**5. Following is the contents of Recovery Services vaults in one of the subscriptions: ![question-image](https://res.cloudinary.com/dezmljkdo/image/upload/v1656356062/az104/table6_d8wqto.png) <br>All of these vaults were created with the default redundancy GRS. Which all vaults can be converted to LRS vaults?**


* [ ] Vault-01 and Vault-02
* [ ] Vault-04 only
* [ ] Vault-02 and Vault-03
* [ ] Vault-03 only

**Correct answer:**
* [x] Vault-03 only

**Explaination**: We can convert GRS vault to the LRS vault before we protect any resources. In this case, vault-03 is empty, which means we can still convert that to LRS.

**6. You have on-premises and Azure VMs that require backup. You are planning to use Microsoft Azure Backup Server to configure the backup. Which of the following on-premises workloads are not supported by MABS?**


* [ ] Windows Server
* [ ] Linux Servers on Hyper-V
* [ ] Oracle workloads
* [ ] Exchange servers

**Correct answer:**
* [x] Oracle workloads

**Explaination**: Oracle workloads are not supported by MABS.

---


## network-monitoring-and-resource-monitoring

**1. Users claim that some of the DNS queries to one of your public facing DNS servers running on Azure VM are failing. You need to check if the requests are hitting the Azure VM. How can we troubleshoot this?**


* [ ] Use NSG flow logs and verify if the clients are reachable from the DNS server
* [ ] Use IP flow to confirm if the port 53 is open for DNS
* [ ] Use Effective Security Rules to verify if the DNS port is open
* [ ] Use Packet Capture to confirm if the request from the clients are hitting the server

**Correct answer:**
* [x] Use Packet Capture to confirm if the request from the clients are hitting the server

**Explaination**: Use Packet Capture on the DNS server and verify if the requests are hitting the server.

**2. You are using Azure Virtual Desktop and users are complaining that a couple of websites take longer to load. They don’t experience the same when they access these websites from their laptops. You need to troubleshoot this by finding the latency and connectivity to the endpoint. Which tool should you use?**


* [ ] Connection Monitor
* [ ] Packet capture
* [ ] IP Flow verify
* [ ] Topology

**Correct answer:**
* [x] Connection Monitor

**Explaination**: Connection Monitor can be used to check the connectivity and latency between your virtual machines and IP addresses/ FQDNs.

**3. Your organization is using ExpressRoute to connect to on-premises. Your network administrators stated that they are seeing performance issues with the circuit. Which tool should you use to troubleshoot this?**


* [ ] ExpressRoute troubleshoot
* [ ] Connection Monitor
* [ ] VPN diagnostics
* [ ] NSG flow logs

**Correct answer:**
* [x] VPN diagnostics

**Explaination**: VPN diagnostics can be run against any gateway to get performance and other relevant metrics. The collected data will be stored in a storage account.

**4. One of your route tables was deleted and you need to filter the activity logs to find who deleted it. Which category should you select to filter the logs?**


* [ ] Alert
* [ ] Policy
* [ ] Resource Health
* [ ] Administrative

**Correct answer:**
* [x] Administrative

**Explaination**: All operations made using Azure Resource Manager will be logged in the Administrative category.

**5. When it comes to the billing of Log Analytics which all meters are considered? Select all that apply.**


* [ ] Data ingestion
* [ ] Connected sources
* [ ] Data retention
* [ ] Number of queries

**Correct answer:**
* [x] Data ingestion
* [x] Data retention

**Explaination**: Microsoft charges for the amount of data ingested and for the number of days data is stored. Retention up to 31 days is free of cost.

**6. What is the default retention period for activity logs?**


* [ ] 30 days
* [ ] 60 days
* [ ] 90 days
* [ ] 120 days

**Correct answer:**
* [x] 90 days

**Explaination**: Activity Logs are stored for 90 days, you can ingest the logs to storage account if you need retention more than 90 days.

**7. In the action group, what are the supported notification methods? Select all that apply.**


* [ ] Secure webhook
* [ ] Webhook
* [ ] Automation Runbook
* [ ] Event Grid

**Correct answer:**
* [x] Secure webhook
* [x] Webhook
* [x] Automation Runbook

**Explaination**: Supported actions are Secure Webhook, Webhook, Logic App, Function App, Automation Runbook and ITSM.

**8. You are ingesting "cron" logs from your Linux servers to Log Analytics. Which table should you query to get the ingested logs?**


* [ ] Heartbeat
* [ ] Syslog
* [ ] Cron
* [ ] Perf

**Correct answer:**
* [x] Syslog

**Explaination**: Cron is one of the facilities provided by syslog, all data ingested will be stored in the syslog table.

**9. Your organization uses Service Now to log internal IT events. You need to set up an alert for high CPU utilization for your production server. When this alert is fired, you need to create a ticket in ServiceNow. What should you do?**


* [ ] In Action groups, use ITSM
* [ ] Create an Event Hub and send the event to Service Now
* [ ] Trigger emails to admins, so that they can create a ticket
* [ ] Use runtime integration

**Correct answer:**
* [x] In Action groups, use ITSM

**Explaination**: Use ITSM to send alerts to your IT service management tools.

**10. Which one of the following is an Application Performance Monitoring tool?**


* [ ] Azure Log Analytics
* [ ] Azure Monitor
* [ ] Azure Application Insights
* [ ] Azure Application Metrics

**Correct answer:**
* [x] Azure Application Insights

**Explaination**: Azure Application Insights is an APM tool that can be used to monitor your application.

**11. Name the datastores used by Azure Monitor to store the collected data? Select all that apply.**


* [ ] Workspace
* [ ] Metrics
* [ ] Activity Logs
* [ ] Logs

**Correct answer:**
* [x] Metrics
* [x] Logs

**Explaination**: Metrics and Logs are the two datastores used by Azure Monitor to store the data collected from a variety of sources.

---


## hashicorp-access-control

**1. As part of an internal initiative your organization has decided to implement the Vault Enterprise feature of Control Groups. The goal is to add a layer of protection for certain activities in Vault. Which of the following statements most accurately describes the function of a Control Group?**


* [ ] Control Groups are a different way of managing access to items in Vault.
* [ ] Control Groups enable fine grained control through "policy as code".
* [ ] Control Groups help organize Vault identities into logical groups to more easily manage access.
* [ ] Control Groups add additional authorization factors to be required before satisfying a request.

**Correct answer:**
* [x] Control Groups add additional authorization factors to be required before satisfying a request.

**2. An internal compliance initiative has been adopted at your organization, and the head of the compliance team is interested in how Sentinel can integrate with Vault. Sentinel can provide a rich set of access control functionality that goes beyond the standard Vault ACL policies. What are the two additional policy types that can be used with Sentinel? (Select two)**


* [ ] Role Governing Policies (RGPs)
* [ ] Secret Governing Policies (SGPs)
* [ ] Authentication Governing Policies (AGPs)
* [ ] Endpoint Governing Policies (EGPs)

**Correct answer:**
* [x] Role Governing Policies (RGPs)
* [x] Endpoint Governing Policies (EGPs)

**3. Your organization has begun using Vault Namespaces to more securely segment data with multi-tenancy. A Vault namespace was created named `NA` at the root level. A secret lives at the `secret/database/postgres` path in the `NA` namespace. Assuming that a Vault policy was granting the `read` permission, which of the following scenarios would properly grant access to that secret. (Select three)**


* [ ] A Vault policy created at the root level with permissions on the `secret/database/postgres` path.
* [ ] A Vault policy created at the root level with permissions on the `NA/secret/database/postgres` path.
* [ ] A Vault policy created in the `NA` Namespace with permissions on the `secret/database/postgres` path.
* [ ] A Vault policy created in the `NA` Namespace with permissions on the `NA/secret/database/postgres` path.
* [ ] A Vault policy created at the root level with permissions on the `+/secret/database/postgres` path.

**Correct answer:**
* [x] A Vault policy created at the root level with permissions on the `NA/secret/database/postgres` path.
* [x] A Vault policy created in the `NA` Namespace with permissions on the `secret/database/postgres` path.
* [x] A Vault policy created at the root level with permissions on the `+/secret/database/postgres` path.

**4. You are currently working on constructing Vault policies to allow other teams to manage secrets for their specific applications. The `acme-secret-manager` policy should grant sufficient permissions to create new secrets, revise existing secrets, and delete secrets under the `secret/finance/acme/` path, as well as ALL child paths. Which Vault policy stanza below would grant the required permissions while ALSO following the concept of "least privilege"? (select one)**


* [ ] `path "secret/finance/acme/" {capabilities = ["create", "update", "delete"]}`
* [ ] `path "secret/finance/acme/*" {capabilities = ["create", "update", "delete"]}`
* [ ] `path "secret/finance/acme/*" {capabilities = ["list", "read", "create", "update", "delete"]}`
* [ ] `path "secret/finance/acme/+" {capabilities = ["create", "update", "delete"]}`
* [ ] `path "secret/finance/acme*" {capabilities = ["create", "update", "delete"]}`

**Correct answer:**
* [x] `path "secret/finance/acme/*" {capabilities = ["create", "update", "delete"]}`

---


## hashicorp-scale-performance

**1. Your organization has already implemented Vault Enterprise and plans to use Performance Replication between the North America and European regions. Due to GDPR compliance, you need to ensure that certain secret data is not replicated from the European region to North America once replication is enabled. Which Vault feature would allow you to enable replication, while also ensuring that specific data in Europe is NOT replicated to North America?**


* [ ] Endpoint Governing Policies (EGPs)
* [ ] Paths filters
* [ ] Namespaces
* [ ] Vault tokens with "token bound CIDRs"

**Correct answer:**
* [x] Paths filters

**Explaination**: EGPs are an integration with Sentinel to restrict access based on paths in Vault.

The paths filter feature enables users to allow or deny which secrets engines are replicated between clusters, and is the best choice.

Vault Namespaces are used for secure tenet isolation, but due not inherently prevent data from being replicated without paths filters.

Tokens with bound CIDRs restrict the range of client IPs allowed to use that token, and would not prevent data from being replicated.


**2. Your organization has recently integrated Vault with a large scale container based application. The application frequently spins up a large number of containers and all will request a token from Vault. Your team has noticed that this activity has a noticeable impact on performance and the storage backend of the Vault cluster. To reduce that impact you are exploring the use of batch tokens. Which statement is true regarding the benefits of batch tokens?**


* [ ] Batch tokens can be set as "periodic", essentially making tokens never expire as long as they are renewed within the TTL.
* [ ] Batch tokens are actually more "heavyweight" than service tokens and require multiple writes to create tokens.
* [ ] You can create batch tokens to act as root tokens.
* [ ] Batch token creations can scale with the number of performance standby nodes.

**Correct answer:**
* [x] Batch token creations can scale with the number of performance standby nodes.

**Explaination**: Batch tokens cannot be created as periodic tokens.

Batch tokens are more "lightweight" than service tokens and have no storage cost for token creations.

Batch tokens cannot be root tokens. Additionally, you should almost never use root tokens during day-to-day operations.

Typical service tokens require a write/create operation, which would need to be forwarded to the leader node. Batch tokens are encrypted blobs that carry enough information for them to be used for Vault actions, and can scale with the number of performance standby nodes in the cluster.

**3. Your team has recently upgraded from open source Vault to Vault Enterprise in order to utilize performance standby nodes. Which of the following is true regarding performance standby nodes?**


* [ ] Performance standby nodes will attempt to locally process client read requests and automatically forward write requests to the leader/active node.
* [ ] Performance standby nodes are only available when using the Consul storage backend.
* [ ] Performance standby nodes scale the overall performance of the Vault cluster by handling both read and write requests locally.
* [ ] Performance standbys can only be used when Performance replication is ALSO enabled.

**Correct answer:**
* [x] Performance standby nodes will attempt to locally process client read requests and automatically forward write requests to the leader/active node.

**Explaination**: Performance standby nodes only handle read requests locally.

Performance standbys can be used with any storage backend that support "high availability" mode, including Integrated Storage (Raft), Consul, ZooKeeper and etcd.

Performance standby nodes will only handle read requests locally and forward write requests to the leader node.

Performance standbys DO NOT require any sort of replication to be enabled.

**4. You are currently working with the executive team to grow the usage of Vault Enterprise to multiple cloud regions and data-centers across your infrastructure. The goal is to have applications in multiple clouds, in multiple regions, all interact with Vault to retrieve secrets. In addition you want to ensure applications can communicate with Vault in the local region to reduce latency for network requests. What would be the best solution the meet these requirements?**


* [ ] Increase the number of nodes in the Vault cluster and use performance standbys to increase performance for the new load from applications.
* [ ] Create new Vault clusters in each region that applications reside. Then enable Disaster Recovery Replication on the new clusters, allowing client requests to be handled locally.
* [ ] Create new Vault clusters in each region that applications reside. Then enable Performance Replication on the new clusters, allowing client requests to be handled locally.
* [ ] Create multiple secret engine mount points on a per-region basis. Each region will have dedicated secret engine mount points, splitting the load of requests from new applications.

**Correct answer:**
* [x] Create new Vault clusters in each region that applications reside. Then enable Performance Replication on the new clusters, allowing client requests to be handled locally.

**Explaination**: Performance standbys increase the performance in a single Vault cluster, but do not meet the requirement of needing to handle Vault requests locally in each region.

DR clusters act as "warm standbys" and cannot handle Vault requests.

PR clusters replicate Vault data to new regions, as well as enabling the handling of client requests locally.

Multiple secret engine mount points would not meet the requirements outlined.

---


## hashicorp-hsm

**1. Due to a new compliance initiative, the ACME company would like to increase security and compliance for their Vault environment. They have sought out your guidence regarding the use of a Hardware Security Module (HSM). They want clarification regarding the availability of their HSM once it is integrated with their Vault cluster as the seal mechanism. ACME currently believes that there is no need for the HSM to be online and available to Vault once the Vault cluster has been unsealed.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**Explaination**: False: The HSM is used for the auto-unseal process, but is also used for other potential operations during normal activity. Such as generating various CSPs or for entropy augmentation, any write operations on mount points with seal wrapping enabled, and Vault token generations with entropy augmentation enabled.

**2. Your organization has already configured the current Vault environment to use an HSM as the seal mechanism for your Vault cluster. Your manager now wants to explore the use of seal wrapping. What statement most accurately describes the benefits of using seal wrapping with an HSM.**


* [ ] Seal wrapping adds an extra layer of security by adding an extra layer to unseal keys. Essentially requiring two different sets of unseal keys to be supplied to unseal the Vault cluster.
* [ ] Seal wrapping is the default encryption used to encrypt Vault data, and does not require a Vault Enterprise license.
* [ ] Seal wrapping adds an extra layer of protection by wrapping values with an extra layer of encryption, and when used with an HSM, conforms with FIPS 140-2 directives on Key Storage and Key Transport.
* [ ] Seal wrapping adds an extra layer of protection by wrapping values with an extra layer of encryption, but does not provide any accredited or certified benefits.

**Correct answer:**
* [x] Seal wrapping adds an extra layer of protection by wrapping values with an extra layer of encryption, and when used with an HSM, conforms with FIPS 140-2 directives on Key Storage and Key Transport.

**Explaination**: Seal wrapping is involved with wrapping RECOVERY keys in an extra layer of protection, but would not create a second set of unseal keys.

Seal wrapping requires a Vault Enterprise license.

Seal wrapping is a mechanism to wrap values with an extra layer of encryption. Vault's Seal Wrap feature has been evaluated by Leidos for compliance with FIPS 140-2 requirements. When used with a FIPS 140-2-compliant HSM, Vault will store Critical Security Parameters (CSPs) in a manner that is compliant with KeyStorage and KeyTransit requirements.

Vault's Seal Wrap feature has been evaluated by Leidos for compliance with FIPS 140-2 requirements.

**3. Many organizations seek to reduce the operational complexity of running Vault by using auto-unseal to automatically unseal the Vault cluster without needing to supply unseal keys. Your leadership team wants to use an HSM in the current Vault environment, and believes that Vault supports the use of an HSM for auto-unseal.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**Explaination**: True: Vault supports the use of an HSM, as well as various cloud KMS services for auto-unseal.

---


## hashicorp-vault-agent

**1. Your organization has begun using the Vault Agent to more easily integrate applications with Vault for secure secrets. Use the provided Vault Agent configuration below to answer. Based on the above configuration, the Vault Agent will authenticate to Vault, retrieve a Vault token, and then exit/shutdown the Vault Agent process.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**Code**: 
```
listener "tcp" {
  address = "127.0.0.1:8200"
}

vault {
  address = "https://vault-demo.com:8200"
}

### Depending on how difficult we want to make the question this can be removed. Default value/behavior is false.
exit_after_auth = false

auto_auth {
  method "approle" {
    config = {
      role_id_file_path = "./roleid"
      secret_id_file_path = "./secretid"
      remove_secret_id_file_after_reading  = false
    }
  }
}

sink {
  type = "file"
  config = {
    path = "/opt/sink_file_1.txt"
    mode = "0640"
  }
}

template {
  source = "./test.tmpl"
  destination = "./test.txt"
}
```

**2. Below is an example Vault Agent template configuration. Given the provided example, select all answers that are correct. (Select three)**


* [ ] The path of the secret in Vault is `secret/data/app1`
* [ ] The secret in Vault contains a key with the name `ID`
* [ ] The secret in Vault contains a key with the name `username`
* [ ] The secret in Vault contains a key with the name `color`
* [ ] The path of the secret in Vault is `Data/data/app1`

**Correct answer:**
* [x] The path of the secret in Vault is `secret/data/app1`
* [x] The secret in Vault contains a key with the name `username`
* [x] The secret in Vault contains a key with the name `color`

**Code**: 
```
{{ with secret "secret/data/app1" }}
ID: {{ .Data.data.username }}
Color: {{ .Data.data.color }}
{{ end }}
```

**3. Your organization has begun using the Vault Agent to more easily integrate applications with Vault for secure secrets. Use the provided Vault Agent configuration below to answer. Based on the configuration the Vault Agent will attempt to connect to the Vault cluster at which address?**


* [ ] `https://127.0.0.1:8200`.
* [ ] It is using a Unix socket listener on localhost.
* [ ] The configuration file does not contain that information and needs to be configured in the unit file or startup command.
* [ ] `https://vault-demo.com:8200`.

**Correct answer:**
* [x] `https://vault-demo.com:8200`.

**Code**: 
```
listener "tcp" {
  address = "127.0.0.1:8200"
}

vault {
  address = "https://vault-demo.com:8200"
}

### Depending on how difficult we want to make the question this can be removed. Default value/behavior is false.
exit_after_auth = false

auto_auth {
  method "approle" {
    config = {
      role_id_file_path = "./roleid"
      secret_id_file_path = "./secretid"
      remove_secret_id_file_after_reading  = false
    }
  }
}

sink {
  type = "file"
  config = {
    path = "/opt/sink_file_1.txt"
    mode = "0640"
  }
}

template {
  source = "./test.tmpl"
  destination = "./test.txt"
}
```

**4. Your organization has begun using the Vault Agent to more easily integrate applications with Vault for secure secrets. Use the provided Vault Agent configuration below to answer. Based on the configuration, which of the following statements are correct? (Select two)**


* [ ] The Vault Agent will store the Vault token retrieved during authentication and make it readable to ALL users on the system.
* [ ] The Vault Agent will attempt to use templating configurations found in the `./test.tmpl` file.
* [ ] The Vault Agent will store the Vault token retrieved during authentication and make it readable to the file owner and members of the same group as the file owner.
* [ ] The Vault Agent is using response wrapping to more securely authenticate to Vault with a role ID and secret ID.
* [ ] The Vault Agent will attempt to use templating instructions found in the `./test.txt` file.

**Correct answer:**
* [x] The Vault Agent will attempt to use templating configurations found in the `./test.tmpl` file.
* [x] The Vault Agent will store the Vault token retrieved during authentication and make it readable to the file owner and members of the same group as the file owner.

**Code**: 
```
listener "tcp" {
  address = "127.0.0.1:8200"
}

vault {
  address = "https://vault-demo.com:8200"
}

### Depending on how difficult we want to make the question this can be removed. Default value/behavior is false.
exit_after_auth = false

auto_auth {
  method "approle" {
    config = {
      role_id_file_path = "./roleid"
      secret_id_file_path = "./secretid"
      remove_secret_id_file_after_reading  = false
    }
  }
}

sink {
  type = "file"
  config = {
    path = "/opt/sink_file_1.txt"
    mode = "0640"
  }
}

template {
  source = "./test.tmpl"
  destination = "./test.txt"
}
```

**5. Which of the following auto-auth methods is ***NOT*** a valid option for the Vault Agent to use to authenticate to Vault. (Select one)**


* [ ] AWS
* [ ] Kubernetes
* [ ] LDAP
* [ ] Azure
* [ ] AppRole

**Correct answer:**
* [x] LDAP

---


## hashicorp-security

**1. Many organizations are moving to hosting applications in Kubernetes clusters. When it comes to Vault, it is important to understand additional considerations when hosting services in a container based environment. Which of the following items is NOT a recommended step to mitigate potential security vulnerabilities when running Vault on Kubernetes?**


* [ ] Ensure mlock is enabled
* [ ] Ensure end-to-end encryption via the use of TLS certificates
* [ ] Ensure the Vault process is not running as the root user
* [ ] Offload TLS by ensuring that traffic is terminated at load balancers

**Correct answer:**
* [x] Offload TLS by ensuring that traffic is terminated at load balancers

**Explaination**: It is always recommended to NOT terminate TLS at load balancers to ensure that the Vault traffic is always encrypted in transit.

**2. The Real Good Foods Company has also approached you regarding the patterns of authentication applications use to integrate with Vault. Currently a vast majority of their applications run in either AWS or GCP. They are considering the use of the platform integration model, but have heard that cloud based auth providers like AWS, GCP, and Azure, are not recommended and that AppRole auth is more secure. True or false, AppRole auth is recommended for use over third-party/cloud auth methods.**


* [ ] True: AppRole should be used over third-party or cloud auth providers
* [ ] False: Cloud based or third-party auth providers should be used when possible, and use AppRole as a fallback

**Correct answer:**
* [x] False: Cloud based or third-party auth providers should be used when possible, and use AppRole as a fallback

**Explaination**: False; If another platform method of authentication is available via a trusted third-party authenticator, it is best practice to use that instead of AppRole. In some situations it may not be possible to use a trusted third-party auth provider, and AppRole exists for those cases.

**3. You have been hired by the Real Good Foods Company to assist with improving their current usage of Vault. A number of applications are already integrated with Vault, but they want to ensure the interactions and workflows they use are secure. Which of the following choices is NOT a good goal for secure introduction of Vault clients?**


* [ ] Using short TTLs on tokens and leases wherever possible
* [ ] Using hardcoded credentials in code to ensure uptime
* [ ] Using a trusted platform to verify the identity of applications/clients
* [ ] Using a trusted orchestrator to inject secrets to applications

**Correct answer:**
* [x] Using hardcoded credentials in code to ensure uptime

**Explaination**: You should try to avoid the use of hardcoded credentials whenever possible. All other choices are great goals for the secure introduction of Vault clients.

**4. You are currently working to onboard applications to leverage Vault for secure secret storage. You have been told to utilize a "trusted orchestrator" to handle the way these applications get their secrets. Which of the following is an example of the "trusted orchestrator" model for secure introduction of Vault clients?**


* [ ] The Vault Agent using auto-auth
* [ ] Using a cloud provider like AWS to gather metadata for an EC2 instance to verify application identities during authentication
* [ ] Terraform using an existing token with capabilities to generate AppRole credentials, and injecting them into an application build
* [ ] The Github authentication method

**Correct answer:**
* [x] Terraform using an existing token with capabilities to generate AppRole credentials, and injecting them into an application build

**Explaination**: The Terraform example best represents the use of a "trusted orchestrator". The Vault agent is another method of securely introducing Vault clients, and "Using a cloud provider like AWS to gather metadata for an EC2 instance to verify application identities during authentication" describes the "platform integration" model. The remaining option is just the Github auth method and does not describe the trusted orchestrator model.

---


## hashicorp-monitor

**1. You have been tasked with increasing the visibility your organization has about the Vault environment. As part of this task you need to configure Vault telemetry settings. Where would you find telemetry settings to forward telemetry data to a collector agent?**


* [ ] In the Vault UI under the "Status" menu
* [ ] sys/telemetry
* [ ] The Vault configuration file
* [ ] sys/tools/telemetry

**Correct answer:**
* [x] The Vault configuration file

**Explaination**: Telemetry is configured in the Vault configuration file using the telemetry stanza.

**2. As part of a new compliance standard you have recently turned on one audit device using the file audit method. After a few days you start getting messages from your team that they cannot interact with Vault and it appears that the service is down. After some initial investigation you discover that the Vault service is running, but you get errors when you run any sort of Vault command. What is the most likely cause?**


* [ ] The new audit device detected malicious activity and automatically locked down your Vault environment
* [ ] After enabling the audit device the servers hosting Vault could not handle the additional resource intensive load and are unresponsive
* [ ] Drive space on the server has filled up and the file audit method cannot write to the log file, causing to Vault stop handling requests
* [ ] It is likely just some sort of network issue that will resolve itself

**Correct answer:**
* [x] Drive space on the server has filled up and the file audit method cannot write to the log file, causing to Vault stop handling requests

**Explaination**: The key piece here is that you just turned on a SINGLE audit device a few days ago. If there are any audit devices enabled, Vault requires that it can write to the log before completing ANY client requests. There is no built-in feature to monitor and lock down Vault, so that option is not true. It may be possible for the servers hosting the Vault to be unable to handle the load and become unresponsive, but unlikely if you are able to login to the server and verify the Vault service is running. "It is likely just some sort of network issue that will resolve itself," is just wishful thinking and is more than likely not true.

**3. Your security and compliance team has concerns regarding Vault audit logs, and the potential of storing sensitive information in plain text. In what case will Vault audit logs store sensitive information like secrets and tokens in plain text?**


* [ ] Sensitive information is hashed, and never stored in plain text
* [ ] Only when using the "File" audit method
* [ ] Only when using the "Syslog" audit method
* [ ] Only when using the "Socket" audit method

**Correct answer:**
* [x] Sensitive information is hashed, and never stored in plain text

**Explaination**: Sensitive information is hashed with a salt using HMAC-SHA256 to ensure secrets and tokens are never in plain text.

**4. Your security and compliance team has approached you with questions regarding logs that are currently being collected from Vault. They are interested in understanding what sort of information different Vault logs contain. Which statement best describes the difference between audit and telemetry logs?**


* [ ] Audit logs are a detailed log of all authenticated requests and responses to Vault​, and telemetry simply tracks Raft leadership elections and changes.
* [ ] Audit logs are a collection of various runtime metrics about the performance of different components of the Vault environment​, and telemetry logs are a detailed log of all authenticated requests and responses to Vault​​.
* [ ] Audit logs only contain information about initial client authentication requests, all other authenticated requests and responses are in telemetry logs.
* [ ] Audit logs are a detailed log of all authenticated requests and responses to Vault​, and telemetry logs are a collection of various runtime metrics about the performance of different components of the Vault environment​.

**Correct answer:**
* [x] Audit logs are a detailed log of all authenticated requests and responses to Vault​, and telemetry logs are a collection of various runtime metrics about the performance of different components of the Vault environment​.

**Explaination**: Audit logs contain detailed info about ALL authenticated requests and responses to Vault, essentially the who, what, when, and where. Telemetry logs contain various runtime metrics like how much memory the Vault process is using, as well as endpoint usage and performance.

---


## hashicorp-basics

**1. Due to an internal compliance audit at your client Binford Tools, they have contacted you about performing a rotate and rekey in their Vault environment. They are unsure if and when they will need their current unseal keys during these processes. True or false, BOTH the rotate and rekey operations require a threshold of key holders with key shares to be performed?**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**Explaination**: False: Only the Rekey operation requires a threshold of key holders with key shares to be performed. Since the encryption key is never available to users or operators, the rotate operations does NOT require a threshold of key holders.

**2. Your management team has approached you regarding the Vault environment at your organization. They recently heard something about an "auto unseal" feature, and what options are available to enable it. Which of the following options is NOT a supported method for auto unseal?**


* [ ] Shamir
* [ ] Cloud Key Management Services (KMS) like AWS KMS
* [ ] Vault Transit Secret Engine
* [ ] Hardware Security Module (HSM)

**Correct answer:**
* [x] Shamir

**Explaination**: Shamir is the default seal mechanism for Vault but does not support auto unseal. When using Shamir a threshold of key shares must be supplied to unseal Vault. All other options support auto unseal.

**3. Vance Refrigeration has an Enterprise Architect that had previously deployed Vault a few years ago using Consul as the storage backend. He is unsure what Integrated Storage (Raft) is, and wants to know why they should use it. Which of the following is NOT a benefit of deploying Vault with Raft as the storage backend, compared to Consul?**


* [ ] Using Raft reduces the number of network ports used for communication
* [ ] Raft can reduce operational costs by lowering the total number of nodes/VMs required for an HA cluster
* [ ] Raft stores all data in memory
* [ ] Raft stores data locally and reduces extra network hops when data needs to be retrieved from the storage backend.

**Correct answer:**
* [x] Raft stores all data in memory

**Explaination**: Raft stores data on local disk rather NOT in memory (Consul does). All other options are valid benefits for using Raft as the storage backend.

**4. You are currently working with Vance Refrigeration on deploying a highly scalable Vault environment with Integrated Storage (Raft) as the storage backend. Performance is a key factor for the team, as they want to ensure Vault is available when clients need to requests secrets from Vault.  When deploying a Vault cluster with Raft, which hardware resource is TYPICALLY the key bottleneck for performance?**


* [ ] CPU
* [ ] Network Throughput
* [ ] Disk IOPS
* [ ] Memory

**Correct answer:**
* [x] Disk IOPS

**Explaination**: Since Integrated Storage stores all data on a local disk, Vault servers should have a relatively high-performance hard disk optimized for high IOPS.

**5. You can only UPGRADE from KV version-1 TO KV version-2, and not DOWNGRADE from KV version-2 TO KV version-1.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**Explaination**: True: You can upgrade a KV-v1 secret engine mount to KV-v2, but it is not possible to downgrade a KV-v2 secret engine mount to KV-v1.

**6. There is sometimes confusion when it comes to "rekey" and "rotate" operations in Vault. While the commands sound similar, it is important to understand the key differences between these unique operations. What statement most accurately describes the difference between a "rekey" and "rotate" operations.**


* [ ] Rekey is used to generate new unseal/recovery keys, and rotate is used to generate a new root key.
* [ ] Rekey is used to generate a new root key and unseal/recovery keys, and rotate is used to change the encryption key used to encrypt data written to the storage backend.
* [ ] Rekey is used to generate a new root key, and rotate is used to generate new unseal/recovery keys.
* [ ] Rekey is used to change the encryption key used to encrypt data written to the storage backend, and rotate is used to generate a new root key and unseal/recovery keys.

**Correct answer:**
* [x] Rekey is used to generate a new root key and unseal/recovery keys, and rotate is used to change the encryption key used to encrypt data written to the storage backend.

**Explaination**: Rekeying is the process of generating a new root key and the unseal/recovery key shares used to reconstruct the root key. Rotating is used to change the underlying key used to encrypt/decrypt Vault data. New keys are added to the keyring and old values can still be decrypted with the old key.

---


## hashicorp-ha

**1. You have been approached by a member of your team that wants to implement an "active-active" architecture of your Vault clusters to maintain high-availability. Vault currently supports active-active implementations where multiple clusters can act as the primary; true or false.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**Explaination**: False: The replication model is not designed for active-active and enabling two primaries should never be done. It is known as a "split-brain" scenario and could result in data loss.

**2. Your operations team has been experiencing outages lately on numerous applications. Your manager has approached you regarding the Vault environment to ensure it is highly available and fault tolerant. Currently the primary Vault cluster is using Integrated Storage (Raft) in a three (3) node cluster. In the current configuration, how many Vault nodes could you lose and still have an available Vault service?**


* [ ] 1
* [ ] 2
* [ ] 3
* [ ] In a 3 node cluster you cannot lose ANY nodes

**Correct answer:**
* [x] 1

**Explaination**: In order to maintain a quorum and have Vault service requests you can lose 1 node, and the remaining two nodes will handle requests. A quorum is a majority of members from a peer set: for a set of size n, quorum requires at least (n+1)/2 members. So a 3 node cluster requires 2 nodes *(3+1)/2 = 2*

**3. Your team has been running Vault for a few months and have created demos for onboarding a few applications team. You have now been tasked with making sure that Vault is ready for a full production deployment. Given the below configuration information, what would be the easiest change to ensure Vault is highly available and ready for production?**


* [ ] Switch to using Consul as a storage backend
* [ ] Increase the number of Vault nodes in the cluster from 5 to 7
* [ ] Migrate the Vault deployment to a public cloud with a shared responsibility model
* [ ] Switch the seal mechanism from Shamir to one supporting auto-unseal

**Correct answer:**
* [x] Switch the seal mechanism from Shamir to one supporting auto-unseal

**Code**: 
- Running in a five (5) node cluster
- Using Integrated Storage (Raft) as the storage backend
- Using Shamir as the seal type
- Virtual machines hosting the Vault service are running in an on-prem datacenter

**Explaination**: Raft as a storage backend has become the standard approach and will be more than adequate for nearly any production deployment, so Consul is not necessary. A 5 node Vault cluster should meet the needs for the vast majority of deployments, and adding additional nodes would be a very rare circumstance. There is no real benefit between using private vs public cloud. Moving from Shamir to an auto-unseal is the clear answer, as it is a relatively simple change and would ensure Vault is unsealed automatically following any service restarts.

**4. Your team has recently upgraded to Vault Enterprise, in part to take advantage of Disaster Recovery (DR) replication. During a discovery meeting one of the on-call managers asked a question regarding DR clusters and if applications would need to re-authenticate in the event of promoting the DR to act as a primary. DR clusters mirror the tokens and leases for applications and users usually interacting with the primary cluster; true or false.**


* [ ] True: DR clusters mirror tokens and leases, so applications would NOT need to re-authenticate
* [ ] False: DR clusters keep track of their own tokens and leases, and applications WOULD need to re-authenticate

**Correct answer:**
* [x] True: DR clusters mirror tokens and leases, so applications would NOT need to re-authenticate

**Explaination**: True: DR clusters act as a warm standby and duplicate all the data of the primary, including tokens and leases.

---


## hashicorp-agent

**1. Your organization has begun using the Vault Agent to more easily integrate applications with Vault for secure secrets. Use the provided Vault Agent configuration below to answer. Based on the configuration the Vault Agent will attempt to connect to the Vault cluster at which address?**


* [ ] `https://127.0.0.1:8200`.
* [ ] It is using a Unix socket listener on localhost.
* [ ] The configuration file does not contain that information and needs to be configured in the unit file or startup command.
* [ ] `https://vault-demo.com:8200`.

**Correct answer:**
* [x] `https://vault-demo.com:8200`.

**Code**: 
```
listener "tcp" {
  address = "127.0.0.1:8200"
}

vault {
  address = "https://vault-demo.com:8200"
}

### Depending on how difficult we want to make the question this can be removed. Default value/behavior is false.
exit_after_auth = false

auto_auth {
  method "approle" {
    config = {
      role_id_file_path = "./roleid"
      secret_id_file_path = "./secretid"
      remove_secret_id_file_after_reading  = false
    }
  }
}

sink {
  type = "file"
  config = {
    path = "/opt/sink_file_1.txt"
    mode = "0640"
  }
}

template {
  source = "./test.tmpl"
  destination = "./test.txt"
}
```

**2. Your organization has begun using the Vault Agent to more easily integrate applications with Vault for secure secrets. Use the provided Vault Agent configuration below to answer. Based on the configuration, which of the following statements are correct? (Select two)**


* [ ] The Vault Agent will store the Vault token retrieved during authentication and make it readable to ALL users on the system.
* [ ] The Vault Agent will attempt to use templating configurations found in the `./test.tmpl` file.
* [ ] The Vault Agent will store the Vault token retrieved during authentication and make it readable to the file owner and members of the same group as the file owner.
* [ ] The Vault Agent is using response wrapping to more securely authenticate to Vault with a role ID and secret ID.
* [ ] The Vault Agent will attempt to use templating instructions found in the `./test.txt` file.

**Correct answer:**
* [x] The Vault Agent will attempt to use templating configurations found in the `./test.tmpl` file.
* [x] The Vault Agent will store the Vault token retrieved during authentication and make it readable to the file owner and members of the same group as the file owner.

**Code**: 
```
listener "tcp" {
  address = "127.0.0.1:8200"
}

vault {
  address = "https://vault-demo.com:8200"
}

### Depending on how difficult we want to make the question this can be removed. Default value/behavior is false.
exit_after_auth = false

auto_auth {
  method "approle" {
    config = {
      role_id_file_path = "./roleid"
      secret_id_file_path = "./secretid"
      remove_secret_id_file_after_reading  = false
    }
  }
}

sink {
  type = "file"
  config = {
    path = "/opt/sink_file_1.txt"
    mode = "0640"
  }
}

template {
  source = "./test.tmpl"
  destination = "./test.txt"
}
```

**3. Your organization has begun using the Vault Agent to more easily integrate applications with Vault for secure secrets. Use the provided Vault Agent configuration below to answer. Based on the above configuration, the Vault Agent will authenticate to Vault, retrieve a Vault token, and then exit/shutdown the Vault Agent process.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**Code**: 
```
listener "tcp" {
  address = "127.0.0.1:8200"
}

vault {
  address = "https://vault-demo.com:8200"
}

### Depending on how difficult we want to make the question this can be removed. Default value/behavior is false.
exit_after_auth = false

auto_auth {
  method "approle" {
    config = {
      role_id_file_path = "./roleid"
      secret_id_file_path = "./secretid"
      remove_secret_id_file_after_reading  = false
    }
  }
}

sink {
  type = "file"
  config = {
    path = "/opt/sink_file_1.txt"
    mode = "0640"
  }
}

template {
  source = "./test.tmpl"
  destination = "./test.txt"
}
```

**4. Which of the following auto-auth methods is ***NOT*** a valid option for the Vault Agent to use to authenticate to Vault. (Select one)**


* [ ] AWS
* [ ] Kubernetes
* [ ] LDAP
* [ ] Azure
* [ ] AppRole

**Correct answer:**
* [x] LDAP

**5. Below is an example Vault Agent template configuration. Given the provided example, select all answers that are correct. (Select three)**


* [ ] The path of the secret in Vault is `secret/data/app1`
* [ ] The secret in Vault contains a key with the name `ID`
* [ ] The secret in Vault contains a key with the name `username`
* [ ] The secret in Vault contains a key with the name `color`
* [ ] The path of the secret in Vault is `Data/data/app1`

**Correct answer:**
* [x] The path of the secret in Vault is `secret/data/app1`
* [x] The secret in Vault contains a key with the name `username`
* [x] The secret in Vault contains a key with the name `color`

**Code**: 
```
{{ with secret "secret/data/app1" }}
ID: {{ .Data.data.username }}
Color: {{ .Data.data.color }}
{{ end }}
```

---


## azure-prac-exam

**1. You are planning to create an Azure Virtual Machine Scale Set using PowerShell. Which command should you use?**


* [ ] New-AzVMSS
* [ ] Set-AzVMScaleSet
* [ ] New-AzVMScaleSet
* [ ] New-AzVM -Type ScaleSet

**Correct answer:**
* [x] New-AzVMSS

**2. You have a VM named VM-1 in the East US region. VM-1 has a network interface named NIC1, this interface is attached to a subnet called “default” which is part of virtual network VNET-01. VM-1 is using a managed disk. You have another virtual network in the West US called VNET-02, you need to move the VM to VNET-02. Which of the following two actions should you perform?**


* [ ] Deallocate the VM-1
* [ ] Delete VM-1 and retain the disk
* [ ] Add a new NIC in VNET-02, attach to VM-1 and remove the one in VNET-01
* [ ] Create a new VM using the disk in West US

**Correct answer:**
* [x] Delete VM-1 and retain the disk
* [x] Create a new VM using the disk in West US

**3. Your business unit uses different virtual machines for your applications. You were asked by your manager to consolidate the cost for these VMs. These VMs are part of different resource groups. What is the easiest way to track the cost for these VMs in Azure Cost Management?**


* [ ] Use Azure Policy to group the VMs and calculate the cost
* [ ] Assign tags at the resource level
* [ ] Assign tags at the resource group level
* [ ] Calculate the cost individually and sum up the cost

**Correct answer:**
* [x] Assign tags at the resource level

**Explaination**: Tags assigned at the resource group level will not be visible in Azure Cost Management, we need to use tags at the resource level to track cost.

**4. Your security team is planning to audit the sign-in logs in Azure AD by ingesting that to a Log Analytics workspace. Which of the following configurations should be done to achieve this?**


* [ ] Ingestion configuration
* [ ] Data source setting
* [ ] Diagnostic setting
* [ ] Data monitor setting

**Correct answer:**
* [x] Diagnostic setting

**5. Which of the following statements about custom domains is true?**


* [ ] You can use TXT record to verify the domain
* [ ] You can have only domain attached to the Azure AD tenant
* [ ] You can remove onmicrosoft.com domain from Azure AD after adding a custom domain
* [ ] You can use MX record to verify the domain

**Correct answer:**
* [x] You can use TXT record to verify the domain
* [x] You can use MX record to verify the domain

**6. Your team hired a new VM administrator to manage the production VMs which are deployed in PROD-RG. You need to assign a role to the new hire by which the administrator should be able to manage all aspects of the VMs including network and storage in PROD-RG. Which of the following roles gives the right amount of access to the user?**


* [ ] Provide Contributor role at the resource group level
* [ ] Provide Virtual Machine Contributor role at the resource group level
* [ ] Provide Owner role at the resource group level
* [ ] Provide Virtual Machine Operator role at the subscription level

**Correct answer:**
* [x] Provide Contributor role at the resource group level

**Explaination**: The Owner role will give additional rights to the user such as access delegation. In this case, the Contributor is the least privilege we can give.

**7. Your organization has two subscriptions, prod-sub and dev-sub. In prod-sub, you have all your production resources and they are protected using resource locks to avoid any accidental changes or deletion. However, the dev-sub doesn’t have any locks, you need to make sure all the resources in the subscription are protected from accidental deletion and at the same time users should be able to modify the resources as required. What is the easiest solution for this?**


* [ ] Use ReadOnly resource lock at the subscription scope
* [ ] Use ReadOnly resource lock for all resource groups
* [ ] Use Delete lock at the subscription scope
* [ ] Use Delete lock for all resource groups

**Correct answer:**
* [x] Use Delete lock at the subscription scope

**Explaination**: Delete lock will stop accidental deletion of resources and at the same time resources can be modified. Since the locks are inherited, we can apply at the subscription scope which will cover our current resource group and even the future ones.

**8. Your organization would like to create some containers using Azure Container Instances. These containers require persistent storage. Which Azure services can be used to create persistent storage for the containers?**


* [ ] Azure Container
* [ ] Azure Files
* [ ] Azure Data Lake Storage Gen2
* [ ] Azure Data Box

**Correct answer:**
* [x] Azure Files

**9. Which table in Log Analytics should you check if the agent is sending data to the Log Analytics workspace?**


* [ ] Health
* [ ] Heartbeat
* [ ] AgentLogs
* [ ] Syslog

**Correct answer:**
* [x] Heartbeat

**10. Which of the following statements about scaling is correct. Select all that apply.**


* [ ] Vertical scaling would require VM reboot
* [ ] Vertical scaling has an upper limit, you cannot scale beyond that
* [ ] Vertical scaling is same as autoscaling
* [ ] Vertical scaling can be done on selected VM sizes only

**Correct answer:**
* [x] Vertical scaling would require VM reboot
* [x] Vertical scaling has an upper limit, you cannot scale beyond that

**11. Your development team wants to use Azure Queue Storage for their application. One of the development teams asked to share the access key and endpoint for the service. You created a storage account called “devkodekloudapp”. You were able to find the access key from Azure Portal. What will be endpoint to access queue storage?**


* [ ] https://devkodekloudapp.core.table.windows.net
* [ ] https://devkodekloudapp.table.azure.net
* [ ] https://devkodekloudapp.table.core.windows.net
* [ ] https://storage.devkodekloudapp.core.table.windows.net

**Correct answer:**
* [x] https://devkodekloudapp.table.core.windows.net

**12. You purchased a domain called kodekloud.com from a domain registrar and created a DNS zone in Azure DNS. You added an A record for www which will resolve to 13.11.13.12, which is one of your public facing web servers. You asked your users to test if they are able to access www.kodekloud.com. The clients were not able to resolve the name. What should you do? Select all that apply.**


* [ ] Make sure public access is enabled in Azure DNS
* [ ] Create a Private Azure DNS zone
* [ ] Reboot the client machines to flush the cache
* [ ] Add the name servers in Azure DNS to domain registrar

**Correct answer:**
* [x] Add the name servers in Azure DNS to domain registrar

**13. You added a ReadOnly lock to one of your Azure VMs running SQL database. Which of the following operations cannot be performed on the database? Select all that apply.**


* [ ] Drop a table
* [ ] Delete database
* [ ] Restart the VM
* [ ] Delete VM

**Correct answer:**
* [x] Restart the VM
* [x] Delete VM

**14. In an ARM template, what's the right way to declare a variable “role” with the value “database”?**


* [ ] variable: {“role” : “database”}
* [ ] variables: {“role” : “database”}
* [ ] variables: (“role” : “database”)
* [ ] variable: [“role” : “database”]

**Correct answer:**
* [x] variables: {“role” : “database”}

**15. Which service is responsible for providing cost recommendations?**


* [ ] Azure Cost Management
* [ ] Azure Monitor
* [ ] Azure Advisor
* [ ] Azure Cost Advisor

**Correct answer:**
* [x] Azure Advisor

**16. Your organization deployed an AKS cluster in Azure and the operations team started to create pods in the cluster. One of the Kubernetes administrators needs to access the cluster from his local computer. The user is able to run kubectl commands from cloud shell, but when the user tries from the Linux terminal, it says kubectl is not recognized. User reached out to you to fix this, how can you resolve this? Azure CLI is already installed on the computer**


* [ ] Download the credentials using az aks get-credentials command and try again
* [ ] Update AKS cluster
* [ ] Install AKS CLI tools using az aks install-cli
* [ ] SSH to the node and verify if the kube-proxy service is running

**Correct answer:**
* [x] Install AKS CLI tools using az aks install-cli

**17. If you are using the Microsoft Azure Recovery Services Agent, how many backups will be taken per day?**


* [ ] 1
* [ ] 2
* [ ] 3
* [ ] 4

**Correct answer:**
* [x] 3

**18. You are planning to use Azure Virtual WAN for connecting your branch offices to Azure via S2S VPN. Due to the cost constraints, you were asked by the management to choose the cheapest SKU that supports S2S. Which SKU will you choose?**


* [ ] Basic
* [ ] Standard
* [ ] Standard V2
* [ ] Premium

**Correct answer:**
* [x] Basic

**19. Your organization wants to use the self-service password reset option for all cloud identities in Azure AD. Which license should you choose?**


* [ ] Microsoft 365 Apps
* [ ] Azure AD Free
* [ ] Azure AD Premium P1
* [ ] Azure AD Premium P2

**Correct answer:**
* [x] Azure AD Premium P2

**20. What is the retention period for the metrics collected by Azure Monitor?**


* [ ] 30 days
* [ ] 60 days
* [ ] 90 days
* [ ] 120 days

**Correct answer:**
* [x] 90 days

**21. Your application development team wants to use Azure Storage Service where they can store application binaries, embed them in a webpage and access via HTTP/HTTPS. Which service should you recommend?**


* [ ] Blob
* [ ] File
* [ ] Queue
* [ ] Table

**Correct answer:**
* [x] Blob

---


## azure-practice-test-1

**1. You would like to control the way the traffic is getting distributed to the backend VMs. Which option in Azure Load Balancer should be configured to control this?**


* [ ] TCP timeout
* [ ] Session stickiness
* [ ] Session affinity
* [ ] Session persistence

**Correct answer:**
* [x] Session persistence

**Explaination**: The distribution mode of the load balancer can be changed using session persistence.

**2. Which port is used to establish WinRM to Windows machines?**


* [ ] 3389
* [ ] 5999
* [ ] 5987
* [ ] 5986

**Correct answer:**
* [x] 5986

**Explaination**: WinRM communication is facilitated over TCP port 5986; you need to ensure that this port is opened for communication in your NSG. 

**3. After creating the load balancer when you reviewed the Azure NSG for the VM, what would you be able to see?**


* [ ] Inbound NAT rule
* [ ] DNAT rule
* [ ] SNAT rule
* [ ] Load balancing rule 

**Correct answer:**
* [x] Inbound NAT rule

**Explaination**: Inbound NAT rules are used to specify a backend resource to route traffic to.

**4. ARM is an example of a(n) ____________________________ solution for Azure.**


* [ ] Template
* [ ] Infrastructure-as-code
* [ ] Platform-as-a-service
* [ ] Infrastructure-as-a-service

**Correct answer:**
* [x] Infrastructure-as-code

**Explaination**: ARM templates are JSON files where you define what you want to deploy to Azure. Templates help you implement an infrastructure-as-code solution for Azure.

**5. Your organization would like to use self-service password reset, and hybrid users should be able to reset their on-premises password. However, you are not able to use this feature due to licensing issues. Which license should you purchase to use this feature?**


* [ ] Azure AD P2
* [ ] Azure AD P1
* [ ] Azure AD Free
* [ ] Azure AD M2

**Correct answer:**
* [x] Azure AD P2

**Explaination**: SSPR is available from the Free license; however, if the hybrid users need to reset their password and write back to on-premises, then they need a Premium P2 license.

**6. Which of the following facts about Azure AD is not true?**


* [ ] Azure AD uses a flat hierarchy.
* [ ] Kerberos and OpenID Connect authentication are supported.
* [ ] LDAP is not used in Azure AD.
* [ ] Group Policy doesn’t exist.

**Correct answer:**
* [x] Kerberos and OpenID Connect authentication are supported.

**Explaination**: Though OpenID Connect is used for authentication, Azure AD doesn’t use Kerberos. So, option B is not true.

**7. Your company has a virtual network with the specifications below. When you were writing a PowerShell script, you needed to provide the resource ID of the virtual network. Which of the following is the right one?**


* [ ] /subscription/00000-00001-00002-00003/resourcegroups/clz-01/providers/Microsoft.Network/virtualNetworks/vnet-hub
* [ ] /subscriptions/00000-00001-00002-00003/resourcegroups/clz-01/providers/Microsoft.Network/virtualNetworks/vnet-hub
* [ ] /subscription/00000-00001-00002-00003/clz-01/providers/Microsoft.Network/virtualNetworks/vnet-hub
* [ ] /subscriptions/00000-00001-00002-00003/resourcegroups/clz-01 /Microsoft.Network/virtualNetworks/vnet-hub

**Correct answer:**
* [x] /subscriptions/00000-00001-00002-00003/resourcegroups/clz-01/providers/Microsoft.Network/virtualNetworks/vnet-hub

**Code**: 
Name: vnet-hub
Resource Group: clz-01
Subscription ID: 00000-00001-00002-00003


**Explaination**: If you have a virtual network with the name vnet-hub, then the resource ID will be /subscriptions/<subscriptionId>/resourcegroups/<resouceGroupName>/ providers/Microsoft.Network/virtualNetworks/vnet-hub.

**8. Your organization has decided to migrate from IaaS to an App Service offering. For your apps, management requires high performance, security, and isolation. Which App Service Plan tier should you use?**


* [ ] Standard
* [ ] Premium
* [ ] Isolated
* [ ] Premium V2

**Correct answer:**
* [x] Isolated

**Explaination**: An isolated plan offers high performance, security, isolation, and native virtual network deployment.

**9. Your organization wants to store binaries and executables for downloads over HTTP/HTTPS. However, they are confused with the storage options in Azure. Which storage service would you suggest?**


* [ ] Queues
* [ ] Tables
* [ ] Blobs
* [ ] Files

**Correct answer:**
* [x] Blobs

**Explaination**: Blob storage can be used for storing binaries and executables for downloads over HTTP/HTTPS.

**10. Does Azure App Service support Azure Marketplace images?**


* [ ] Yes
* [ ] No

**Correct answer:**
* [x] Yes

**Explaination**: Popular applications like WordPress, Joomla, and Drupal can be deployed to App Services from Azure Marketplace.

**11. In which of the following backup scenarios can you use the MARS agent?**


* [ ] SharePoint backup
* [ ] VMware backup
* [ ] Azure Files backup
* [ ] On-premises files and folders

**Correct answer:**
* [x] On-premises files and folders

**Explaination**: MARS can be used to back up files and folders only.

**12. Which of the following data can be collected using Azure Monitor? (Select all that apply.)**


* [ ] Application monitoring data
* [ ] Guest OS monitoring data
* [ ] Resource monitoring data
* [ ] Subscription monitoring data

**Correct answer:**
* [x] Application monitoring data
* [x] Guest OS monitoring data
* [x] Resource monitoring data
* [x] Subscription monitoring data

**Explaination**: Azure Monitor can collect all the aforementioned data sources along with tenant monitoring data.

**13. What is the durability offered by GRS storage?**


* [ ] 99.99999999999 (11 nines)
* [ ] 99.99999999999999 (14 nines)
* [ ] 99.9999999999999999 (16 nines)
* [ ] 99.999999999999999999 (18 nines)

**Correct answer:**
* [x] 99.9999999999999999 (16 nines)

**Explaination**: GRS offers a durability of 99.9999999999999999 percent (16 nines) over a given year.

**14. Cost data exported from Azure Cost Management will be stored in the ____________________.**


* [ ] Azure Log Analytics workspace
* [ ] Power BI workspace
* [ ] Azure Event Hub
* [ ] Azure Storage account

**Correct answer:**
* [x] Azure Storage account

**Explaination**: The export feature in Azure Cost Management can be used to export the billing data to an Azure storage account.

**15. Your management has requested to collect all logs related to health events for resources that are part of your subscription. Which category should you select to get this data from the Activity Log?**


* [ ] Administrative
* [ ] Resource health
* [ ] Service health
* [ ] Subscription health

**Correct answer:**
* [x] Resource health

**Explaination**: The resource health category logs all the health events happening to resources that are there in your subscription.

**16. What is the maximum number of gateways you can add to a virtual network?**


* [ ] 500
* [ ] 100
* [ ] 10
* [ ] 1

**Correct answer:**
* [x] 1

**Explaination**: You can have only one VPN gateway per virtual network. However, you can establish multiple connections to it.

**17. Which of the following PowerShell commands can be used to create a virtual network? (Select all that apply.)**


* [ ] New-AzureVNet
* [ ] New-AzureVirtualNetwork
* [ ] New-AzVirtualNetwork
* [ ] New-AzNetwork

**Correct answer:**
* [x] New-AzVirtualNetwork

**Explaination**: The New-AzVirtualNetwork command is used to create a virtual network.

**18. Which of the following statements is false?**


* [ ] The ARM template can be used for creating Azure resources only; resource groups and subscriptions should be deployed using PowerShell or the CLI.
* [ ] The ARM template is an example of declarative automation.
* [ ] The ARM templates can be exported and reused in different subscriptions.
* [ ] The ARM template has an option to preview the changes before the deployment.

**Correct answer:**
* [x] The ARM template can be used for creating Azure resources only; resource groups and subscriptions should be deployed using PowerShell or the CLI.

**Explaination**: ARM templates can be used to create resources, resource groups, and subscriptions.

**19. Which of the following facts about resource groups is true? (Select all that apply.)**


* [ ] A resource group is a global service and can be used to group resources from multiple regions.
* [ ] Resource groups will help manage the lifecycle of resources.
* [ ] Resource groups can act as a scope for managing access.
* [ ] Deleting the resource group deletes all the resources that are part of the resource group.

**Correct answer:**
* [x] Resource groups will help manage the lifecycle of resources.
* [x] Resource groups can act as a scope for managing access.
* [x] Deleting the resource group deletes all the resources that are part of the resource group.

**Explaination**: Though we can use resource groups to group resources from multiple regions, a resource group is not a global service. Every resource group will have a region where it will store the metadata about the resources that are part of the group.

**20. What is the key length of the key pairs used for Azure Linux virtual machines?**


* [ ] 1024-bit
* [ ] 2048-bit
* [ ] 4096-bit
* [ ] 128-bit

**Correct answer:**
* [x] 2048-bit

**Explaination**: Azure uses a 2048-bit key length and SSH-RSA format for public and private keys.

**21. Your organization is creating a virtual machine with a public IP address. You need to make sure that the public IP address is zone redundant. Which of the following SKU and assignment types should you select to achieve this?**


* [ ] Basic dynamic
* [ ] Basic static
* [ ] Standard dynamic
* [ ] Standard static

**Correct answer:**
* [x] Standard static

**Explaination**: Standard SKU comes only with static IP assignment, and it offers zone redundancy.

**22. You are planning to set up a Log Analytics workspace for collecting logs. Your manager asked you to prepare a budget for Log Analytics. What are two meters you should include as part of the Log Analytics cost calculation?**


* [ ] Data queries
* [ ] Data ingestion
* [ ] Data retention
* [ ] Data sources

**Correct answer:**
* [x] Data ingestion
* [x] Data retention

**Explaination**: The Log Analytics cost is calculated using the amount of data ingested and the number of days the data is retained for (data retention). Thirty-one days of retention is free of cost.

**23. Which of the statements about MARS is not true?**


* [ ] MARS can be used to back up files and folders stored in physical Windows servers.
* [ ] MARS can back up files without the need to deploy the Backup server.
* [ ] MARS can back up files stored in RedHat VMs.
* [ ] File, folder, and volume level restore is available in MARS.

**Correct answer:**
* [x] MARS can back up files stored in RedHat VMs.

**Explaination**: Linux workloads are not supported in MARS.

**24. Your organization wants to store binaries and executables for downloads over HTTP/HTTPS. However, they are confused with the storage options in Azure. Which storage service would you suggest?**


* [ ] Queues
* [ ] Tables
* [ ] Blobs
* [ ] Files

**Correct answer:**
* [x] Blobs

**Explaination**: Blob storage can be used for storing binaries and executables for downloads over HTTP/HTTPS.

**25. You are using deployment slots for your web apps. Currently, you are using the Standard plan, and when you swapped the slots, CORS settings were not swapped. What could be the reason?**


* [ ] CORS swap is supported only from the Premium tier onward.
* [ ] CORS settings cannot be swapped during slot swap.
* [ ] Enable the CORS Across Slots option from app configuration settings.
* [ ] Enable CORS Across Slots option from the app general settings.

**Correct answer:**
* [x] CORS settings cannot be swapped during slot swap.

**Explaination**: CORS settings cannot be swapped during a deployment slot swap.

**26. Which of the following user types cannot be created from the Azure portal? (Select all that apply.)**


* [ ] Cloud identities
* [ ] Guest accounts
* [ ] M365 cloud identities
* [ ] Directory synchronized users

**Correct answer:**
* [x] Directory synchronized users

**Explaination**: Directory-synchronized users cannot be created from the Azure portal; these users should be synchronized from an on-premises domain controller with the help of the Azure AD Connect tool.

**27. After creating the load balancer when you reviewed the Azure NSG for the VM, what can you see?**


* [ ] Inbound NAT rule
* [ ] DNAT rule
* [ ] SNAT rule
* [ ] Load balancing rule

**Correct answer:**
* [x] Inbound NAT rule

**Explaination**: Inbound NAT rules are used to specify a backend resource to route traffic to.

**28. You want to use an ARM template to deploy a VM. You need to make sure that the password should be added as a parameter to the template. What parameter type should you choose to make sure that the password is secure?**


* [ ] secureString
* [ ] secureObject
* [ ] passwordString
* [ ] notClearTextString

**Correct answer:**
* [x] secureString

**Explaination**: To mark a string as secure, we need to set the object as secureString.

**29. For using Azure Bastion, you need to create a dedicated subnet. What should be the name and minimum size of the subnet?**


* [ ] AzureBastionSubnet (minimum /27)
* [ ] BastionSubnet (minimum /26)
* [ ] AzureBastion (minimum /27)
* [ ] AzureBastionSubnets (minimum /26)

**Correct answer:**
* [x] AzureBastionSubnet (minimum /27)

**Explaination**: To work with Azure Bastion, you need to deploy the Bastion host to the virtual network where your VM is deployed to. Azure Bastion requires a dedicated subnet of minimum size /27 called AzureBastionSubnet.

**30. You need to establish a highly available site-to-site connection to an on-premises environment. Which of the following topologies offers the highest availability?**


* [ ] Active-passive
* [ ] Active-cold standby
* [ ] Active-active
* [ ] Passive-passive

**Correct answer:**
* [x] Active-active

**Explaination**: Site-to-site connections will be established from both instances to your on-premises VPN device if you are using an active-active configuration.

**31. Which of the following facts about management groups is true? (Select all that apply.)**


* [ ] Using the management group, you can logically group subscriptions.
* [ ] You can easily apply policies and access a set of subscriptions.
* [ ] Budgets can be created at the management group level, which is ideal for teams and projects having multiple subscriptions.
* [ ] The management group can be created to isolate the resources that you don’t want to be part of any subscription.

**Correct answer:**
* [x] Budgets can be created at the management group level, which is ideal for teams and projects having multiple subscriptions.
* [x] You can easily apply policies and access a set of subscriptions.
* [x] Using the management group, you can logically group subscriptions.

**Explaination**: All resources need to be part of a subscription; we cannot deploy resources to a management group.

**32. You are planning to restore an Azure VM that you were backing up to a Recovery Services Vault. As part of the restore process, you would like to mount the restore point as a drive to our VM and recover files. Which option should you select while restoring the VM?**


* [ ] Volume recovery
* [ ] File recovery
* [ ] Folder recovery
* [ ] File system recovery

**Correct answer:**
* [x] File recovery

**Explaination**: Using File Recovery, you will be able to mount the restore point as a drive to your VM and recover files without the need to restore the entire VM.

**33. You want to use allowedValues for one of your parameters. How can you supply values to allowedValues?**


* [ ] “allowedValues”: “one”, “two”, “three”
* [ ] “allowedValues”: {“one”, “two”, “three”}
* [ ] “allowedValues”: (“one”, “two”, “three”)
* [ ] “allowedValues”: [“one”, “two”, “three”]

**Correct answer:**
* [x] “allowedValues”: [“one”, “two”, “three”]

**Explaination**: The allowed values are provided in an array.

**34. Which of the following bulk operations are available for users in Azure AD? (Select all that apply.)**


* [ ] Bulk create
* [ ] Bulk delete
* [ ] Bulk invite
* [ ] Download all users

**Correct answer:**
* [x] Bulk create
* [x] Bulk delete
* [x] Bulk invite
* [x] Download all users

**Explaination**: All of the options are considered as bulk operations for users.

**35. You would like to ingest metrics from the VM host and analyze them using Metrics Explorer. What should you do to collect these metrics from the VM?**


* [ ] Metrics are collected by default without any additional configuration.
* [ ] During VM deployment, you can enable log collection. By default, this will be enabled.
* [ ] You need to create a Log Analytics workspace to collect these metrics.
* [ ] Data sources.

**Correct answer:**
* [x] During VM deployment, you can enable log collection. By default, this will be enabled.
* [x] You need to create a Log Analytics workspace to collect these metrics.

**Explaination**: The Log Analytics cost is calculated using the amount of data ingested and the number of days the data is retained for (data retention). Thirty-one days of retention is free.

**36. You have been asked to access the table named `customers` in the storage account `axfg03`. Which is the right endpoint to access the table?**


* [ ] https:// axfg03.table.core.windows.net/Customers
* [ ] https:// axfg03.table.core.windows.net/customers
* [ ] https:// axfg03.table.core.windows.net/CUSTOMERS
* [ ] https:// axfg03.table.core.windows.net/tables/customers

**Correct answer:**
* [x] https:// axfg03.table.core.windows.net/customers

**Explaination**: Table names are case sensitive. The right approach is to use the table endpoint followed by the table name.

**37. Which of the following statements is correct?**


* [ ] RBAC targets authorization, and a policy targets resource properties.
* [ ] RBAC always supersedes the policy.
* [ ] RBAC and a policy are required to grant access.
* [ ] A policy is not required when you are using RBAC.

**Correct answer:**
* [x] RBAC targets authorization, and a policy targets resource properties.

**Explaination**: RBAC targets authorization and access to a specific resource, while a policy targets resource properties such as size, location, type, etc.

**38. Which of the following is an example of manual deployment in App Services?**


* [ ] GitHub
* [ ] BitBucket
* [ ] Local Git
* [ ] External Git

**Correct answer:**
* [x] External Git

**Explaination**: External Git is an example of manual deployment.

**39. What is the maximum number of nodes that can be added to an AKS cluster with the Standard Load Balancer?**


* [ ] 100
* [ ] 1,000
* [ ] 1,500
* [ ] 2,000

**Correct answer:**
* [x] 1,000

**Explaination**: Refer to the following: 
https://docs.microsoft.com/en-us/azure/aks/quotas-skus-regions#service-quotas-and-limits

**40. Your organization has a requirement that all Azure VM backups stored in Azure should be encrypted. What needs to be done to enable encryption at rest?**


* [ ] Volume recovery
* [ ] File recovery
* [ ] Folder recovery
* [ ] File system recovery

**Correct answer:**
* [x] File recovery

**Explaination**: Using File Recovery, you will be able to mount the restore point as a drive to your VM and recover files without the need to restore the entire VM.

**41. You need to connect to Linux virtual machines using a key pair from the Windows Terminal application. The key name is vm-01, and it is stored in ~/.ssh/key. What is the right syntax for connecting to a VM called vm-01 using the username admin?**


* [ ] ssh -k ~/.ssh/key/vm-01 admin@vm-01
* [ ] ssh -p ~/.ssh/key/ -i vm-01 admin@vm-01
* [ ] ssh -i ~/.ssh/key/vm-01 admin@vm-01
* [ ] ssh -c ~/.ssh/key/vm-01 admin@vm-01

**Correct answer:**
* [x] ssh -i ~/.ssh/key/vm-01 admin@vm-01

**Explaination**: The -i parameter is used to specify the path to the private key, followed by the username@IP/DNS name format.

**42. To create a hybrid environment, you are planning to deploy a basic VPN gateway. To get zone redundancy, your manager asked you to use a standard SKU public IP address for the VPN gateway. When you were creating a VPN gateway, you were not able to see the option to attach a standard SKU public IP address. What could be the reason?**


* [ ] A standard SKU is not supported for VPN gateways.
* [ ] You need to upgrade to VpnGw1 or higher to use a standard SKU.
* [ ] You don’t have permission to attach a standard SKU to the VPN gateway.
* [ ] The public IP is deployed in a different region. You can only select the IP address that is in the same region as the gateway.

**Correct answer:**
* [x] A standard SKU is not supported for VPN gateways.

**Explaination**: The standard SKU is supported by VM NICs and public load balancers only. The basic SKU supports VM NICs, VPN gateways, application gateways, and public load balancers.

**43. Select the correct sequence for setting up a virtual network to virtual network connection.**


* [ ] Create virtual networks, add gateway subnets, deploy VPN gateways, establish a connection
* [ ] Create a VPN gateway, select the virtual networks to connect, create the connection
* [ ] Create virtual networks, create gateway virtual networks, deploy gateways to gateway virtual network
* [ ] Create a VPN gateway and then create a site-to-site connection

**Correct answer:**
* [x] Create virtual networks, add gateway subnets, deploy VPN gateways, establish a connection

**Explaination**: The process is to first create virtual networks, then add gateway subnets, and then deploy VPN gateways. Finally, you establish a connection.

**44. You are setting up Azure Load Balancer, and you would like to implement cookie affinity. However, you are not able to find this while configuring the load balancer. What could be the reason?**


* [ ] Cookie affinity is also known as session persistence; use it instead.
* [ ] Cookie affinity requires an L7 load balancer; use an application gateway instead of Azure Load Balancer.
* [ ] Ensure that you are using Azure Load Balancer Standard as cookie affinity is available only on Standard.
* [ ] Set session persistence to a two-tuple hash to enable cookie persistence.

**Correct answer:**
* [x] Cookie affinity requires an L7 load balancer; use an application gateway instead of Azure Load Balancer.

**Explaination**: To use cookie affinity, you need to have an L7 load balancer like Azure Application Gateway.

**45. You have an on-premises endpoint that is publicly exposed. You want to check the latency and connectivity from a virtual machine to the FQDN. Which service should you choose?**


* [ ] Connection Monitor
* [ ] IP Flow Verify
* [ ] VPN diagnostics
* [ ] Topology

**Correct answer:**
* [x] Connection Monitor

**Explaination**: Connection Monitor will verify if the destination is reachable or not. It will also show the hops taken to reach the destination and latency.

**46. What is the maximum throughput offered by a VPN gateway?**


* [ ] 1 Gbps
* [ ] 10 Gbps
* [ ] 100 Gbps
* [ ] 1000 Gbps

**Correct answer:**
* [x] 10 Gbps

**Explaination**: The bandwidth of the VPN gateway is limited to 10 Gbps.

**47. Which is the cheapest licensing option if you have fewer than 500,000 users and require single sign-on?**


* [ ] Azure AD P2
* [ ] Azure AD P1
* [ ] Azure AD Free
* [ ] Azure AD M2

**Correct answer:**
* [x] Azure AD Free

**Explaination**: Azure AD Free edition offers 500,000 directory objects, SSO, and B2B collaboration.

**48. Which action will a User Access administrator be able to perform? (Select all that apply.)**


* [ ] View resources
* [ ] Modify resources
* [ ] Delegate access
* [ ] Create resources

**Correct answer:**
* [x] View resources
* [x] Delegate access

**Explaination**: A User Access administrator can delegate access to other users; however, this role cannot manage any resources.

**49. You have implemented alerts for all your critical workloads. As of now, you are using ServiceNow for creating internal IT tickets. Which action in action groups offers the easiest way to connect to ServiceNow?**


* [ ] Email/SMS/push
* [ ] Email Azure Resource Manager role
* [ ] ITSM
* [ ] ITIL

**Correct answer:**
* [x] ITSM

**Explaination**: ITSM connectors can be created in Azure to connect to service management tools like ServiceNow. This connection can be referenced in action groups to create tickets automatically whenever an alert is fired.

**50. Which of the following statements is not true? (Select all that apply.)**


* [ ] The Azure Bastion host requires you to create RDP access to the host machine to establish an RDP/SSH connection to other VMs.
* [ ] The Azure Bastion service requires a dedicated subnet.
* [ ] Azure Bastion can be used to establish a connection from the Azure portal over SSL.
* [ ] The Azure Bastion host is charged only when you are connecting to VMs using RDP/SSH.

**Correct answer:**
* [x] The Azure Bastion host requires you to create RDP access to the host machine to establish an RDP/SSH connection to other VMs.
* [x] The Azure Bastion host is charged only when you are connecting to VMs using RDP/SSH.

**Explaination**: Azure Bastion can be used to establish a connection from the Azure portal over SSL; there is no need to download any clients. You will be charged for the entire hour regardless of whether you are using Azure Bastion. The only way to stop being billed is to delete the Azure Bastion service.

---


## azure-practice-test-2

**1. You created a web app named azwebapp. What will the default URL of the web app be?**


* [ ] https://azwebapp.azure.websites.net 
* [ ] https://azwebapp.app.azurewebsites.net
* [ ] https://azwebapp.azurewebsites.net
* [ ] https://azwebapp.azurewebapp.net

**Correct answer:**
* [x] https://azwebapp.azurewebsites.net

**Explaination**: The default domain will be azurewebsites.net

**2. You are trying to add dynamic devices under M365 groups, and for some reason you are not able to perform that action. What could be the reason?**


* [ ] You need to have a Premium P2 license to add dynamic devices under M365 groups.
* [ ] You should have only Windows 10 and Windows Server 2019 devices to add to M365 groups.
* [ ] Dynamic devices are not supported for M365 groups.
* [ ] You need to have the Device Administrator role assigned to group devices in Azure AD.

**Correct answer:**
* [x] You need to have the Device Administrator role assigned to group devices in Azure AD.

**Explaination**: Dynamic devices are supported only for security groups.

**3. Which of the following is not mandatory for an ARM template? (Select all that apply.)**


* [ ] apiProfile
* [ ] parameters
* [ ] functions
* [ ] variables

**Correct answer:**
* [x] apiProfile
* [x] parameters
* [x] functions
* [x] variables

**Explaination**: The only required fields for an ARM template are $schema, contentVersion, and resources.

**4. Which of the following statements about custom domains is true? (Select all that apply.)**


* [ ] You need to add a TXT/MX record to validate the domain.
* [ ] You can add multiple domains.
* [ ] You can delete the onmicrosoft.com domain after adding the custom domain.
* [ ] You can remove a domain where users are mapped to the domain.

**Correct answer:**
* [x] You need to add a TXT/MX record to validate the domain.
* [x] You can add multiple domains.

**Explaination**: You can add multiple custom domains to your Azure AD tenant by adding the TXT/MX record to your DNS domain for validation.

**5. Your organization wants to ingest logs from Azure AD to a Log Analytics workspace. Which setting should be configured for this?**


* [ ] Ingestion setting
* [ ] Data source setting
* [ ] Diagnostic setting
* [ ] Data collection setting

**Correct answer:**
* [x] Diagnostic setting

**Explaination**: Azure AD logs can be streamed to Azure Log Analytics by enabling the diagnostic settings.

**6. Which table should you query to verify if the data collection from the Log Analytics agent is stopped or not?**


* [ ] Event
* [ ] Heartbeat
* [ ] Syslog
* [ ] AgentLogs

**Correct answer:**
* [x] Heartbeat

**Explaination**: The Heartbeat table will help you identify computers that haven’t had a heartbeat in a specific time frame.

**7. Which one of the following can be used as a persistent storage for Azure Container Instances?**


* [ ] Azure Blob Storage
* [ ] Azure Files
* [ ] Azure Data Lake Storage
* [ ] Azure Container Storage plug-in

**Correct answer:**
* [x] Azure Files

**Explaination**: Azure Files can be used as a persistent storage for Azure Container Instances.

**8. Which of the following facts are correct? (Select all that apply.)**


* [ ] A VM requires a reboot during vertical scaling.
* [ ] There is a size limit to which you can vertically scale; no further scaling will be possible beyond that.
* [ ] Vertical scaling will increase the number of instances without changing the hardware.
* [ ] Vertical scaling can be used to implement true autoscaling.

**Correct answer:**
* [x] A VM requires a reboot during vertical scaling.
* [x] There is a size limit to which you can vertically scale; no further scaling will be possible beyond that.

**Explaination**: A VM requires a reboot during vertical scaling, and there is a size limit to which you can vertically scale; no further scaling will be possible beyond that.

**9. Which classic role is equivalent to the Contributor role?**


* [ ] Account administrator
* [ ] Global administrator
* [ ] Co-administrator
* [ ] Service administrator

**Correct answer:**
* [x] Co-administrator

**Explaination**: Co-administrators can manage all aspects of resources the same as service administrators; however, co-admins cannot delegate access. For example, if you are co-admin, you cannot add another person as a co-admin. This action can be done only by a service administrator. The co-administrator role is similar to the Contributor role in RBAC.

**10. When you compare the features between Azure Application Gateway and Azure Front Door, you find that most of the features are the same. What is the key difference between Azure Front Door and Azure Application Gateway?**


* [ ] Azure Application Gateway is an L7 load balancer, while Azure Front Door is a DNS load balancer.
* [ ] Azure Application Gateway supports path-based and multiple site routing, which is not supported by Azure Front Door.
* [ ] Azure Application Gateway is a regional service, and Azure Front Door is a global service.
* [ ] Application Gateway supports only VMs, and Azure Front Door supports only App Services.

**Correct answer:**
* [x] Azure Application Gateway is a regional service, and Azure Front Door is a global service.

**Explaination**: Azure Application Gateway is a regional service, and Azure Front Door is a global service.

**11. MARS takes a backup _____ time(s) per day.**


* [ ] 1
* [ ] 2
* [ ] 3
* [ ] 4

**Correct answer:**
* [x] 3

**Explaination**: MARS takes a backup three times per day.

**12. You have been asked to access the table named customers in the storage account axfg03. Which is the right endpoint to access the table?**


* [ ] https:// axfg03.table.core.windows.net/Customers
* [ ] https:// axfg03.table.core.windows.net/customers
* [ ] https:// axfg03.table.core.windows.net/CUSTOMERS
* [ ] https:// axfg03.table.core.windows.net/tables/customers

**Correct answer:**
* [x] https:// axfg03.table.core.windows.net/customers

**Explaination**: Table names are case sensitive. The right approach is to use the table endpoint followed by the table name.

**13. Your organization added a ReadOnly lock to one of your production SQL databases. Which of the following operations cannot be accomplished?**


* [ ] Modify queries in the SQL database
* [ ] Create a new table in the SQL database
* [ ] Resize the SQL database
* [ ] Delete the table from the SQL database

**Correct answer:**
* [x] Resize the SQL database

**Explaination**: Locks prevent changes to a resource, but they don’t restrict how resources perform their own functions. Any operations done within the SQL database cannot be prevented using locks.

**14. You have deployed an AKS cluster and have downloaded the credentials; however, when using kubectl, you are not able to get the node information from your local computer. As per the terminal, kubectl is not recognized as a known command. What needs to be done to resolve this problem? (Select all that apply.)**


* [ ] Download the credentials again and save them to your home directory
* [ ] Install the AKS CLI tools using az aks install-cli
* [ ] SSH to the nodes and verify if the kube-proxy service is running
* [ ] Try accessing the cluster from the cloud shell

**Correct answer:**
* [x] Install the AKS CLI tools using az aks install-cli
* [x] SSH to the nodes and verify if the kube-proxy service is running

**Explaination**: You can install the kubectl binaries to your local computer by executing "az aks install-cli". Kubectl is already installed in the cloud shell, so verify if you can get the node information to rule out any provisioning issues.

**15. Select the correct syntax to declare a variable with the name vmName and a value of webserver-01.**


* [ ] variables: ( “vmName”: “webserver-01”)
* [ ] variables: [ “vmName”: “webserver-01”]
* [ ] variables: { “vmName”: “webserver-01”}
* [ ] variables: { “vmName” = “webserver-01”}

**Correct answer:**
* [x] variables: { “vmName”: “webserver-01”}

**Explaination**: Variables are declared in the format "variables": { "name": "value"}.

**16. In your organization, administrators created Azure DNS zones, and most of the production zones are hosted on an on-premises Windows DNS server. You want to implement name resolution for your on-premises servers; they should be able to resolve DNS names from both Azure DNS zones and on-premises DNS zones. The on-premises infrastructure is dynamic where VMs are created and deleted dynamically based on the demand. What would be the easiest solution to make the name resolution happen without management overhead?**


* [ ] Add Azure DNS servers to all the VMs
* [ ] Create a conditional forwarder on the on-premises domain controller
* [ ] Migrate the on-premises zones to Azure
* [ ] Synchronize Azure DNS zones to on-premises

**Correct answer:**
* [x] Create a conditional forwarder on the on-premises domain controller

**Explaination**: Though you can add Azure DNS to all the VMs and make the name resolution happen, as the VMs are dynamically created, it’s not a feasible solution. The easiest way is to add a conditional forwarder to the on-premises DNS server, which will forward all the requests matching the condition to the Azure DNS for name resolution.

**17. Azure Cost Management can generate cost recommendations based on your usage. Which of the following services is responsible for deriving these recommendations?**


* [ ] Azure Cost Management Recommendations
* [ ] Azure Cost Advisor
* [ ] Azure Monitor
* [ ] Azure Advisor

**Correct answer:**
* [x] Azure Advisor

**Explaination**: Azure Advisor generates the cost recommendations along with Operation Excellence, Performance, Reliability, and Security recommendations.

**18. Your organization wants to use a virtual WAN with an S2S VPN. Which is the cheapest WAN type you can use?**


* [ ] Basic
* [ ] Standard
* [ ] Advanced
* [ ] Premium

**Correct answer:**
* [x] Basic

**Explaination**: Basic Virtual WAN supports S2S VPN, and this is the cheapest option.

**19. Your security wants to make sure that public access to the storage account from all networks should be denied. All communications should happen via a VPN S2S connection from on-premises. What needs to be done about this requirement?**


* [ ] Create a service endpoint
* [ ] Create network rules and add the on-premises network to the storage account
* [ ] Create a private endpoint
* [ ] Create Azure Firewall

**Correct answer:**
* [x] Create a private endpoint

**Explaination**: Private endpoints can be created for the storage account that will create an interface for the storage account in the virtual network that can be accessed privately.

**20. How long will the metrics be retained by Microsoft?**


* [ ] 90 days
* [ ] 91 days
* [ ] 92 days
* [ ] 93 days

**Correct answer:**
* [x] 93 days

**Explaination**: There is no limit to the amount of metric data you can collect, but this data is stored for a maximum of 93 days.

**21. You are trying to connect your Cost Management data to Power BI, and you are not able to do so. Currently, you have five pay-as-you-go subscriptions that run production workloads. What could be the reason for this failure?**


* [ ] You need to generate the API key to connect to Power BI.
* [ ] You need to sign in as the administrator to use the Power BI integration.
* [ ] You need to enable Power BI access in Cost Management to download the data.
* [ ] Only the subscription offer doesn’t support the Power BI connector.

**Correct answer:**
* [x] Only the subscription offer doesn’t support the Power BI connector.

**Explaination**: The Power BI connector supports only the Enterprise Agreement and Microsoft Customer Agreement subscriptions.

**22. You are using Azure DNS, and one of your colleagues has asked you to add the IPv6 address of a server to the DNS zone. What should you do?**


* [ ] Inform the colleague that Azure DNS only supports IPv4 records
* [ ] Create an AAAA record
* [ ] Create a CNAME recording pointing to the IPv6 address
* [ ] Create an A record pointing to the IPv6 address

**Correct answer:**
* [x] Create an AAAA record

**Explaination**: Azure DNS supports IPv6 records, and you can create AAAA records to map the name to IPv6 addresses.

**23. Your reliability team wants to distribute the instances of a virtual machine scale set across fault domains. Which option do you need to configure to accomplish this requirement?**


* [ ] Scaling policy
* [ ] Uniform distribution
* [ ] Spreading algorithm
* [ ] Availability set

**Correct answer:**
* [x] Spreading algorithm

**Explaination**: The spreading algorithm determines how the instances should be spread across fault domains. Microsoft recommends using max spreading to spread the instances across all fault domains.

**24. You need to create a storage account using an ARM template. As per the naming conventions, you should take the name of the resource group and append that with a string that will be provided by the user during runtime. Assume that the string shared by the user will be stored in `name`. For example, if the name of the resource group is `rg-01` and the string provided by the user is storage, then the name should be `rg-01-storage`. What is the right format to construct the storage account name while writing the resource block?**


* [ ] [concat(resourceGroup.name(),’-’, variables(‘name’)]
* [ ] [concat(resourceGroup().name(),’-’, variables(‘name’)]
* [ ] [concat(resourceGroup.name(),’-’, parameters(‘name’)]
* [ ] [concat(resourceGroup().name,’-’, parameters(‘name’)]

**Correct answer:**
* [x] [concat(resourceGroup().name,’-’, parameters(‘name’)]

**Explaination**: As the user is providing the value during the runtime, you cannot use variables; you need to use parameters. The resource group name can be captured using resourcegroup().name. The values can be appended using the concat function.

**25. You would like to enable Network Watcher for your subscription to view the topology. What needs to be done to enable Network Watcher?**


* [ ] Network Watcher is enabled automatically for every subscription.
* [ ] You need to deploy the Network Watcher service manually in the region where your deployment is.
* [ ] Network Watcher gets enabled when you create a virtual network.
* [ ] Manual deployment is required for only one region as Network Watcher is a global service.

**Correct answer:**
* [x] Network Watcher is enabled automatically for every subscription.

**Explaination**: The Network Watcher service is enabled automatically for every subscription.

**26. You are setting up a VPN gateway and would like to add your custom DNS servers. Where will you configure this?**


* [ ] Under VPN configuration, you can specify the DNS servers.
* [ ] A VPN gateway inherits the DNS configuration of the virtual network.
* [ ] A private DNS needs to be created to link the VPN gateway to use custom DNS.
* [ ] A VPN uses the DNS servers mentioned in the local network gateway.

**Correct answer:**
* [x] A VPN gateway inherits the DNS configuration of the virtual network.

**Explaination**: If you specified a DNS server or servers when you created your virtual network, the VPN gateway will use the DNS servers that you specified.

**27. Which component in AKS managed nodes is responsible for managing requests coming from the master node?**


* [ ] kube-proxy
* [ ] kubelet
* [ ] docker
* [ ] api-server

**Correct answer:**
* [x] kubelet

**Explaination**: Kubelet is responsible for processing the requests that are coming from the Azure managed node or the master node.

**28. Which parameters are taken into consideration for a three-tuple hash?**


* [ ] Source IP address, destination IP address, and protocol
* [ ] Source IP address, source port, and destination IP address
* [ ] Source IP address, destination IP address, and destination port
* [ ] Protocol, destination IP address, and destination port

**Correct answer:**
* [x] Source IP address, destination IP address, and protocol

**Explaination**: In a three-tuple hash we are taking the hash of the source IP address, destination IP address, and protocol to map the servers.

**29. You need to use Azure Storage in AKS; what are the storage options available to you via Storage classes? (Select all that apply.)**


* [ ] Azure Standard SSD
* [ ] Azure Premium SSD
* [ ] Azure Premium File Storage
* [ ] Azure Tables

**Correct answer:**
* [x] Azure Standard SSD
* [x] Azure Premium SSD
* [x] Azure Premium File Storage

**Explaination**: Azure Managed SSD Disks (Standard, Premium) and Azure Files (Standard, Premium) are supported for AKS Storage classes.

**30. Azure Hybrid Benefit can be used with which of the following services? (Select all that apply.)**


* [ ] Windows VMs
* [ ] SQL servers
* [ ] Linux servers
* [ ] Azure Functions

**Correct answer:**
* [x] Windows VMs
* [x] SQL servers
* [x] Linux servers

**Explaination**: Azure Hybrid Benefit can be applied to Windows, SQL, and Linux servers. Azure Functions is a serverless service.

**31. You have several Linux virtual machines running in a virtual network. Your management wants to deploy an Azure Firewall for securing the workloads. As soon as you deployed the firewall, the Linux machines are not able to download the updates. What needs to be done to make sure that the updates are allowed? Choose a solution that will have the lowest management overhead.**


* [ ] Whitelist IP addresses of update repositories to the firewall
* [ ] Ignore manual updates as Azure automatically updates all virtual machines
* [ ] Add an application rule and whitelist the domains
* [ ] Add a NAT rule to enable updates

**Correct answer:**
* [x] Add an application rule and whitelist the domains

**Explaination**: Adding IP addresses is not an efficient solution as chances of these IP addresses getting changed are high. It’s easy to add the FQDNs to an application rule so that requests to these domains are not blocked.

**32. What are the additional cost implications if you are setting up an active-active configuration for a VPN gateway?**


* [ ] The VPN gateway cost will be doubled for VPN gateway as your second instance is active.
* [ ] There is no change in the cost as the redundant instances need to be deployed for passive as well.
* [ ] The cost will be comparatively higher as data transfer happens via both tunnels.
* [ ] The cost will be double for data transfer as the data is transferred via both tunnels simultaneously.

**Correct answer:**
* [x] There is no change in the cost as the redundant instances need to be deployed for passive as well.

**Explaination**: There won’t be any billing implications or additional cost for switching to active-active. The redundant instances are always running regardless of active-passive or active-active, so the cost of that is always included in the price of the VPN gateway.

**33. What does reclaimPolicy in AKS Storage do?**


* [ ] Helps to claim the pod configuration 
* [ ] Controls the persistence of the disk
* [ ] Chooses the type of storage used for the cluster
* [ ] Maps the underlying storage to the node pool

**Correct answer:**
* [x] Controls the persistence of the disk

**Explaination**: When you delete the pod and the persistent volume is no longer required, reclaimPolicy controls the behavior of the underlying Azure storage resource. The underlying storage resource can be either deleted or kept for use with a future pod.

**34. You would like to have VMs of different configurations in a virtual machine scale set. Which configuration should you select to achieve this?**


* [ ] Spreading algorithm
* [ ] Uniform orchestration
* [ ] Flexible orchestration
* [ ] Azure Spot instance

**Correct answer:**
* [x] Flexible orchestration

**Explaination**: Flexible orchestration is the option you can use to have VMs of different configurations in a virtual machine scale set

**35. You have linked a virtual network to a private zone with autoregistration enabled. In the virtual network there are 150 VMs, and you accidentally deleted all the DNS records. You need to recover these DNS records as soon as possible to avoid a DNS outage for the virtual network. What needs to be done?**


* [ ] The records cannot be recovered. 
* [ ] Export the VM and IP addresses to a CSV file and import to the zone.
* [ ] Create a new zone and register the VMs there.
* [ ] No action is required.

**Correct answer:**
* [x] No action is required.

**Explaination**: The automatic registration happens again as long as the virtual machine still exists and has a private IP address attached to it. The DNS record is re-created automatically in the zone.

**36. You have implemented a policy-based VPN gateway and set up an S2S connection. Later, you decide to switch to a route-based VPN. What’s the right approach to change from a policy-based VPN to a route-based VPN?**


* [ ] Delete the S2S connection and update the VPN gateway; then re-create the connection.
* [ ] Disable the connection, update the gateway, and enable the connection again.
* [ ] Delete the connection, delete the gateway, and create a new route-based VPN.
* [ ] Changing via the portal is not supported; use PowerShell to change the gateway type.

**Correct answer:**
* [x] Delete the connection, delete the gateway, and create a new route-based VPN.

**Explaination**: You cannot change a policy-based VPN to route-based VON without redeploying the VPN gateway. Deleting the connection, deleting the gateway, and creating a new route-based VPN is the right approach.

**37. One of the previous administrators of your Azure environment created a General Purpose v1 Hot Standard Storage account in Azure. This account is used to store tables and blobs. Now your management is asking you to implement Lifecycle Management for blobs. However, you are not able to find this option in the Azure portal. What needs to be done?**


* [ ] Change the tier to Premium
* [ ] Enable Lifecycle Management from Advanced Properties
* [ ] Change the account to the Cool tier
* [ ] Upgrade to General Purpose v2

**Correct answer:**
* [x] Upgrade to General Purpose v2

**Explaination**: Only General Purpose v2 storage accounts support the Lifecycle Management option, so you need to upgrade your storage account.

**38. Your manager has asked you to remove a deployment from the deployment history. How can you achieve this?**


* [ ] The deployment history cannot be cleared; Azure stores it for 90 days.
* [ ] Use the az group deployment remove command.
* [ ] Use the az group deployment delete command.
* [ ] Use the az group deployment cancel command.

**Correct answer:**
* [x] Use the az group deployment delete command.

**Explaination**: You can use the "az group deployment delete" command to delete a deployment from the deployment history.

**39. What option do you have if the region doesn’t support availability zones?**


* [ ] The availability zone is available for all regions.
* [ ] Use an availability set.
* [ ] Flexible orchestration.
* [ ] Azure Spot instance.

**Correct answer:**
* [x] Use an availability set.

**Explaination**: Use an availability set in regions where availability zones are not available.

**40. You are planning to implement an Azure VPN gateway, and you would like to direct packets through IPsec tunnels based on the combinations of address prefixes between your on-premises network and the Azure VNet. What configuration should you select to achieve this?**


* [ ] Zone-redundant VPN
* [ ] Policy-based VPN
* [ ] Route-based VPN
* [ ] IPsec VPN

**Correct answer:**
* [x] Policy-based VPN

**Explaination**: Policy-based gateways implement policy-based VPNs. Policy-based VPNs encrypt and direct packets through IPsec tunnels based on the combinations of address prefixes between your on-premises network and the Azure virtual network. The policy (or traffic selector) is usually defined as an access list in the VPN configuration.

**41. What is the maximum duration of the script that can be run using Custom Script Extension for Linux machines?**


* [ ] 9 minutes
* [ ] 90 minutes
* [ ] 95 minutes
* [ ] 20 minutes

**Correct answer:**
* [x] 90 minutes

**Explaination**: The script is allowed 90 minutes to run. Anything longer will result in a failed provision of the extension.

**42. What option do you have if the region doesn’t support availability zones?**


* [ ] The availability zone is available for all regions.
* [ ] Use an availability set.
* [ ] Flexible orchestration.
* [ ] Azure Spot instance.

**Correct answer:**
* [x] Use an availability set.

**Explaination**: Use an availability set in regions where availability zones are not available.

**43. Your organization wants to use Custom Script Extension stored in one of the source controls to be executed on a Linux machine. Which property should be used to specify the command that needs to be executed?**


* [ ] fileUris
* [ ] scriptToExecute
* [ ] commandsToExecute
* [ ] commandToExecute

**Correct answer:**
* [x] commandToExecute

**Explaination**: The URL to the script can be added using the fileUris property, and then you can execute the script using the commandToExecute option.

**44. Which Azure CLI command is used to validate if the ARM template is syntactically correct?**


* [ ] az group template validate
* [ ] az group deployment whatif
* [ ] az group deployment validate
* [ ] az group template whatif

**Correct answer:**
* [x] az group deployment validate

**Explaination**: In Azure CLI, you can use "az group deployment validate" to validate whether a template is syntactically correct.

**45. Your security wants to make sure that public access to the storage account from all networks should be denied. All communications should happen via a VPN S2S connection from on-premises. What needs to be done about this requirement?**


* [ ] Create a service endpoint
* [ ] Create network rules and add the on-premises network to the storage account
* [ ] Create a private endpoint
* [ ] Use Azure Firewall

**Correct answer:**
* [x] Create a private endpoint

**Explaination**: Private endpoints can be created for the storage account that will create an interface for the storage account in the virtual network that can be accessed privately.

**46. Your organization is planning to save VHD files in Azure Blob Storage. Which type of blob should be selected for this?**


* [ ] Append blob
* [ ] Block blob
* [ ] General blob
* [ ] Page blob

**Correct answer:**
* [x] Page blob

**Explaination**: A page blob is ideal for frequent read/write operations and can be up to 8 TB in size. Azure stores virtual machine OS disks and data disks in page blob format.

**47. Your organization wants to bring your own keys for SSE. Which service should you use to store these keys?**


* [ ] Azure Key Vault
* [ ] Key Management Service
* [ ] Recovery Services Vault
* [ ] Windows Key Store

**Correct answer:**
* [x] Azure Key Vault

**Explaination**: Azure Key Vault can be used to store SSE and ADE keys.

**48. You have an Azure subscription. You are deploying an Azure Kubernetes Service (AKS) cluster that will contain multiple pods. The pods will use Kubernetes networking; you need to restrict network traffic between the pods. What should you configure on the AKS cluster?**


* [ ] Security policy
* [ ] Kubernetes ingress policy
* [ ] Azure network policy
* [ ] Services

**Correct answer:**
* [x] Azure network policy

**Explaination**: An Azure network policy needs to be created to restrict the traffic between the pods.

**49. You have an Azure subscription named `az-sub01` that contains an Azure Log Analytics workspace named `la-workspace-01`. You need to view the error events from a table named Event. Which query should you run in `la-workspace-01`?**


* [ ] Get-Event Event | where {$_.EventType == "error"}
* [ ] search in (Event) "error"
* [ ] select * from Event where EventType == "error"
* [ ] search in (Event) * | where EventType -eq "error"

**Correct answer:**
* [x] search in (Event) "error"

**Explaination**: You can use search in followed by the table name and keyword for searching in a table. You can also use Event | search "error" or Event | where EventType == "error".

**50. One of your Linux virtual machines has the following NSGs attached to it. ![question-image](https://res.cloudinary.com/dezmljkdo/image/upload/v1658341172/az104/az1042_xvcrzk.png) <br> You are not able to connect to the VM over SSH. How can you fix this?**


* [ ] Change the priority of rule 1 of subnet level NSG to 100
* [ ] Delete rule 1 from NIC level NSG
* [ ] Change the source of rule 1 of subnet level NSG
* [ ] Delete the NIC level NSG

**Correct answer:**
* [x] Delete rule 1 from NIC level NSG

**Explaination**: Delete rule 1 from the NIC level so NSG will allow SSH traffic.

---


## consul-architecture

**1. Which of the following is responsible for managing membership and broadcasting messages within the Consul cluster?**


* [ ] Serf Protocol
* [ ] Raft Protocol
* [ ] Prepared Queries
* [ ] Proxies

**Correct answer:**
* [x] Serf Protocol

**Explaination**: Serf is the gossip protocol that is used across all servers and client members within a Consul cluster for broadcasting messages about server and client membership.

**2. A Consul cluster containing (5) Consul server nodes can tolerate a maximum of how many node failures before the Consul cannot establish a quorum and continue to operate?**


* [ ] 1
* [ ] 2
* [ ] 3
* [ ] 4

**Correct answer:**
* [x] 2

**Explaination**: The following chart can be used to determine the number of failures a Consul cluster can handle before being unable to establish a quorum and the Consul service becoming unavailable. https://www.consul.io/docs/internals/consensus.html#deployment-table

**3. Which of the following is `not` true about running Consul in development mode?**


* [ ] development mode is not secure
* [ ] development mode is easily scalable
* [ ] development mode allows you to easily experiment with most of the Consul's functionality
* [ ] development mode should never be used in the production environment

**Correct answer:**
* [x] development mode is easily scalable

**Explaination**: Dev mode only runs on a single node, such as your desktop or laptop. Therefore, it is not a scalable solution when using it for Consul services

**4. In order to provide high availability and ensure that Consul's state is preserved even if a server fails, HashiCorp suggests that Consul should be configured to run  __________.**


* [ ] in a cluster made up of three to five servers
* [ ] on as many servers as needed to scale for performance
* [ ] only Kubernetes to provide scheduling for new Consul nodes
* [ ] on a public cloud platform that can provide Hardware redundancy

**Correct answer:**
* [x] in a cluster made up of three to five servers

**Explaination**: In order to make sure that Consul's state is preserved even if a server fails, you should always run either three or five servers in production. The odd number of servers (and no more than five of them) strikes a balance between performance and failure tolerance. When scaled beyond (7) servers, the network requirements needed to maintain replication between the clusters may negatively impact the performance of the Consul.

**5. Which nodes in a Consul datacenter do not participate in the LAN gossip pool?**


* [ ] Consul server nodes
* [ ] Consul client nodes
* [ ] Consul non-voting server nodes
* [ ] Consul consumers (meaning any client accessing Consul, such as user desktop)

**Correct answer:**
* [x] Consul consumers (meaning any client accessing Consul, such as user desktop)

**Explaination**: Consumers of the Consul service do not participate in the gossip pool. However, all members of the Consul do participate in the LAN and possibly a WAN pool, when federated.

---


## consul-deploy-single-datacenter

**1. What command can be used for new Consul agents to join an existing cluster?**


* [ ] consul connect
* [ ] consul join
* [ ] consul cluster -join
* [ ] consul exec -join

**Correct answer:**
* [x] consul join

**Explaination**: This is the only valid command for joining the cluster and is the proper way for a Consul agent to join an existing cluster

**2. True or False? When joining a new Consul agent to a cluster, the consul join command must include all the server nodes that make up the cluster.**


* [ ] TRUE
* [ ] FALSE

**Correct answer:**
* [x] FALSE

**Explaination**:  A new Consul agent reference any node in the existing cluster. After joining with one member, the gossip communication will propagate the updated membership state across the cluster.


**3. What command below can be used to display the participating servers and clients within the local Consul cluster?**


* [ ] consul members
* [ ] consul monitor
* [ ] consul info
* [ ] consul validate

**Correct answer:**
* [x] consul members

**Explaination**: The 'consul members' command outputs the current list of members that a Consul agent knows about, along with their state. The state of a node can only be "alive", "left", or "failed".

**4. Scenario: You are automating the deployment of a new 3-node Consul cluster using Terraform, but not all the nodes are joining the cluster as expected. It seems some nodes are being provisioned faster than others. Because of this, a leader is never elected and the cluster is never established. You are using the Consul configuration that is shown below, but you continue to get the following error on multiple nodes. What can be changed in the configuration file in order to ensure the Consul cluster is bootstrapped and a leader is elected?**


* [ ] change `join` to `retry_join`
* [ ] update the node name to include numbers instead of letters
* [ ] update `node_name` to `node_names` to indicate there is one node in the cluster

**Correct answer:**
* [x] change `join` to `retry_join`

**Code**: 
[WARN] raft: EnableSingleNode disabled, and no known peers. Aborting election.

# Configuration file
{
  "acl": {
    "enabled": true,
    "default_policy": "allow",
    "down_policy": "extend-cache"
  },
  "bind_addr": "0.0.0.0",
  "bootstrap_expect": 3,
  "client_addr": "0.0.0.0",
  "datacenter": "primary",
  "data_dir": "/var/consul/data",
  "join": ["10.0.15.76, 10.0.15.35"],
  "log_level": "INFO",
  "node_name": "consul-node-a.example.com",
  "performance": {
    "raft_multiplier": 1
  },  
  "server": true,
  "ui": true,
}



**Explaination**: The key to this question is the phrase "It seems some nodes are being provisioned faster than others." Nodes are spinning up faster than others, and since the configuration file only includes a join statement, the nodes being deployed faster cannot communicate with the other nodes, therefore the Consul agent fails. join does not reattempt communication with the listed node(s), therefore the cluster is never bootstrapped

**5. Based on the Consul agent configuration below, what parameter will determine which interface the Consul will use for internal cluster communications?**


* [ ] "bind_addr": "10.0.30.186"
* [ ] "client_addr": "0.0.0.0"
* [ ] "node_name": "node-a.example.com"
* [ ] "verify_outgoing": true

**Correct answer:**
* [x] "bind_addr": "10.0.30.186"

**Code**: 
{
  "log_level": "INFO",
  "server": true,
  "node_name": "node-a.example.com",
  "key_file": "/etc/consul.d/cert.key",
  "cert_file": "/etc/consul.d/client.pem",
  "ca_file": "/etc/consul.d/chain.pem",
  "verify_incoming": true,
  "verify_outgoing": true,
  "encrypt": "xxxxxxxxxxxxxxxxxxxxxxxx",
  "data_dir": "/opt/consul/data",
  "datacenter": "us-east-1",
  "bind_addr": "10.0.30.186",
  "client_addr": "0.0.0.0",
  "retry_join": ["provider=aws tag_key=Environment-Name tag_value=consul region=us-east-1"],
  "enable_syslog": true,
  "acl": {
     "tokens": {
        "agent": "xxxxxxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxx"
        }
  }
}

**Explaination**: The parameter bind_addr is used to determine the address that should be bound to for internal cluster communications. This is an IP address that should be reachable by all other nodes in the cluster. By default, this is "0.0.0.0", meaning Consul will bind to all addresses on the local machine and will advertise the private IPv4 address to the rest of the cluster.

---


## consul-services

**1. Which of the following is NOT a valid type of a Consul health check?**


* [ ] Script Health Check
* [ ] Port Health Check
* [ ] TTL Health Check
* [ ] TCP Health Check

**Correct answer:**
* [x] Port Health Check

**Explaination**: Port health check is incorrect as only a TCP health check can detect whether or not a connection can be established using an IP address and port

**2. You have registered a new service using the service definition below. What DNS record can you query to get the results of healthy nodes hosting the service?**


* [ ] front-end-eCommerce.service.consul
* [ ] web-server-01.service.consul

**Correct answer:**
* [x] front-end-eCommerce.service.consul

**Code**: 
{
"service": {
"id": "web-server-01",
"name": "front-end-eCommerce",
"tags": ["v7.05", "production"],
"address": "10.3.13.112",
"port": 8080,
"checks": [
   {
    "args": ["/usr/local/bin/check_mem.py"],
    "interval": "30s"
   }
  ],
 }
}

**Explaination**: the service being registered here is named front-end-eCommerce and the default address of a service is <name>.service.consul

**3. A developer named Terry wants to query Consul and only retrieve the hosts providing the "eCommerce-web" service tagged with v7.5. What feature of Consul should Terry use?**


* [ ] failover policy
* [ ] prepared query
* [ ] health check
* [ ] service definition

**Correct answer:**
* [x] prepared query

**Explaination**:  A prepared query allows you to create a more complex service query, including the ability to filter results based on tags

**4. You have deployed a virtual machine that hosts two different web applications named web-01 and web-02. You have multiple health checks configured, including two application (service level) health checks and one host-level health check. When the services are queried, which of the following will be returned as part of the service query?**


* [ ] web-01 will be returned but not web-02
* [ ] web-01 and web-02 will both be returned, but web-02 will be returned as failing
* [ ] neither of the services will be returned

**Correct answer:**
* [x] neither of the services will be returned

**Code**: 
The current status of the health checks is as follows:

Host-Level Health Check: Failing

Web-01 Health Check: Passing

Web-02 Health Check: Failing


**Explaination**: the host-level health check is failing. Therefore, neither service running on that host will be returned in service query

**5. Which of the following is not a valid method of registering a new service with the Consul?**


* [ ] create definition as part of the Consul agent configuration
* [ ] place a file alongside the Consul agent configuration file when using the `-config-dir` parameter
* [ ] run the `consul services register` command while referencing a service configuration file
* [ ] register the service in the Consul UI using service definition file

**Correct answer:**
* [x] register the service in the Consul UI using service definition file

**Explaination**: The Consul UI doesn't support registering a Consul service directly

---


## consul-kv-store

**1. You want to set up a watch to invoke an API if a specific value changes in the Consul K/V. What type of watch is the most specific for watching this value?**


* [ ] event
* [ ] key prefix
* [ ] key
* [ ] services

**Correct answer:**
* [x] key

**Explaination**: a key watch can watch a specific KV pair and alert on changes

**2. Data in the Consul K/V is replicated across what type of node(s)?**


* [ ] across all consul server nodes in the cluster
* [ ] across all consul server nodes in all federated datacenters
* [ ] across Consul server and client nodes
* [ ] across Consul client nodes

**Correct answer:**
* [x] across all consul server nodes in the cluster

**Explaination**: data is replicated across all server nodes within a cluster

**3. What feature can be used to protect access to the Consul K/V?**


* [ ] Consul ACLs
* [ ] encryption
* [ ] mutual TLS
* [ ] consul watch

**Correct answer:**
* [x] Consul ACLs

**Explaination**: Consul ACLs provides an RBAC feature in Consul to restrict access to data

**4. Which of the following should NOT be stored in the Consul K/V store?**


* [ ] configuration parameters
* [ ] metadata
* [ ] database passwords
* [ ] variables

**Correct answer:**
* [x] database passwords

**Explaination**: remember that the Consul K/V is not an encrypted store, so sensitive credentials shouldn't be stored in the K/V. Use something like Vault instead

**5. You are using the Consul HTTP API to retrieve data from the Consul K/V. Instead of getting the expected value, you receive a value such as J2VuYWJsZWQn . Why do you not get the value in plain-text?**


* [ ] the data is encrypted on the Consul K/V store
* [ ] Consul K/V values are base64 encoded
* [ ] Consul uses `Vault` to encrypt any data saved to the Consul K/V

**Correct answer:**
* [x] Consul K/V values are base64 encoded

**Explaination**: You can easily decode this value to get the plain-text value

---


## consul-backup-restore

**1. The Consul Snapshot Agent provides many different features. Which is NOT one of the features provided by the Consul Snapshot Agent?**


* [ ] failover
* [ ] high-availability
* [ ] automated snapshots
* [ ] automated recovery

**Correct answer:**
* [x] automated recovery

**Explaination**: Automated recovery is not a feature in Consul

**2. True or False? Data can be selectively restored from a Consul snapshot, meaning I can pick and choose what data needs to be restored to my Consul cluster.**


* [ ] TRUE
* [ ] FALSE

**Correct answer:**
* [x] FALSE

**Explaination**: snapshot restore is an "all or nothing" type of action

**3. Consul operators can take manual snapshots of the cluster by using multiple Consul interfaces. Which Consul interface does not provide the ability to create a snapshot ?**


* [ ] API
* [ ] UI
* [ ] CLI

**Correct answer:**
* [x] UI

**Explaination**: the User Interface does not offer a way to take a Consul snapshot

**4. A Consul snapshot saves all the information in Consul except which of the following?**


* [ ] ACLs
* [ ] Key/Value data
* [ ] Consul agent configuration
* [ ] Prepared Queries

**Correct answer:**
* [x] Consul agent configuration

**Explaination**: snapshots do not include the agent configuration

**5. By default, Consul snapshots are taken in ____________ mode, meaning that the leader performs the snapshot.**


* [ ] stale
* [ ] consistent
* [ ] encrypted
* [ ] performance

**Correct answer:**
* [x] consistent

**Explaination**: a consistent snapshot means that it was indeed taken by the leader node

---


## consul-service-proxy

**1. Which interface provides the most feature-rich options for creating a Service Intention?**


* [ ] API
* [ ] CLI
* [ ] UI

**Correct answer:**
* [x] API

**Explaination**: the API at /connect/intentions/exact should be used and provides the most features

**2. The service named `eCommerce-FrontEnd` relies on a backend database service called `customer-db` in order to properly service customer orders. Select the correct statement below based on this configuration.**


* [ ] the `customer-db` service is downstream from the `eCommerce-FrontEnd`
* [ ] the `customer-db` service is upstream from the `eCommerce-FrontEnd`

**Correct answer:**
* [x] the `customer-db` service is upstream from the `eCommerce-FrontEnd`

**Explaination**: the customer-db service is upstream from the eCommerce-FrontEnd service. The eCommerce-FrontEnd is downstream from the customer-db service

**3. True or False? When registering a service proxy, Consul automatically starts a new service proxy if a custom one is not specified.**


* [ ] TRUE
* [ ] FALSE

**Correct answer:**
* [x] FALSE

**Explaination**: Registration does not start the sidecar proxy - you must do that manually or programmatically

**4. True or False? Intentions follow a top-down ruleset and precedence cannot be overridden.**


* [ ] TRUE
* [ ] FALSE

**Correct answer:**
* [x] TRUE

**Explaination**: Intentions follow a top-down ruleset using Allow or Deny intentions. More specific rules are evaluated first.

**5. Which of the following statement is NOT true regarding Consul Service Mesh architecture?**


* [ ] applications may not be aware of the consul service mesh is present
* [ ] applications can be written for native support of Consul service mesh
* [ ] Service Configuration file often declare the downstream service(s) that the local service relies on
* [ ] intentions define access control for Services

**Correct answer:**
* [x] Service Configuration file often declare the downstream service(s) that the local service relies on

**Explaination**: service files would include upstream services, not the downstream

---


## consul-secure-agent-conf

**1. Which component of the Consul security model uses a pre-shared key to secure communications throughout the Consul cluster?**


* [ ] Consul Agent
* [ ] gossip protocol
* [ ] Consul ACL system
* [ ] Consul Certificate Authority

**Correct answer:**
* [x] gossip protocol

**Explaination**: You can use consul keygen or consul keyring to manage the gossip encryption key

**2. If you are using your own private certificate authority (CA) to issue certificates for Consul, what additional subject alternative name (SAN) should you add to the certificate to ensure you don't receive errors when configuring the most secure Consul environment?**


* [ ] server.<datacenter>.<domain>
* [ ] add a SAN for the IP Address
* [ ] add a SAN for the DNS friendly name of the Consul cluster
* [ ] add a SAN for each DNS name of all nodes in the cluster

**Correct answer:**
* [x] server.<datacenter>.<domain>

**Explaination**: `server.<datacenter>.<domain>` ensures a client cannot modify the Consul Agent config and restart as a server


**3. In order to ensure a Consul client cannot modify the agent configuration and restart as a server, what feature should organizations enable in the Consul configuration file?**


* [ ] verify_incoming
* [ ] verfiy_server_hostname
* [ ] verify_outgoing
* [ ] encrypt <key>

**Correct answer:**
* [x] verfiy_server_hostname

**Explaination**: All outgoing connections will perform hostname verification. It ensures that servers have a certificate valid for server.<datacenter>.<domain>

**4. True or False? Consul's flexibility allows you to use certificates from multiple internal certificate authorities.**


* [ ] TURE
* [ ] FALSE

**Correct answer:**
* [x] FALSE

**Explaination**: all certificates must be signed by the same certificate authority

**5. Consul can create multiple types of certificates when it is configured as a certificate authority. Which is `NOT` one of the types of certificates you can create?**


* [ ] server
* [ ] client
* [ ] API
* [ ] CLI

**Correct answer:**
* [x] API

**Explaination**: Consul can create certificates types of server, client, and CLI

---


## consul-acl

**1. True or False? Before the ACL system can be used, it must be enabled in the agent configuration file and bootstrapped.**


* [ ] TRUE
* [ ] FALSE

**Correct answer:**
* [x] TRUE

**Explaination**: It must be enabled in the configuration file and bootstrapped before policies or tokens can be created

**2. James is making some changes using the Consul CLI and has been provided a token for authentication. Which of the following is NOT a valid way that James can provide the token when executing Consul CLI commands?**


* [ ] set up the environment variable `CONSUL_HTTP_TOKEN` with the value of the token
* [ ] save the token in the local file and reference that file using the `-consul-token-file` parameter
* [ ] use the `-token` parameter as the part of the command
* [ ] set environment variable `CONSUL_HTTP_TOKEN_FILE` to the path of the file where you have saved the token

**Correct answer:**
* [x] save the token in the local file and reference that file using the `-consul-token-file` parameter

**Explaination**: `-consul-token-file` is not a valid parameter - you could use -token-file instead.

**3. You have completed the configuration of Consul ACLs, the default_policy is set to Deny, and you created a new policy for your end-users to query Consul for a critical service they depend on to perform their job. How can the end-users successfully query Consul using DNS without providing them an ACL token?**


* [ ] disable the ACL system for those specific users
* [ ] configure a policy for `node_prefix` to allow all local nodes to query Consul
* [ ] update the anonymous token with the new policy that you created
* [ ] configure the policy to allow queries from all nodes in the Consul datacenter

**Correct answer:**
* [x] update the anonymous token with the new policy that you created

**Explaination**: any request to Consul that does not specifically include a token will try and use the anonymous token for authentication

**4. Julie is working on a Consul cluster with ACLs enabled, but she is able to create new Service Mesh intentions using the command line without providing an ACL token. What could be the reason that her commands are successful?**


* [ ] the ACL system has not yet been bootstrapped
* [ ] the default_policy is said to "Allow"
* [ ] the token is being sent using `X-Consul-Token` header
* [ ] Consul intention creation doesn't require an ACL Token

**Correct answer:**
* [x] the default_policy is said to "Allow"

**Explaination**: the ACL system can be enabled and working but if the default_policy is set to allow, any requests to Consul would not require a token

**5. You have created a token for a consultant that should permit access to the Consul cluster until the end of the week. What built-in feature of Consul can you use to automatically ensure that the token is no longer valid at the end of the week?**


* [ ] Sign in and manually revoke the token at the end of the week
* [ ] set the optional expiration date when creating the token
* [ ] set up a `Cron job` that runs at the end of the week and revokes the token
* [ ] restart the consul service at the end of the week

**Correct answer:**
* [x] set the optional expiration date when creating the token

**Explaination**: the optional configuration sets the duration on how long the token is valid. After that time, it will be automatically revoked



---


## consul-gossip-encryption

**1. True or False? By default, Consul automatically encrypts gossip communication/messages using a self-signed certificate.**


* [ ] TRUE
* [ ] FALSE

**Correct answer:**
* [x] FALSE

**2. You need to create a gossip encryption key. What Consul built-in tool/command can you use to easily create one or many keys?**


* [ ] consul keyring -create
* [ ] consul keygen
* [ ] consul tls cert create -server
* [ ] consul keyrin -install

**Correct answer:**
* [x] consul keygen

**Explaination**: consul keygen will quickly create a new encryption key for you

**3. You run a consul keyring -list command and notice that the gossip encryption key being used doesn't match the key displayed in the configuration file. How can this be?**


* [ ] the key has been rotated since the initial configuration
* [ ] gossip encryption has been disabled
* [ ] the consul agent was loaded with a different configuration file
* [ ] the key is displayed in the consul keyring command resulting in the creation of the new keys and doesn't show the current keys

**Correct answer:**
* [x] the key has been rotated since the initial configuration

**Explaination**: the key being used may not always match the one displayed in the configuration file, since Consul only reads that key one time during the initial configuration of a Consul agent

**4. Gossip uses what type of security method for encryption?**


* [ ] mTLS certificate from a trusted CA
* [ ] a self-signed cert minted from the built-in Consul certificate Authority
* [ ] a symmetric key
* [ ] an ACL using the Gossip resource

**Correct answer:**
* [x] a symmetric key

**Explaination**: Remember symmetric key also uses a 32-byte, Base64 encoded key.

**5. Which of the following functions/actions can the consul keyring command NOT perform?**


* [ ] list the keys installed on the cluster
* [ ] create a new key to be used for the cluster
* [ ] remove a key from the cluster
* [ ] distribute a new key from the cluster

**Correct answer:**
* [x] create a new key to be used for the cluster

**Explaination**: the consul keyring command does not have key creation functionality. Use consul keygen instead

---


## consul-practice-exam

**1. True or False? The Consul UI and the API can only be accessed from a Consul server itself.**


* [ ] TRUE
* [ ] FALSE

**Correct answer:**
* [x] FALSE

**Explaination**: This is false. The UI and API are intended to be consumed from remote systems, such as a user's desktop or an application looking to discover a remote service in which it needs to establish connectivity. In addition, most consumers of the Consul service wouldn't normally have access to connect (SSH) to a Consul server anyway

**2. In order to provide high availability and ensure that Consul's state is preserved even if a server fails, HashiCorp suggests that Consul should be configured to run  __________.**


* [ ] in a cluster made up of three to five servers
* [ ] on as many servers as needed to scale for performance
* [ ] only Kubernetes to provide scheduling for new Consul nodes
* [ ] on a public cloud platform that can provide Hardware redundancy

**Correct answer:**
* [x] in a cluster made up of three to five servers

**Explaination**: In order to make sure that Consul's state is preserved even if a server fails, you should always run either three or five servers in production. The odd number of servers (and no more than five of them) strikes a balance between performance and failure tolerance. When scaled beyond (7) servers, the network requirements needed to maintain replication between the clusters may negatively impact the performance of the Consul.

**3. Based on the file below, in what directory would you place the consul.hcl configuration file in order to start the Consul service?**


* [ ] /etc/consul.d/
* [ ] /usr/local/bin
* [ ] /etc/systemd/system
* [ ] /opt/services/consul

**Correct answer:**
* [x] /etc/consul.d/

**Code**: 
[Unit]
Description="HashiCorp Consul - A service mesh solution"
Documentation=https://www.consul.io/
Requires=network-online.target
After=network-online.target
 
[Service]
Type=notify
User=consul
Group=consul
ExecStart=/usr/local/bin/consul agent -config-dir=/etc/consul.d/
ExecReload=/usr/local/bin/consul reload
ExecStop=/usr/local/bin/consul leave
KillMode=process
Restart=on-failure
LimitNOFILE=65536
 
[Install]
WantedBy=multi-user.target


**Explaination**: Based on the executable statement in the service file, all .hcl configuration files for Consul would be placed in /etc/consul.d/ . You can break up your desired Consul configurations across multiple .hcl files if you wish. It's common to put the actual Consul configuration in a consul.hcl file while putting node_meta data into a node_meta.hcl file, therefore both will be read upon starting or refreshing the Consul service. This is because the executable statement in the service file is using -config-dir rather than pointing to a single file using -config-file

**4. In most organizations, a service will be run on multiple nodes to provide redundancy and high availability. In the following example, what is the name of the Consul service that this service definition will create?**


* [ ] web-a
* [ ] web-frontend
* [ ] green
* [ ] web-server-health

**Correct answer:**
* [x] web-frontend

**Code**: 
{
  "service": {
    "id": "web-a",
    "name": "web-frontend",
    "port": 80,
    "tags": [
          "web",
          "green"
      ],
    "enable_tag_override": false,
    "checks": [
          {
              "interval": "10s",
              "name": "web-server-health",
              "tcp": "localhost:80",
              "DeregisterCriticalServiceAfter": "60s"
          }
      ]
  }
}


**Explaination**: The service definition above will create a service named web-frontend, and will register a new node named web-a that will host the web-frontend service. As long as the health check passes, web-a will register as healthy and traffic destined to the web-frontend service will be directed to this node.


**5. Based on the Consul agent configuration below, what parameter will determine which interface the Consul will use for internal cluster communications?**


* [ ] "bind_addr": "10.0.30.186"
* [ ] "client_addr": "0.0.0.0"
* [ ] "node_name": "node-a.example.com"
* [ ] "verify_outgoing": true

**Correct answer:**
* [x] "bind_addr": "10.0.30.186"

**Code**: 
{
  "log_level": "INFO",
  "server": true,
  "node_name": "node-a.example.com",
  "key_file": "/etc/consul.d/cert.key",
  "cert_file": "/etc/consul.d/client.pem",
  "ca_file": "/etc/consul.d/chain.pem",
  "verify_incoming": true,
  "verify_outgoing": true,
  "encrypt": "xxxxxxxxxxxxxxxxxxxxxxxx",
  "data_dir": "/opt/consul/data",
  "datacenter": "us-east-1",
  "bind_addr": "10.0.30.186",
  "client_addr": "0.0.0.0",
  "retry_join": ["provider=aws tag_key=Environment-Name tag_value=consul region=us-east-1"],
  "enable_syslog": true,
  "acl": {
     "tokens": {
        "agent": "xxxxxxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxx"
        }
  }
}

**Explaination**: The parameter bind_addr is used to determine the address that should be bound to for internal cluster communications. This is an IP address that should be reachable by all other nodes in the cluster. By default, this is "0.0.0.0", meaning Consul will bind to all addresses on the local machine and will advertise the private IPv4 address to the rest of the cluster.

**6. A Consul cluster containing (5) Consul server nodes can tolerate a maximum of how many node failures before the Consul cannot establish a quorum and continue to operate?**


* [ ] 1
* [ ] 2
* [ ] 3
* [ ] 4

**Correct answer:**
* [x] 2

**Explaination**: The following chart can be used to determine the number of failures a Consul cluster can handle before being unable to establish a quorum and the Consul service becoming unavailable. https://www.consul.io/docs/internals/consensus.html#deployment-table

**7. Complete the sentence:  The main restriction on Consul's K/V store is an object's size, which can be a maximum of _______?**


* [ ] 512KB
* [ ] 64KB
* [ ] 8KB
* [ ] 1MB

**Correct answer:**
* [x] 512KB

**Explaination**: The main restriction on an object is size - the maximum is 512 KB. Due to the maximum object size and main use cases, you should not need extra storage

**8. Scenario: You are storing configuration settings for your application in Consul's K/V store, and each setting is critical to the successful implementation of the application. A developer recently updated the value for app1, causing the deployment to fail.  What Consul feature can be used to monitor the K/V store for updates and automatically take action to remediate the issue?**


* [ ] set up health checks to monitor for changes to the K/V store
* [ ] configure a watch and execute the script to update the application
* [ ] set up an ACL to automatically restart the consul service when a value is changed
* [ ] use the raft consensus protocol to replicate the changes from the other nodes when the value is changed

**Correct answer:**
* [x] configure a watch and execute the script to update the application

**Explaination**: Watches are a way of specifying a view of data (e.g. list of nodes, KV pairs, health checks) which is monitored for updates. When an update is detected, an external handler is invoked. A handler can be any executable or HTTP endpoint. As an example, you could set up a key watch type that executes a python script when the value of a key changes

**9. True or False? The open-source tools Consul Template and Envconsul require a Consul cluster to operate**


* [ ] TRUE
* [ ] FALSE

**Correct answer:**
* [x] FALSE

**Explaination**: Despite the name, Consul Template, nor Envconsul, does not require a Consul cluster to operate. Consul Template can retrieve secrets from Vault and manage the acquisition and renewal lifecycle. Envconsul can launch a subprocess that dynamically populates environment variables from secrets read from Vault.


**10. Which of the following Consul features is responsible for securing inter-service communication with mutual TLS by using sidecar proxies?**


* [ ] Envoy
* [ ] Consul ACLs
* [ ] Consul Gossip
* [ ] Consul Connect

**Correct answer:**
* [x] Consul Connect

**Explaination**: Consul Connect provides service-to-service connection authorization and encryption using mutual Transport Layer Security (TLS). Applications can use sidecar proxies in a service mesh configuration to establish TLS connections for inbound and outbound connections without being aware of Connect at all. Applications may also natively integrate with Connect for optimal performance and security. Connect can help you secure your services and provide data about service-to-service communications.

**11. During leadership election, which members in the local datacenter get a vote to elect a new leader?**


* [ ] Consul servers, read-only server nodes, and clients
* [ ] Consul servers and clients
* [ ] Consul Clients
* [ ] Consul server nodes

**Correct answer:**
* [x] Consul server nodes

**Explaination**: Consul's consensus protocol (Serf) is responsible for electing a leader during a new cluster creation or if an existing leader fails. The consensus protocol is only used on Consul server nodes, therefore clients do not participate in voting for a new cluster leader. Furthermore, read-only nodes are also known as non-voting nodes and do not participate in voting for a new leader (hence the name).

**12. Complete the following sentence with the proper order of answers:**


* [ ] mutual TLS, TLS
* [ ] TLS, mutual TLS
* [ ] SSL, TLS
* [ ] mutual TLS, SSL

**Correct answer:**
* [x] TLS, mutual TLS

**Code**: 
Consul uses two types of certificates for encryption. Consul agent communications are secured by _________ and Consul Connect uses __________ between registered services.

**Explaination**: Consul agent communications are done using TLS certificates that can be created by the built-in CA or an external CA if you need more control over certificates. Consul Connect uses mutual TLS for authorization and encryption.

**13. Your colleague has deployed a new Consul cluster, and you want to double-check the encryption key used for gossip communication. You open up an SSH session to a Consul node and type the command consul keyring -list but receive the following error.  From the error message below, what is missing from the Consul agent configuration file?**


* [ ]  the `gossip` parameter and a value of `true`
* [ ] the `encrypt` parameter and the corresponding value
* [ ] the `encryption` parameter with a valid of `true` or `1`
* [ ] configuration for the TLS certificate, the private key, and the CA bundle

**Correct answer:**
* [x] the `encrypt` parameter and the corresponding value

**Code**: 
$  consul keyring -list
 
==> Gathering installed encryption keys...
error: Unexpected response code: 500 (12 errors occurred:
        * WAN error: 5/5 nodes reported failure
        * CONSUL-NODE-D.dc-1: Keyring is empty (encryption not enabled)
        * CONSUL-NODE-C.dc-1: Keyring is empty (encryption not enabled)
        * CONSUL-NODE-A.dc-1: Keyring is empty (encryption not enabled)
        * CONSUL-NODE-E.dc-1: Keyring is empty (encryption not enabled)
        * CONSUL-NODE-B.dc-1: Keyring is empty (encryption not enabled)
        * dc-1 (LAN) error: 5/5 nodes reported failure
        * CONSUL-NODE-E: Keyring is empty (encryption not enabled)
        * CONSUL-NODE-D: Keyring is empty (encryption not enabled)
        * CONSUL-NODE-B: Keyring is empty (encryption not enabled)
        * CONSUL-NODE-A: Keyring is empty (encryption not enabled)
        * CONSUL-NODE-C: Keyring is empty (encryption not enabled)


**Explaination**: In the configuration file, the encrypt parameter must be used to enable gossip encryption and set the gossip encryption key. The provided key is automatically persisted to the data directory and loaded automatically whenever the agent is restarted. The fact that the key is persisted in the data directory means that in order to encrypt Consul's gossip protocol, this option only needs to be provided once on each agent's initial startup sequence.

**14. A user is defining a new prepared query named web-app for an application that includes a failover policy for providing high availability for a service. However, when the user accesses the app using the DNS name web-app.service.consul, access to the application isn't failing to the secondary datacenter as expected. What could be the issue?**


* [ ] Consul Federation isn't supported for Web Applications
* [ ] Prepared queries doesn’t support failover policies
* [ ] The user needs to use `web-app.query.consul` instead
* [ ] Prepared queries are only accessible by API, and not DNS

**Correct answer:**
* [x] The user needs to use `web-app.query.consul` instead

**Explaination**: When defining a prepared query, the default endpoint for using the prepared query is <name>.query.consul. If the user is trying to access it using web-app.service.consul, the user may be hitting the service directly, and not taking advantage of the prepared query at all. Therefore, if the local service were to go down, the failover policy would not be used, since you must hit the DNS name of the prepared query to take advantage of a failover policy.

**15. Which token is always assigned the Accessor ID of 00000000-0000-0000-0000-000000000002, regardless of what cluster you are working on?**


* [ ] master token
* [ ] the second token create by the user
* [ ] anonymous token
* [ ] Consul DNS token

**Correct answer:**
* [x] anonymous token

**Explaination**: The anonymous token is used when a request is made to Consul without specifying a bearer token. The anonymous token's description and policies may be updated but Consul will prevent this token's deletion. When created, it will be assigned 00000000-0000-0000-0000-000000000002 for its Accessor ID and anonymous for its Secret ID.


**16. Consul uses a gossip protocol that is powered by Serf. How is this communication protected between all participating servers and clients?**


* [ ] mutual TLS
* [ ] TLS
* [ ] username and password
* [ ] shared secret

**Correct answer:**
* [x] shared secret

**Explaination**: Consul's gossip protocol is protected by a symmetric key, or a shared secret, that is configured as part of the configuration file or in a separate file that is read when the Consul service starts. For example, you can add the parameter "encrypt" to the configuration file with 32-byte, Base64 encoded shared secret. All nodes in the Consul cluster, including WAN joined datacenters, must use the same encryption key. An example of this key would be pUqJrVyVRj5jsiYEkM/tFQYfWyJIv4s3XkvDwy7Cu5s= Furthermore, you can generate this 32-byte, Base64 encoded shared secret by using the built-in command

**17. Using the service configuration below, what service will be registered, and what port will the service run on?**


* [ ] `watermark_web_a` running on port `8080`
* [ ] `picture_app` running on port `8080`
* [ ] `watermark_web_a` running on port `80`
* [ ] `picture_app` running on port `80`

**Correct answer:**
* [x] `picture_app` running on port `8080`

**Code**: 
{
  "service": {
    "ID": "watermark_web_a",
    "name": "picture_app",
    "tags": ["front-end", "watermark"],
    "port": 8080,
    "check": {
      "id": "picture_app_check",
      "name": "Check Counter health 80",
      "tcp": "localhost:80",
      "interval": "10s",
      "timeout": "1s"
    }
  }
}


**Explaination**: For this service definition, the instance of the service is watermark_web_a but the service itself will be named picture_app. The service definition has a port defined of 8080, so that is the port the service will run on. The health check is checking that a service is running on port 80 on the same host, but that is not the port that the picture_app will be registered on.

**18. Assuming Consul default configurations, which of the following DNS records would be used to access the service referenced by the following configuration:**


* [ ] inventory-app.query.service.consul
* [ ] inventory-app.query.consul
* [ ] retail-app.service.consul
* [ ] retail-app.query.consul

**Correct answer:**
* [x] retail-app.query.consul

**Code**: 
{
  "Name": "retail-app",
  "Service": {
    "Service": "inventory-app",
    "Tags": ["v1.2.3"],
    "Failover": {
      "Datacenters": ["dc2", "dc3"]
    }
  }
}


**Explaination**: The configuration provided in the question is a prepared query. The name of the prepared query is retail-app, therefore the DNS record used to query this prepared query is retail-app.query.consul since all prepared queries use the query namespace.

**19. True or False? Using the / character, Consul organizes the data in a directory structure, similar to a file system.**


* [ ] TRUE
* [ ] FALSE

**Correct answer:**
* [x] FALSE

**Explaination**: The character / will be treated like any other character and is not fixed to the file system. Meaning, including / in a key does not fix it to a directory structure. This model is similar to Amazon S3 buckets. However, / is still useful for organizing data and when recursively searching within the data store.

**20. True or False? You want to restore a Consul from a snapshot. On a five-node Consul cluster, the consul snapshot restore filename.snap command must be run on each individual node before starting the Consul service.**


* [ ] TRUE
* [ ] FALSE

**Correct answer:**
* [x] FALSE

**Explaination**: Running the restore process should be straightforward. However, there are a couple of actions you can take to ensure the process goes smoothly. First, make sure the datacenter you are restoring is stable and has a leader. You can see this using consul operator raft list-peers and checking server logs and telemetry for signs of leader elections or network issues. You will only need to run the process once, on the leader. The Raft consensus protocol ensures that all servers restore the same state.


**21. After Consul ACLs have been enabled in the configuration file, what is the next step to begin using ACLs in your environment?**


* [ ] create new ACL tokens
* [ ] create new ACL policies
* [ ] bootstrap the ACL system
* [ ] disable the anonymous token

**Correct answer:**
* [x] bootstrap the ACL system

**Explaination**: Once ACLs have been enabled, you must first bootstrap the ACL system in order to begin using it. To do this, you would run the command consul acl bootstrap and Consul will return the bootstrap token

**22. You need to determine the leader node for the Consul cluster. What command allows you to quickly identify the nodes and their current roles within the cluster?**


* [ ] consul members
* [ ] consul nodes-leader
* [ ] consul raft -list
* [ ] consul operator raft list-peers

**Correct answer:**
* [x] consul operator raft list-peers

**Code**: 
Example of the command's output:

```
Node           ID                                    Address           State     Voter  RaftProtocol
CONSUL-NODE-A  121abb4c-16fb-c8ec-2e2b-9595925de4dc  10.0.10.238:8300  follower  true   3
CONSUL-NODE-C  4bead426-4471-0924-598f-cd6ce0015ebc  10.0.10.48:8300   follower  true   3
CONSUL-NODE-E  c44e8ab1-1132-1b22-9501-479c690c9e1b  10.0.10.105:8300  leader    true   3
CONSUL-NODE-D  ba86541f-cd93-6ada-b763-709b0fc6c09f  10.0.11.163:8300  follower  true   3
CONSUL-NODE-B  2528cba1-06ea-4837-fc7b-13e44af19b0d  10.0.11.141:8300  follower  true   3
```


**Explaination**: The Raft operator command is used to interact with Consul's Raft subsystem. Specifically, the list-peers command will display the current Raft peer configuration, which shows the state of each node, being either a leader or a follower.

**23. What is one of the benefits of deploying non-voting servers in a Consul Enterprise cluster environment?**


* [ ] they can write data to the cluster to reduce write latency
* [ ] they can expand the number of nodes to take part in quorum election operations
* [ ] they do not receive data from cluster replication, therefore reducing latency
* [ ] they provide enhanced read scalability

**Correct answer:**
* [x] they provide enhanced read scalability

**Explaination**: Consul Enterprise provides the ability to scale clustered Consul servers to include voting and non-voting servers. Non-voting servers still receive data from the cluster replication, however, they do not take part in quorum election operations. Expanding your Consul cluster in this way can scale reads without impacting write latency.

**24. Based on the configuration file provided below, how would a new Consul agent discover other Consul datacenter members in order to join the Consul datacenter?**


* [ ] using auto-discovery information through DNS
* [ ] discover other members using DC1 name
* [ ] using cloud auto-join
* [ ] using the information provided by a DHCP

**Correct answer:**
* [x] using cloud auto-join

**Code**: 
{
  "server": false,
  "node_name": "web-app-01",
  "datacenter": "DC1",
  "data_dir": "/opt/consul/data",
  "bind_addr": "10.0.42.84",
  "client_addr": "10.0.42.84",
  "retry_join": ["provider=aws tag_key=consul tag_value=true"],
  "log_level": "INFO",
  "enable_syslog": true,
}


**Explaination**: Consul can join a datacenter by using the cloud auto-join feature, which does automatic cluster joining using cloud metadata. In the configuration file above, the retry_join parameter states that the Consul agent should query AWS and discover instances that have a tag of consul with a value of true

**25. Based on the payload below, what Consul feature is being created with the API?**


* [ ] prepared query
* [ ] service
* [ ] Consul agent
* [ ] service mesh intention

**Correct answer:**
* [x] prepared query

**Code**: 
{
  "Name": "db-service",
  "Service": {
    "Service": "redis",
    "Failover": {
      "NearestN": 3,
      "Datacenters": ["dc1", "dc2"]
    },
    "Near": "node1",
    "OnlyPassing": false,
    "Tags": ["primary", "!experimental"],
    "NodeMeta": { "instance_type": "m3.large" },
    "ServiceMeta": { "environment": "production" }
  },
  "DNS": {
    "TTL": "10s"
  }
}


**Explaination**: The payload in the question is the configuration of a prepared query. You can immediately tell that it's a prepared query since it includes a failover policy as well. Failover policies are only configured in a prepared query.

---


## azure-case-studies

**1. You are planning to create a web application which is internet facing. The application should be deployed on Linux Ubuntu VM running Apache webserver. For load balancing the requests, you have created a standard application gateway and added these VMs as backend servers to the application gateway. Your management would like to ensure that the VMs are highly available by deploying VMs in different physical data centers within the same region. Should you deploy VMs across availability sets?**


* [ ] Yes
* [ ] No

**Correct answer:**
* [x] No

**2. You are planning to create a web application which is internet facing. The application should be deployed on Linux Ubuntu VM running Apache webserver. For load balancing the requests, you have created a standard application gateway and added these VMs as backend servers to the application gateway. Your management would like to ensure that the VMs are highly available by deploying VMs in different physical data centers within the same region. Should you deploy VMs across availability zones?**


* [ ] Yes
* [ ] No

**Correct answer:**
* [x] Yes

**3. You are planning to create a web application which is internet facing. The application should be deployed on Linux Ubuntu VM running Apache webserver. For load balancing the requests, you have created a standard application gateway and added these VMs as backend servers to the application gateway. Your management would like to ensure that the VMs are highly available by deploying VMs in different physical data centers within the same region. Is the following true or false: No need for deployment across datacenters, Application gateway will handle high availability.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**4. You are planning to create a web application which is internet facing. The application should be deployed on Linux Ubuntu VM running Apache webserver. For load balancing the requests, you have created a standard application gateway and added these VMs as backend servers to the application gateway. Your management would like to ensure that the VMs are highly available by deploying VMs in different physical data centers within the same region. Should you upgrade to StandardV2 Application Gateway?**


* [ ] Yes
* [ ] No

**Correct answer:**
* [x] No

**5. Your web application is currently running on Azure Virtual Machines and you would like to migrate to Azure App Service. Before you move to production, you would like to test the application. Ideally, the testing requires 24 hours. Which tier would you select for testing your application?**


* [ ] Use the Free Tier as it is free of cost
* [ ] Use Shared tier
* [ ] Use Basic Tier
* [ ] Use Standard Tier

**Correct answer:**
* [x] Use Basic Tier

---


## vault associate auth methods

**1. True or False? Vault auth methods are responsible for validating a user's identity and associating policies for authorization?**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**Explaination**: Vault auth methods are used for validating identity and associating policies that are attached to a token.

**2. When using any auth method beyond the token, what is the result of using an auth method?**


* [ ] obtaining a Vault token
* [ ] validating your credentials
* [ ] to get access to a Vault policy
* [ ] to generate new dynamic credentials

**Correct answer:**
* [x] obtaining a Vault token

**Explaination**: The correct option is the "Obtaining a Vault token" because all subsequent requests to Vault will use the token for authentication. 

**3. Which of the following are valid auth methods that can be enabled in Vault?**


* [ ] OIDC
* [ ] Azure AD
* [ ] SAML 2.0
* [ ] MySQL

**Correct answer:**
* [x] OIDC

**Explaination**: OIDC is the correct auth method. Apart from that, Azure AD is not a valid auth method, although you can use OIDC to configure Vault to use Azure AD for authentication. You might feel this is tricky, but it's important to know what are actual auth methods and what options can be enabled by using the official auth methods, like Azure AD. 

**Documentation Link**: https://www.vaultproject.io/docs/auth/jwt#oidc-authentication

**4. You are using Terraform in your environment to deploy infrastructure to your public cloud platform. Terraform is being executed on a server running in AWS. The Security team has mandated that any credentials for Terraform must be short-lived and rotated often. What auth method should you use to authenticate to Vault and satisfy these requirements?**


* [ ] AWS auth method
* [ ] UserPass
* [ ] AppRole
* [ ] LDAP

**Correct answer:**
* [x] AWS auth method

**Explaination**: The credentials are generated only when Terraform needs them and are automatically revoked after the lease. On other hand. 
AppRole probably shouldn't be used here since the AWS auth method provides a better alternative, especially since Terraform is being executed from a server running in AWS. AppRole could work, but the AWS auth method is a better choice here.
LDAP wouldn't really provide any benefit here, since Terraform needs AWS credentials.
UserPass credentials don't have any requirements behind how long they live, so you could have a userpass user that lives forever without having to rotate the password.


**5. Which of the following auth methods are generally associated with machine-to-machine authentication?**


* [ ] AppRole
* [ ] OIDC
* [ ] LDAP
* [ ] UserPass

**Correct answer:**
* [x] AppRole

**Explaination**: The correct option is the "AppRole".  It is frequently the primary auth method used for machine-to-machine authentication. On the other hand.                                                                               
LDAP - is generally associated with a human-based authentication method. It generally requires somebody to type in a credential.                                                                                                
UserPass - is generally associated with a human-based authentication method. It generally requires somebody to type in a credential.                                                                                               
OIDC - is generally associated with a human-based authentication method, such as Azure AD or Ping Federate. It generally requires somebody to type in a credential.

**Documentation Link**: https://www.vaultproject.io/docs/auth/approle

---


## vault associate policies

**1. You have a new team member on the Vault operations team. Their first task is to rotate the encryption key in Vault as part of the organization's security policy. However, when they log in, they get an access denied error when attempting to rotate the key. The policy being used is below. Why can't the user rotate the encryption key?**


* [ ] The policy requires `sudo` privileges since it is a root-protected path
* [ ] The policy doesn't include `create` privileges so a new encryption key can't be created
* [ ] The policy should include `sys/rotate/<name of key>` as part of the path
* [ ] The encryption key has a minimum TTL. Therefore the key cannot be rotated until that time expires

**Correct answer:**
* [x] The policy requires `sudo` privileges since it is a root-protected path

**Code**: 
path "auth/*" {
 capabilities = ["create", "read", "update", "delete", "list"]
}
# Rotate encryption key
path "sys/rotate" {
 capabilities = ["read", "update"]
}

**Explaination**: Rotating the encryption requires sudo or root access to the path sys/rotate.

**2. Every Vault deployment will have two default policies that are created automatically. What are these two policies?**


* [ ] The `anonymous` policy and the `default` policy
* [ ] The `admin` policy and the `user` policy
* [ ] The `primary` policy and the `secondary` policy
* [ ] The `root` policy and the `default` policy

**Correct answer:**
* [x] The `root` policy and the `default` policy

**Explaination**: The default policies are root and default.

**3. Vault policies contain multiple parts, including the path and capabilities. What are the valid capabilities that can be used in a policy?**


* [ ] read,write,delete,list,sudo,update
* [ ] read,create,delete,list,sudo,update,deny
* [ ] get,write,remove,list,sudo,update,deny
* [ ] delete,list,update,root,default,deny create,sudo

**Correct answer:**
* [x] read,create,delete,list,sudo,update,deny

**Explaination**: Neither get, write, root, or remove is a valid capability. The correct answer is read, create, delete, list, sudo, update, and deny. 

**4. You've been provided a Vault token that is attached to the following policy. Select the action below that will be permitted.**


* [ ] List secrets stored at `kv/data/apps/jenkins`
* [ ] Modify an existing Vault policy
* [ ] Store a new secret stored at `kv/data/apps/jenkins`
* [ ] List the roles for the AWS secrets engine mounted at `aws/`

**Correct answer:**
* [x] Modify an existing Vault policy

**Code**: 
path "kv/data/apps/jenkins" {
capabilities = ["read","update","delete"]
}
path "sys/policies/*" {
capabilities = ["create","update","list","delete"]
}
path "aws/creds/web-app" {
capabilities = ["read"]
}

**Explaination**: The correct answer is "Modify an existing Vault policy" because the capability update has been permitted for all policies (sys/policies/*). 
For "List secrets stored at kv/data/apps/jenkins", you could read these secrets, but you won't be able to list the secrets that are stored at this path because that capability doesn't exist on the path.
For "Store a new secret stored at kv/data/apps/jenkins", This is wrong because the policy doesn't grant the "create" capability at this path.
For "List the roles for the AWS secrets engine mounted at aws/", you have permission to generate credentials against a specific role, but not list the roles at aws/roles.


**5. Given the following policy, select the action that would <b>NOT</b> be permitted.**


* [ ] Read a secret at the path `kv/data/teams/cloud/database/db2`
* [ ] Store a new secret at `secret/apps/database/prod-db`
* [ ] Read a secret at `kv/apps/webapp/ecommerce/production`
* [ ] List the secrets stored at `kv/data/teams/developers/database/db-001`

**Correct answer:**
* [x] Read a secret at the path `kv/data/teams/cloud/database/db2`

**Code**: 
path "kv/apps/webapp/*" {
capabilities = ["read"]
}
path "secret/apps/database/prod-db" {
capabilities = ["read", "create", "update", "delete", "list"]
}
path "kv/data/teams/+/database/db-*" {
capabilities = ["read", "list"]
}

**Explaination**: The correct answer is "Read a secret at the path kv/data/teams/cloud/database/db2" because this isn't permitted because the wildcard at the end requires that the ending segment be db-. The path selected does not include the dash ("-") character.

For "Store a new secret at secret/apps/database/prod-db", this will be permitted by the second policy since it has the create capability, therefore it is the incorrect answer.

For "Read a secret at kv/apps/webapp/ecommerce/production", this will be permitted by the first policy because there is a wildcard after kv/apps/webapp.

For "List the secrets stored at kv/data/teams/developers/database/db-001", this will be permitted by the last policy because it follows the correct pattern.


---


## vault associate tokens

**1. What is the difference between the `TTL` and the `Max TTL`?**


* [ ] The TTL defines when the token will expire and be revoked. 
* [ ] They are essentially the same.
* [ ] The TTL defines when another token will be generated.
* [ ] The max TTL defines the timeframe for which a token cannot be used.

**Correct answer:**
* [x] The TTL defines when the token will expire and be revoked. 

**Explaination**: TTL defines when the token will expire and Max TTL defines the maximum timeframe for which the token can be renewed.

**2. What of the following feature is true about batch tokens in Vault?**


* [ ] Batch tokens are not persisted (written) to storage
* [ ] Batch tokens can create child tokens
* [ ] Batch tokens are written to the storage

**Correct answer:**
* [x] Batch tokens are not persisted (written) to storage

**Explaination**: Batch tokens are NOT written to storage.

**3. True or False? To prepare for day-to-day operations, the root token should be safely saved outside of Vault in order to administer Vault.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**Explaination**: For day-to-day operations, the root token should be deleted after configuring other auth methods which will be used by admins and Vault clients.

**4. Sara uses the Vault command-line interface (CLI) to perform various administrative tasks on the production Vault cluster. However, Sara is receiving permission denied errors when attempting to make changes. She needs to figure out what policies are attached to her token so she can view the policy and determine what permissions need to be added. </br>What CLI command can Sara run on the Vault node to determine what policies are attached to the current token?**


* [ ] `vault token lookup`
* [ ] `vault operator diagnose`
* [ ] `vault policy list`
* [ ] `vault token capabilities` 

**Correct answer:**
* [x] `vault token lookup`

**5. Which of the following best describes a token accessor?**


* [ ] A value that acts as a reference to a token that can be used to perform limited actions against the token.
* [ ] A token is used for Consul to access Vault auth methods.
* [ ] Describes the value associated with the tokens TTL.
* [ ] A value that describes which clients have access to the attached token.  

**Correct answer:**
* [x] A value that acts as a reference to a token that can be used to perform limited actions against the token.

---


## Quiz 1 - Open Source

**1. Identify some open source software from these**


* [ ] MacOS
* [ ] Windows
* [ ] Linux
* [ ] Docker

**Correct answer:**
* [x] Linux
* [x] Docker

**Explaination**: Linux and Docker are open source while MacOS and Windows are closed source software

**2. Early Stellwart of open source, Richard Stallman named his project GNU which is a recursive acronym. Using the internet find out which other works if Richard Stallman popular for.**


* [ ] Free Software Foundation
* [ ] GNU GPL License
* [ ] Linux
* [ ] Mozilla

**Correct answer:**
* [x] Free Software Foundation
* [x] GNU GPL License

**Explaination**: Richard stallman created GNU GPL License giving the concept of Copy Left and also started the Free Software Foundation.

**3. For a software to be called, open source software it must ______**


* [ ] Have code in clean architecture 
* [ ] Have an associated open source license
* [ ] be free of cost
* [ ] be debuggable

**Correct answer:**
* [x] Have an associated open source license

**Explaination**: To be called open-source software, there must be a finite set of laid down principles that must be respected. There are called “Open Source Licences”. An open-source licence should regulate each open-source software. A licence defines the conditions of using, modifying and redistributing the source code associated with the open-source software.

**4. In context of open source, "Free" refers to ______**


* [ ] Freedom
* [ ] Free of cost
* [ ] Free of bugs
* [ ]  Free of governance

**Correct answer:**
* [x] Freedom

**Explaination**: "Free" does not necessarily mean without cost/payment,  but it tries to define the aspects of freedom to use, modify and redistribute the code. 

**5. Facebook’s open source javascript based UI library “React” is an open source project. Use the internet to find out this repository and find which open source license is used for this project?**


* [ ] GNU GPL
* [ ] MIT License
* [ ] FreeBSD
* [ ] Apache license 2.0

**Correct answer:**
* [x] MIT License

**Explaination**: React is licensed under MIT license.
https://github.com/facebook/react/blob/main/LICENSE

---


## Quiz 2 - Open Source

**1. One of the pioneer organisations in Open source "Open Source Initiative" was founded in 1998. OSI is the steward if Open Source Definition (OSD).  Using the official docs (https://opensource.org/docs/osd) choose the original source from which OSD was derived**


* [ ] Debian Free Software Guidelines (DFSG)
* [ ] World Wide Web Consortium (W3C)
* [ ] OpenGL
* [ ] The Cathedral and the Bazaar

**Correct answer:**
* [x] Debian Free Software Guidelines (DFSG)

**Explaination**: he Open Source Definition was originally derived from the Debian Free Software Guidelines (DFSG). Debian, the producers of the Debian system, have created the Debian Social Contract. The Debian Free Software Guidelines (DFSG) part of the contract, initially designed as a set of commitments that we agree to abide by, has been adopted by the free software community as the basis of the Open Source Definition.

**2. Apart from MIT, Which other organisation among these is associated with MIT License**


* [ ] IBM
* [ ] Google 
* [ ] Facebook
* [ ] Linux Foundation

**Correct answer:**
* [x] IBM

**Explaination**: The Open Source Definition was originally derived from the Debian Free Software Guidelines (DFSG). Debian, the producers of the Debian system, have created the Debian Social Contract. The Debian Free Software Guidelines (DFSG) part of the contract, initially designed as a set of commitments that we agree to abide by, has been adopted by the free software community as the basis of the Open Source Definition.

**3. Any work which is freely available over the internet can be considered open source?**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**Explaination**: The open source licence grants other users permission to access, modify and distribute the work while also defining the boundaries, conditions, and other nuances with it.

**4. Open source software systems offer reliability because of anyone can anytime modify the source code?**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**Explaination**: For any system, reliability is the ability to consistently perform according to its specifications. Open source systems offer reliability because vulnerabilities are fixed and patches and new versions are released a lot faster than a typical closed source system.

**5. Unlike copyleft software licenses, the MIT License also permits reuse within proprietary software**


* [ ] True
* [ ] Flase

**Correct answer:**
* [x] True

**Explaination**: Unlike copyleft software licenses, the MIT License also permits reuse within proprietary software
, provided that all copies of the software or its substantial portions include a copy of the terms of the MIT License and also a copyright notice

**6. Continuous Evolution in open source systems is enabled by**


* [ ] Community Support
* [ ] Open Source License
* [ ] Source Code
* [ ] Open Data

**Correct answer:**
* [x] Community Support

**Explaination**: The practice of open source involves a community of contributors which helps in the evolution of the idea, project and the scope of the project.

**7. Using Internet, find which of the given are permissions granted by MIT license. You can use the following resource - https://choosealicense.com/licenses/**


* [ ] Commercial use
* [ ] modification
* [ ] Liability
* [ ] Warranty

**Correct answer:**
* [x] Commercial use
* [x] modification

**Explaination**: MIT license is a short and simple permissive license allowing Modifications and Commercial use

---


## Quiz 3 - Getting Started with Open Source

**1. Participating in open source might also include contributing to ______**


* [ ] Documentation
* [ ] Design
* [ ] Advocacy
* [ ] Code

**Correct answer:**
* [x] Documentation
* [x] Design
* [x] Advocacy
* [x] Code

**Explaination**: Participating in open source has many forms. It is not just limited to contributing to the code base. It also includes design, documentation, content, communities, and advocacy, among several other ways. 

**2. Express is a minimal and flexible Node.js web application framework that provides a robust set of features for web and mobile applications. Using the official documentation, select the organisation which maintains Express https://expressjs.com/**


* [ ] OpenJS Foundation
* [ ] Open Source Initiative
* [ ] Mozilla Foundation
* [ ] Apple Inc,

**Correct answer:**
* [x] OpenJS Foundation

**Explaination**: Express is a project of the OpenJS Foundation.

**3. Contributing to open source means to contribute to the source code of an open source software**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**Explaination**: open-source in its fundamental principle is accepting in nature. In practice, there are more than one ways to be involved in open-source.

**4. A branch is when a new line of development is created that diverges from the main line of development.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**Explaination**: A branch is when a new line of development is created that diverges from the main line of development. By default, the straight line of development is called “Master”.

**5. In terminology of the version control system Git, a commit refers to**


* [ ] Data Snapshots
* [ ] User's Commitment to code
* [ ] Files in Repository
* [ ] Staging Index

**Correct answer:**
* [x] Data Snapshots

**Explaination**: Commit Refers to data snapshots. Upon every commit, a snapshot of the look of the file is stored. In general terms, changes made to files inside a repository are made using commits.

**6. Forking a repository means**


* [ ] creating a copy of the repository
* [ ] sending files to github
* [ ] initialising a new git repository
* [ ] downloading the repository to local system

**Correct answer:**
* [x] creating a copy of the repository

**Explaination**: Forking a repository means creating a copy of the repository. Once a repository has been forked, the repository can be viewed under one’s own account.

**7. On using git add, the changes/files are added to the**


* [ ] Git Repository
* [ ] Staging Index
* [ ] VCS Logs
* [ ] Working Directory

**Correct answer:**
* [x] Staging Index

**Explaination**: The first step in using git to keep track of changes to these files is to add the change (the file which was changed) to a different area called the “Staging Index”. Before you call these changes permanent (using a commit), this area acts like a bridge between the working directory and the actual git repository. On being sure of the changes made, you would explicitly commit the file moving it to the repository from the staging index.

---


## Quiz 4 - Open Source Projects

**1. Which is one of the popular system for Internet hosting service and Version control among the following**


* [ ] Github
* [ ] Stackoverflow
* [ ] Quora
* [ ] Ansible

**Correct answer:**
* [x] Github

**Explaination**: Open source software projects are typically hosted on the internet using platforms that allow version control. Github, Gitlab, BigBucket and are some popular examples where you can find millions of projects

**2. In context of an open source project, the person/s or organization that created the project is typicall the _____**


* [ ] Author 
* [ ] Coder
* [ ] Contributor
* [ ] Member

**Correct answer:**
* [x] Author 

**3. Since there is no boundary whatsoever, open communities promote _____ in technology and innovation.**


* [ ] Diversity
* [ ] Business
* [ ] Costs
* [ ] Regulations

**Correct answer:**
* [x] Diversity

**4. The documentation file which typically acts like an instruction manual that welcomes new community members to the project is famously known as -**


* [ ] CONTRIBUTING.MD
* [ ] README.MD
* [ ] INTRODUCTION.MD
* [ ] WELCOME.MD

**Correct answer:**
* [x] README.MD

**Explaination**: The README is the instruction manual that welcomes new community members to the project. It explains why the project is useful and how to get started.

**5. The ground rules for participants’ behavior are regulated by the**


* [ ] Morals
* [ ] Code of conduct
* [ ] Legal policies
* [ ] NDA

**Correct answer:**
* [x] Code of conduct

**Explaination**: The code of conduct sets ground rules for participants’ behaviour associated and helps to facilitate a friendly, welcoming environment. While not every project has a CODE_OF_CONDUCT file, its presence signals that this is a welcoming project to contribute to.

**6. Project governance in an open source project typically includes**


* [ ] Operations
* [ ] Development Methodology
* [ ] Legality
* [ ] Finances

**Correct answer:**
* [x] Operations
* [x] Development Methodology

---


## Quiz 5 - Starting an Open Source Project

**1. Ways to manage interaction and communication in an open community are ?**


* [ ] IRCs
* [ ] GitHub Discussions
* [ ] Community Forum
* [ ] VS code

**Correct answer:**
* [x] IRCs
* [x] GitHub Discussions
* [x] Community Forum

**2. Open Source Projects could belong to**


* [ ] Individuals
* [ ] Organisations
* [ ] Groups 
* [ ] Students

**Correct answer:**
* [x] Individuals
* [x] Organisations
* [x] Groups 
* [x] Students

**3. People around an open source project make up the**


* [ ] Community
* [ ] Project Management Board
* [ ] Advocates
* [ ] Owners

**Correct answer:**
* [x] Community

**4. The purpose of README is to describe the project, serve as an early interaction point for all new users, and provide sufficient knowledge about the usability and working of the project**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**Explaination**: Its purpose is to describe the project, serve as an early interaction point for all new users, and provide sufficient knowledge about the usability and working of the project. For software projects, you might see READMEs containing instructions on how to install and run the project locally.

**5. Which of the following is a critical aspect of documentation in open source projects**


* [ ] Describing personal details about author
* [ ] Describing the ways to contribute
* [ ] Describing conversations, threads and discussions
* [ ] Describing the Project

**Correct answer:**
* [x] Describing the ways to contribute
* [x] Describing conversations, threads and discussions
* [x] Describing the Project

---


## Getting Started With OpenShift

**1. If you want an OpenShift environment for dev purposes that you control, which option would you choose?**


* [ ] Dev Sandbox
* [ ] OpenShift ReadyContainers (OpenShift Local)
* [ ] Minikube

**Correct answer:**
* [x] OpenShift ReadyContainers (OpenShift Local)

**Explaination**: OpenShift Local, previously known as ReadyContainers, is a method that you can use
to install OpenShift locally. It looks and feels the same way as any other OpenShift
cluster, except it’s free and running on your local computer.

**2. You can install OpenShift in production on both Azure and AWS**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**Explaination**: You can install OpenShift on any cloud, whether it has a specific OpenShift service or not.

**3. If you use the OpenShift Trial, you will still be charged by the cloud provider you run OpenShift on**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**Explaination**: Even though you won’t be charged for OpenShift itself, you’ll be charged for where
OpenShift runs. For example, if you run OpenShift on AWS, you’ll be charged for the
EC2 instances

**4. Is the Dev Sandbox the same as OpenShift Local?**


* [ ] yes
* [ ] no

**Correct answer:**
* [x] no

**Explaination**: OpenShift Local is installed locally on a desktop or laptop. Sandbox is run in the cloud.

**5. Under the Developer section of OpenShift, there’s an observability tab where you can see**


* [ ] Metrics
* [ ] Prometheus dashboards
* [ ] Cluster information
* [ ] Events

**Correct answer:**
* [x] Metrics
* [x] Events

**Explaination**: The Observability tab allows you to look at metrics, events, alerts, and the dashboard.

---


## Openshift Concepts – Projects and Users

**1. On the Pod section of the OpenShift UI, there’s a tab called ___ so you can exec into the Pod**


* [ ] Exec  
* [ ] Terminal  
* [ ] Events
* [ ] Logs

**Correct answer:**
* [x] Terminal  

**Explaination**: The Terminal tab allows you to exec into a Pod, very much like when you’re on the terminal and run kubectl exec -ti

**2. Projects in OpenShift are the same, or very similar, to _____ in Kubernetes**


* [ ] RBAC 
* [ ] Worker nodes 
* [ ] Namespaces 
* [ ] User creation

**Correct answer:**
* [x] Namespaces 

**Explaination**: Projects in OpenShift are an isolation of Pods, Deployments, and other Kubernetes
resources. They are almost identical to Namespaces in Kubernetes.

**3. __ is the same as kubectl apply -f**


* [ ] test apply -y
* [ ] deploy apply -y
* [ ] oc apply -f
* [ ] openshift apply -f

**Correct answer:**
* [x] oc apply -f

**Explaination**: The oc CLI allows you to control and retrieve Kubernetes resources much like the
kubectl CLI.

**4. When you deploy a Pod to OpenShift and you don’t specify a Project, it’ll be deployed to the ___ project**


* [ ] A new one gets automatically created
* [ ] The Default project
* [ ] The kube-system project
* [ ] The Pod doesn’t get deployed to a Project

**Correct answer:**
* [x] The Default project

**Explaination**: Pods, by default, get deployed to the default Project in OpenShift if a Namespace isn’t specified.

**5. You can deploy standard Kubernetes Manifests to OpenShift**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**Explaination**: Kubernetes Manifests work the same way in OpenShift from a deployment and
management perspective.

---


## Concepts – Builds and Deployments

**1. A Build allows you to**


* [ ] Automatically deployment workloads
* [ ] Takes code and turns it into a container image
* [ ] Takes a container image and turns it into code
* [ ] Automatically deploys Pods

**Correct answer:**
* [x] Takes code and turns it into a container image

**Explaination**: Builds allow you to take code that’s in source control and turn it into a container image to be deployed to Kubernetes

**2. What’s the biggest difference between Deployments and DeploymentConfigs?**


* [ ] Pods don’t work on both
* [ ] DeploymentConfigs can’t have Services attached
* [ ] ReplicaSets and ReplicaControllers
* [ ] There are no replicas in DeploymentConfigs

**Correct answer:**
* [x] ReplicaSets and ReplicaControllers

**Explaination**: DeploymentConfigs have ReplicaControllers and Deployments have ReplicaSets

**3. S2i allows you to create container images without Dockerfiles**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**Explaination**: S2i allows you to take code and create a docker-compliant container image.

**4. Image streams allow you to**


* [ ] Update deployments on the fly with tags
* [ ] Update Pods with tags
* [ ] Delete deployments automatically
* [ ] Automatically deploy DeploymentConfigs

**Correct answer:**
* [x] Update deployments on the fly with tags

**Explaination**: Image streams constantly update and create container images for automatic updates of applications and auto-deployments (if configured)

**5. What would you use in OpenShift if you want to have a public-facing app that users can reach via the web browser?**


* [ ] Pods with public IPs 
* [ ] Deployments with public IPs 
* [ ] Routes 
* [ ] DaemonSets

**Correct answer:**
* [x] Routes 

**Explaination**: Routes in OpenShift are very similar to having a Kubernetes Service with a type:
LoadBalancer associated with it. Routes look at backend Pods and expose apps via a URL.

---


## Networks, Services, Routes and Scaling

**1. What command would you use to scale Deployments?**


* [ ] oc scale 
* [ ] oc deploy 
* [ ] oc create -f 
* [ ] oc autoscale

**Correct answer:**
* [x] oc autoscale

**Explaination**: The oc autoscale command allows you to set minimum Pods and maximum Pods using the --min and --max flags.

**2. Horizontal autoscaling increases CPU/memory on a Pod or a server**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**Explaination**: Horizontal autoscaling increases the amount of Pods or servers that you have. For example, if you have 2 replicas and you want 4 replicas, horizontal autoscaling would take care of that.

**3. Horizontal autoscaling allows you to increase or decrease replicas based on**


* [ ] Increased load for an app
* [ ] Decreased load for an app
* [ ] New ingress routes
* [ ] New projects created

**Correct answer:**
* [x] Increased load for an app
* [x] Decreased load for an app

**Explaination**: Autoscaling is all about increased load or decreased load. If there are more users reaching an application, replicas will increase. If there are less users reaching an application, replicas will decrease.

**4. Vertical autoscaling is increasing CPU/memory on a Pod or a server**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**Explaination**: Vertical autoscaling increases hardware resources on a Pod or server

**5. You can set the minimum Pod count by using the following flag:**


* [ ] —minus  
* [ ] —minusPods  
* [ ] —minuz  
* [ ] —min

**Correct answer:**
* [x] —min

**Explaination**: The --min flags tells Kubernetes that for a specific Deployment, there should be a
minimum Pod of X Pods running.

---


## Storage, Templates and Catalog

**1. Limits do the following**


* [ ] Allow for resources to be requested
* [ ] Puts hard limits on CPU (if specified)
* [ ] Puts hard limits on memory (if specified)
* [ ] Increases autoscaling

**Correct answer:**
* [x] Puts hard limits on CPU (if specified)
* [x] Puts hard limits on memory (if specified)

**Explaination**: Limits allow you to specify what resources a Pod can get and what the limit is. For
example, if a Pod is limited to 10Mi, even if the Pod needs 15Mi to keep running the app
inside of the Pod, it won’t get it

**2. Dynamic storage allows you to:**


* [ ] Create your own storage provider
* [ ] Use a StorageClass so you don’t have to create PersisitentVolumes
* [ ] Use PersistentVolumes so you don’t have to create StorageClasses
* [ ] Use local storage as a PersistentVolume

**Correct answer:**
* [x] Use a StorageClass so you don’t have to create PersisitentVolumes

**Explaination**: Dynamic storage allows engineers to not have to worry about creating
PersistentVolumes and having to manage said volumes across the cluster. Instead, a PersistentVolumeClaim can point to a StorageClass to utilize X amount of storage on demand.

**3. A StorageClass automatically creates a PersistentVolume**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**Explaination**: A StorageClass gives you the necessary tools to utilize storage, but it does not
automatically create a PersistentVolume

**4. Requests do the following:**


* [ ] Ask for CPU/memory
* [ ] Can keep asking
* [ ] Vertically scales Pods 
* [ ]  Specify what type of scaling goes into a Pod

**Correct answer:**
* [x] Ask for CPU/memory
* [x] Can keep asking

**Explaination**: Requests give Pods the ability to request CPU and memory to allocate resources for apps running inside of the Pods. They can also keep asking for resources
(CPU/memory) if the resources are available.

**5. You can specify limits, requests, and overall quotas per Namespace**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**Explaination**: You can set specific limits and requests per Namespace. That way, you don’t have to
manually set them per Pod.

---


## Openshift Security

**1. You can create a user with the following command**


* [ ] oc create user username 
* [ ] oc deploy user username
* [ ] oc create person username
* [ ] kubectl create user username

**Correct answer:**
* [x] oc create user username 

**Explaination**: The oc create user command is used to create a new user in OpenShift

**2. What’s the difference between ClusterRoles and Roles?**


* [ ] ClusterRoles are worker-node specific 
* [ ] Roles are tied to a worker node
* [ ] ClusterRoles are namespace-scoped
* [ ] Roles are namespace-scoped

**Correct answer:**
* [x] Roles are namespace-scoped

**Explaination**: ClusterRoles are cluster-wide and Roles are namespace-scoped.

**3. Security Context Constraints (SCC) allow you to control permissions for Pods**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**Explaination**: Along with the ability to use SELinux, SCC can also control permissions for Pods

**4. Kubernetes secrets allow you to**


* [ ] Store sensitive information 
* [ ] Create users
* [ ] Delete users 
* [ ] Create configmaps

**Correct answer:**
* [x] Store sensitive information 

**Explaination**: A Kubernetes Secret could be anything from an API key to a password to a username.
Whatever you define as sensitive information

---


## PCAP-ERRORS AND EXCEPTIONS

**1. When does Python throw a ValueError?**


* [ ] for errors related to variables in our code
* [ ] for wrong values passed in operations and functions
* [ ] for wrong data types passed in operations and functions

**Correct answer:**
* [x] for wrong values passed in operations and functions

**2. We can build specific exceptions for certain errors.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**3. What is the output of the following python code?**


* [ ] 5Sally
* [ ] 9
* [ ] Error
* [ ] 5+Sally

**Correct answer:**
* [x] 5Sally

**Code**: 
x = 5
y = "Sally"
print(str(x) + y)

**4. What may happen if our Python program does not handle errors?**


* [ ] The console returns errors   
* [ ] Both of the above
* [ ] None of the above   
* [ ] Our program stops running and errors appear in the console

**Correct answer:**
* [x] Both of the above

**5. What is the order of the try/except code blocks?**


* [ ] try/except codeblocks are executed one after the other
* [ ] the try codeblock only gets executed if the except codeblock runs without errors
* [ ] the except codeblock only gets executed if the try codeblock fails

**Correct answer:**
* [x] the except codeblock only gets executed if the try codeblock fails

**6. How many exceptions branches may get executed during errors?**


* [ ] As many as our exception branches  
* [ ]  Depends on how many different types of errors we have   
* [ ] At most one

**Correct answer:**
* [x] At most one

**7. When does Python throw a TypeError?**


* [ ] for errors related to the extension of our python files
* [ ] for wrong data types passed in operations and functions
* [ ] for wrong string and numeric data types

**Correct answer:**
* [x] for wrong data types passed in operations and functions

**8. What mechanisms does Python provide to deal with errors?**


* [ ]  Built-in error functions
* [ ] Built-in error methods
* [ ] Exception raising
* [ ] The special keywords 'try', 'exception'
* [ ]  The special variables try, 'exception'

**Correct answer:**
* [x] Exception raising
* [x] The special keywords 'try', 'exception'

**9. What is the error thrown by the following code snippet?**


* [ ] ValueError 
* [ ]   IndexError   
* [ ] TypeError   
* [ ] ZeroDivisionError

**Correct answer:**
* [x] ZeroDivisionError

**Code**: 
  >>> 3 // 0 == 3 % 3

**10. What is the error thrown by the following code snippet?**


* [ ] No error 
* [ ]   ValueError   
* [ ] TypeError
* [ ] Index error

**Correct answer:**
* [x] Index error

**Code**: 
>>> list1 = [1, 2, 3, 4]
>>> list1[4]

**11. What is the output of the following code block?**


* [ ] zero error  
* [ ]  zero error  other error  
* [ ]  index error   
* [ ] other error

**Correct answer:**
* [x] zero error  

**Code**: 
try:
      print("my_string"[1/0])
except IndexError:
      print("index error")
except ZeroDivisionError:
      print("zero error")
except:
      print("other error")

**12. Which of the following code snippet would raise an unhandled exception?**


* [ ] C
* [ ] A
* [ ] B

**Correct answer:**
* [x] C

**Code**: 
 A.  
try:
      x = y + 1
except NameError:
      print("y is not defined")

B. 
try:
      x = 'seasalt'[7]
except IndexError:
      print("No character found in that index")

C. 
try:
      x = 'y' + 1
except ValueError:
      print("y is not a number value")

---


## Python Exception Handling

**1. Which of the following code snippet would raise an unhandled exception?**


* [ ] B
* [ ] A
* [ ] C

**Correct answer:**
* [x] C

**Code**: 
 A.  
try:
    x = y + 1
except NameError:
    print("y is not defined")
    x = y + 1

B.  
try:
    x = 'seasalt'[7]
except IndexError:
    print("No character found in that index")

C. 
try:
    x = 'y' + 1
except ValueError:
    print("y is not a number value")

**2. When does the unnamed exception run?**


* [ ]  When there is no dedicated named exception
* [ ] After all other named exceptions
* [ ]  When it is the only exception branch

**Correct answer:**
* [x]  When there is no dedicated named exception
* [x]  When it is the only exception branch

**3. Which of the following code lines is the correct one if we want to break out of the while loop?**


* [ ] if i == 4   
* [ ] else   
* [ ] except IndexError   
* [ ] except ValueError

**Correct answer:**
* [x] except IndexError   

**Code**: 
flowers = ['roses', 'daisies', 'dahlias', 'camellias']
i = 0
while True:
    try:
        print(flowers[i])
        i += 1
    ... :
        break

**4. What happens when an exception branch is executed?**


* [ ] None of the above
* [ ] Next exception branches shall run if they are related to the error
* [ ] Previous branches shall run if they are related to the error
* [ ] No other branch after or before that is executed

**Correct answer:**
* [x] No other branch after or before that is executed

---


## PCAP-Hierarchy of Exceptions

**1. When can we use the raise keyword?**


* [ ] Inside the try block together with a named exception 
* [ ] Inside any try/except block
* [ ] Inside the except block, but unnamed 
* [ ]  Inside the try or except block with a named exception

**Correct answer:**
* [x] Inside the try block together with a named exception 
* [x] Inside the except block, but unnamed 

**2. What is the output of the following code snippet?   try:**


* [ ] We have a problem!   
* [ ] Zero won't work!   
* [ ] Zero won't work!  We have a problem!

**Correct answer:**
* [x] Zero won't work!   

**Code**: 
try:
    z = 3 // 0
except ZeroDivisionError:
    print("Zero won't work!")
except ArithmeticError:
    print("We have a problem!")

**3. ZeroDivisionError is a special case of:**


* [ ] ValueError  
* [ ] ArithmeticError 
* [ ]  TypeError 
* [ ]  Exception

**Correct answer:**
* [x] ArithmeticError 
* [x]  Exception

**4. What are the abstract exceptions?**


* [ ] Exceptions that we call without a specific name
* [ ] General exceptions that they include other exceptions
* [ ] Exception functions that take no arguments

**Correct answer:**
* [x] General exceptions that they include other exceptions

**5. Why shouldn't we put more general exceptions before specific ones?**


* [ ] Because we risk having both general and specific exceptions raised
* [ ] Because the tree of exceptions dictate us to do so
* [ ] Because the specific exceptions would never execute and become useless

**Correct answer:**
* [x] Because the specific exceptions would never execute and become useless

**6. What is common between AssertionError and ArithmeticError?**


* [ ] Both options  
* [ ]  None of the options
* [ ] They both stem from Exception
* [ ] They both have more specific error cases

**Correct answer:**
* [x] They both stem from Exception

**Explaination**: The statement "They both have more specific error classes" is incorrect. While ArithmeticError does have more specific error classes that inherit from it (e.g., ZeroDivisionError, OverflowError), AssertionError does not have any more specific error classes.

**7. In the following snippet, which exception could replace the existing one?   try:**


* [ ] ValueError  
* [ ]  RuntimeError   
* [ ] BaseException

**Correct answer:**
* [x] BaseException

**Code**: 
try:
    y = 5 / 0
except ZeroDivisionError:
    print("Can't divide with zero.")

**8. What are the specific exceptions included in the LookupError?**


* [ ] TypeError  
* [ ]  KeyError  
* [ ]  ValueError  
* [ ]  IndexError

**Correct answer:**
* [x]  KeyError  
* [x]  IndexError

**9. Which of the below code snippets is correct?**


* [ ] A
* [ ] B
* [ ] C

**Correct answer:**
* [x] C

**Code**: 
A. 
a = input("Enter a number: ")
try:
    float(a) / 0
except Exception:
    print("Hmm an error occurred.")
except TypeError, ZeroDivisionError:
    print("Please enter valid numbers, besides 0.")

B.  
a = input("Enter a number: ")
try:
    float(a) / 0
except TypeError, ZeroDivisionError:
    print("Please enter valid numbers, besides 0.")

C.  
a = input("Enter a number: ")
try:
    float(a) / 0
except (TypeError, ZeroDivisionError):
    print("Please enter valid numbers, besides zero.")



---


## GCP Compute

**1. VM in GCP can run ______?**


* [ ] Only public images 
* [ ] Only private images
* [ ] Both public and private images
* [ ] Custom images built by organization

**Correct answer:**
* [x] Both public and private images

**2. Default time on all GCP VM is ?**


* [ ] GMT 
* [ ] UTC 
* [ ] Synced with the region your VM is created
* [ ] PST

**Correct answer:**
* [x] UTC 

**Explaination**: Regardless of the region where you create your VM instance, the default time for your VM instance is Coordinated Universal Time (UTC).


**3. GCP compute instance billing is done ______ ?**


* [ ] Monthly 
* [ ] Hourly 
* [ ] Based on mins 
* [ ] Based on seconds

**Correct answer:**
* [x] Based on seconds

**Explaination**:  Google bills in second-level increments. You pay only for the compute time that you use.

**4. Your organization wants to test a open source database. Can we setup this open soure database on GCP compute?**


* [ ] YES
* [ ] NO

**Correct answer:**
* [x] YES

**Explaination**: We can setup the database on compute but its not a scalable solution


**5. _______ node is a physical Compute Engine server that is dedicated to hosting only your project's VM instances**


* [ ] Solo tenant 
* [ ] Multi tenant

**Correct answer:**
* [x] Solo tenant 

---


## GCP Big Data 

**1. Big Query pricing is based on ?**


* [ ] Number of query you run
* [ ] Amount of data scanned during the query
* [ ] Amount of data stored ?
* [ ] Number of people in the organization

**Correct answer:**
* [x] Amount of data scanned during the query

**2. A client of yours has data in AWS and want to connect to this data from GCP cloud.  Such a connection is possible between two clouds ?**


* [ ] Yes
* [ ] No

**Correct answer:**
* [x] Yes

**3. Which tool is best to find insights and visualize of the data stored in Big Query?**


* [ ] Vertex AI 
* [ ] Looker 
* [ ] GCP monitoring 
* [ ] Big Query Dashboards

**Correct answer:**
* [x] Looker 

**4. 4 Vs of Big Data are?**


* [ ] Volume, velocity, variety and veracity
* [ ] Velocity, volume, visibility, veracity
* [ ] Veracity, variety, value, volume

**Correct answer:**
* [x] Volume, velocity, variety and veracity

---


## GCP Compute 

**1. Default time on all GCP VM is ?**


* [ ] GMT 
* [ ] UTC 
* [ ] Synced with the region your VM is created
* [ ]  PST

**Correct answer:**
* [x] UTC 

**Explaination**: Regardless of the region where you create your VM instance, the default time for your VM instance is Coordinated Universal Time (UTC).


**2. Your organization wants to test an open source database. Can we setup this open source database on GCP compute?**


* [ ] Yes
* [ ] No

**Correct answer:**
* [x] Yes

**Explaination**: We can setup the database on compute but its not a scalable solution

**3. GCP compute instance billing is done ______ ?**


* [ ] Monthly 
* [ ] Hourly 
* [ ] Based on mins
* [ ]  Based on seconds

**Correct answer:**
* [x]  Based on seconds

**Explaination**: Google bills in second-level increments. You pay only for the compute time that you use.

**4. VM in GCP can run ______?**


* [ ] Only public images 
* [ ] Only private images
* [ ] Both public and private images
* [ ] Custom images built by organization

**Correct answer:**
* [x] Both public and private images

**5. _______ node is a physical Compute Engine server that is dedicated to hosting only your project's VM instances**


* [ ] Solo tenant 
* [ ] Multi tenant

**Correct answer:**
* [x] Solo tenant 

---


## GCP Database 

**1. You have been tasked with selecting a database for storing key and value pairs for your application. What kind of DB suits best?**


* [ ] SQL 
* [ ] NoSQL 
* [ ] Both SQL and NoSQL 
* [ ] Inmemory DB

**Correct answer:**
* [x] NoSQL 

**2. When using Cloud SQL in GCP , Data Replication can only be done within the region and can’t be done outside the region.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**Explaination**:  Database replication can be done outside the region


**3. AlloyDB is a fully managed ______?**


* [ ] SQL server 
* [ ] Postgres SQL server 
* [ ] MysQL server 
* [ ] Maria DB

**Correct answer:**
* [x] Postgres SQL server 

---


## GCP Final Quiz 

**1. Which GCP services can be used for flat- structured object storage?**


* [ ] Cloud Firestore 
* [ ] Cloud SQL 
* [ ] Cloud Storage

**Correct answer:**
* [x] Cloud Storage

**2. In Cloud Storage, which storage class is ideal for files you access rarely, say, once every five years?**


* [ ] Standard 
* [ ] Nearline 
* [ ] Coldline 
* [ ] Archive

**Correct answer:**
* [x] Archive

**3. How many zones are present in a GCP?**


* [ ] 1
* [ ] 2
* [ ] 3
* [ ] 4

**Correct answer:**
* [x] 3

**4. What is a managed instance group?**


* [ ] Managed instance groups are expensive compared to unmanaged instance groups
* [ ] Managed Instance group can be created using an instance template to create identical instances 
* [ ] Managed instance group is a GKE concept that autopilots the nodes
* [ ] Managed instance group is a set of computing engines that can be easily managed

**Correct answer:**
* [x] Managed instance group is a set of computing engines that can be easily managed

**5. We need a NoSQL database to be used for client-side web and mobile apps. Which DB would best suit the use case?**


* [ ] Cloud Firestore
* [ ]  Cloud Bigtable 
* [ ] Cloud Memorystore 
* [ ] Firebase Realtime Database

**Correct answer:**
* [x] Cloud Firestore

**6. Which of the following GCP services can be used to securely connect compute instance in GCP to on-premise database service?**


* [ ] Cloud VPC 
* [ ] Cloud VPN 
* [ ] Cloud Interconnect 
* [ ] VPC Network Peering

**Correct answer:**
* [x] Cloud Interconnect 

**7. What would be the best way to connect to a GCP compute instance?**


* [ ] Set up a bastion host; then use this to log in to compute instance
* [ ] Connect via NAT Gateway
* [ ] Connect via serial console
* [ ] Connect via your service account

**Correct answer:**
* [x] Connect via serial console

**8. What type of firewall rule(s) does Google Cloud's networking support?**


* [ ] deny
* [ ] allow
* [ ] allow and deny
* [ ] allow, deny, and logical route

**Correct answer:**
* [x] allow and deny

**9. GKE is suitable for ________.**


* [ ] Monolith service 
* [ ] Microservice design

**Correct answer:**
* [x] Microservice design

**10. A pharma company wants to ensure that its users are notified in real time when its packages are delivered to pharmacies. Which Google Cloud service would you recommend for this?**


* [ ] Cloud Pub/Sub
* [ ] Cloud IoT
* [ ] Cloud DataFlow
* [ ] Cloud Firestore

**Correct answer:**
* [x] Cloud Pub/Sub

**11. Which of the following statements is true for “application load testing"?**


* [ ] Load testing should be done to sustain 5x the expected traffic to the application.
* [ ] Load testing is to verify if your application and DB connections are working.
* [ ] Load testing can’t really help you determine the scalability of your system.

**Correct answer:**
* [x] Load testing should be done to sustain 5x the expected traffic to the application.

**12. What is data warehouse service in GCP?**


* [ ] Cloud Spanner
* [ ] BigQuery
* [ ] Cloud Storage
* [ ] Bigtable

**Correct answer:**
* [x] BigQuery

**13. Your organization is estimating the GCP expenditure for the upcoming quarter. What should they do to control costs?**


* [ ] Cloud cost can be a capital expenditure; hence, assign 2x the budget used for the data center.
* [ ] Audit the cost for the previous year and set that as an estimate.
* [ ] Review cloud resource costs frequently because costs depend on usage.
* [ ] Assess and audit organic growth in cloud cost, compare it with expected cloud utilization based on an upcoming project, and share the estimate.

**Correct answer:**
* [x] Assess and audit organic growth in cloud cost, compare it with expected cloud utilization based on an upcoming project, and share the estimate.

**14. What is the biggest factor to consider when building an application for business continuity?**


* [ ] Meantime for recovery in case of disaster recovery
* [ ] Latency of the application for end users
* [ ] Cost of the region/zone
* [ ] Business revenue generated from application

**Correct answer:**
* [x] Meantime for recovery in case of disaster recovery

**15. When granting permissions to GCP, what principle should we follow?**


* [ ] Basic view access to all resources and content
* [ ] Least access privilege; provide only the required access 
* [ ] Share Editor access
* [ ] Replicate team member access directly

**Correct answer:**
* [x] Least access privilege; provide only the required access 

**16. What factors must be considered when choosing a region?**


* [ ] Region-specific restrictions
* [ ] Where will the developer team work from?
* [ ] Latency requirements of your app
* [ ] Time to deploy changes to your application

**Correct answer:**
* [x] Region-specific restrictions
* [x] Latency requirements of your app

**17. Your organization decides to move to GCP cloud. How should they start adopting the transformation steps?**


* [ ] Adopt changes manually with automation
* [ ] Adopt changes programmatically
* [ ] Adopt changes for applications when problems arise

**Correct answer:**
* [x] Adopt changes programmatically

**18. Our pharmaceutical company wants to observe the behavior of users placing orders and design its application to make it more user-friendly. What might have prompted the company to consider this change?**


* [ ] Users/Customers expecting discounts and waiting for orders to be placed 
* [ ] Customers expecting a more personalized experience when using the app
* [ ] Company wants to deploy changes immediately
* [ ] Company wants insights on its spending

**Correct answer:**
* [x] Customers expecting a more personalized experience when using the app

**19. You have a billing application that runs only during the month end. This application has a very high SLA, and a failure will delay the commission paid out to sales employees.  Which deployment setup would be best for such a use case?**


* [ ] Deploy the application on compute engine and choose preemptible instances
* [ ] Develop the application on compute engine with default settings
* [ ] Reserve a compute engine for 3 years and use it to run the billing application
* [ ] Run this application on-premises

**Correct answer:**
* [x] Deploy the application on compute engine and choose preemptible instances

**20. Which databases does Cloud SQL currently support?**


* [ ] MySQL, PostgreSQL, SQL Server
* [ ] OracleDB, PostgreSQL, MySQL
* [ ] MariaDB, SQL Server, MySQL
* [ ] MySQL, MongoDB, SQL Server

**Correct answer:**
* [x] MySQL, PostgreSQL, SQL Server

**21. What would the DevOps philosophy recommend to measure first in the modern cloud design?**


* [ ] Customer satisfaction
* [ ] Speed of cloud adoption
* [ ] Reliability and health of our production systems
* [ ] Disaster recovery setup

**Correct answer:**
* [x] Reliability and health of our production systems

---


## 30-130-metrics-quiz

**1. How many labels does the following time series have? http_errors_total{instance="1.1.1.1:80", job="api", code="400", endpoint="/user", method="post"} 55234**


* [ ] 1
* [ ] 2
* [ ] 3
* [ ] 4
* [ ] 5

**Correct answer:**
* [x] 5

**Explaination**: The 5 labels are instance, job, code, endpoint, method

**2. What metric should be used to report the amount of time a process has been running?**


* [ ] counter
* [ ] gauge 
* [ ] histogram
* [ ] summary

**Correct answer:**
* [x] counter

**Explaination**: For this a counter metric would be used as it would count the number of seconds a process has been running for. The uptime of a process can never go down, so a gauge metric shouldn’t be used.

**3. Which of these is not a valid metric?**


* [ ] 404_error_count
* [ ] net_conntrack_listener_conn_closed_total
* [ ] http_errors_total
* [ ] node_netstat_Icmp_InErrors

**Correct answer:**
* [x] 404_error_count

**Explaination**: Metric names cannot start with a number.

**4. For the metric http_requests_total{path=”/auth”, instance=”node1”, job=”api”} 7782 ; What is the metric name?**


* [ ] node1
* [ ] http_requests_total
* [ ] path=”auth”
* [ ] api
* [ ] 7782

**Correct answer:**
* [x] http_requests_total

**5. What are the 4 types of prometheus metrics?**


* [ ] counter, float, histogram, summary
* [ ] incrementer, gauge, histogram, summary
* [ ] counter, spatial, linear, summary
* [ ] counter, gauge, histogram, summary

**Correct answer:**
* [x] counter, gauge, histogram, summary

**6. What metric should be used to report the current memory utilization?**


* [ ] counter
* [ ] gauge
* [ ] histogram
* [ ] summary

**Correct answer:**
* [x] gauge

**Explaination**: Since memory utilization can go up and down, we want to use a gauge metric instead of a counter.

**7. How many total unique time series are there in this output?**


* [ ] 9
* [ ] 3
* [ ] 8
* [ ] 5

**Correct answer:**
* [x] 9

**Code**: 
node_arp_entries{instance="node1" job="node"} 200
node_arp_entries{instance="node2" job="node"} 150

node_cpu_seconds_total{cpu="0", instance="node"1", mode="iowait"}
node_cpu_seconds_total{cpu="1", instance="node"1", mode="iowait"}
node_cpu_seconds_total{cpu="0", instance="node"1", mode="idle"}
node_cpu_seconds_total{cpu="1", instance="node"1", mode="idle"}
node_cpu_seconds_total{cpu="1", instance="node"2", mode="idle"}

node_memory_Active_bytes{instance="node1" job="node"} 419124
node_memory_Active_bytes{instance="node2" job="node"} 55589


**Explaination**: Each unique combination of metrics & labels is a separate time series. There are 9 separate instances of that

**8. What are the two labels every metric is assigned by default?**


* [ ] node, instance
* [ ] target, job
* [ ] node, job
* [ ] instance, job

**Correct answer:**
* [x] instance, job

**9. What are the two attributes provided by a metric?**


* [ ] Category, Detail
* [ ] Type, Detail
* [ ] Help, Type
* [ ] Summary, Type
* [ ] Identity, Help

**Correct answer:**
* [x] Help, Type

**Explaination**: HELP - Description of what the metric is
TYPE - Specific type of prometheus metric(counter, gauge, histogram, summary)


**10. A web app is being built that allows users to upload pictures, management would like to be able to track the size of uploaded pictures and report back the number of photos that were less than 10Mb, 50Mb, 100MB, 500MB, and 1Gb. What metric would be best for this?**


* [ ] counter
* [ ] gauge
* [ ] histogram 
* [ ] summary

**Correct answer:**
* [x] histogram 

**Explaination**: Histograms should be used to calculate how long or how big something is and allows you to group observations into configurable bucket sizes

---


## PCA mock 1

**1. What are the 3 components of the prometheus server?**


* [ ] influxdb, api gateway, retrieval node
* [ ] retrieval node, tsdb, http server
* [ ] retrieval node, alerting, push gateway
* [ ] exporters, tsdb, http server

**Correct answer:**
* [x] retrieval node, tsdb, http server

**Explaination**: The 3 main components of the Prometheus instance or retrieval node tsdb, and the HTTP server. The retrieval node is responsible for scraping targets and collecting metrics. After the metrics are scraped, they will be stored in a time series database. To retrieve scraped metrics a query can be sent to the HTTP server.

**2. The metric `http_errors_total` has 3 labels, `path`, `method`, `error`. Which of the following queries will give the total number of errors for a path of `/auth`, method of `POST`, and error code of `401`?**


* [ ] http_errors_total
* [ ] http_errors_total{path="/auth",method="POST",code="401"}
* [ ] http_errors_total{path="/auth",method="POST"}
* [ ] http_errors_total{path="/auth",method="PUT",code="401"}

**Correct answer:**
* [x] http_errors_total{path="/auth",method="POST",code="401"}

**3. Which component of the Prometheus architecture should be used to collect metrics of short-lived jobs?**


* [ ] service discovery
* [ ] exporters
* [ ] push gateway
* [ ] alertmanager

**Correct answer:**
* [x] push gateway

**Explaination**: Since Prometheus follows a pull-based model, this makes gathering metrics from short-lived jobs difficult. The push gateway allows short-lived jobs to push metrics to the push gateway, and the Prometheus server can scrape the push gateway.

**4. What is the name of the cli utility that comes with Prometheus?**


* [ ] prom-validator
* [ ] prometheus-cli
* [ ] prom-cli
* [ ] promtool

**Correct answer:**
* [x] promtool

**Explaination**: The cli utility that ships with Prometheus is called promtool.

**5. What are the 3 components of observability?**


* [ ] logging, metrics, traces
* [ ] logging, metrics, alerting
* [ ] traces, exporters, instrumentation
* [ ] metrics, client libraries, traces

**Correct answer:**
* [x] logging, metrics, traces

**6. What metric should be used to track the uptime of a server?**


* [ ] counter 
* [ ] gauge 
* [ ] histogram
* [ ] summary

**Correct answer:**
* [x] counter 

**Explaination**: For this, a counter metric would be used as it would count the number of seconds a server has been running for. The uptime of a server can never go down, so a gauge metric shouldn’t be used.

**7. What type of metric should be used for measuring internal temperature of a server?**


* [ ] counter
* [ ] gauge
* [ ] histogram
* [ ] summary

**Correct answer:**
* [x] gauge

**Explaination**: Since temperature readings can go up or down, a gauge metric should be used in this case.

**8. A metric to track requests to an api `http_requests_total` is created. Which of the following would not be a good choice for a label?**


* [ ] path
* [ ] status code
* [ ] email
* [ ] method

**Correct answer:**
* [x] email

**Explaination**: Email is a poor choice for a label as there will be a large number of different emails for an application and this will lead to high cardinality

**9. Which of the following is not a valid way to reload Prometheus configuration?**


* [ ] restart prometheus server(systemctl restart prometheus) 
* [ ] promtool config reload
* [ ] send a POST request to /-/reload with the –web.enable-lifecycle flag 
* [ ] send a SIGHUP signal to the prometheus process

**Correct answer:**
* [x] promtool config reload

**Explaination**: There are 3 ways to reload Prometheus configurations
1) Restart prometheus
2) send a SIGUP signal to the prometheus process
3) SEND a POST/PUT request to /-/reload with the –web.enable-lifecycle flag


**10. Which query below will give the active bytes on instance `10.1.1.1:9100` 45m ago?**


* [ ] node_memory_Active_bytes offset 45m
* [ ] node_memory_Active_bytes{instance="10.1.1.1:9100"} offset 45m
* [ ] node_memory_Active_bytes{instance="10.1.1.1:9100"}[45]
* [ ] node_memory_Active_bytes{instance="10.1.1.1:9100"} @45m

**Correct answer:**
* [x] node_memory_Active_bytes{instance="10.1.1.1:9100"} offset 45m

**Explaination**: To get the value of a metric 45 minutes ago, use the offset modifier and specify "offset 45m".

**11. Which query below will get all time series for metric `node_disk_read_bytes_total` for job=web, and job=node?**


* [ ] node_disk_read_bytes_total{job=~"web|node"}
* [ ] node_disk_read_bytes_total{job="web|node"}
* [ ] node_disk_read_bytes_total{job=~"web + node"}
* [ ] node_disk_read_bytes_total{job="web", job="node"}

**Correct answer:**
* [x] node_disk_read_bytes_total{job=~"web|node"}

**Explaination**: To get all time series for both jobs `node`, and `web, we will need to use a regular expression matcher `=~`. The regular expression to get both of those jobs is `web|node`. 

**12. Which query will give sum of all filesystems on the machine? The metric `node_filesystem_size_bytes` will list out all of the filesystems and their total size.**


* [ ] sum(node_filesystem_size_bytes{instance="192.168.1.168:9100"})
* [ ] node_filesystem_size_bytes{instance="192.168.1.168:9100"} + node_filesystem_size_bytes
* [ ] node_filesystem_size_bites(sum)
* [ ] count(node_filesystem_size_bytes{instance="192.168.1.168:9100"})

**Correct answer:**
* [x] sum(node_filesystem_size_bytes{instance="192.168.1.168:9100"})

**Code**: 
node_filesystem_size_bytes{device="/dev/sda2", fstype="vfat", instance="192.168.1.168:9100", mountpoint="/boot/efi"} 536834048
node_filesystem_size_bytes{device="/dev/sda3", fstype="ext4", instance="192.168.1.168:9100", mountpoint="/"} 13268975616
node_filesystem_size_bytes{device="tmpfs", fstype="tmpfs", instance="192.168.1.168:9100", mountpoint="/run"} 727924736
node_filesystem_size_bytes{device="tmpfs", fstype="tmpfs", instance="192.168.1.168:9100", mountpoint="/run/lock"} 5242880
node_filesystem_size_bytes{device="tmpfs", fstype="tmpfs", instance="192.168.1.168:9100", mountpoint="/run/snapd/ns"} 727924736
node_filesystem_size_bytes{device="tmpfs", fstype="tmpfs", instance="192.168.1.168:9100", mountpoint="/run/user/1000"} 727920640


**Explaination**: The `sum` aggregation operator will give the sum of all of the filesystems

**13. What method does Prometheus use to collect metrics from targets?**


* [ ] push
* [ ] batch pub
* [ ] pull
* [ ] streams

**Correct answer:**
* [x] pull

**Explaination**: Prometheus follows a pull-based model. The prometheus.yml file will have a list of all targets Prometheus will need to scrape, which involves sending an HTTP request to the target.

**14. Which query will return all time series for the metric `node_network_transmit_drop_total` this is greater than 20 and less than 100?**


* [ ] node_network_transmit_drop_total > 20 and < 100
* [ ] node_network_transmit_drop_total > 20 or node_network_transmit_drop_total < 100
* [ ] node_network_transmit_drop_total > 20 and node_network_transmit_drop_total < 100
* [ ] node_network_transmit_drop_total > 20 or < 100

**Correct answer:**
* [x] node_network_transmit_drop_total > 20 and node_network_transmit_drop_total < 100

**Explaination**: To get all-time series greater than 20 and less than 10, comparison operators will need to be used `>20`, `<100`. Both operators can be combined with the `and` operator.

**15. Which query will return targets who have more than 50 arp entries?**


* [ ] node_arp_entries{job="node"} < 50
* [ ] node_arp_entries{job="node"} > 50
* [ ] node_arp_entries{job="node"} = 50
* [ ] node_arp_entries{job="node" >50}

**Correct answer:**
* [x] node_arp_entries{job="node"} > 50

**Explaination**: To return all-time series greater than a specific value(50 in this case), use a comparison operator to find instances with arp entries > 50.

**16. Which query below will return a range vector?**


* [ ] node_cpu_seconds_total{cpu="0"}
* [ ] node_boot_time_seconds[5m]
* [ ] http_upload_bytes @1670487164
* [ ] process_virtual_memory_bytes offset 2h

**Correct answer:**
* [x] node_boot_time_seconds[5m]

**Explaination**: The range vector selector `[5m]` will return all values for a metric over the past 5 minutes, so it returns a range vector.

**17. Regarding histogram and summary metrics, which of the following are true?**


* [ ] histogram is calculated client side and summary is calculated server side
* [ ] histogram is calculated client side and summary is calculated client side
* [ ] histogram is calculated server side and summary is calculated client side
* [ ] histogram is calculated server side and summary is calculated server side

**Correct answer:**
* [x] histogram is calculated server side and summary is calculated client side

**Explaination**: For histograms, quantiles must be calculated server side, thus they are less taxing on client libraries. whereas summary metrics are the opposite, as everything is calculated client side.

**18. What are the different states a Prometheus alert can be in?**


* [ ] inactive, triggered, complete
* [ ] ok, pending,  firing
* [ ] silenced, firing, triggered
* [ ] inactive, pending, firing

**Correct answer:**
* [x] inactive, pending, firing

**Explaination**: Alerts can have 3 states:
1) inactive - Alert expression has not returned any results
2) pending - the state of an alert that has been active for less than the configured threshold duration
3) firing - the state of an alert that has been active for longer than the configured threshold duration


**19. Which of the following would make for a poor SLI?**


* [ ] high disk utilization
* [ ] availability
* [ ] error rate
* [ ] Latency

**Correct answer:**
* [x] high disk utilization

**Explaination**: For `useful` SLIs, you want to find metrics that accurately measure a user’s experience. So things like high CPU, memory, and disk utilization would make for poor SLIs, as a user `may` not experience any degradation of service during these events.

**20. Which query below will give the 99% quantile of the metric `http_requests_total`?**


* [ ] histogram_quantile(0.99, http_requests_total_bucket)
* [ ] http_requests_total(quantile=”0.99”}
* [ ] http_requests_total < 99%
* [ ] quantile(http_requests_total, 0.99)

**Correct answer:**
* [x] histogram_quantile(0.99, http_requests_total_bucket)

**Explaination**: For histogram metrics, to calculate a quantile use the `histogram_quantile` function. The function takes two arguments, the desired percentile, and the histogram metric, make sure to pass in the _bucket sub metric.



**21. What is the purpose of Prometheus `scrape_interval`?**


* [ ] defines what targets to scrape
* [ ] Defines how long to wait for a scrape before timing out
* [ ] Defines how long Prometheus waits before clearing out the TSDB
* [ ] Defines how frequently to scrape a target

**Correct answer:**
* [x] Defines how frequently to scrape a target

**Explaination**: scrape_interval configs determine how often to scrape a target. If scrape_interval is set to 30s then each target will get scraped every 30s.

**22. Management has decided to offer a file upload service where the SLO states that 97% of all upload should complete within 30s. A histogram metric is configured to track the upload time, which of the following bucket configurations is recommended for the desired slo?**


* [ ] 1, 5, 10, 25, 35, 50
* [ ] 10, 25, 27, 30, 32, 35, 40, 50
* [ ] 35, 45, 55, 65, 75, 100
* [ ] 1, 3, 8, 10, 12, 15, 17

**Correct answer:**
* [x] 10, 25, 27, 30, 32, 35, 40, 50

**Explaination**: Since histogram quantiles are approximations, to find out if a slo has been met, make sure that a bucket is specified at the desired slo value.

**23. Which of the following statements are true regarding Alert `labels` and `annotations`?**


* [ ] Both Alert `labels` and `annotations` can be used for routing on Alertmanager
* [ ] Both Alert `labels` and `annotations` are used purely for descriptive purposes
* [ ] Alert `labels` can be used as metadata so alertmanager can match on them and performing routing policies, `Annotations` should be used for cosmetic descriptions of the alerts
* [ ] Alert `annotations` can be used as metadata so alertmanager can match on them and performing routing policies, `labels` should be used for cosmetic description of the alerts

**Correct answer:**
* [x] Alert `labels` can be used as metadata so alertmanager can match on them and performing routing policies, `Annotations` should be used for cosmetic descriptions of the alerts

**Code**: 
route:
  receiver: staff
  group_by: ['severity']
  group_wait: 30s
  group_interval: 5m
  repeat_interval: 12h
  routes:
    - matchers:
        job: kubernetes
      receiver: infra
      group_by: ['severity']

**Explaination**: In the Alertmanager configs, under matchers and group_by, we can match on labels to determine who gets notified for which alerts.

**24. What type of database does Prometheus use?**


* [ ] Relational
* [ ] NoSQL
* [ ] Key-Value
* [ ] Time Series
* [ ] Graph

**Correct answer:**
* [x] Time Series

**Explaination**: Prometheus uses a time series database.

**25. What is this an example of?  	`Service provider guaranteed 99.999% uptime each month or else customer will be awarded $10k'**


* [ ] SLA
* [ ] SLO
* [ ] SLI
* [ ] SLU

**Correct answer:**
* [x] SLA

**Explaination**: An SLA is a contract between a vendor and a user that guarantees a certain SLO and states the consequences for not meeting said SLA.

**26. What configuration will make it so Prometheus doesn’t scrape targets with a label of `team: frontend`?**


* [ ] Option A
* [ ] Option B
* [ ] Option C
* [ ] Option D

**Correct answer:**
* [x] Option A

**Code**: 
Option A:

relabel_configs:
  - source_labels: [team]
    regex: frontend
    action: drop

Option B:

relabel_configs:
  - source_labels: [frontend]
    regex: team
    action: drop

Option C:

metric_relabel_configs:
  - source_labels: [team]
    regex: frontend
    action: drop

Option D:

relabel_configs:
  - match: [team]
    regex: frontend
    action: drop

**Explaination**: Option A is the correct answer. The relabel_configs is where you will define which targets Prometheus should scrape. To match on label `team: frontend`
1) Set the source_labels to `team
2) regex field should represent the value of the label `frontend
3) since we don’t want to scrape the targets, specify `action: drop`


**27. Analayze the example alertmanager configs and determine when an alert with the following labels arrives on alertmanager, what receiver will it send the alert to  `team: backend` and `severity: critical`**


* [ ] general-email
* [ ] backend-email
* [ ] backend
* [ ] backend-pager
* [ ] frontend-pager

**Correct answer:**
* [x] backend-pager

**Code**: 
route:
  receiver: general-email
  routes:
    - receiver: frontend-email
      matchers:
        - team: frontend
      routes:
        - matchers:
            severity: critical
          receiver: frontend-pager
    - receiver: backend-email
      matchers:
        - team: backend
      routes:
        - matchers:
            severity: critical
          receiver: backend-pager
    - receiver: auth-email
      matchers:
        - team: auth
      routes:
        - matchers:
            severity: critical
          receiver: auth-pager
  receiver: auth-pager


**Explaination**: Since the alert has the label `team: backend,` it will match the second route. By default, it will send an alert to the backend-email receiver. However, there is a subroute. The subroute matches on a label of `severity: critical`. Since `severity: critical` has been set on the label, alertmanager will send it to the receiver backend-pager

**28. Analyze the example alertmanager configs and determine when an alert with the following labels arrives on alertmanager, what receiver will it send the alert to  `team: api` and  `severity: critical`?**


* [ ] general-email
* [ ] backend-email
* [ ] auth-email
* [ ] backend-pager
* [ ] frontend-pager

**Correct answer:**
* [x] general-email

**Code**: 
route:
  receiver: general-email
  routes:
    - receiver: frontend-email
      matchers:
        - team: frontend
      routes:
        - matchers:
            severity: critical
          receiver: frontend-pager
    - receiver: backend-email
      matchers:
        - team: backend
      routes:
        - matchers:
            severity: critical
          receiver: backend-pager
    - receiver: auth-email
      matchers:
        - team: auth
      routes:
        - matchers:
            severity: critical
          receiver: auth-pager
  receiver: auth-pager

**Explaination**: The label `team: api` does not match with any of the parent routes, so it goes to the default route, which uses the general-email receiver

**29. The following PromQL expression is trying to divide the the `node_filesystem_avail_bytes` by `node_filesystem_size_bytes` , and   	`node_filesystem_avail_bytes` / `node_filesystem_size_bytes`. The PromQL expression does not return any results, fix the expression so that it successfully divides the two metric.  This is what the two metrics look like before the division operation.**


* [ ] SLA
* [ ] node_filesystem_avail_bytes / ignoring(class) node_filesystem_size_bytes
* [ ] node_filesystem_avail_bytes / on(class) node_filesystem_size_bytes
* [ ] node_filesystem_avail_bytes{class!=”SSD”} / on(class) node_filesystem_size_bytes

**Correct answer:**
* [x] node_filesystem_avail_bytes / ignoring(class) node_filesystem_size_bytes

**Code**: 
node_filesystem_avail_bytes{device="/dev/sda2", fstype="vfat", class=”SSD” instance="192.168.1.168:9100", job="test", mountpoint="/boot/efi"}


	node_filesystem_size_bytes{device="/dev/sda2", fstype="vfat", instance="192.168.1.168:9100", job="test", mountpoint="/boot/efi"}


**Explaination**: The `node_filesystem_avail_bytes` metric has the following labels:
- device 
- fstype 
- class 
- instance 
- job
- mountpoint

	The `node_filesystem_size_bytes` has the following labels:
- device 
- fstype 
- instance 
- job
- mountpoint

	The node_filesystem_size_bytes` is missing the `class` label, so there will never be a match. Use the `ignoring` keyword and pass in the `class` label so Prometheus will ignore it.


**30. Which type of observability would be used to track a request/transaction as it traverses a system?**


* [ ] logs
* [ ] traces
* [ ] metric
* [ ] events

**Correct answer:**
* [x] traces

**Explaination**: Traces allow you to follow operations as they traverse through various systems & services, allowing you to follow a request hop by hop through a system.

**31. Which component of the Prometheus architecture should be used to automatically discover all nodes in a Kubernetes cluster?**


* [ ] service discovery
* [ ] push gateway
* [ ] alertmanager
* [ ] exporters

**Correct answer:**
* [x] service discovery

**Explaination**: Service Discovery allows Prometheus to automatically generate a list of targets to scrape. In highly dynamic environments where nodes/applications are continuously spun and shutdown, like in Kubernetes, Prometheus can automatically update the list of targets.

**32. Analyze the alertmanager configs below. For all the alerts that got generated, how many total notifications will be sent out.?**


* [ ] 1
* [ ] 2
* [ ] 3
* [ ] 4
* [ ] 5

**Correct answer:**
* [x] 3

**Code**: 
route:
  receiver: general-email
  group_by: [alertname]
  routes:
    - receiver: frontend-email
      group_by: [env]
      matchers:
        - team: frontend
 



The following alerts get generated by Prometheus with the defined labels.

alert1
	team: frontend
	env: dev


alert2
	team: frontend
	env: dev


alert3
	team: frontend
	env: prod

alert4
	team: frontend
	env: prod

alert5
	team: frontend
	env: staging


**Explaination**: In the configuration, the group_by configuration is setup to `env`, which means all alerts will be grouped by the `env` and a single notification gets sent out. There are 2 alerts with `env=dev` and another 2 alerts with `env=prod` and one alert with `env=staging`. So 3 notifications get generated

**33. What is the default path Prometheus will scrape to collect metrics?**


* [ ] /metrics
* [ ] /swagger-stats/metrics
* [ ] /prometheus
* [ ] /stats

**Correct answer:**
* [x] /metrics

**Explaination**: The default endpoint Prometheus will scrape is `/metrics` however, this can be modified with `metrics_path`

**34. What is the Prometheus client library used for?**


* [ ] Instrumenting applications  to generate prometheus metrics and to push metrics to the Push Gateway
* [ ] Sending Alerts to Alertmanager
* [ ] Used for Service discovery 
* [ ] Generate logs & traces

**Correct answer:**
* [x] Instrumenting applications  to generate prometheus metrics and to push metrics to the Push Gateway

**Explaination**: Client libraries are to instrument applications so Prometheus can collect metrics from them. In addition, client libraries can also be used to push metrics to a Push Gateway.

**35. The metric http_errors_total{code=”404”} tracks the number of 404 errors a web server has seen. Which query returns what is the average rate of 404s a server has seen for the past 2 hours? Use a 2m sample range and a query interval of 1m**


* [ ] avg_over_time(rate(http_errors_total{code=”404”}[2h]))
* [ ] avg_over_time(rate(http_errors_total{code=”404”}[2m])) [2h:1m]
* [ ] avg_over_time(rate(http_errors_total{code=”404”}[2m]) [2h:1m])
* [ ] avg_over_time(rate(http_errors_total{code=”404”}[2m]) [1m:2h])

**Correct answer:**
* [x] avg_over_time(rate(http_errors_total{code=”404”}[2m]) [2h:1m])

**Explaination**: Since the question is asking for the what is the average `rate` of 404 errors over the past 2 hours, rate function will need to be used:
rate(http_errors_total{code=”404”}[2m])

To get the average over the past 2 hours, use the avg_over_time function. The avg_over_time function requires a range vector to be passed in, a subquery will need to be performed on the rate to get the range vector that contains the rate of errors for the past 2 hours. Since we need the average for the past 2 hours, the first value in the subquery will be `2h` and the second number is the query interval. Thus the final query looks like this:


avg_over_time(rate(http_errors_total{code=”404”}[2m]) [2h:1m])


**36. Which of the following is not a valid time value to be used in a range selector?**


* [ ] 1h15m
* [ ] 2y
* [ ] 200ms
* [ ] 4w
* [ ] 2mo

**Correct answer:**
* [x] 2mo

**37. Based off the metrics below, which query will return the same result as the query database_write_timeouts / ignoring(error) database_error_total**


* [ ] database_write_timeouts / group_left database_error_total
* [ ] database_write_timeouts / on(instance, job, type) database_error_total
* [ ] database_write_timeouts / merge(error) database_error_total
* [ ] database_error_total ignoring(error) / database_write_timeouts

**Correct answer:**
* [x] database_write_timeouts / on(instance, job, type) database_error_total

**Code**: 
database_write_timeouts{instance="db1", job="db", error="212, type="mysql"} 12

database_error_total{instance="db1", job="db", type="mysql"} 67


**Explaination**: To perform division between two vectors, all labels must match for the division to occur; that is why the ignoring keyword is used to filter out the error label, which does not exist on the `database_error_total` metric. The opposite of the ignoring keyword is the `on` keyword, which tells Prometheus which labels to match. If we specify the `on` keyword and pass all labels except for the `error` label, it will have the same effect as doing `ignoring(error)`

**38. For a histogram metric, what are the different submetrics?**


* [ ] `_count`, `_bucket`
* [ ] `_bucket`
* [ ] `_count`, `_bucket`, `_sum`
* [ ] `_total`, `_sum`, `_bucket`

**Correct answer:**
* [x] `_count`, `_bucket`, `_sum`

**Explaination**: Histogram metrics have 3 submetrics:
		count: total number of observations
		sum: sum of all observations
		bucket: number of observations for a specific bucket

**39. Add an annotation to the alert called `description` that will print out the message that looks like this  Instance <insert name of instance> has low disk space on filesystem <insert mountpoint name>, current free space is at <insert percentage of free space>%**


* [ ] Option A
* [ ] Option B
* [ ] Option C
* [ ] Option D

**Correct answer:**
* [x] Option B

**Code**: 
groups:
  - name: node
    rules:
      - alert: node_filesystem_free_percent
        expr: 100 * node_filesystem_free_bytes{job="node"} / node_filesystem_size_bytes{job="node"} < 10

// Examples of the two metrics used in the alert can be seen below.

node_filesystem_free_bytes{device="/dev/sda3", fstype="ext4", instance="node1", job="web", mountpoint="/home"}

node_filesystem_size_bytes{device="/dev/sda3", fstype="ext4", instance="nodde1", job="web", mountpoint="/home"}

Choose the correct answer:
Option A:
description: Instance << $Labels.instance >> has low disk space on filesystem << $Labels.mountpoint >>, current free space is at << .Value >>%

Option B:
description: Instance {{ .Labels.instance }} has low disk space on filesystem {{ .Labels.mountpoint }}, current free space is at {{ .Value }}%

Option C:
description: Instance {{ .Labels=instance }} has low disk space on filesystem {{ .Labels=mountpoint }}, current free space is at {{ .Value }}%

Option D:
description: Instance {{ .instance }} has low disk space on filesystem {{ .mountpoint }}, current free space is at {{ .Value }}%



**40. Add an annotation to the alert called `description` that will print out the message that looks like this  Instance <insert name of instance> has low disk space on filesystem <insert mountpoint name>, current free space is at <insert percentage of free space>%**


* [ ] Option A
* [ ] Option B
* [ ] Option C
* [ ] Option D

**Correct answer:**
* [x] Option B

**Code**: 
groups:
  - name: node
    rules:
      - alert: node_filesystem_free_percent
        expr: 100 * node_filesystem_free_bytes{job="node"} / node_filesystem_size_bytes{job="node"} < 10

Examples of the two metrics used in the alert can be seen below

node_filesystem_free_bytes{device="/dev/sda3", fstype="ext4", instance="node1", job="web", mountpoint="/home"}

node_filesystem_size_bytes{device="/dev/sda3", fstype="ext4", instance="nodde1", job="web", mountpoint="/home"}

Choose the correct option:

Option A:
description: Instance << $Labels.instance >> has low disk space on filesystem << $Labels.mountpoint >>, current free space is at << .Value >>%

Option B:
description: Instance {{ .Labels.instance }} has low disk space on filesystem {{ .Labels.mountpoint }}, current free space is at {{ .Value }}%

Option C:
description: Instance {{ .Labels=instance }} has low disk space on filesystem {{ .Labels=mountpoint }}, current free space is at {{ .Value }}%

Option D:
description: Instance {{ .instance }} has low disk space on filesystem {{ .mountpoint }}, current free space is at {{ .Value }}%


**Explaination**: Using the go templating syntax, we can access instance labels with {{ .Labels.instance }} and the mountpoint can be accessed with {{ .Labels.mountpoint }} and the value can be accessed with {{ .Value}}

**41. An engineer forgot to address an alert, based off the alertmanager config below, how long will they need to wait to see the alert again?**


* [ ] 10s
* [ ] 4h
* [ ] 5m
* [ ] 15s

**Correct answer:**
* [x] 4h

**Code**: 
route:
  receiver: pager
  group_by: [alertname]
  group_wait: 10s
  repeat_interval: 4h
  group_interval: 5m
  routes:
    - match:
        team: api
      receiver: api-pager
    - match:
        team: frontend
      receiver: frontend-pager

**Explaination**: The repeat interval determines how long alertmanager will wait before sending another notification.

**42. You are writing your own exporter for a Redis database. Which of the following would be the correct name for a metric to represent used memory on the by the Redis instance?**


* [ ] used_mem_redis_bytes
* [ ] redis_mem_used_kilobytes
* [ ] redis_used_bytes_mem
* [ ] redis_mem_used_bytes

**Correct answer:**
* [x] redis_mem_used_bytes

**Explaination**: When naming metrics, the first word should be the application/library, which in this case is `redis`. The name second part of the metric name should be the metric name. The last part of the name should be the unit which should be unprefixed, so that means we prefer bytes, seconds over kilobytes, and milliseconds.

**43. Which of the following components is responsible for collecting metrics from an instance and exposing them in a format Prometheus expects?**


* [ ] exporters
* [ ] alertmanager
* [ ] pushgateway
* [ ] Grafana
* [ ] TSDB

**Correct answer:**
* [x] exporters

**44. The metric `node_cpu_temp_celcius` reports the current temperature of a nodes CPU in celsius. What query will return the average temperature across all CPUs on a per node basis? The query should return 	{instance="node1"} 23.5     //average temp across all CPUs on node1 {instance="node2"} 33.5    //average temp across all CPUs on node2**


* [ ] avg(node_cpu_temp_celsius)
* [ ] avg on(instance) node_cpu_temp_celsius
* [ ] avg by(instance) (node_cpu_temp_celsius)
* [ ] avg_over_time(node_cpu_temp_celsius)

**Correct answer:**
* [x] avg by(instance) (node_cpu_temp_celsius)

**Code**: 
node_cpu_temp_celsius{instance="node1", cpu="0"} 28 

node_cpu_temp_celsius{instance="node1", cpu="1"} 19 

node_cpu_temp_celsius{instance="node2", cpu="0"} 36 

node_cpu_temp_celsius{instance="node2", cpu="1"} 31 





**Explaination**: The avg aggregator is used to find the average across multiple time series. To aggregate along each instance to get the average temp across all CPUs on a per-node basis, we will use the `by` clause and aggregate along the `instance` label.

**45. In the scrape configs for a pushgateway, what is the purpose of the `honor_labels: true`**


* [ ] Tells prometheus this is a pushgateway
* [ ] Prometheus will drop all labels without this config
* [ ] Allows metrics to specify the instance and job labels instead of pulling it from scrape_configs
* [ ] Used to save disk space when a lot of labels are used

**Correct answer:**
* [x] Allows metrics to specify the instance and job labels instead of pulling it from scrape_configs

**Code**: 
scrape_configs:
  - job_name: pushgateway
    honor_labels: true
    static_configs:
      - targets: ["192.168.1.168:9091"]

**46. How can alertmanager prevent certain alerts from generating notification for a temporary period of time?**


* [ ] add a new route in alertmanager config and specify `silence_period:` config
* [ ] Configuring a Silence
* [ ] Configuring a AlertMute
* [ ] Using inhibition rule

**Correct answer:**
* [x] Configuring a Silence

**Explaination**: Silences allow you to temporarily pause the generation of notifications for a specific alert(s) 

**47. What does the following `metric_relabel_config` do?**


* [ ] renames the metric `database_errors_total` to `database_failures_total`
* [ ] drops metric with the name `database_errors_total
* [ ] merges the metrics database_errors_total and `database_failures_total`
* [ ] tells prometheus to only scrape targets with the metric `database_errors_total`

**Correct answer:**
* [x] renames the metric `database_errors_total` to `database_failures_total`

**Code**: 
scrape_configs:
  - job_name: example
    metric_relabel_configs:
      - source_labels: [__name__]
        regex: database_errors_total
        action: replace
        target_label: __name__
        replacement: database_failures_total


**48. Where should alerting rules be defined?**


* [ ] prometheus configuration file
* [ ] alertmanager configuration file
* [ ] separate rules file
* [ ] export configuration file

**Correct answer:**
* [x] separate rules file

**Code**: 
scrape_configs:
  - job_name: example
    metric_relabel_configs:
      - source_labels: [__name__]
        regex: database_errors_total
        action: replace
        target_label: __name__
        replacement: database_failures_total


**Explaination**: Alerts and recording rules should be defined in a separate rules file. This file then needs to be referenced in the prometheus.yml file as per:

	prometheus.yml

rule_files:
 - rules.yml

**49. What is the default web port of Prometheus?**


* [ ] 9090
* [ ] 9001
* [ ] 9100
* [ ] 9003

**Correct answer:**
* [x] 9090

**50. Which statement is true about the rate/irate functions?**


* [ ] rate() and irate() operate in the same exact way
* [ ] rate() calculates rate by using the first two datapoints over an interval, irate() calculates the rate only between the last two datapoints in an interval
* [ ] irate() calculates average rate over entire interval, rate() calculates the rate only between the last two datapoints in an interval
* [ ] rate() calculates average rate over entire interval, irate() calculates the rate only between the last two datapoints in an interval

**Correct answer:**
* [x] rate() calculates average rate over entire interval, irate() calculates the rate only between the last two datapoints in an interval

**51. What is the purpose of the `for` attribute in a Prometheus alert rule?**


* [ ] Determines which team `team` the alert is for 
* [ ] Determines how long a rule must be true before firing an alert
* [ ] Tells alertmanager how long to wait before firing a notification
* [ ] Specifies how frequently to fire the alert

**Correct answer:**
* [x] Determines how long a rule must be true before firing an alert

**52. What does the following `metric_relabel_config` do?**


* [ ] Changes the `datacenter` label to `location` and prepends the value with `dc-`
* [ ] removes the datacenter label
* [ ] Drops all metrics that are not from DC-1
* [ ] changes the value of the datacenter label to be prepended with `dc-`

**Correct answer:**
* [x] Changes the `datacenter` label to `location` and prepends the value with `dc-`

**Code**: 
scrape_configs:
  - job_name: example
    metric_relabel_configs:
      - source_labels: [datacenter]
        regex: (.*)
        action: replace
        target_label: location
        replacement: dc-$1


**Explaination**: The source_labels specifies the labels to match, which in this case is `datacenter.` Since the action is `replace` the label name will be swapped out with the value in target_label, and the value will be swapped out with the `replacement`. 

**53. Which of the following is not something that is tracked in a span within a trace?**


* [ ] start time
* [ ] duration
* [ ] complexity
* [ ] parent-id

**Correct answer:**
* [x] complexity

**54. What selector will match on time series whose `mountpoint` label doesn’t start with /run**


* [ ] node_filesystem_avail_bytes{mountpoint!~"/run"}
* [ ] node_filesystem_avail_bytes{mountpoint!="/run.*"}
* [ ] node_filesystem_avail_bytes{mountpoint!~"/run.*"}
* [ ] node_filesystem_avail_bytes{mountpoint!~"/run.^**"}

**Correct answer:**
* [x] node_filesystem_avail_bytes{mountpoint!~"/run.*"}

**Code**: 
node_filesystem_avail_bytes{device="/dev/sda2", fstype="vfat", instance="node1", mountpoint="/boot/efi"}​
node_filesystem_avail_bytes{device="/dev/sda2", fstype="vfat", instance="node2", mountpoint="/boot/efi"}​
node_filesystem_avail_bytes{device="/dev/sda3", fstype="ext4", instance="node1", mountpoint="/"}​
node_filesystem_avail_bytes{device="/dev/sda3", fstype="ext4", instance="node2", mountpoint="/"}​
node_filesystem_avail_bytes{device="tmpfs", fstype="tmpfs", instance="node1", mountpoint="/run"}​
node_filesystem_avail_bytes{device="tmpfs", fstype="tmpfs", instance="node1", mountpoint="/run/lock"}​
node_filesystem_avail_bytes{device="tmpfs", fstype="tmpfs", instance="node1", mountpoint="/run/snapd/ns"}​
node_filesystem_avail_bytes{device="tmpfs", fstype="tmpfs", instance="node1", mountpoint="/run/user/1000"}​
node_filesystem_avail_bytes{device="tmpfs", fstype="tmpfs", instance="node2", mountpoint="/run"}​
node_filesystem_avail_bytes{device="tmpfs", fstype="tmpfs", instance="node2", mountpoint="/run/lock"}​
node_filesystem_avail_bytes{device="tmpfs", fstype="tmpfs", instance="node2", mountpoint="/run/snapd/ns"}​
node_filesystem_avail_bytes{device="tmpfs", fstype="tmpfs", instance="node2", mountpoint="/run/user/1000"}


**Explaination**: To match on any mountpoint starting with /run a regular expression /run.* must be used. Since we want to `not` match on it use a negative regular expression matcher !~

**55. Which of the following is Prometheus’ built in dashboarding/visualization feature?**


* [ ] Go Templates
* [ ] Grafana
* [ ] client libraries
* [ ] Console Templates

**Correct answer:**
* [x] Console Templates

**Explaination**: Console templates allow you to create custom dashboards to display metrics and graphs.


**56. Which cli command can be used to verify/validate prometheus configurations?**


* [ ] prom-util validate config
* [ ] prom-cli check config
* [ ] promtool check config
* [ ] promtool validate config

**Correct answer:**
* [x] promtool check config

**Explaination**: promtool is the cli utility for performing config validation. The `check config` subcommands perform validation of the passed in the configuration


**57. What type of data should Prometheus monitor?**


* [ ] Events
* [ ] numeric
* [ ] traces
* [ ] system logs

**Correct answer:**
* [x] numeric

**58. What does the double underscore `__` before a label name signify?**


* [ ] The label was set by a pushgateway
* [ ] The label is a malformed label
* [ ] The label was set by a client library
* [ ] The label is a reserved label

**Correct answer:**
* [x] The label is a reserved label

**59. What two labels are assigned to every metric by default?**


* [ ] target, job
* [ ] target, group
* [ ] instance, group
* [ ] instance, job

**Correct answer:**
* [x] instance, job

**60. Which of the following is not a valid method for reloading alertmanager configuration?**


* [ ] restart alertmanager process
* [ ] send a SIGHUP signal to alertmanager process
* [ ] HTTP post to /-/reload endpoint
* [ ] hit the `reload config` button in alertmanager web-ui

**Correct answer:**
* [x] hit the `reload config` button in alertmanager web-ui

---


## LPIC-hardware-settings

**1. A friend is visiting you and brings a portable USB hard drive to share some files with you. You have one SATA drive in  your Linux machine. Which name will be given to your friend's portable USB hard drive in the /dev/ directory of your Linux machine?**


* [ ] /dev/sata2
* [ ] /dev/sba
* [ ] /dev/sdb
* [ ] /dev/usb1

**Correct answer:**
* [x] /dev/sdb

**Explaination**: In Linux, USB drives are treated as SATA drives and follow the same naming conventions. Your initial drive is already identified as /dev/sda, so your friend's USB hard drive will be identified as /dev/sdb.

**2. You have added a new hard drive to a system. You know that the hard drive has no defects, but the system cannot boot. Where would you go to begin troubleshooting this problem?**


* [ ] The BIOS setup utility.
* [ ] The manual for the hard drive.
* [ ] The manufacturer's website.
* [ ] An Internet forum.

**Correct answer:**
* [x] The BIOS setup utility.

**Explaination**: The BIOS setup utility is the correct place to begin troubleshooting this hardware problem. It could be that the boot device order is incorrect.

**3. You need to know which hardware devices are inside of a Linux system, but you cannot open the system's case. How would you get Linux to produce a list of hardware connected to the PCI bus?**


* [ ] cat /proc/hardware
* [ ] ls /dev/
* [ ] lspci
* [ ] modprobe

**Correct answer:**
* [x] lspci

**Explaination**: lspci gives information about hardware that is connected to the system's PCI bus.

**4. Which command will list kernel modules in use by hardware?**


* [ ] lspci -k
* [ ] lspci -m
* [ ] lspci --modules
* [ ] lspci -l

**Correct answer:**
* [x] lspci -k

**Explaination**: The -k option for lspci will list kernel modules in use by hardware.

**5. Which file would you search to determine the CPU features on a Linux machine?**


* [ ] /proc/cpuinfo
* [ ] /dev/cpuinfo
* [ ] /proc/cpu
* [ ] /proc/hardware/cpuinfo

**Correct answer:**
* [x] /proc/cpuinfo

**Explaination**: Information on the CPU and its features can be found in the cpuinfo file inside the /proc virtual filesystem (/proc/cpuinfo).

---


## LPIC-boot

**1. You have compiled a custom kernel, and the filesystem type used by your Linux system's root filesystem is compiled as a module. What would you need to include along with the kernel to make sure your system loads the root filesystem correctly?**


* [ ] initramfs
* [ ] EFI applications
* [ ] A copy of /etc/modules.conf.d/
* [ ] bootstrap

**Correct answer:**
* [x] initramfs

**Explaination**: An initial RAM filesystem (initramfs) should be included if the root filesystem is compiled as a module.

**2. Which option would you pass to journalctl to change the directory it uses to search for log files?**


* [ ] --log-directory=
* [ ] -D
* [ ] -d
* [ ] --log-dir

**Correct answer:**
* [x] -D

**Explaination**: The -D option allows you to specify a different directory for log files other than the default directory. Example: journalctl -D /mnt/external/var/log/journal.

**3. On a system equipped with BIOS, what should be placed in the MBR of the first storage device in order to boot the system?**


* [ ] bootstrap
* [ ] firmware
* [ ] initrd
* [ ] kernel

**Correct answer:**
* [x] bootstrap

**Explaination**: The bootstrap binary must be located in the MBR of the first storage device for a system equipped with BIOS to boot correctly.

**4. Which filesystem would you use for the ESP partition?**


* [ ] ext2
* [ ] fat32
* [ ] xfs
* [ ] ntfs

**Correct answer:**
* [x] fat32

**Explaination**: EFI System Partitions (ESP) should use a FAT filesystem, like FAT12, FAT16, or FAT32.

**5. Which command would you use to view only the last ten lines of the initialization log?**


* [ ] dmesg --last
* [ ] dmesg | tail
* [ ] lastlog
* [ ] dmesg -H

**Correct answer:**
* [x] dmesg | tail

**Explaination**: Directing the output of dmesg to the tail command will only show the last ten lines of the initialization log.

---


## LPIC-runlevels

**1. Given a theoretical file located at /etc/rc3.d/S45 ethernet, what would happen to any services listed in this file when the system enters runlevel 3?**


* [ ] They will stop.
* [ ] They will start.
* [ ] They will be suspended.
* [ ] They will be restarted.

**Correct answer:**
* [x] They will start.

**Explaination**: The letter "S" at the beginning of the file name indicates that services listed in this file will be started.

**2. On SysV systems, which file contains the default init configuration?**


* [ ] /etc/init.d/defaults
* [ ] /etc/defaults/inittab
* [ ] /etc/inittab
* [ ] /etc/conf.d/inittab

**Correct answer:**
* [x] /etc/inittab

**Explaination**: The default init configuration is stored in /etc/inittab on SysV systems.

**3. Which command would cause the system to shutdown 45 minutes from now?**


* [ ] shutdown 00:45
* [ ] shutdown now+45
* [ ] shutdown +45
* [ ] shutdown +45M

**Correct answer:**
* [x] shutdown +45

**Explaination**: Specifying +45 after the shutdown command will cause the system to shutdown 45 minutes from the time the command is run.

**4. Using systemctl, how would you determine if a service named myservice.service is configured to start when the system boots?**


* [ ] systemctl status myservice.service
* [ ] systemctl is-enabled myservice.service
* [ ] systemctl is-active myservice.service
* [ ] systemctl on-boot myservice.service

**Correct answer:**
* [x] systemctl is-enabled myservice.service

**Explaination**: The is-enabled option for systemctl will return "enabled" if a service is enabled to start at boot, or "disabled" if it is not.

**5. If you want to warn other users of a Linux system that the system is going down for maintenance, which command could you use to send a message to all logged-in users?**


* [ ] write
* [ ] wall
* [ ] warn
* [ ] wipe

**Correct answer:**
* [x] wall

**Explaination**: The wall command is use to send messages to all users logged into a system.

---


## LPIC-disk-layout

**1. Where are the files for the GRUB bootloader stored?**


* [ ] /etc/grub/
* [ ] /boot/
* [ ] /boot/grub/
* [ ] /grub/boot/

**Correct answer:**
* [x] /boot/grub/

**Explaination**: The files for the GRUB bootloader are stored in the /boot/grub/ directory.

**2. When a Linux system mounts the EFI partition, where is it usually mounted?**


* [ ] /boot/efi/
* [ ] /boot/grub/efi/
* [ ] /mnt/efi/
* [ ] /efi/

**Correct answer:**
* [x] /boot/efi/

**Explaination**: Linux usually mounts the EFI partition at /boot/efi/.

**3. Which formula would give the size of a Logical Volume?**


* [ ] Size of physical extents divided by number of extents
* [ ] Number of extents divided by size of physical extents
* [ ] Number of extents multiplied by physical extent size
* [ ] Disk sectors multiplied by number of extents

**Correct answer:**
* [x] Number of extents multiplied by physical extent size

**Explaination**: The size of a Logical Volume is equal to the number of physical extents multiplied by the total number of extents on the volume.

**4. In addition to swap partitions, Linux can make use of swap files. Where is the swap file typically located?**


* [ ] /mnt/swap
* [ ] /swapfile
* [ ] /mnt/swapfile
* [ ] /swap

**Correct answer:**
* [x] /swapfile

**Explaination**: If used, a swap file is typically located at /swapfile on Linux systems.

**5. Volume Groups are divided into:**


* [ ] Extents
* [ ] Sectors
* [ ] Slices
* [ ] Chunks

**Correct answer:**
* [x] Extents

**Explaination**: Volume groups are divided into extents.

---


## LPIC-boot-manager

**1. When using GRUB Legacy, how is the first partition of the first disk labeled?**


* [ ] (hd1,1)
* [ ] (hd0,1)
* [ ] (hd0,0)
* [ ] (hd1,0)

**Correct answer:**
* [x] (hd0,0)

**Explaination**: GRUB Legacy begins counting from 0, so the first partition of the first drive would be labeled as (hd0,0).

**2. To change the menu entries for GRUB Legacy, which file would you edit?**


* [ ] /boot/grub/menu.lst
* [ ] /etc/grub/grub.cfg
* [ ] /etc/grub/menu.lst
* [ ] /boot/grub/grub.cfg

**Correct answer:**
* [x] /boot/grub/menu.lst

**Explaination**: Menu entries for GRUB Legacy are stored in /boot/grub/menu.lst.

**3. When using GRUB 2, which menu entry parameter determines how long GRUB 2 will pause before booting the default menu entry?**


* [ ] GRUB_TIMEOUT=
* [ ] GRUB_WAIT=
* [ ] GRUB_PAUSE=
* [ ] GRUB_TIMER=

**Correct answer:**
* [x] GRUB_TIMEOUT=

**Explaination**: The menu entry that controls how long GRUB 2 will pause before booting the default menu entry is GRUB_TIMEOUT=.

**4. GRUB 2 stores its configuration file at:**


* [ ] /etc/grub/grub.cfg
* [ ] /etc/grub.d/
* [ ] /boot/grub/grub.cfg
* [ ] /boot/grub.d/

**Correct answer:**
* [x] /etc/grub/grub.cfg

**Explaination**: The default configuration file for GRUB 2 is /etc/grub/grub.cfg.

**5. If you update the configuration for GRUB 2, which command would you run to write that configuration to the correct directory for GRUB 2 to use the next time you boot the system?**


* [ ] grub-config -o /boot/grub/grub.cfg
* [ ] grub-mkconfig -o /boot/grub/grub.cfg
* [ ] grub-mkconfig -o /boot/grub/grub.conf
* [ ] grub-config -o /boot/grub/grub.conf

**Correct answer:**
* [x] grub-mkconfig -o /boot/grub/grub.cfg

**Explaination**: After updating the configuration for GRUB 2, you must run grub-mkconfig -o /boot/grub/grub.cfg to make GRUB 2 use the changes on the next system boot.

---


## LPIC-shared-libraries

**1. The naming format for shared libraries is:**


* [ ] libraryname.so.versionnumber
* [ ] libraryname.versionnumber.so
* [ ] libraryname-versionnumber.so
* [ ] libraryname-so.versionnumber

**Correct answer:**
* [x] libraryname.so.versionnumber

**Explaination**: The naming format for shared libraries is the library name . so suffix . version number. Example: libpthread.so.1.

**2. You have added a new shared library directory to your system, and you have written a custom configuration file including the full path to to the new shared library location. Which directory would you put the configuration file into?**


* [ ] /usr/share/
* [ ] /etc/share
* [ ] /etc/ld.so.conf.d/
* [ ] /usr/ld.so.conf.d/

**Correct answer:**
* [x] /etc/ld.so.conf.d/

**Explaination**: Configuration files that define shared library paths go in the /etc/ld.so.conf.d/ directory.

**3. Static libraries have which file extension?**


* [ ] .lib
* [ ] .a
* [ ] .out
* [ ] .aout

**Correct answer:**
* [x] .a

**Explaination**: Static libraries use the .a file extension.

**4. After adding a new shared library directory to your system, you copied a configuration file with its full path to the correct directory. Which command would you use to notify the system of the change?**


* [ ] ldd
* [ ] ldconfig
* [ ] libupdate
* [ ] updatedb

**Correct answer:**
* [x] ldconfig

**Explaination**: When making changes to shared libraries on the system, you must run ldconfig to notify the system of the changes.

**5. Which command would you use to list the shared libraries used by the /usr/bin/systemctl program?**


* [ ] which /usr/bin/systemctl
* [ ] lib /usr/bin/systemctl
* [ ] ldd /usr/bin/systemctl
* [ ] shared /usr/bin/systemctl

**Correct answer:**
* [x] ldd /usr/bin/systemctl

**Explaination**: The ldd command can be used to list all of the shared libraries used by a program.

---


## LPIC-debian-packages

**1. Which command would you use to install a .deb file located on your local Linux system?**


* [ ] dpkg -I
* [ ] dpkg -i
* [ ] dpkg install
* [ ] dpkg -install

**Correct answer:**
* [x] dpkg -i

**Explaination**: To install a .deb file located on a local machine, you would you use the dpkg -i command. It is also possible to use dpkg --install (with two dashes).

**2. Which apt-cache command will provide information for a package?**


* [ ] apt-cache show
* [ ] apt-cache info
* [ ] apt-cache print
* [ ] apt-cache search

**Correct answer:**
* [x] apt-cache show

**Explaination**: The apt-cache show command, followed by a package name, will provide information for a package.

**3. Which apt-file command would show the package that contains the file /usr/bin/systemctl?**


* [ ] apt-file search /usr/bin/systemctl
* [ ] apt-file show /usr/bin/systemctl
* [ ] apt-file provides /usr/bin/systemctl
* [ ] apt-file query /usr/bin/systemctl

**Correct answer:**
* [x] apt-file search /usr/bin/systemctl

**Explaination**: You can use apt-file search followed by the full path to a file to show which package contains that file.

**4. Which parameter would you add to dpkg-query to show which package contains a particular file?**


* [ ] -s
* [ ] -S
* [ ] -l
* [ ] -W

**Correct answer:**
* [x] -S

**Explaination**: Using the -S parameter with dpkg-query will show which package contains a particular file.

**5. On a Debian system, which command would remove a package and all of its configuration files?**


* [ ] dpkg -r
* [ ] dpkg --remove
* [ ] dkpg --erase
* [ ] dpkg -P

**Correct answer:**
* [x] dpkg -P

**Explaination**: Running dpkg with the -P (purge) flag will remove a package and all of its configuration files.

---


## LPIC-rpm-packages

**1. Using DNF, how would you install the httpd package?**


* [ ] dnf install httpd
* [ ] dnf -i httpd
* [ ] dnf httpd
* [ ] dnf provide httpd

**Correct answer:**
* [x] dnf install httpd

**Explaination**: When using DNF, the dnf install command, followed by a package name, will install that package. Example: dnf install httpd.

**2. Using YUM or DNF, which option would uninstall a package from the system, along with any packages that depend on that package?**


* [ ] remove
* [ ] purge
* [ ] uninstall
* [ ] delete

**Correct answer:**
* [x] remove

**Explaination**: Using YUM or DNF, the remove option will uninstall a package from the system, along with all packages that depend on it. Example: dnf remove httpd.

**3. Which yum command will bring repository metadata to the most recent version?**


* [ ] yum update
* [ ] yum metadata
* [ ] yum upgrade
* [ ] yum check-metadata

**Correct answer:**
* [x] yum update

**Explaination**: The yum update command will update repository metadata.

**4. Using zypper, how would you find out which package provides the file /usr/bin/systemctl?**


* [ ] zypper se /usr/bin/systemctl
* [ ] zypper se --provides /usr/bin/systemctl
* [ ] zypper --provides /usr/bin/systemctl
* [ ] zypper se provides /usr/bin/systemctl

**Correct answer:**
* [x] zypper se --provides /usr/bin/systemctl

**Explaination**: When using zypper, the se (search) command, followed by --provides and the file path will show which package provides that file. Example: zypper se --provides /usr/bin/systemctl.

**5. Where are repository files stored for DNF and YUM?**


* [ ] /etc/repos.d/
* [ ] /etc/yum.repos.d/
* [ ] /etc/yum/repos/
* [ ] /etc/yum/repos.d/

**Correct answer:**
* [x] /etc/yum.repos.d/

**Explaination**: Repository files (.repo) are stored in /etc/yum.repos.d/.

---


## LPIC-virtualization

**1. Intel VT-x and AMD-V are examples of _____ used to provide support for fully virtualized guests?**


* [ ] CPU extensions
* [ ] paravirtualized drivers
* [ ] hypervisors
* [ ] virtual machines

**Correct answer:**
* [x] CPU extensions

**Explaination**: Intel VT-x and AMD-V are CPU extensions.

**2. Which type of virtualization requires the guest to be capable of running all instructions on virtual hardware?**


* [ ] full virtualization
* [ ] paravirtualization
* [ ] hybrid virtualization
* [ ] bare metal

**Correct answer:**
* [x] full virtualization

**Explaination**: Fully virtualized machines must be able to run all instructions on the virtual hardware. They are not aware that they are running as virtual machines.

**3. Which program is used to start Linux virtual instances in a cloud environment?**


* [ ] upstart
* [ ] kickstart
* [ ] cloud-start
* [ ] cloud-init

**Correct answer:**
* [x] cloud-init

**Explaination**: Linux virtual instances for the cloud can be started with cloud-init.

**4. KVM is a _____ hypervisor. Select all that apply.**


* [ ] Type-1
* [ ] Type-2
* [ ] Bare metal
* [ ] Hybrid

**Correct answer:**
* [x] Type-2
* [x] Type-1

**Explaination**: Kernel Virtual Machine (KVM) is a Type-1 and Type-2 hypervisor. 

**5. Which virtualization method provides the highest performance?**


* [ ] Full virtualization
* [ ] Paravirtualization
* [ ] VirtualBox
* [ ] Virsh

**Correct answer:**
* [x] Paravirtualization

**Explaination**: Paravirtualization provides the best performance through the use of paravirtualized drivers for networking and storage.

---


## LPIC-command-line-1

**1. Before the apropos command will work on a new system, it may be necessary to run another command first. Which command would you run to make sure apropos has access to all of the information it needs?**


* [ ] apropos -update
* [ ] updatedb
* [ ] dbupdate
* [ ] mandb

**Correct answer:**
* [x] mandb

**Explaination**: The mandb command must be run to update the database for apropos. Otherwise, apropos will not have any results to return.

**2. Which command is used to search the short description of man pages for keywords?**


* [ ] apropos
* [ ] grep
* [ ] find
* [ ] cat

**Correct answer:**
* [x] apropos

**Explaination**: Each manual page has a short description available within it and apropos searches the descriptions for instances of keyword.

**3. If you are unsure of the location of an executable mycommand on a Linux system, how would you find it?**


* [ ] whereis mycommand
* [ ] which mycommand
* [ ] find mycommand
* [ ] fetch mycommand

**Correct answer:**
* [x] which mycommand

**Explaination**: The which command provides the location of a specified executable. While it is possible to use find to locate the executable, it would require more options than were shown with that answer choice, and is not as straightforward or fast as using which for this task.

**4. To create an empty file named emptyfile in the current working directory, which command would you use?**


* [ ] touch emptyfile
* [ ] new emptyfile
* [ ] echo emptyfile
* [ ] blank emptyfile

**Correct answer:**
* [x] touch emptyfile

**Explaination**: The touch command can be used to create empty files.

**5. Which command will cause Bash to print the path of the current directory?**


* [ ] whereami
* [ ] which directory
* [ ] cwd
* [ ] pwd

**Correct answer:**
* [x] pwd

**Explaination**: To print the current working directory, use the pwd command.

---


## LPIC-command-line-2

**1. You have set a variable called kodekloud using the export command. Which command would print the value of the kodekloud variable?**


* [ ] print kodekloud
* [ ] echo kodekloud
* [ ] echo $kodekloud
* [ ] read $kodekloud

**Correct answer:**
* [x] echo $kodekloud

**Explaination**: The echo command will print the value of a variable, and the variable name must have a $ before it. Example: echo $kodekloud will return the value assigned to the kodekloud variable.

**2. The _____ command can be used to assign values to environment variables for the current shell session.**


* [ ] export
* [ ] env
* [ ] echo
* [ ] import

**Correct answer:**
* [x] export

**Explaination**: The export command is used to assign values to environment variables for the current shell session.

**3. The _____ command can be used to clear environment variables for the current shell session.**


* [ ] clear
* [ ] unset
* [ ] import
* [ ] echo

**Correct answer:**
* [x] unset

**Explaination**: The unset command is used to clear environment variable values for the current shell session.

**4. Which command would return the list of directories used by the shell to find executable commands?**


* [ ] echo $PATH
* [ ] print $PATH
* [ ] read $PATH
* [ ] pwd $PATH

**Correct answer:**
* [x] echo $PATH

**Explaination**: The command echo $PATH would return a list of directories used by the shell to find executable commands.

**5. The _____ command will print a list of all current environment variables.**


* [ ] env
* [ ] path
* [ ] export
* [ ] import

**Correct answer:**
* [x] env

**Explaination**: The env command will print a list of all current environment variables.

---


## LPIC-filters

**1. The _____ command can be used to perform search and replace operations on text files.**


* [ ] grep
* [ ] sed
* [ ] find
* [ ] substitute

**Correct answer:**
* [x] sed

**Explaination**: The sed command can be used to perform search and replace operations on text files.

**2. Which command can be used to search file content using regular expression patterns?**


* [ ] grep
* [ ] find
* [ ] read
* [ ] search

**Correct answer:**
* [x] grep

**Explaination**: The grep command can be used to search file content using regular expressions. Grep stands for "get regular expression."

**3. Which command can be used to print only the desired field from text?**


* [ ] sed
* [ ] grep
* [ ] cut
* [ ] find

**Correct answer:**
* [x] cut

**Explaination**: The cut command can specify which field(s) to print from text.

**4. The _____ command will print the contents of a text file to standard output.**


* [ ] cat
* [ ] print
* [ ] grep
* [ ] wc

**Correct answer:**
* [x] cat

**Explaination**: The cat command will print the contents of a text file to standard output.

**5. Which command sequence would put the contents of a text file named myfile.txt in alphabetical order and remove all duplicates? Select all that apply.**


* [ ] sort myfile.txt | uniq
* [ ] sort myfile.txt | sed -s 'duplicates//g'
* [ ] uniq myfile.txt | sort
* [ ] sed -i -s 'duplicates//g' myfile.txt && sort myfile.txt

**Correct answer:**
* [x] sort myfile.txt | uniq

**Explaination**: The sort command will place contents in alphabetical order and the uniq command will print only unique entries (remove duplicates). 

---


## LPIC-file-management-1

**1. Which option(s) would you use with the ls command to print file sizes in human readable format?**


* [ ] ls -h
* [ ] ls --human
* [ ] ls -lh
* [ ] ls -l --human

**Correct answer:**
* [x] ls -lh

**Explaination**: The -h option for ls lists file sizes in human readable format, and it must always be used with the -l option. Therefore, ls -lh is the correct answer.

**2. To remove a directory called mydir along with all of its files and subdirectories, which command would you use?**


* [ ] rm mydir
* [ ] rm -R mydir
* [ ] rm -rf mydir
* [ ] rm mydir/*

**Correct answer:**
* [x] rm -rf mydir

**Explaination**: To remove a directory along with its files and subdirectories, we have to specify the -r (recursive) option, and will also need to specify the -f (force) option.

**3. Which command can be used to rename an existing file or directory?**


* [ ] mv
* [ ] cp
* [ ] touch
* [ ] ls

**Correct answer:**
* [x] mv

**Explaination**: The mv command can be used to rename an existing file or directory by "moving" it from the original name to the new name. Example: mv myfile myfile2.

**4. Which command(s) would list all of the files in the current directory that start with the word "file" followed by one number and .txt?**


* [ ] ls file[0-9].txt
* [ ] ls | grep 'file[0-9].txt'
* [ ] ls file*.txt
* [ ] ls | grep 'file*.txt'

**Correct answer:**
* [x] ls file[0-9].txt
* [x] ls | grep 'file[0-9].txt'

**Explaination**: Using the bracketed range [0-9] with either ls or grep as shown above will match exactly one number. Using the * wildcard would match zero or more numbers, and would not produce a list that contained only filenames that started with file followed by one number and .txt. For example, that sequence would also match "file.txt" and "file10.txt".

**5. Which option can be used with ls to show hidden files?**


* [ ] -a
* [ ] -h
* [ ] --hidden
* [ ] -?

**Correct answer:**
* [x] -a

**Explaination**: The -a option with ls will show "all" including hidden files that begin with a period (.).

---


## LPIC-file-management-2

**1. Which of the following commands could be used to back up an entire disk, including its filesystem?**


* [ ] dd
* [ ] tar
* [ ] cpio
* [ ] gzip

**Correct answer:**
* [x] dd

**Explaination**: The dd command can be used to back up an entire disk, including its file system. Example: dd if=/dev/sdb of=/mnt/backups/backup.dd bs=4096.

**2. Which command could be used to create an archive named backup.cpio containing all of the files and directories in the current working directory?**


* [ ] cpio -o > backup.cpio
* [ ] ls | cpio -o > backup.cpio
* [ ] cpio * > backup.cpio
* [ ] cpio -o * > backup.cpio

**Correct answer:**
* [x] ls | cpio -o > backup.cpio

**Explaination**: Since cpio takes its file list as input from standard input, we would use the output of the ls command to list the files and directories in the current working directory and pass that to cpio as input. The -o option with cpio tells cpio to create an archive, and the > operator is used before the archive's filename to direct cpio to create the archive with that name.

**3. Which option(s) would you use with tar to extract the archive located at /home/kodekloud/archive.tar.gz?**


* [ ] cvf
* [ ] xf
* [ ] xfz
* [ ] cfvz

**Correct answer:**
* [x] xf
* [x] xfz

**Explaination**: The options for extract (x) and file (f) would be required. The verbose (v) option is optional, and the option for gzip (z) is only required to create a .gzip compressed archive, not to extract one.

**4. Which of the following find command would find files in the current directory that have an extension of .bak and are larger than 1 gigabyte?**


* [ ] find . -name *.bak -size +1G
* [ ] find pwd -name *.bak -size -1G
* [ ] find . -name *.back -size 1G
* [ ] find . -name *.bak -size =1G+

**Correct answer:**
* [x] find . -name *.bak -size +1G

**Explaination**: The command find . -name *.bak -size +1G would: search the current directory (.) for files with any name followed by the .bak extension (-name *.bak), with a size that is 1 gigabyte or more (-size +1G).

**5. Which command would take the contents of the /home/kodekloud/ directory and create a gzipped tar archive of the contents in a file called kodekloud.tar.gz?**


* [ ] tar -czvf kodekloud.tar.gz /home/kodekloud/
* [ ] tar -czvf /home/kodekloud/ kodekloud.tar.gz
* [ ] tar -cvf kodekloud.tar.gz /home/kodekloud/
* [ ] tar -cvf /home/kodekloud/ kodekloud.tar.gz

**Correct answer:**
* [x] tar -czvf kodekloud.tar.gz /home/kodekloud/

**Explaination**: When using tar to create gzipped tar archives, we must use the create (c), gzip (z), and file (f) options, followed by the name of the file we wish to create (kodekloud.tar.gz), and finally, the directory or files we wish to archive and compress (/home/kodekloud/).

---


## LPIC-streams-1

**1. Which of the following would search the contents of the /etc/ directory for the pattern "kodekloud" and write only the errors to a file a /home/kodekloud/errors.list?**


* [ ] grep -r 'kodekloud' /etc/ > /home/kodekloud/errors.list
* [ ] grep -r 'kodekloud' /etc/ 2> /home/kodekloud/errors.list
* [ ] grep -r 'kodekloud' /etc/ 2>&1 /home/kodekloud/errors.list
* [ ] grep -r 'kodekloud' /etc/ 1> /home/kodekloud/errors.list

**Correct answer:**
* [x] grep -r 'kodekloud' /etc/ 2> /home/kodekloud/errors.list

**Explaination**: The operator "2>" would cause stderr to be written to the file at "/home/kodekloud/errors.list" while the stdout content would be displayed to the screen.

**2. Which operator would be used to designate a herestring?**


* [ ] <
* [ ] <<
* [ ] <<<
* [ ] <<<<

**Correct answer:**
* [x] <<<

**Explaination**: The "<<<" operator designates a herestring.

**3. Which symbol(s) would be used to specify a stdout redirect? Select all that apply.**


* [ ] 1>
* [ ] 2>
* [ ] 0>
* [ ] \>

**Correct answer:**
* [x] 1>
* [x] \>

**Explaination**: Stdout is implied by the ">" operator. Stdout is also designated by "1", therefore "1>" also refers to stdout.

**4. Which command would search the /etc/ directory for the pattern "kodekloud" and redirect all output (stdout and stderr) to the file at /home/kodekloud/output.list?**


* [ ] grep -r 'kodekloud' /etc/ 2> /home/kodekloud/output.list
* [ ] grep -r 'kodekloud' /etc/ 1> /home/kodekloud/output.list
* [ ] grep -r 'kodekloud' /etc/ > /home/kodekloud/output.list
* [ ] grep -r 'kodekloud' /etc/ \&> /home/kodekloud/output.list

**Correct answer:**
* [x] grep -r 'kodekloud' /etc/ \&> /home/kodekloud/output.list

**Explaination**: The operator "&>" will direct both stderr and stdout to a file. This will cause the file to contain all of the contents that would normally be seen on the screen, including error messages.

**5. Which operator can be used to append text to a file, without overwriting the existing contents?**


* [ ] ">"
* [ ] ">>"
* [ ] "<<"
* [ ] "<"

**Correct answer:**
* [x] ">>"

**Explaination**: The ">>" operator will append text to a file without overwriting the existing contents.

---


## LPIC-streams-2

**1. _____ is an intermediary program used to pass the output from one program as arguments to another program.**


* [ ] xargs
* [ ] xout
* [ ] xin
* [ ] xinput

**Correct answer:**
* [x] xargs

**Explaination**: The xargs program can be used as an intermediary program to pass the output of one program as arguments to another.

**2. Given that the "whoami" command prints the current user's username, what would be the result of the following command: rm -rf /home/$(whoami)**


* [ ] It would delete the directory /home/whoami
* [ ] It would delete the user's home directory
* [ ] Nothing; this is not a valid command
* [ ] An endless loop

**Correct answer:**
* [x] It would delete the user's home directory

**Explaination**: Since the "whoami" command returns the current user's username, the command would delete the user's home directory (and all subdirectories and files). Example: for the user "kodekloud" this would delete "/home/kodekloud".

**3. The output of _____ is passed as an argument to _____ by xargs in the following command: ps -ef | grep 'httpd' | xargs kill**


* [ ] ps to kill
* [ ] grep to kill
* [ ] ps to grep
* [ ] httpd to kill

**Correct answer:**
* [x] grep to kill

**Explaination**: The initial output from "ps" is passed to "grep." The output of "grep" is passed to "kill" by xargs. Therefore, the output of "grep" is passed by "xargs" to the "kill" command.

**4. Which operator(s) can be used for command substitution? Select all that apply.**


* [ ] ``
* [ ] $()
* [ ] %{}
* [ ] ""

**Correct answer:**
* [x] ``
* [x] $()

**Explaination**: Placing a command inside of backquotes (``) or inside of the parenthesis in $() will cause the shell to use the output of the command in that place. Example: echo "Today is $(date) and this system is `uname -a`." would  produce: Today is Mon Dec 19 02:48:03 PM CST 2022 and this system is Linux kodekloud 5.14.0-210.el9.x86_64 #1 SMP PREEMPT_DYNAMIC Fri Dec 9 20:01:51 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux.

**5. The standard operator used to pass the output of one program to the input of another is:**


* [ ] |
* [ ] &
* [ ] @
* [ ] %

**Correct answer:**
* [x] |

**Explaination**: The vertical pipe (|) is used to pass the output of one command to the input of another. Example: cat /proc/cpuinfo | grep 'bugs'.

---


## LPIC-processes

**1. To view processes which have been sent to the background, we can use the _____ command.**


* [ ] bg
* [ ] fg
* [ ] jobs
* [ ] cron

**Correct answer:**
* [x] jobs

**Explaination**: The jobs command will show a list of processes that have been sent to the background or stopped.

**2. To view dynamically updated information on system processes and resources, we can use the _____ command.**


* [ ] top
* [ ] ps
* [ ] jobs
* [ ] uptime

**Correct answer:**
* [x] top

**Explaination**: The top command provides dynamically updated information on processes and system resources, such as CPU usage and RAM.

**3. To run a command detached from the current session, we can use the _____ command.**


* [ ] nohup
* [ ] SIGTERM
* [ ] jobs
* [ ] bg

**Correct answer:**
* [x] nohup

**Explaination**: The "no hangup" (nohup) command runs another command detached from the current session. This is often paired with the & symbol to send the command to the background. Example: nohup ping 8.8.8.8 &.

**4. Which command(s) can we use to view the priority of a process that is already running? Select all that apply.**


* [ ] ps
* [ ] top
* [ ] nice
* [ ] renice

**Correct answer:**
* [x] ps
* [x] top

**Explaination**: Both the ps command (with the -el or -Al flags) and the top command can be used to view the priority of a process that is already running.

**5. To modify a the priority of an existing process, we can use the _____ command.**


* [ ] renice
* [ ] nice
* [ ] ps
* [ ] top

**Correct answer:**
* [x] renice

**Explaination**: The renice command allows us to change the priority ("niceness") of a process that is already running. Example: renice -10 -p 1127.

**6. To get detailed information about a process when we already have the PID (process ID), we can use the _____ command.**


* [ ] proc
* [ ] pgrep
* [ ] ps
* [ ] pkill

**Correct answer:**
* [x] ps

**Explaination**: The ps command provides detailed information about a process when we already know the PID. Example: ps 1127.

**7. To set the priority for a process when it is run, we can use the _____ command.**


* [ ] nice
* [ ] renice
* [ ] top
* [ ] ps

**Correct answer:**
* [x] nice

**Explaination**: The nice command allows us to set the process priority ("niceness") when a command is run. Example: nice -n 15 top.

**8. To bring a job with job ID 1 from the background to the foreground, which command could we run?**


* [ ] fg %1
* [ ] fg $1
* [ ] bg %1
* [ ] bg $1

**Correct answer:**
* [x] fg %1

**Explaination**: We can use the fg command, followed by % and the job number to bring a process from the background to the foreground.

**9. True or false: A regular user can only lower the process niceness one time.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**Explaination**: A normal user can only lower the niceness of a process one time. The root user can do so multiple times.

**10. Which user account(s) can lower the niceness of a process to a value less than zero? Select all that apply.**


* [ ] normal users
* [ ] the root user
* [ ] any user with sudo access
* [ ] only the user who started the process

**Correct answer:**
* [x] the root user
* [x] any user with sudo access

**Explaination**: Only the root user or a user with sudo access using the sudo command can lower the niceness of a process below zero.

---


## LPIC-screen-tmux

**1. The default configuration file for tmux is located at _____.**


* [ ] /etc/tmux.conf.d/tmux.conf
* [ ] /etc/tmux.conf
* [ ] /etc/conf/tmux.conf
* [ ] /etc/tmux/conf/tmux.conf

**Correct answer:**
* [x] /etc/tmux.conf

**Explaination**: The default configuration file for tmux is located at /etc/tmux.conf.

**2. The _____ terminal multiplexer uses a client-server model.**


* [ ] GNU screen
* [ ] tmux

**Correct answer:**
* [x] tmux

**Explaination**: The tmux terminal multiplexer uses a client-server model.

**3. The default configuration file for GNU screen is located at _____.**


* [ ] /etc/screenrc
* [ ] /etc/screen/screenrc
* [ ] /etc/screen.conf.d/screenrc
* [ ] /etc/screen/conf/screenrc

**Correct answer:**
* [x] /etc/screenrc

**Explaination**: The default configuration file for GNU screen is located at /etc/screenrc.

**4. The default command prefix for GNU screen is:**


* [ ] CTRL + a
* [ ] CTRL + b
* [ ] CTRL + v
* [ ] CTRL + s

**Correct answer:**
* [x] CTRL + a

**Explaination**: The default command prefix for GNU screen is CTRL + a.

**5. The default command prefix for tmux is:**


* [ ] CTRL + a
* [ ] CTRL + b
* [ ] CTRL + v
* [ ] CTRL + s

**Correct answer:**
* [x] CTRL + b

**Explaination**: The default command prefix for tmux is CTRL + b.

---


## LPIC-regular-expressions-1

**1. Which regular expression symbol matches a single instance of a any character?**


* [ ] .
* [ ] *
* [ ] $
* [ ] ^

**Correct answer:**
* [x] .

**Explaination**: The period (.) matches exactly one instance of any character.

**2. Which regular expression symbol matches any characters from a list provided with that symbol?**


* [ ] [ ]
* [ ] ( )
* [ ] *
* [ ] ?

**Correct answer:**
* [x] [ ]

**Explaination**: Brackets ([ ]) provide a list of characters, and match any characters provided in the brackets. Example grep 'l[oe]t' would match "lot" and "let."

**3. Which of the following utilities makes use of regular expressions? Select all that apply.**


* [ ] grep
* [ ] egrep
* [ ] sed
* [ ] ps

**Correct answer:**
* [x] grep
* [x] egrep
* [x] sed

**Explaination**: The grep, egrep, and sed utilities make use of regular expressions for pattern matching.

**4. Which regular expression symbol indicates that a line ends with a regular expression pattern?**


* [ ] ^
* [ ] $
* [ ] .
* [ ] *

**Correct answer:**
* [x] $

**Explaination**: The dollar sign ($) is used to indicate that a line ends with a regular expression pattern.

**5. Which regular expression symbol indicates a line that begins with a regular expression pattern?**


* [ ] ^
* [ ] $
* [ ] .
* [ ] *

**Correct answer:**
* [x] ^

**Explaination**: The caret (^) is used to indicate that a line begins with a regular expression pattern.

---


## LPIC-regular-expressions-2

**1. When using grep, we should always enclose our regular expression inside of _____.**


* [ ] ' '
* [ ] ( )
* [ ] { }
* [ ] [ ]

**Correct answer:**
* [x] ' '

**Explaination**: We should always enclose our regular expressions in single quotes (' ') when using grep.

**2. Which option for sed indicates that every instance of a term should be replaced?**


* [ ] g
* [ ] *
* [ ] a
* [ ] ?

**Correct answer:**
* [x] g

**Explaination**: The global (g) option with sed indicates that every instance of a term should be replaced. Example: sed  's/day/night/g'.

**3. Which option can be used with sed to edit the file as-is, without specifying a different file name as a target?**


* [ ] -i
* [ ] -e
* [ ] --here
* [ ] --same

**Correct answer:**
* [x] -i

**Explaination**: The in-place (-i) option with sed can be used to edit a file without specifying a different file name for output.

**4. Which grep option can be used to search an entire directory and its sub directories?**


* [ ] -r
* [ ] *
* [ ] -R
* [ ] .

**Correct answer:**
* [x] -r

**Explaination**: The recursive (-r) option with grep will cause grep to search an entire directory and its sub directories. Example: grep -r 'kodekloud' /etc/.

**5. Which option can be used with grep when the case (upper or lower) of the term does not matter?**


* [ ] -i
* [ ] -e
* [ ] -f
* [ ] -k

**Correct answer:**
* [x] -i

**Explaination**: The ignore-case (-i) option can be used when the case of the term does not matter.

---


## LPIC-file-editing

**1. In command mode, which command will allow us to save changes and exit vi?**


* [ ] :wq
* [ ] :q
* [ ] :q!
* [ ] :w

**Correct answer:**
* [x] :wq

**Explaination**: The command :wq or "write and quite" will allow us to save changes and exit vi.

**2. When using vi, in normal mode, which key allows us to search the file?**


* [ ] /
* [ ] s
* [ ] f
* [ ] t

**Correct answer:**
* [x] /

**Explaination**: The / character allows us to specify a search term while in normal mode.

**3. In vi normal mode, the pp command will paste. Which command can be used to copy?**


* [ ] cc
* [ ] yy
* [ ] :copy
* [ ] :yank

**Correct answer:**
* [x] yy

**Explaination**: The "yy" command will copy (yank) text.

**4. When using vi in normal mode, which command will delete the line at the cursor?**


* [ ] d
* [ ] dd
* [ ] yy
* [ ] ZZ

**Correct answer:**
* [x] dd

**Explaination**: Pressing "d" twice or "dd" will delete the line at the current cursor position.

**5. In addition to vi, what are some other common editors for Linux?**


* [ ] nano
* [ ] Emacs
* [ ] Word
* [ ] notes

**Correct answer:**
* [x] nano
* [x] Emacs

**Explaination**: GNU nano and Emacs are common editors in addition to vi.

---


## LPIC-filesystems

**1. Which command(s) can be used to work with partitions? Select all that apply.**


* [ ] fdisk
* [ ] gdisk
* [ ] parted
* [ ] vdisk

**Correct answer:**
* [x] fdisk
* [x] gdisk
* [x] parted

**Explaination**: The fdisk, gdisk, and parted commands can all be used to work with partitions.

**2. Which methods can be used on Linux to move memory pages from RAM to the hard disk?**


* [ ] swap partition
* [ ] swap file
* [ ] virtual memory
* [ ] disk cache

**Correct answer:**
* [x] swap partition
* [x] swap file

**Explaination**: Swap partitions and swap files can be used to move memory pages from RAM to the hard disk.

**3. Which command is used to create filesystems?**


* [ ] mkswap
* [ ] mkfs
* [ ] fdisk
* [ ] parted

**Correct answer:**
* [x] mkfs

**Explaination**: The mkfs command can be used to create a variety of filesystems on existing partitions.

**4. Which command can be used to determine how much disk space is available?**


* [ ] du
* [ ] df
* [ ] free
* [ ] diskfree

**Correct answer:**
* [x] df

**Explaination**: The "disk free" (df) command can be used to see how much free space is left on a disk.

**5. Which command can be used to show how much space is currently taken up on a disk and display that output in human-readable format?**


* [ ] du -h
* [ ] df -h
* [ ] free -h
* [ ] diskfree -h

**Correct answer:**
* [x] du -h

**Explaination**: The disk usage (du) command with the human-readable (-h) flag can be used to see how much disk space is currently taken up and display it in a human-readable format.

**6. Which command will check a filesystem for errors and repair it if it is offline?**


* [ ] fsck
* [ ] fdisk
* [ ] sfc
* [ ] chkdsk

**Correct answer:**
* [x] fsck

**Explaination**: The filesystem checker (fsck) will check a filesystem for errors, and can be used to repair the filesystem while it is offline (not mounted).

**7. The two types of partition tables commonly used in Linux are (select two):**


* [ ] MBR
* [ ] GPT
* [ ] DVH
* [ ] BSD

**Correct answer:**
* [x] MBR
* [x] GPT

**Explaination**: Linux commonly uses MBR and GPT partition tables (though others are supported).

**8. Which of the following are features of BTRFS? Select all that apply.**


* [ ] compression
* [ ] subvolumes
* [ ] snapshots
* [ ] encryption

**Correct answer:**
* [x] compression
* [x] subvolumes
* [x] snapshots

**Explaination**: BTRFS supports compression, subvolumes, and snapshots.

**9. Before checking a filesystem for errors and repairing any that are found, you should first use the ____ command on the filesystem to get it ready for this process.**


* [ ] mount
* [ ] fsck
* [ ] xfs_repair
* [ ] umount

**Correct answer:**
* [x] umount

**Explaination**: A filesystem must first be offline for errors to be repaired, so a mounted filesystem must be unmounted before running fsck or xfs_repair.

**10. Which utility can be used to check and repair XFS filesystems?**


* [ ] xfs_repair
* [ ] xfs_fsck
* [ ] xfs_check
* [ ] xfs_fix

**Correct answer:**
* [x] xfs_repair

**Explaination**: The xfs_repair command can be used to check and repair XFS filesystems.

---


## LPIC-mount-unmount

**1. A list of filesystems to be mounted when the system boots can be found in _____.**


* [ ] /etc/fstab
* [ ] /etc/fs
* [ ] /boot/fstab
* [ ] /boot/fs

**Correct answer:**
* [x] /etc/fstab

**Explaination**: The /etc/fstab file contains information about filesystems that should be mounted when the system boots.

**2. Which command will mount every filesystem defined in /etc/fstab?**


* [ ] mount -a
* [ ] mount -t
* [ ] mount -e
* [ ] mount -o

**Correct answer:**
* [x] mount -a

**Explaination**: The mount -a command will mount all filesystems defined in /etc/fstab.

**3. When trying to unmount a filesystem, you get an error saying the target is busy. Which command can you use to see the program that is keeping the disk busy?**


* [ ] lsof
* [ ] lsfiles
* [ ] iowait
* [ ] lsio

**Correct answer:**
* [x] lsof

**Explaination**: The "list open files" or "lsof" command can be used to see which program is currently using files on a disk that is busy. Example: lsof /dev/sda1.

**4. In addition to the device's disk identifier and partition number (e.g., /dev/sda1), what else can be used in /etc/fstab to identify a partition? Select all that apply.**


* [ ] UUID
* [ ] blkid
* [ ] label
* [ ] FSID

**Correct answer:**
* [x] UUID
* [x] label

**Explaination**: Both the UUID and the filesystem label can be used in /etc/fstab to identify a filesystem.

**5. Which command can be used to show information about all block devices on the system?**


* [ ] lsblk
* [ ] mount
* [ ] disks
* [ ] ls /dev/

**Correct answer:**
* [x] lsblk

**Explaination**: The lsblk command will show information about all block devices on the system and their filesystems.

---


## LPIC-permissions-ownership

**1. To add execute permissions for the user-owner of a file to existing permissions, which command would you use?**


* [ ] chmod u+x
* [ ] chmod +x
* [ ] chmod u-x
* [ ] chmod -x

**Correct answer:**
* [x] chmod u+x

**Explaination**: The command chmod u+x would add execute permissions for the file's user-owner while keeping all other existing permissions.

**2. Which command would change the user owner of a directory named "mydir" to "kodekloud" and also make this change for all files and subdirectories contained in the directory?**


* [ ] chown mydir kodekloud
* [ ] chown kodekloud mydir
* [ ] chown -R kodekloud mydir
* [ ] chown -R mydir kodekloud

**Correct answer:**
* [x] chown -R kodekloud mydir

**Explaination**: The format for using chown is to specify the owner and then the file or directory, and the -R (recursive) option would apply the changes to all files and subdirectories.

**3. A file named "myfile" has the user owner "kodekloud" and the group owner "users." Which command(s) could be use to change the group owner to "friends"? Select all that apply.**


* [ ] chgrp friends myfile
* [ ] chown kodekloud:friends myfile
* [ ] chgrp myfile friends
* [ ] chown myfile kodekloud:friends

**Correct answer:**
* [x] chgrp friends myfile
* [x] chown kodekloud:friends myfile

**Explaination**: The chgrp command can be used to change group ownership, and the chown command can also be used if the group is specified after a user owner and a colon. With both commands, the group comes before the file name.

**4. Which command would change the permissions of the file "myfile" to read, write, and execute for the user owner, read and write for the group owner, and no permissions for all other users? Select all that apply.**


* [ ] chmod 760 myfile
* [ ] chmod 660 myfile
* [ ] chmod u=rwx,g=rw,u= myfile
* [ ] chmod u=rw,g=rw=u= myfile

**Correct answer:**
* [x] chmod 760 myfile
* [x] chmod u=rwx,g=rw,u= myfile

**Explaination**: We can use octal, where 7 is "read, write, and execute" for the user owner, 6 is "read and write" for the group owner, and 0 is "nothing" for all other users, or we can use ugo format, were u=rwx gives "read, write, and execute" to the user owner, g=rw gives "read and write" to the group owner, and u= gives nothing to all other users.

**5. How can we set the sticky bit on a file named "myfile"? Other permissions are unimportant. Select all that apply.**


* [ ] chmod +t myfile
* [ ] chmod 1777 myfile
* [ ] chmod 0777 myfile
* [ ] chmod 4777 myfile

**Correct answer:**
* [x] chmod +t myfile
* [x] chmod 1777 myfile

**Explaination**: We can add the sticky bit to existing permissions by using chmod +t. We can also specify the sticky bit using the octal value of 1 in a four-digit format.

---


## LPIC-links

**1. True or false: When a hard link is deleted, the original file is also deleted.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**Explaination**: A hard link is treated as the original file, but in a different location. If a hard link is deleted, the original file is also deleted.

**2. Which command would create a soft link between myphoto.jpg and /home/kodekloud/photo?**


* [ ] ln -s myphoto.jpg /home/kodekloud/photo
* [ ] ln -s /home/kodekloud/photo myphoto.jpg
* [ ] ln myphoto.jpg /home/kodekloud/photo
* [ ] ln /home/kodekloud/photo myphoto.jpg

**Correct answer:**
* [x] ln -s myphoto.jpg /home/kodekloud/photo

**Explaination**: The ln -s command creates a soft link, and the format for ln in the specify the file being linked (myphoto.jpg) and then the target (/home/kodekloud/photo).

**3. The command ln myphoto.jpg /home/kodekloud/Pictures/photo would create a _____ link.**


* [ ] Symbolic
* [ ] Hard

**Correct answer:**
* [x] Hard

**Explaination**: The ln command, when used without other flags, creates a hard link to a file.

**4. You see myphoto.jpg -> photo in the output of the ls command. What type of link does this show?**


* [ ] Hard
* [ ] Soft

**Correct answer:**
* [x] Soft

**Explaination**: Symbolic or "soft" links are indicated by -> in the output of ls.

**5. True or false: When a soft link is deleted, the original file is also deleted.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**Explaination**: A soft link points to the original file, but is not considered to be the same as the original file. Therefore, a soft link can be deleted while leaving the original file intact.

---


## LPIC-file-locations

**1. The standard layout for Linux directories and their contents, as determined by the Linux Foundation, is known as the _____.**


* [ ] Filesystem Hierarchy Standard
* [ ] Filesystem Standard Hierarchy
* [ ] Hierarchical Filesystem Standard 
* [ ] Standard Hierarchical Filesystem

**Correct answer:**
* [x] Filesystem Hierarchy Standard

**Explaination**: The Filesystem Hierarchy Standard (FHS) is a layout determined by the Linux Foundation as an option (but encouraged) standard for Linux directories and their contents.

**2. To locate files, we use the _____ command.**


* [ ] find
* [ ] search
* [ ] spotlight
* [ ] cortana

**Correct answer:**
* [x] find

**Explaination**: Linux uses the find command to locate files.

**3. Removable storage, such as CD-ROMs and flash drives are mounted to the _____ directory.**


* [ ] /mnt
* [ ] /media
* [ ] /home
* [ ] /run

**Correct answer:**
* [x] /media

**Explaination**: According to the FHS, user-mountable removable media such as flash drives and CD-ROMS should be mounted to the /media directory.

**4. Temporary files can be located in _____. Select any that apply.**


* [ ] /tmp
* [ ] /var/tmp
* [ ] /run
* [ ] /usr/tmp

**Correct answer:**
* [x] /tmp
* [x] /var/tmp
* [x] /run

**Explaination**: According to the FHS, temporary files that are cleared during system boot are stored in /tmp; temporary files that are not cleared during system boot are stored in /var/tmp; run-time data used by running processes is stored in /run.

**5. Essential programs, available to all users, are found in _____.**


* [ ] /bin
* [ ] /sbin
* [ ] /usr/bin
* [ ] /usr/sbin

**Correct answer:**
* [x] /bin

**Explaination**: According to the FHS, essential binaries available to everyone are to be placed in the /bin directory.

---


## LPIC-mock-1

**1. Which option would you pass to journalctl to change the directory it uses to search for log files?**


* [ ] --log-directory=
* [ ] -D
* [ ] -d
* [ ] --log-dir

**Correct answer:**
* [x] -D

**Explaination**: The -D option allows you to specify a different directory for log files other than the default directory. Example: journalctl -D /mnt/external/var/log/journal.

**2. A friend is visiting you and brings a portable USB hard drive to share some files with you. You have one SATA drive in  your Linux machine. Which name will be given to your friend's portable USB hard drive in the /dev/ directory of your Linux machine?**


* [ ] /dev/sata2
* [ ] /dev/sba
* [ ] /dev/sdb
* [ ] /dev/usb1

**Correct answer:**
* [x] /dev/sdb

**Explaination**: In Linux, USB drives are treated as SATA drives and follow the same naming conventions. Your initial drive is already identified as /dev/sda, so your friend's USB hard drive will be identified as /dev/sdb.

**3. Which command would you use to view only the last ten lines of the initialization log?**


* [ ] dmesg --last
* [ ] dmesg | tail
* [ ] lastlog
* [ ] dmesg -H

**Correct answer:**
* [x] dmesg | tail

**Explaination**: Directing the output of dmesg to the tail command will only show the last ten lines of the initialization log.

**4. Which command would cause the system to shutdown 45 minutes from now?**


* [ ] shutdown 00:45
* [ ] shutdown now+45
* [ ] shutdown +45
* [ ] shutdown +45M

**Correct answer:**
* [x] shutdown +45

**Explaination**: Specifying +45 after the shutdown command will cause the system to shutdown 45 minutes from the time the command is run.

**5. If you want to warn other users of a Linux system that the system is going down for maintenance, which command could you use to send a message to all logged-in users?**


* [ ] write
* [ ] wall
* [ ] warn
* [ ] wipe

**Correct answer:**
* [x] wall

**Explaination**: The wall command is use to send messages to all users logged into a system.

**6. Given a theoretical file located at /etc/rc3.d/S45 ethernet, what would happen to any services listed in this file when the system enters runlevel 3?**


* [ ] They will stop.
* [ ] They will start.
* [ ] They will be suspended.
* [ ] They will be restarted.

**Correct answer:**
* [x] They will start.

**Explaination**: The letter "S" at the beginning of the file name indicates that services listed in this file will be started.

**7. You need to know which hardware devices are inside of a Linux system, but you cannot open the system's case. How would you get Linux to produce a list of hardware connected to the PCI bus?**


* [ ] cat /proc/hardware
* [ ] ls /dev/
* [ ] lspci
* [ ] modprobe

**Correct answer:**
* [x] lspci

**Explaination**: lspci gives information about hardware that is connected to the system's PCI bus.

**8. Which filesystem would you use for the ESP partition?**


* [ ] ext2
* [ ] fat32
* [ ] xfs
* [ ] ntfs

**Correct answer:**
* [x] fat32

**Explaination**: EFI System Partitions (ESP) should use a FAT filesystem, like FAT12, FAT16, or FAT32.

**9. Which formula would give the size of a Logical Volume?**


* [ ] Size of physical extents divided by number of extents
* [ ] Number of extents divided by size of physical extents
* [ ] Number of extents multiplied by physical extent size
* [ ] Disk sectors multiplied by number of extents

**Correct answer:**
* [x] Number of extents multiplied by physical extent size

**Explaination**: The size of a Logical Volume is equal to the number of physical extents multiplied by the total number of extents on the volume.

**10. When using GRUB Legacy, how is the first partition of the first disk labeled?**


* [ ] (hd1,1)
* [ ] (hd0,1)
* [ ] (hd0,0)
* [ ] (hd1,0)

**Correct answer:**
* [x] (hd0,0)

**Explaination**: GRUB Legacy begins counting from 0, so the first partition of the first drive would be labeled as (hd0,0).

**11. After adding a new shared library directory to your system, you copied a configuration file with its full path to the correct directory. Which command would you use to notify the system of the change?**


* [ ] ldd
* [ ] ldconfig
* [ ] libupdate
* [ ] updatedb

**Correct answer:**
* [x] ldconfig

**Explaination**: When making changes to shared libraries on the system, you must run ldconfig to notify the system of the changes.

**12. On a Debian system, which command would remove a package and all of its configuration files?**


* [ ] dpkg -r
* [ ] dpkg --remove
* [ ] dkpg --erase
* [ ] dpkg -P

**Correct answer:**
* [x] dpkg -P

**Explaination**: Running dpkg with the -P (purge) flag will remove a package and all of its configuration files.

**13. If you update the configuration for GRUB 2, which command would you run to write that configuration to the correct directory for GRUB 2 to use the next time you boot the system?**


* [ ] grub-config -o /boot/grub/grub.cfg
* [ ] grub-mkconfig -o /boot/grub/grub.cfg
* [ ] grub-mkconfig -o /boot/grub/grub.conf
* [ ] grub-config -o /boot/grub/grub.conf

**Correct answer:**
* [x] grub-mkconfig -o /boot/grub/grub.cfg

**Explaination**: After updating the configuration for GRUB 2, you must run grub-mkconfig -o /boot/grub/grub.cfg to make GRUB 2 use the changes on the next system boot.

**14. Which parameter would you add to dpkg-query to show which package contains a particular file?**


* [ ] -s
* [ ] -S
* [ ] -l
* [ ] -W

**Correct answer:**
* [x] -S

**Explaination**: Using the -S parameter with dpkg-query will show which package contains a particular file.

**15. Which command would you use to install a .deb file located on your local Linux system?**


* [ ] dpkg -I
* [ ] dpkg -i
* [ ] dpkg install
* [ ] dpkg -install

**Correct answer:**
* [x] dpkg -i

**Explaination**: To install a .deb file located on a local machine, you would you use the dpkg -i command. It is also possible to use dpkg --install (with two dashes).

**16. Volume Groups are divided into:**


* [ ] Extents
* [ ] Sectors
* [ ] Slices
* [ ] Chunks

**Correct answer:**
* [x] Extents

**Explaination**: Volume groups are divided into extents.

**17. Where are repository files stored for DNF and YUM?**


* [ ] /etc/repos.d/
* [ ] /etc/yum.repos.d/
* [ ] /etc/yum/repos/
* [ ] /etc/yum/repos.d/

**Correct answer:**
* [x] /etc/yum.repos.d/

**Explaination**: Repository files (.repo) are stored in /etc/yum.repos.d/.

**18. Using DNF, how would you install the httpd package?**


* [ ] dnf install httpd
* [ ] dnf -i httpd
* [ ] dnf httpd
* [ ] dnf provide httpd

**Correct answer:**
* [x] dnf install httpd

**Explaination**: When using DNF, the dnf install command, followed by a package name, will install that package. Example: dnf install httpd.

**19. Which virtualization method provides the highest performance?**


* [ ] Full virtualization
* [ ] Paravirtualization
* [ ] VirtualBox
* [ ] Virsh

**Correct answer:**
* [x] Paravirtualization

**Explaination**: Paravirtualization provides the best performance through the use of paravirtualized drivers for networking and storage.

**20. Using YUM or DNF, which option would uninstall a package from the system, along with any packages that depend on that package?**


* [ ] remove
* [ ] purge
* [ ] uninstall
* [ ] delete

**Correct answer:**
* [x] remove

**Explaination**: Using YUM or DNF, the remove option will uninstall a package from the system, along with all packages that depend on it. Example: dnf remove httpd.

**21. The _____ command will print a list of all current environment variables.**


* [ ] env
* [ ] path
* [ ] export
* [ ] import

**Correct answer:**
* [x] env

**Explaination**: The env command will print a list of all current environment variables.

**22. Which command would return the list of directories used by the shell to find executable commands?**


* [ ] echo $PATH
* [ ] print $PATH
* [ ] read $PATH
* [ ] pwd $PATH

**Correct answer:**
* [x] echo $PATH

**Explaination**: The command echo $PATH would return a list of directories used by the shell to find executable commands.

**23. To create an empty file named emptyfile in the current working directory, which command would you use?**


* [ ] touch emptyfile
* [ ] new emptyfile
* [ ] echo emptyfile
* [ ] blank emptyfile

**Correct answer:**
* [x] touch emptyfile

**Explaination**: The touch command can be used to create empty files.

**24. Which command is used to search the short description of man pages for keywords?**


* [ ] apropos
* [ ] grep
* [ ] find
* [ ] cat

**Correct answer:**
* [x] apropos

**Explaination**: Each manual page has a short description available within it and apropos searches the descriptions for instances of keyword.

**25. The _____ command will print the contents of a text file to standard output.**


* [ ] cat
* [ ] print
* [ ] grep
* [ ] wc

**Correct answer:**
* [x] cat

**Explaination**: The cat command will print the contents of a text file to standard output.

**26. Which command sequence would put the contents of a text file named myfile.txt in alphabetical order and remove all duplicates? Select all that apply.**


* [ ] sort myfile.txt | uniq
* [ ] sort myfile.txt | sed -s 'duplicates//g'
* [ ] uniq myfile.txt | sort
* [ ] sed -i -s 'duplicates//g' myfile.txt && sort myfile.txt

**Correct answer:**
* [x] sort myfile.txt | uniq

**Explaination**: The sort command will place contents in alphabetical order and the uniq command will print only unique entries (remove duplicates). 

**27. Which option can be used with ls to show hidden files?**


* [ ] -a
* [ ] -h
* [ ] --hidden
* [ ] -?

**Correct answer:**
* [x] -a

**Explaination**: The -a option with ls will show "all" including hidden files that begin with a period (.).

**28. Which option(s) would you use with the ls command to print file sizes in human readable format?**


* [ ] ls -h
* [ ] ls --human
* [ ] ls -lh
* [ ] ls -l --human

**Correct answer:**
* [x] ls -lh

**Explaination**: The -h option for ls lists file sizes in human readable format, and it must always be used with the -l option. Therefore, ls -lh is the correct answer.

**29. Which command could be used to create an archive named backup.cpio containing all of the files and directories in the current working directory?**


* [ ] cpio -o > backup.cpio
* [ ] ls | cpio -o > backup.cpio
* [ ] cpio * > backup.cpio
* [ ] cpio -o * > backup.cpio

**Correct answer:**
* [x] ls | cpio -o > backup.cpio

**Explaination**: Since cpio takes its file list as input from standard input, we would use the output of the ls command to list the files and directories in the current working directory and pass that to cpio as input. The -o option with cpio tells cpio to create an archive, and the > operator is used before the archive's filename to direct cpio to create the archive with that name.

**30. Which of the following commands could be used to back up an entire disk, including its filesystem?**


* [ ] dd
* [ ] tar
* [ ] cpio
* [ ] gzip

**Correct answer:**
* [x] dd

**Explaination**: The dd command can be used to back up an entire disk, including its file system. Example: dd if=/dev/sdb of=/mnt/backups/backup.dd bs=4096.

**31. Which command would take the contents of the /home/kodekloud/ directory and create a gzipped tar archive of the contents in a file called kodekloud.tar.gz?**


* [ ] tar -czvf kodekloud.tar.gz /home/kodekloud/
* [ ] tar -czvf /home/kodekloud/ kodekloud.tar.gz
* [ ] tar -cvf kodekloud.tar.gz /home/kodekloud/
* [ ] tar -cvf /home/kodekloud/ kodekloud.tar.gz

**Correct answer:**
* [x] tar -czvf kodekloud.tar.gz /home/kodekloud/

**Explaination**: When using tar to create gzipped tar archives, we must use the create (c), gzip (z), and file (f) options, followed by the name of the file we wish to create (kodekloud.tar.gz), and finally, the directory or files we wish to archive and compress (/home/kodekloud/).

**32. Which of the following would search the contents of the /etc/ directory for the pattern "kodekloud" and write only the errors to a file a /home/kodekloud/errors.list?**


* [ ] grep -r 'kodekloud' /etc/ > /home/kodekloud/errors.list
* [ ] grep -r 'kodekloud' /etc/ 2> /home/kodekloud/errors.list
* [ ] grep -r 'kodekloud' /etc/ 2>&1 /home/kodekloud/errors.list
* [ ] grep -r 'kodekloud' /etc/ 1> /home/kodekloud/errors.list

**Correct answer:**
* [x] grep -r 'kodekloud' /etc/ 2> /home/kodekloud/errors.list

**Explaination**: The operator "2>" would cause stderr to be written to the file at "/home/kodekloud/errors.list" while the stdout content would be displayed to the screen.

**33. _____ is an intermediary program used to pass the output from one program as arguments to another program.**


* [ ] xargs
* [ ] xout
* [ ] xin
* [ ] xinput

**Correct answer:**
* [x] xargs

**Explaination**: The xargs program can be used as an intermediary program to pass the output of one program as arguments to another.

**34. Given that the "whoami" command prints the current user's username, what would be the result of the following command: rm -rf /home/$(whoami)**


* [ ] It would delete the directory /home/whoami
* [ ] It would delete the user's home directory
* [ ] Nothing; this is not a valid command
* [ ] An endless loop

**Correct answer:**
* [x] It would delete the user's home directory

**Explaination**: Since the "whoami" command returns the current user's username, the command would delete the user's home directory (and all subdirectories and files). Example: for the user "kodekloud" this would delete "/home/kodekloud".

**35. To bring a job with job ID 1 from the background to the foreground, which command could we run?**


* [ ] fg %1
* [ ] fg $1
* [ ] bg %1
* [ ] bg $1

**Correct answer:**
* [x] fg %1

**Explaination**: We can use the fg command, followed by % and the job number to bring a process from the background to the foreground.

**36. To view dynamically updated information on system processes and resources, we can use the _____ command.**


* [ ] top
* [ ] ps
* [ ] jobs
* [ ] uptime

**Correct answer:**
* [x] top

**Explaination**: The top command provides dynamically updated information on processes and system resources, such as CPU usage and RAM.

**37. The default command prefix for tmux is:**


* [ ] CTRL + a
* [ ] CTRL + b
* [ ] CTRL + v
* [ ] CTRL + s

**Correct answer:**
* [x] CTRL + b

**Explaination**: The default command prefix for tmux is CTRL + b.

**38. To set the priority for a process when it is run, we can use the _____ command.**


* [ ] nice
* [ ] renice
* [ ] top
* [ ] ps

**Correct answer:**
* [x] nice

**Explaination**: The nice command allows us to set the process priority ("niceness") when a command is run. Example: nice -n 15 top.

**39. The default command prefix for GNU screen is:**


* [ ] CTRL + a
* [ ] CTRL + b
* [ ] CTRL + v
* [ ] CTRL + s

**Correct answer:**
* [x] CTRL + a

**Explaination**: The default command prefix for GNU screen is CTRL + a.

**40. Which user account(s) can lower the niceness of a process to a value less than zero? Select all that apply.**


* [ ] normal users
* [ ] the root user
* [ ] any user with sudo access
* [ ] only the user who started the process

**Correct answer:**
* [x] the root user
* [x] any user with sudo access

**Explaination**: Only the root user or a user with sudo access using the sudo command can lower the niceness of a process below zero.

**41. Which regular expression symbol matches a single instance of a any character?**


* [ ] .
* [ ] *
* [ ] $
* [ ] ^

**Correct answer:**
* [x] .

**Explaination**: The period (.) matches exactly one instance of any character.

**42. Which regular expression symbol indicates a line that begins with a regular expression pattern?**


* [ ] ^
* [ ] $
* [ ] .
* [ ] *

**Correct answer:**
* [x] ^

**Explaination**: The caret (^) is used to indicate that a line begins with a regular expression pattern.

**43. Which option can be used with sed to edit the file as-is, without specifying a different file name as a target?**


* [ ] -i
* [ ] -e
* [ ] --here
* [ ] --same

**Correct answer:**
* [x] -i

**Explaination**: The in-place (-i) option with sed can be used to edit a file without specifying a different file name for output.

**44. Which regular expression symbol indicates that a line ends with a regular expression pattern?**


* [ ] ^
* [ ] $
* [ ] .
* [ ] *

**Correct answer:**
* [x] $

**Explaination**: The dollar sign ($) is used to indicate that a line ends with a regular expression pattern.

**45. When using vi, in normal mode, which key allows us to search the file?**


* [ ] /
* [ ] s
* [ ] f
* [ ] t

**Correct answer:**
* [x] /

**Explaination**: The / character allows us to specify a search term while in normal mode.

**46. Which command is used to create filesystems?**


* [ ] mkswap
* [ ] mkfs
* [ ] fdisk
* [ ] parted

**Correct answer:**
* [x] mkfs

**Explaination**: The mkfs command can be used to create a variety of filesystems on existing partitions.

**47. In command mode, which command will allow us to save changes and exit vi?**


* [ ] :wq
* [ ] :q
* [ ] :q!
* [ ] :w

**Correct answer:**
* [x] :wq

**Explaination**: The command :wq or "write and quite" will allow us to save changes and exit vi.

**48. Which command(s) can be used to work with partitions? Select all that apply.**


* [ ] fdisk
* [ ] gdisk
* [ ] parted
* [ ] vdisk

**Correct answer:**
* [x] fdisk
* [x] gdisk
* [x] parted

**Explaination**: The fdisk, gdisk, and parted commands can all be used to work with partitions.

**49. Which command can be used to show how much space is currently taken up on a disk and display that output in human-readable format?**


* [ ] du -h
* [ ] df -h
* [ ] free -h
* [ ] diskfree -h

**Correct answer:**
* [x] du -h

**Explaination**: The disk usage (du) command with the human-readable (-h) flag can be used to see how much disk space is currently taken up and display it in a human-readable format.

**50. Which command will check a filesystem for errors and repair it if it is offline?**


* [ ] fsck
* [ ] fdisk
* [ ] sfc
* [ ] chkdsk

**Correct answer:**
* [x] fsck

**Explaination**: The filesystem checker (fsck) will check a filesystem for errors, and can be used to repair the filesystem while it is offline (not mounted).

**51. Which command will mount every filesystem defined in /etc/fstab?**


* [ ] mount -a
* [ ] mount -t
* [ ] mount -e
* [ ] mount -o

**Correct answer:**
* [x] mount -a

**Explaination**: The mount -a command will mount all filesystems defined in /etc/fstab.

**52. A list of filesystems to be mounted when the system boots can be found in _____.**


* [ ] /etc/fstab
* [ ] /etc/fs
* [ ] /boot/fstab
* [ ] /boot/fs

**Correct answer:**
* [x] /etc/fstab

**Explaination**: The /etc/fstab file contains information about filesystems that should be mounted when the system boots.

**53. In addition to the device's disk identifier and partition number (e.g., /dev/sda1), what else can be used in /etc/fstab to identify a partition? Select all that apply.**


* [ ] UUID
* [ ] blkid
* [ ] label
* [ ] FSID

**Correct answer:**
* [x] UUID
* [x] label

**Explaination**: Both the UUID and the filesystem label can be used in /etc/fstab to identify a filesystem.

**54. Which command would change the user owner of a directory named "mydir" to "kodekloud" and also make this change for all files and subdirectories contained in the directory?**


* [ ] chown mydir kodekloud
* [ ] chown kodekloud mydir
* [ ] chown -R kodekloud mydir
* [ ] chown -R mydir kodekloud

**Correct answer:**
* [x] chown -R kodekloud mydir

**Explaination**: The format for using chown is to specify the owner and then the file or directory, and the -R (recursive) option would apply the changes to all files and subdirectories.

**55. Which command would change the permissions of the file "myfile" to read, write, and execute for the user owner, read and write for the group owner, and no permissions for all other users? Select all that apply.**


* [ ] chmod 760 myfile
* [ ] chmod 660 myfile
* [ ] chmod u=rwx,g=rw,u= myfile
* [ ] chmod u=rw,g=rw=u= myfile

**Correct answer:**
* [x] chmod 760 myfile
* [x] chmod u=rwx,g=rw,u= myfile

**Explaination**: We can use octal, where 7 is "read, write, and execute" for the user owner, 6 is "read and write" for the group owner, and 0 is "nothing" for all other users, or we can use ugo format, were u=rwx gives "read, write, and execute" to the user owner, g=rw gives "read and write" to the group owner, and u= gives nothing to all other users.

**56. How can we set the sticky bit on a file named "myfile"? Other permissions are unimportant. Select all that apply.**


* [ ] chmod +t myfile
* [ ] chmod 1777 myfile
* [ ] chmod 0777 myfile
* [ ] chmod 4777 myfile

**Correct answer:**
* [x] chmod +t myfile
* [x] chmod 1777 myfile

**Explaination**: We can add the sticky bit to existing permissions by using chmod +t. We can also specify the sticky bit using the octal value of 1 in a four-digit format.

**57. Which command would create a soft link between myphoto.jpg and /home/kodekloud/photo?**


* [ ] ln -s myphoto.jpg /home/kodekloud/photo
* [ ] ln -s /home/kodekloud/photo myphoto.jpg
* [ ] ln myphoto.jpg /home/kodekloud/photo
* [ ] ln /home/kodekloud/photo myphoto.jpg

**Correct answer:**
* [x] ln -s myphoto.jpg /home/kodekloud/photo

**Explaination**: The ln -s command creates a soft link, and the format for ln in the specify the file being linked (myphoto.jpg) and then the target (/home/kodekloud/photo).

**58. Temporary files can be located in _____. Select any that apply.**


* [ ] /tmp
* [ ] /var/tmp
* [ ] /run
* [ ] /usr/tmp

**Correct answer:**
* [x] /tmp
* [x] /var/tmp
* [x] /run

**Explaination**: According to the FHS, temporary files that are cleared during system boot are stored in /tmp; temporary files that are not cleared during system boot are stored in /var/tmp; run-time data used by running processes is stored in /run.

**59. The command ln myphoto.jpg /home/kodekloud/Pictures/photo would create a _____ link.**


* [ ] Symbolic
* [ ] Hard

**Correct answer:**
* [x] Hard

**Explaination**: The ln command, when used without other flags, creates a hard link to a file.

**60. Essential programs, available to all users, are found in _____.**


* [ ] /bin
* [ ] /sbin
* [ ] /usr/bin
* [ ] /usr/sbin

**Correct answer:**
* [x] /bin

**Explaination**: According to the FHS, essential binaries available to everyone are to be placed in the /bin directory.

---


## LPIC-mock-2

**1. Which command will list kernel modules in use by hardware?**


* [ ] lspci -k
* [ ] lspci -m
* [ ] lspci --modules
* [ ] lspci -l

**Correct answer:**
* [x] lspci -k

**Explaination**: The -k option for lspci will list kernel modules in use by hardware.

**2. Which file would you search to determine the CPU features on a Linux machine?**


* [ ] /proc/cpuinfo
* [ ] /dev/cpuinfo
* [ ] /proc/cpu
* [ ] /proc/hardware/cpuinfo

**Correct answer:**
* [x] /proc/cpuinfo

**Explaination**: Information on the CPU and its features can be found in the cpuinfo file inside the /proc virtual filesystem (/proc/cpuinfo).

**3. You have compiled a custom kernel, and the filesystem type used by your Linux system's root filesystem is compiled as a module. What would you need to include along with the kernel to make sure your system loads the root filesystem correctly?**


* [ ] initramfs
* [ ] EFI applications
* [ ] A copy of /etc/modules.conf.d/
* [ ] bootstrap

**Correct answer:**
* [x] initramfs

**Explaination**: An initial RAM filesystem (initramfs) should be included if the root filesystem is compiled as a module.

**4. Which command would you use to view only the last ten lines of the initialization log?**


* [ ] dmesg --last
* [ ] dmesg | tail
* [ ] lastlog
* [ ] dmesg -H

**Correct answer:**
* [x] dmesg | tail

**Explaination**: Directing the output of dmesg to the tail command will only show the last ten lines of the initialization log.

**5. Using systemctl, how would you determine if a service named myservice.service is configured to start when the system boots?**


* [ ] systemctl status myservice.service
* [ ] systemctl is-enabled myservice.service
* [ ] systemctl is-active myservice.service
* [ ] systemctl on-boot myservice.service

**Correct answer:**
* [x] systemctl is-enabled myservice.service

**Explaination**: The is-enabled option for systemctl will return "enabled" if a service is enabled to start at boot, or "disabled" if it is not.

**6. Given a theoretical file located at /etc/rc3.d/S45 ethernet, what would happen to any services listed in this file when the system enters runlevel 3?**


* [ ] They will stop.
* [ ] They will start.
* [ ] They will be suspended.
* [ ] They will be restarted.

**Correct answer:**
* [x] They will start.

**Explaination**: The letter "S" at the beginning of the file name indicates that services listed in this file will be started.

**7. On a system equipped with BIOS, what should be placed in the MBR of the first storage device in order to boot the system?**


* [ ] bootstrap
* [ ] firmware
* [ ] initrd
* [ ] kernel

**Correct answer:**
* [x] bootstrap

**Explaination**: The bootstrap binary must be located in the MBR of the first storage device for a system equipped with BIOS to boot correctly.

**8. On SysV systems, which file contains the default init configuration?**


* [ ] /etc/init.d/defaults
* [ ] /etc/defaults/inittab
* [ ] /etc/inittab
* [ ] /etc/conf.d/inittab

**Correct answer:**
* [x] /etc/inittab

**Explaination**: The default init configuration is stored in /etc/inittab on SysV systems.

**9. When a Linux system mounts the EFI partition, where is it usually mounted?**


* [ ] /boot/efi/
* [ ] /boot/grub/efi/
* [ ] /mnt/efi/
* [ ] /efi/

**Correct answer:**
* [x] /boot/efi/

**Explaination**: Linux usually mounts the EFI partition at /boot/efi/.

**10. Where are the files for the GRUB bootloader stored?**


* [ ] /etc/grub/
* [ ] /boot/
* [ ] /boot/grub/
* [ ] /grub/boot/

**Correct answer:**
* [x] /boot/grub/

**Explaination**: The files for the GRUB bootloader are stored in the /boot/grub/ directory.

**11. To change the menu entries for GRUB Legacy, which file would you edit?**


* [ ] /boot/grub/menu.lst
* [ ] /etc/grub/grub.cfg
* [ ] /etc/grub/menu.lst
* [ ] /boot/grub/grub.cfg

**Correct answer:**
* [x] /boot/grub/menu.lst

**Explaination**: Menu entries for GRUB Legacy are stored in /boot/grub/menu.lst.

**12. Which apt-file command would show the package that contains the file /usr/bin/systemctl?**


* [ ] apt-file search /usr/bin/systemctl
* [ ] apt-file show /usr/bin/systemctl
* [ ] apt-file provides /usr/bin/systemctl
* [ ] apt-file query /usr/bin/systemctl

**Correct answer:**
* [x] apt-file search /usr/bin/systemctl

**Explaination**: You can use apt-file search followed by the full path to a file to show which package contains that file.

**13. When using GRUB 2, which menu entry parameter determines how long GRUB 2 will pause before booting the default menu entry?**


* [ ] GRUB_TIMEOUT=
* [ ] GRUB_WAIT=
* [ ] GRUB_PAUSE=
* [ ] GRUB_TIMER=

**Correct answer:**
* [x] GRUB_TIMEOUT=

**Explaination**: The menu entry that controls how long GRUB 2 will pause before booting the default menu entry is GRUB_TIMEOUT=.

**14. Which parameter would you add to dpkg-query to show which package contains a particular file?**


* [ ] -s
* [ ] -S
* [ ] -l
* [ ] -W

**Correct answer:**
* [x] -S

**Explaination**: Using the -S parameter with dpkg-query will show which package contains a particular file.

**15. You have added a new shared library directory to your system, and you have written a custom configuration file including the full path to to the new shared library location. Which directory would you put the configuration file into?**


* [ ] /usr/share/
* [ ] /etc/share
* [ ] /etc/ld.so.conf.d/
* [ ] /usr/ld.so.conf.d/

**Correct answer:**
* [x] /etc/ld.so.conf.d/

**Explaination**: Configuration files that define shared library paths go in the /etc/ld.so.conf.d/ directory.

**16. Which apt-cache command will provide information for a package?**


* [ ] apt-cache show
* [ ] apt-cache info
* [ ] apt-cache print
* [ ] apt-cache search

**Correct answer:**
* [x] apt-cache show

**Explaination**: The apt-cache show command, followed by a package name, will provide information for a package.

**17. Which yum command will bring repository metadata to the most recent version?**


* [ ] yum update
* [ ] yum metadata
* [ ] yum upgrade
* [ ] yum check-metadata

**Correct answer:**
* [x] yum update

**Explaination**: The yum update command will update repository metadata.

**18. Where are repository files stored for DNF and YUM?**


* [ ] /etc/repos.d/
* [ ] /etc/yum.repos.d/
* [ ] /etc/yum/repos/
* [ ] /etc/yum/repos.d/

**Correct answer:**
* [x] /etc/yum.repos.d/

**Explaination**: Repository files (.repo) are stored in /etc/yum.repos.d/.

**19. Using zypper, how would you find out which package provides the file /usr/bin/systemctl?**


* [ ] zypper se /usr/bin/systemctl
* [ ] zypper se --provides /usr/bin/systemctl
* [ ] zypper --provides /usr/bin/systemctl
* [ ] zypper se provides /usr/bin/systemctl

**Correct answer:**
* [x] zypper se --provides /usr/bin/systemctl

**Explaination**: When using zypper, the se (search) command, followed by --provides and the file path will show which package provides that file. Example: zypper se --provides /usr/bin/systemctl.

**20. Which command will cause Bash to print the path of the current directory?**


* [ ] whereami
* [ ] which directory
* [ ] cwd
* [ ] pwd

**Correct answer:**
* [x] pwd

**Explaination**: To print the current working directory, use the pwd command.

**21. The _____ command can be used to clear environment variables for the current shell session.**


* [ ] clear
* [ ] unset
* [ ] import
* [ ] echo

**Correct answer:**
* [x] unset

**Explaination**: The unset command is used to clear environment variable values for the current shell session.

**22. Intel VT-x and AMD-V are examples of _____ used to provide support for fully virtualized guests?**


* [ ] CPU extensions
* [ ] paravirtualized drivers
* [ ] hypervisors
* [ ] virtual machines

**Correct answer:**
* [x] CPU extensions

**Explaination**: Intel VT-x and AMD-V are CPU extensions.

**23. The _____ command can be used to assign values to environment variables for the current shell session.**


* [ ] export
* [ ] env
* [ ] echo
* [ ] import

**Correct answer:**
* [x] export

**Explaination**: The export command is used to assign values to environment variables for the current shell session.

**24. You have set a variable called kodekloud using the export command. Which command would print the value of the kodekloud variable?**


* [ ] print kodekloud
* [ ] echo kodekloud
* [ ] echo $kodekloud
* [ ] read $kodekloud

**Correct answer:**
* [x] echo $kodekloud

**Explaination**: The echo command will print the value of a variable, and the variable name must have a $ before it. Example: echo $kodekloud will return the value assigned to the kodekloud variable.

**25. Which command can be used to search file content using regular expression patterns?**


* [ ] grep
* [ ] find
* [ ] read
* [ ] search

**Correct answer:**
* [x] grep

**Explaination**: The grep command can be used to search file content using regular expressions. Grep stands for "get regular expression."

**26. Which command can be used to print only the desired field from text?**


* [ ] sed
* [ ] grep
* [ ] cut
* [ ] find

**Correct answer:**
* [x] cut

**Explaination**: The cut command can specify which field(s) to print from text.

**27. Which option can be used with the find command to show only directories?**


* [ ] -type d
* [ ] --type -d
* [ ] -type directory
* [ ] --type --directory

**Correct answer:**
* [x] -type d

**Explaination**: The option -type d will cause find to limit its output to directories that match the search pattern.

**28. Which command(s) would list all of the files in the current directory that start with the word "file" followed by one number and .txt?**


* [ ] ls file[0-9].txt
* [ ] ls | grep 'file[0-9].txt'
* [ ] ls file*.txt
* [ ] ls | grep 'file*.txt'

**Correct answer:**
* [x] ls file[0-9].txt
* [x] ls | grep 'file[0-9].txt'

**Explaination**: Using the bracketed range [0-9] with either ls or grep as shown above will match exactly one number. Using the * wildcard would match zero or more numbers, and would not produce a list that contained only filenames that started with file followed by one number and .txt. For example, that sequence would also match "file.txt" and "file10.txt".

**29. Which of the following find command would find files in the current directory that have an extension of .bak and are larger than 1 gigabyte?**


* [ ] find . -name *.bak -size +1G
* [ ] find pwd -name *.bak -size -1G
* [ ] find . -name *.back -size 1G
* [ ] find . -name *.bak -size =1G+

**Correct answer:**
* [x] find . -name *.bak -size +1G

**Explaination**: The command find . -name *.bak -size +1G would: search the current directory (.) for files with any name followed by the .bak extension (-name *.bak), with a size that is 1 gigabyte or more (-size +1G).

**30. Which option(s) would you use with tar to extract the archive located at /home/kodekloud/archive.tar.gz?**


* [ ] cvf
* [ ] xf
* [ ] xfz
* [ ] cfvz

**Correct answer:**
* [x] xf
* [x] xfz

**Explaination**: The options for extract (x) and file (f) would be required. The verbose (v) option is optional, and the option for gzip (z) is only required to create a .gzip compressed archive, not to extract one.

**31. Which symbol(s) would be used to specify a stdout redirect? Select all that apply.**


* [ ] 1>
* [ ] 2>
* [ ] 0>
* [ ] \>

**Correct answer:**
* [x] 1>
* [x] \>

**Explaination**: Stdout is implied by the ">" operator. Stdout is also designated by "1", therefore "1>" also refers to stdout.

**32. Which operator would be used to designate a herestring?**


* [ ] <
* [ ] <<
* [ ] <<<
* [ ] <<<<

**Correct answer:**
* [x] <<<

**Explaination**: The "<<<" operator designates a herestring.

**33. Which operator(s) can be used for command substitution? Select all that apply.**


* [ ] ``
* [ ] $()
* [ ] %{}
* [ ] ""

**Correct answer:**
* [x] ``
* [x] $()

**Explaination**: Placing a command inside of backquotes (``) or inside of the parenthesis in $() will cause the shell to use the output of the command in that place. Example: echo "Today is $(date) and this system is `uname -a`." would  produce: Today is Mon Dec 19 02:48:03 PM CST 2022 and this system is Linux kodekloud 5.14.0-210.el9.x86_64 #1 SMP PREEMPT_DYNAMIC Fri Dec 9 20:01:51 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux.

**34. The standard operator used to pass the output of one program to the input of another is:**


* [ ] |
* [ ] &
* [ ] @
* [ ] %

**Correct answer:**
* [x] |

**Explaination**: The vertical pipe (|) is used to pass the output of one command to the input of another. Example: cat /proc/cpuinfo | grep 'bugs'.

**35. To view processes which have been sent to the background, we can use the _____ command.**


* [ ] bg
* [ ] fg
* [ ] jobs
* [ ] cron

**Correct answer:**
* [x] jobs

**Explaination**: The jobs command will show a list of processes that have been sent to the background or stopped.

**36. The _____ terminal multiplexer uses a client-server model.**


* [ ] GNU screen
* [ ] tmux

**Correct answer:**
* [x] tmux

**Explaination**: The tmux terminal multiplexer uses a client-server model.

**37. To get detailed information about a process when we already have the PID (process ID), we can use the _____ command.**


* [ ] proc
* [ ] pgrep
* [ ] ps
* [ ] pkill

**Correct answer:**
* [x] ps

**Explaination**: The ps command provides detailed information about a process when we already know the PID. Example: ps 1127.

**38. To run a command detached from the current session, we can use the _____ command.**


* [ ] nohup
* [ ] SIGTERM
* [ ] jobs
* [ ] bg

**Correct answer:**
* [x] nohup

**Explaination**: The "no hangup" (nohup) command runs another command detached from the current session. This is often paired with the & symbol to send the command to the background. Example: nohup ping 8.8.8.8 &.

**39. To modify a the priority of an existing process, we can use the _____ command.**


* [ ] renice
* [ ] nice
* [ ] ps
* [ ] top

**Correct answer:**
* [x] renice

**Explaination**: The renice command allows us to change the priority ("niceness") of a process that is already running. Example: renice -10 -p 1127.

**40. True or false: A regular user can only lower the process niceness one time.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**Explaination**: A normal user can only lower the niceness of a process one time. The root user can do so multiple times.

**41. Which of the following utilities makes use of regular expressions? Select all that apply.**


* [ ] grep
* [ ] egrep
* [ ] sed
* [ ] ps

**Correct answer:**
* [x] grep
* [x] egrep
* [x] sed

**Explaination**: The grep, egrep, and sed utilities make use of regular expressions for pattern matching.

**42. Which regular expression symbol matches any characters from a list provided with that symbol?**


* [ ] [ ]
* [ ] ( )
* [ ] *
* [ ] ?

**Correct answer:**
* [x] [ ]

**Explaination**: Brackets ([ ]) provide a list of characters, and match any characters provided in the brackets. Example grep 'l[oe]t' would match "lot" and "let."

**43. Which grep option can be used to search an entire directory and its sub directories?**


* [ ] -r
* [ ] *
* [ ] -R
* [ ] .

**Correct answer:**
* [x] -r

**Explaination**: The recursive (-r) option with grep will cause grep to search an entire directory and its sub directories. Example: grep -r 'kodekloud' /etc/.

**44. In vi normal mode, the pp command will paste. Which command can be used to copy?**


* [ ] cc
* [ ] yy
* [ ] :copy
* [ ] :yank

**Correct answer:**
* [x] yy

**Explaination**: The "yy" command will copy (yank) text.

**45. When using vi in normal mode, which command will delete the line at the cursor?**


* [ ] d
* [ ] dd
* [ ] yy
* [ ] ZZ

**Correct answer:**
* [x] dd

**Explaination**: Pressing "d" twice or "dd" will delete the line at the current cursor position.

**46. The two types of partition tables commonly used in Linux are (select two):**


* [ ] MBR
* [ ] GPT
* [ ] DVH
* [ ] BSD

**Correct answer:**
* [x] MBR
* [x] GPT

**Explaination**: Linux commonly uses MBR and GPT partition tables (though others are supported).

**47. Which of the following are features of BTRFS? Select all that apply.**


* [ ] compression
* [ ] subvolumes
* [ ] snapshots
* [ ] encryption

**Correct answer:**
* [x] compression
* [x] subvolumes
* [x] snapshots

**Explaination**: BTRFS supports compression, subvolumes, and snapshots.

**48. In addition to vi, what are some other common editors for Linux?**


* [ ] nano
* [ ] Emacs
* [ ] Word
* [ ] notes

**Correct answer:**
* [x] nano
* [x] Emacs

**Explaination**: GNU nano and Emacs are common editors in addition to vi.

**49. Which command can be used to determine how much disk space is available?**


* [ ] du
* [ ] df
* [ ] free
* [ ] diskfree

**Correct answer:**
* [x] df

**Explaination**: The "disk free" (df) command can be used to see how much free space is left on a disk.

**50. Which utility can be used to check and repair XFS filesystems?**


* [ ] xfs_repair
* [ ] xfs_fsck
* [ ] xfs_check
* [ ] xfs_fix

**Correct answer:**
* [x] xfs_repair

**Explaination**: The xfs_repair command can be used to check and repair XFS filesystems.

**51. Which command can be used to show information about all block devices on the system?**


* [ ] lsblk
* [ ] mount
* [ ] disks
* [ ] ls /dev/

**Correct answer:**
* [x] lsblk

**Explaination**: The lsblk command will show information about all block devices on the system and their filesystems.

**52. When trying to unmount a filesystem, you get an error saying the target is busy. Which command can you use to see the program that is keeping the disk busy?**


* [ ] lsof
* [ ] lsfiles
* [ ] iowait
* [ ] lsio

**Correct answer:**
* [x] lsof

**Explaination**: The "list open files" or "lsof" command can be used to see which program is currently using files on a disk that is busy. Example: lsof /dev/sda1.

**53. A file named "myfile" has the user owner "kodekloud" and the group owner "users." Which command(s) could be use to change the group owner to "friends"? Select all that apply.**


* [ ] chgrp friends myfile
* [ ] chown kodekloud:friends myfile
* [ ] chgrp myfile friends
* [ ] chown myfile kodekloud:friends

**Correct answer:**
* [x] chgrp friends myfile
* [x] chown kodekloud:friends myfile

**Explaination**: The chgrp command can be used to change group ownership, and the chown command can also be used if the group is specified after a user owner and a colon. With both commands, the group comes before the file name.

**54. In addition to the device's disk identifier and partition number (e.g., /dev/sda1), what else can be used in /etc/fstab to identify a partition? Select all that apply.**


* [ ] UUID
* [ ] blkid
* [ ] label
* [ ] FSID

**Correct answer:**
* [x] UUID
* [x] label

**Explaination**: Both the UUID and the filesystem label can be used in /etc/fstab to identify a filesystem.

**55. Which command would change the permissions of the file "myfile" to read, write, and execute for the user owner, read and write for the group owner, and no permissions for all other users? Select all that apply.**


* [ ] chmod 760 myfile
* [ ] chmod 660 myfile
* [ ] chmod u=rwx,g=rw,u= myfile
* [ ] chmod u=rw,g=rw=u= myfile

**Correct answer:**
* [x] chmod 760 myfile
* [x] chmod u=rwx,g=rw,u= myfile

**Explaination**: We can use octal, where 7 is "read, write, and execute" for the user owner, 6 is "read and write" for the group owner, and 0 is "nothing" for all other users, or we can use ugo format, were u=rwx gives "read, write, and execute" to the user owner, g=rw gives "read and write" to the group owner, and u= gives nothing to all other users.

**56. To add execute permissions for the user-owner of a file to existing permissions, which command would you use?**


* [ ] chmod u+x
* [ ] chmod +x
* [ ] chmod u-x
* [ ] chmod -x

**Correct answer:**
* [x] chmod u+x

**Explaination**: The command chmod u+x would add execute permissions for the file's user-owner while keeping all other existing permissions.

**57. True or false: When a hard link is deleted, the original file is also deleted.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**Explaination**: A hard link is treated as the original file, but in a different location. If a hard link is deleted, the original file is also deleted.

**58. True or false: When a soft link is deleted, the original file is also deleted.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**Explaination**: A soft link points to the original file, but is not considered to be the same as the original file. Therefore, a soft link can be deleted while leaving the original file intact.

**59. The standard layout for Linux directories and their contents, as determined by the Linux Foundation, is known as the _____.**


* [ ] Filesystem Hierarchy Standard
* [ ] Filesystem Standard Hierarchy
* [ ] Hierarchical Filesystem Standard 
* [ ] Standard Hierarchical Filesystem

**Correct answer:**
* [x] Filesystem Hierarchy Standard

**Explaination**: The Filesystem Hierarchy Standard (FHS) is a layout determined by the Linux Foundation as an option (but encouraged) standard for Linux directories and their contents.

**60. Removable storage, such as CD-ROMs and flash drives are mounted to the _____ directory.**


* [ ] /mnt
* [ ] /media
* [ ] /home
* [ ] /run

**Correct answer:**
* [x] /media

**Explaination**: According to the FHS, user-mountable removable media such as flash drives and CD-ROMS should be mounted to the /media directory.

---


## LPIC-mock-3

**1. Which command would you use to view only the last ten lines of the initialization log?**


* [ ] dmesg --last
* [ ] dmesg | tail
* [ ] lastlog
* [ ] dmesg -H

**Correct answer:**
* [x] dmesg | tail

**Explaination**: Directing the output of dmesg to the tail command will only show the last ten lines of the initialization log.

**2. On SysV systems, which file contains the default init configuration?**


* [ ] /etc/init.d/defaults
* [ ] /etc/defaults/inittab
* [ ] /etc/inittab
* [ ] /etc/conf.d/inittab

**Correct answer:**
* [x] /etc/inittab

**Explaination**: The default init configuration is stored in /etc/inittab on SysV systems.

**3. A friend is visiting you and brings a portable USB hard drive to share some files with you. You have one SATA drive in  your Linux machine. Which name will be given to your friend's portable USB hard drive in the /dev/ directory of your Linux machine?**


* [ ] /dev/sata2
* [ ] /dev/sba
* [ ] /dev/sdb
* [ ] /dev/usb1

**Correct answer:**
* [x] /dev/sdb

**Explaination**: In Linux, USB drives are treated as SATA drives and follow the same naming conventions. Your initial drive is already identified as /dev/sda, so your friend's USB hard drive will be identified as /dev/sdb.

**4. Given a theoretical file located at /etc/rc3.d/S45 ethernet, what would happen to any services listed in this file when the system enters runlevel 3?**


* [ ] They will stop.
* [ ] They will start.
* [ ] They will be suspended.
* [ ] They will be restarted.

**Correct answer:**
* [x] They will start.

**Explaination**: The letter "S" at the beginning of the file name indicates that services listed in this file will be started.

**5. You have added a new hard drive to a system. You know that the hard drive has no defects, but the system cannot boot. Where would you go to begin troubleshooting this problem?**


* [ ] The BIOS setup utility.
* [ ] The manual for the hard drive.
* [ ] The manufacturer's website.
* [ ] An Internet forum.

**Correct answer:**
* [x] The BIOS setup utility.

**Explaination**: The BIOS setup utility is the correct place to begin troubleshooting this hardware problem. It could be that the boot device order is incorrect.

**6. Which filesystem would you use for the ESP partition?**


* [ ] ext2
* [ ] fat32
* [ ] xfs
* [ ] ntfs

**Correct answer:**
* [x] fat32

**Explaination**: EFI System Partitions (ESP) should use a FAT filesystem, like FAT12, FAT16, or FAT32.

**7. You have compiled a custom kernel, and the filesystem type used by your Linux system's root filesystem is compiled as a module. What would you need to include along with the kernel to make sure your system loads the root filesystem correctly?**


* [ ] initramfs
* [ ] EFI applications
* [ ] A copy of /etc/modules.conf.d/
* [ ] bootstrap

**Correct answer:**
* [x] initramfs

**Explaination**: An initial RAM filesystem (initramfs) should be included if the root filesystem is compiled as a module.

**8. Using systemctl, how would you determine if a service named myservice.service is configured to start when the system boots?**


* [ ] systemctl status myservice.service
* [ ] systemctl is-enabled myservice.service
* [ ] systemctl is-active myservice.service
* [ ] systemctl on-boot myservice.service

**Correct answer:**
* [x] systemctl is-enabled myservice.service

**Explaination**: The is-enabled option for systemctl will return "enabled" if a service is enabled to start at boot, or "disabled" if it is not.

**9. When a Linux system mounts the EFI partition, where is it usually mounted?**


* [ ] /boot/efi/
* [ ] /boot/grub/efi/
* [ ] /mnt/efi/
* [ ] /efi/

**Correct answer:**
* [x] /boot/efi/

**Explaination**: Linux usually mounts the EFI partition at /boot/efi/.

**10. In addition to swap partitions, Linux can make use of swap files. Where is the swap file typically located?**


* [ ] /mnt/swap
* [ ] /swapfile
* [ ] /mnt/swapfile
* [ ] /swap

**Correct answer:**
* [x] /swapfile

**Explaination**: If used, a swap file is typically located at /swapfile on Linux systems.

**11. Which apt-cache command will provide information for a package?**


* [ ] apt-cache show
* [ ] apt-cache info
* [ ] apt-cache print
* [ ] apt-cache search

**Correct answer:**
* [x] apt-cache show

**Explaination**: The apt-cache show command, followed by a package name, will provide information for a package.

**12. On a Debian system, which command would remove a package and all of its configuration files?**


* [ ] dpkg -r
* [ ] dpkg --remove
* [ ] dkpg --erase
* [ ] dpkg -P

**Correct answer:**
* [x] dpkg -P

**Explaination**: Running dpkg with the -P (purge) flag will remove a package and all of its configuration files.

**13. Which parameter would you add to dpkg-query to show which package contains a particular file?**


* [ ] -s
* [ ] -S
* [ ] -l
* [ ] -W

**Correct answer:**
* [x] -S

**Explaination**: Using the -S parameter with dpkg-query will show which package contains a particular file.

**14. If you update the configuration for GRUB 2, which command would you run to write that configuration to the correct directory for GRUB 2 to use the next time you boot the system?**


* [ ] grub-config -o /boot/grub/grub.cfg
* [ ] grub-mkconfig -o /boot/grub/grub.cfg
* [ ] grub-mkconfig -o /boot/grub/grub.conf
* [ ] grub-config -o /boot/grub/grub.conf

**Correct answer:**
* [x] grub-mkconfig -o /boot/grub/grub.cfg

**Explaination**: After updating the configuration for GRUB 2, you must run grub-mkconfig -o /boot/grub/grub.cfg to make GRUB 2 use the changes on the next system boot.

**15. When using GRUB 2, which menu entry parameter determines how long GRUB 2 will pause before booting the default menu entry?**


* [ ] GRUB_TIMEOUT=
* [ ] GRUB_WAIT=
* [ ] GRUB_PAUSE=
* [ ] GRUB_TIMER=

**Correct answer:**
* [x] GRUB_TIMEOUT=

**Explaination**: The menu entry that controls how long GRUB 2 will pause before booting the default menu entry is GRUB_TIMEOUT=.

**16. The naming format for shared libraries is:**


* [ ] libraryname.so.versionnumber
* [ ] libraryname.versionnumber.so
* [ ] libraryname-versionnumber.so
* [ ] libraryname-so.versionnumber

**Correct answer:**
* [x] libraryname.so.versionnumber

**Explaination**: The naming format for shared libraries is the library name . so suffix . version number. Example: libpthread.so.1.

**17. Which yum command will bring repository metadata to the most recent version?**


* [ ] yum update
* [ ] yum metadata
* [ ] yum upgrade
* [ ] yum check-metadata

**Correct answer:**
* [x] yum update

**Explaination**: The yum update command will update repository metadata.

**18. Using YUM or DNF, which option would uninstall a package from the system, along with any packages that depend on that package?**


* [ ] remove
* [ ] purge
* [ ] uninstall
* [ ] delete

**Correct answer:**
* [x] remove

**Explaination**: Using YUM or DNF, the remove option will uninstall a package from the system, along with all packages that depend on it. Example: dnf remove httpd.

**19. If you are unsure of the location of an executable mycommand on a Linux system, how would you find it?**


* [ ] whereis mycommand
* [ ] which mycommand
* [ ] find mycommand
* [ ] fetch mycommand

**Correct answer:**
* [x] which mycommand

**Explaination**: The which command provides the location of a specified executable. While it is possible to use find to locate the executable, it would require more options than were shown with that answer choice, and is not as straightforward or fast as using which for this task.

**20. Where are repository files stored for DNF and YUM?**


* [ ] /etc/repos.d/
* [ ] /etc/yum.repos.d/
* [ ] /etc/yum/repos/
* [ ] /etc/yum/repos.d/

**Correct answer:**
* [x] /etc/yum.repos.d/

**Explaination**: Repository files (.repo) are stored in /etc/yum.repos.d/.

**21. Which type of virtualization requires the guest to be capable of running all instructions on virtual hardware?**


* [ ] full virtualization
* [ ] paravirtualization
* [ ] hybrid virtualization
* [ ] bare metal

**Correct answer:**
* [x] full virtualization

**Explaination**: Fully virtualized machines must be able to run all instructions on the virtual hardware. They are not aware that they are running as virtual machines.

**22. You have set a variable called kodekloud using the export command. Which command would print the value of the kodekloud variable?**


* [ ] print kodekloud
* [ ] echo kodekloud
* [ ] echo $kodekloud
* [ ] read $kodekloud

**Correct answer:**
* [x] echo $kodekloud

**Explaination**: The echo command will print the value of a variable, and the variable name must have a $ before it. Example: echo $kodekloud will return the value assigned to the kodekloud variable.

**23. Before the apropos command will work on a new system, it may be necessary to run another command first. Which command would you run to make sure apropos has access to all of the information it needs?**


* [ ] apropos -update
* [ ] updatedb
* [ ] dbupdate
* [ ] mandb

**Correct answer:**
* [x] mandb

**Explaination**: The mandb command must be run to update the database for apropos. Otherwise, apropos will not have any results to return.

**24. Which command is used to search the short description of man pages for keywords?**


* [ ] apropos
* [ ] grep
* [ ] find
* [ ] cat

**Correct answer:**
* [x] apropos

**Explaination**: Each manual page has a short description available within it and apropos searches the descriptions for instances of keyword.

**25. The _____ command can be used to perform search and replace operations on text files.**


* [ ] grep
* [ ] sed
* [ ] find
* [ ] substitute

**Correct answer:**
* [x] sed

**Explaination**: The sed command can be used to perform search and replace operations on text files.

**26. Which command can be used to print only the desired field from text?**


* [ ] sed
* [ ] grep
* [ ] cut
* [ ] find

**Correct answer:**
* [x] cut

**Explaination**: The cut command can specify which field(s) to print from text.

**27. Which command can be used to rename an existing file or directory?**


* [ ] mv
* [ ] cp
* [ ] touch
* [ ] ls

**Correct answer:**
* [x] mv

**Explaination**: The mv command can be used to rename an existing file or directory by "moving" it from the original name to the new name. Example: mv myfile myfile2.

**28. To remove a directory called mydir along with all of its files and subdirectories, which command would you use?**


* [ ] rm mydir
* [ ] rm -R mydir
* [ ] rm -rf mydir
* [ ] rm mydir/*

**Correct answer:**
* [x] rm -rf mydir

**Explaination**: To remove a directory along with its files and subdirectories, we have to specify the -r (recursive) option, and will also need to specify the -f (force) option.

**29. Which command would take the contents of the /home/kodekloud/ directory and create a gzipped tar archive of the contents in a file called kodekloud.tar.gz?**


* [ ] tar -czvf kodekloud.tar.gz /home/kodekloud/
* [ ] tar -czvf /home/kodekloud/ kodekloud.tar.gz
* [ ] tar -cvf kodekloud.tar.gz /home/kodekloud/
* [ ] tar -cvf /home/kodekloud/ kodekloud.tar.gz

**Correct answer:**
* [x] tar -czvf kodekloud.tar.gz /home/kodekloud/

**Explaination**: When using tar to create gzipped tar archives, we must use the create (c), gzip (z), and file (f) options, followed by the name of the file we wish to create (kodekloud.tar.gz), and finally, the directory or files we wish to archive and compress (/home/kodekloud/).

**30. Which option can be used with ls to show hidden files?**


* [ ] -a
* [ ] -h
* [ ] --hidden
* [ ] -?

**Correct answer:**
* [x] -a

**Explaination**: The -a option with ls will show "all" including hidden files that begin with a period (.).

**31. Which command could be used to create an archive named backup.cpio containing all of the files and directories in the current working directory?**


* [ ] cpio -o > backup.cpio
* [ ] ls | cpio -o > backup.cpio
* [ ] cpio * > backup.cpio
* [ ] cpio -o * > backup.cpio

**Correct answer:**
* [x] ls | cpio -o > backup.cpio

**Explaination**: Since cpio takes its file list as input from standard input, we would use the output of the ls command to list the files and directories in the current working directory and pass that to cpio as input. The -o option with cpio tells cpio to create an archive, and the > operator is used before the archive's filename to direct cpio to create the archive with that name.

**32. Which of the following would search the contents of the /etc/ directory for the pattern "kodekloud" and write only the errors to a file a /home/kodekloud/errors.list?**


* [ ] grep -r 'kodekloud' /etc/ > /home/kodekloud/errors.list
* [ ] grep -r 'kodekloud' /etc/ 2> /home/kodekloud/errors.list
* [ ] grep -r 'kodekloud' /etc/ 2>&1 /home/kodekloud/errors.list
* [ ] grep -r 'kodekloud' /etc/ 1> /home/kodekloud/errors.list

**Correct answer:**
* [x] grep -r 'kodekloud' /etc/ 2> /home/kodekloud/errors.list

**Explaination**: The operator "2>" would cause stderr to be written to the file at "/home/kodekloud/errors.list" while the stdout content would be displayed to the screen.

**33. Which command would search the /etc/ directory for the pattern "kodekloud" and redirect all output (stdout and stderr) to the file at /home/kodekloud/output.list?**


* [ ] grep -r 'kodekloud' /etc/ 2> /home/kodekloud/output.list
* [ ] grep -r 'kodekloud' /etc/ 1> /home/kodekloud/output.list
* [ ] grep -r 'kodekloud' /etc/ > /home/kodekloud/output.list
* [ ] grep -r 'kodekloud' /etc/ \&> /home/kodekloud/output.list

**Correct answer:**
* [x] grep -r 'kodekloud' /etc/ \&> /home/kodekloud/output.list

**Explaination**: The operator "&>" will direct both stderr and stdout to a file. This will cause the file to contain all of the contents that would normally be seen on the screen, including error messages.

**34. The default configuration file for tmux is located at _____.**


* [ ] /etc/tmux.conf.d/tmux.conf
* [ ] /etc/tmux.conf
* [ ] /etc/conf/tmux.conf
* [ ] /etc/tmux/conf/tmux.conf

**Correct answer:**
* [x] /etc/tmux.conf

**Explaination**: The default configuration file for tmux is located at /etc/tmux.conf.

**35. Which operator can be used to append text to a file, without overwriting the existing contents?**


* [ ] ">"
* [ ] ">>"
* [ ] "<<"
* [ ] "<"

**Correct answer:**
* [x] ">>"

**Explaination**: The ">>" operator will append text to a file without overwriting the existing contents.

**36. To view dynamically updated information on system processes and resources, we can use the _____ command.**


* [ ] top
* [ ] ps
* [ ] jobs
* [ ] uptime

**Correct answer:**
* [x] top

**Explaination**: The top command provides dynamically updated information on processes and system resources, such as CPU usage and RAM.

**37. To get detailed information about a process when we already have the PID (process ID), we can use the _____ command.**


* [ ] proc
* [ ] pgrep
* [ ] ps
* [ ] pkill

**Correct answer:**
* [x] ps

**Explaination**: The ps command provides detailed information about a process when we already know the PID. Example: ps 1127.

**38. To set the priority for a process when it is run, we can use the _____ command.**


* [ ] nice
* [ ] renice
* [ ] top
* [ ] ps

**Correct answer:**
* [x] nice

**Explaination**: The nice command allows us to set the process priority ("niceness") when a command is run. Example: nice -n 15 top.

**39. The default configuration file for GNU screen is located at _____.**


* [ ] /etc/screenrc
* [ ] /etc/screen/screenrc
* [ ] /etc/screen.conf.d/screenrc
* [ ] /etc/screen/conf/screenrc

**Correct answer:**
* [x] /etc/screenrc

**Explaination**: The default configuration file for GNU screen is located at /etc/screenrc.

**40. Which command(s) can we use to view the priority of a process that is already running? Select all that apply.**


* [ ] ps
* [ ] top
* [ ] nice
* [ ] renice

**Correct answer:**
* [x] ps
* [x] top

**Explaination**: Both the ps command (with the -el or -Al flags) and the top command can be used to view the priority of a process that is already running.

**41. Which option can be used with grep when the case (upper or lower) of the term does not matter?**


* [ ] -i
* [ ] -e
* [ ] -f
* [ ] -k

**Correct answer:**
* [x] -i

**Explaination**: The ignore-case (-i) option can be used when the case of the term does not matter.

**42. When using vi, in normal mode, which key allows us to search the file?**


* [ ] /
* [ ] s
* [ ] f
* [ ] t

**Correct answer:**
* [x] /

**Explaination**: The / character allows us to specify a search term while in normal mode.

**43. In command mode, which command will allow us to save changes and exit vi?**


* [ ] :wq
* [ ] :q
* [ ] :q!
* [ ] :w

**Correct answer:**
* [x] :wq

**Explaination**: The command :wq or "write and quite" will allow us to save changes and exit vi.

**44. Which option for sed indicates that every instance of a term should be replaced?**


* [ ] g
* [ ] *
* [ ] a
* [ ] ?

**Correct answer:**
* [x] g

**Explaination**: The global (g) option with sed indicates that every instance of a term should be replaced. Example: sed  's/day/night/g'.

**45. When using vi in normal mode, which command will delete the line at the cursor?**


* [ ] d
* [ ] dd
* [ ] yy
* [ ] ZZ

**Correct answer:**
* [x] dd

**Explaination**: Pressing "d" twice or "dd" will delete the line at the current cursor position.

**46. Which of the following are features of BTRFS? Select all that apply.**


* [ ] compression
* [ ] subvolumes
* [ ] snapshots
* [ ] encryption

**Correct answer:**
* [x] compression
* [x] subvolumes
* [x] snapshots

**Explaination**: BTRFS supports compression, subvolumes, and snapshots.

**47. Which grep option can be used to search an entire directory and its sub directories?**


* [ ] -r
* [ ] *
* [ ] -R
* [ ] .

**Correct answer:**
* [x] -r

**Explaination**: The recursive (-r) option with grep will cause grep to search an entire directory and its sub directories. Example: grep -r 'kodekloud' /etc/.

**48. Which methods can be used on Linux to move memory pages from RAM to the hard disk?**


* [ ] swap partition
* [ ] swap file
* [ ] virtual memory
* [ ] disk cache

**Correct answer:**
* [x] swap partition
* [x] swap file

**Explaination**: Swap partitions and swap files can be used to move memory pages from RAM to the hard disk.

**49. Which command can be used to determine how much disk space is available?**


* [ ] du
* [ ] df
* [ ] free
* [ ] diskfree

**Correct answer:**
* [x] df

**Explaination**: The "disk free" (df) command can be used to see how much free space is left on a disk.

**50. Which command can be used to show information about all block devices on the system?**


* [ ] lsblk
* [ ] mount
* [ ] disks
* [ ] ls /dev/

**Correct answer:**
* [x] lsblk

**Explaination**: The lsblk command will show information about all block devices on the system and their filesystems.

**51. When trying to unmount a filesystem, you get an error saying the target is busy. Which command can you use to see the program that is keeping the disk busy?**


* [ ] lsof
* [ ] lsfiles
* [ ] iowait
* [ ] lsio

**Correct answer:**
* [x] lsof

**Explaination**: The "list open files" or "lsof" command can be used to see which program is currently using files on a disk that is busy. Example: lsof /dev/sda1.

**52. Before checking a filesystem for errors and repairing any that are found, you should first use the ____ command on the filesystem to get it ready for this process.**


* [ ] mount
* [ ] fsck
* [ ] xfs_repair
* [ ] umount

**Correct answer:**
* [x] umount

**Explaination**: A filesystem must first be offline for errors to be repaired, so a mounted filesystem must be unmounted before running fsck or xfs_repair.

**53. In addition to the device's disk identifier and partition number (e.g., /dev/sda1), what else can be used in /etc/fstab to identify a partition? Select all that apply.**


* [ ] UUID
* [ ] blkid
* [ ] label
* [ ] FSID

**Correct answer:**
* [x] UUID
* [x] label

**Explaination**: Both the UUID and the filesystem label can be used in /etc/fstab to identify a filesystem.

**54. To add execute permissions for the user-owner of a file to existing permissions, which command would you use?**


* [ ] chmod u+x
* [ ] chmod +x
* [ ] chmod u-x
* [ ] chmod -x

**Correct answer:**
* [x] chmod u+x

**Explaination**: The command chmod u+x would add execute permissions for the file's user-owner while keeping all other existing permissions.

**55. A file named "myfile" has the user owner "kodekloud" and the group owner "users." Which command(s) could be use to change the group owner to "friends"? Select all that apply.**


* [ ] chgrp friends myfile
* [ ] chown kodekloud:friends myfile
* [ ] chgrp myfile friends
* [ ] chown myfile kodekloud:friends

**Correct answer:**
* [x] chgrp friends myfile
* [x] chown kodekloud:friends myfile

**Explaination**: The chgrp command can be used to change group ownership, and the chown command can also be used if the group is specified after a user owner and a colon. With both commands, the group comes before the file name.

**56. Which command would change the permissions of the file "myfile" to read, write, and execute for the user owner, read and write for the group owner, and no permissions for all other users? Select all that apply.**


* [ ] chmod 760 myfile
* [ ] chmod 660 myfile
* [ ] chmod u=rwx,g=rw,u= myfile
* [ ] chmod u=rw,g=rw=u= myfile

**Correct answer:**
* [x] chmod 760 myfile
* [x] chmod u=rwx,g=rw,u= myfile

**Explaination**: We can use octal, where 7 is "read, write, and execute" for the user owner, 6 is "read and write" for the group owner, and 0 is "nothing" for all other users, or we can use ugo format, were u=rwx gives "read, write, and execute" to the user owner, g=rw gives "read and write" to the group owner, and u= gives nothing to all other users.

**57. You see myphoto.jpg -> photo in the output of the ls command. What type of link does this show?**


* [ ] Hard
* [ ] Soft

**Correct answer:**
* [x] Soft

**Explaination**: Symbolic or "soft" links are indicated by -> in the output of ls.

**58. Removable storage, such as CD-ROMs and flash drives are mounted to the _____ directory.**


* [ ] /mnt
* [ ] /media
* [ ] /home
* [ ] /run

**Correct answer:**
* [x] /media

**Explaination**: According to the FHS, user-mountable removable media such as flash drives and CD-ROMS should be mounted to the /media directory.

**59. Which command would create a soft link between myphoto.jpg and /home/kodekloud/photo?**


* [ ] ln -s myphoto.jpg /home/kodekloud/photo
* [ ] ln -s /home/kodekloud/photo myphoto.jpg
* [ ] ln myphoto.jpg /home/kodekloud/photo
* [ ] ln /home/kodekloud/photo myphoto.jpg

**Correct answer:**
* [x] ln -s myphoto.jpg /home/kodekloud/photo

**Explaination**: The ln -s command creates a soft link, and the format for ln in the specify the file being linked (myphoto.jpg) and then the target (/home/kodekloud/photo).

**60. Temporary files can be located in _____. Select any that apply.**


* [ ] /tmp
* [ ] /var/tmp
* [ ] /run
* [ ] /usr/tmp

**Correct answer:**
* [x] /tmp
* [x] /var/tmp
* [x] /run

**Explaination**: According to the FHS, temporary files that are cleared during system boot are stored in /tmp; temporary files that are not cleared during system boot are stored in /var/tmp; run-time data used by running processes is stored in /run.

---


## PCA mock 2

**1. You are writing an exporter for RabbitMQ and are creating a metric to track the size of the message queue. Which of the following would be an appropriate name for the metric.**


* [ ] rabbitmq_message_kilobytes
* [ ] message_rabbitmq_bytes
* [ ] rabbitmq_message_bytes
* [ ] bytes_rabbitmq_message

**Correct answer:**
* [x] rabbitmq_message_bytes

**Explaination**: When naming metrics, the first word should be the application/library which in this case is `rabbitmq`. The name second part of the metric name should be the metric name. The last part of the name should be the unit which should be unprefixed so that means we prefer bytes, seconds over kilobytes and milliseconds


**2. Which of the following components is responsible for receiving metrics from short lived jobs?**


* [ ] alertmanager 
* [ ] prometheus 
* [ ] exporter 
* [ ] pushgateway

**Correct answer:**
* [x] pushgateway

**Explaination**: Short lived jobs should push metrics to pushgateway before exiting. Prometheus can then scrape the pushgateway like any other target


**3. What is the default port that Prometheus listens on?**


* [ ] 9100 
* [ ] 9200 
* [ ] 9090 
* [ ] 9001

**Correct answer:**
* [x] 9090 

**Explaination**: Prometheus listens on port 9090 by default


**4. `kafka_topic_partition_replicas` metric tracks the number of partitions for a topic/partition. Which query will get the number of partitions for the past 2 hours. Result should return a range vector**


* [ ] kafka_topic_partition_replicas[2h]
* [ ] kafka_topic_partition_replicas offset 2h
* [ ] kafka_topic_partition_replicas<2h>
* [ ] kafka_topic_partition_replicas range 2h

**Correct answer:**
* [x] kafka_topic_partition_replicas[2h]

**Explaination**: To get value of time series for the past 2hours use a range selector of [2h]. This will return a range vector with the value of the metric for the past 2 hours.


**5. How many labels does the following time series have  node_fan_speed{instance="node8", job="server", fan="2"}**


* [ ] 3
* [ ] 2
* [ ] 1
* [ ] 4

**Correct answer:**
* [x] 3

**Explaination**: There are 3 labels `instance`, `job`, `fan`.


**6. Which of the following is not a component of the Prometheus solution?**


* [ ] pushgateway 
* [ ] alertmanager 
* [ ] exporters 
* [ ] influxdb

**Correct answer:**
* [x] influxdb

**7. What type of metric should be used for measuring a users heart rate?**


* [ ] counter 
* [ ] gauge 
* [ ] histogram 
* [ ] summary

**Correct answer:**
* [x] gauge 

**Explaination**: Since heart rate can go up or down, a gauge metric should be used in this case.

**8. What type of database does Prometheus use?**


* [ ] Mysql 
* [ ] Postgres 
* [ ] Time-Series database 
* [ ] Mongo 
* [ ] dynamoDB 
* [ ] AuroraDB

**Correct answer:**
* [x] Time-Series database 

**Explaination**: Prometheus uses a time series database


**9. What is this an example of?  	`99% availability with a median latency less than 300ms`?**


* [ ] SLA 	
* [ ] SLO 	
* [ ] SLI 	
* [ ] SLU

**Correct answer:**
* [x] SLO 	

**Explaination**: This is an example of an SLO. The SLI is the availability and latency, and the SLO is the target value or range for these SLIs, which in this case is 99% availability and 300ms latency. If this was an SLA it would state the consequences for not meeting the SLO.


**10. Which of the following is not a form of observability?**


* [ ] metrics 
* [ ] streams 
* [ ] logs 
* [ ] traces

**Correct answer:**
* [x] streams 

**Explaination**: The three forms of observability are metrics, logs, and traces


**11. Which of the following would make for a good SLI?**


* [ ] request failures
* [ ] disk utilization
* [ ] memory utilization
* [ ] Fan Speed
* [ ] Server temperature

**Correct answer:**
* [x] request failures

**Explaination**: For good SLIs metrics, use metrics that impact the user's experience. Disk utilization, memory utilization, fan speed, and server temperature are not things that impact the user. Request failures will impact a user’s experience



**12. What are the different states a Prometheus alert can be in?**


* [ ] inactive, triggered, complete
* [ ] ok, pending,  firing
* [ ] silenced, firing, triggered
* [ ] inactive, pending, firing

**Correct answer:**
* [x] inactive, pending, firing

**Explaination**: Alerts can have 3 states:
inactive - Alert expression has not returned any results
pending - the state of an alert that has been active for less than the configured threshold duration
firing - the state of an alert that has been active for longer than the configured threshold duration


**13. Which query below will give the 95% quantile of the metric `http_file_upload_bytes`?**


* [ ] histogram_quantile(0.95, http_file_upload_bytes_bucket)
* [ ] http_file_upload_bytes(quantile=”0.95”}
* [ ] http_file_upload_bytes < 95%
* [ ] quantile(http_file_upload_bytes, 0.95)

**Correct answer:**
* [x] histogram_quantile(0.95, http_file_upload_bytes_bucket)

**Explaination**: For histogram metrics, to calculate a quantile use the `histogram_quantile` function. The function takes two arguments, the desired percentile, and the histogram metric, make sure to pass in the _bucket sub metric.


**14. What method does Prometheus use to collect metrics from targets?**


* [ ] push 
* [ ] pull 
* [ ] batch upload 
* [ ] rsync

**Correct answer:**
* [x] pull 

**Explaination**: Prometheus follows a pull based model. The prometheus.yml file will have a list of all targets prometheus will need to scrape, which involves sending an http request to the target


**15. An application is advertising metrics at the path `/monitoring/stats`. What property in the scrape configs needs to be modified?**


* [ ] targets: [“node1:9100/monitoring/stats”]
* [ ] metrics_path: “/monitoring/stats”
* [ ] http_path: “/monitoring/stats”
* [ ] scrape_path: “/monitoring/stats”

**Correct answer:**
* [x] metrics_path: “/monitoring/stats”

**Explaination**: the `metrics_path` property should be updated with the new path `/monitoring/stats`



**16. What command should be used to verify that a Prometheus config is valid?**


* [ ] prom-util validate config prometheus.yml
* [ ] promcli check config prometheus.yml
* [ ] promtool validate config prometheus.yml
* [ ] promtool check config prometheus.yml

**Correct answer:**
* [x] promtool check config prometheus.yml

**Explaination**: Promtool is the cli utility used to verify prometheus configs. The `check config` subcommand will verify the prometheus config passed to it.


**17. What are the two attributes that metrics can have?**


* [ ] TYPE, HELP 
* [ ] INFO, CATEGORY
* [ ] INFO, TYPE 
* [ ] HELP, INFO
* [ ] TYPE, DESCRIPTION

**Correct answer:**
* [x] TYPE, HELP 

**Explaination**: The two types of attributes metrics can have are:
	help - description of what the metric is
	type - specifies what type of metric(counter, gauge, histogram, summary)


**18. The metric `mealplanner_consumed_calories` tracks the number of calories that have been consumed by the user. What query will return the amount of calories that had been consumed 4 days ago?**


* [ ] mealplanner_consumed_calories[4d]
* [ ] mealplanner_consumed_calories  fallback 4d
* [ ] mealplanner_consumed_calories{offset=”4d”}
* [ ] mealplanner_consumed_calories offset 4d

**Correct answer:**
* [x] mealplanner_consumed_calories offset 4d

**Explaination**: 
To get the value of a metric 4 days ago, use the offset modifier and specify `4d`


**19. What type of metric should be used to track the number of miles a car has driven?**


* [ ] counter
* [ ]  gauge 
* [ ] histogram
* [ ]  summary

**Correct answer:**
* [x] counter

**Explaination**: A counter metric should be used as the number of miles a car has been driven cannot go down.


**20. What is the name of the Prometheus query language?**


* [ ] PromQuery 
* [ ] PromSearch 
* [ ] PromQL 
* [ ] SQL

**Correct answer:**
* [x] PromQL 

**21. The metric `node_fan_speed_rpm` tracks the current fan speeds. The `location` label specifies where on the server the fan is located. Which query will return the fan speeds for all fans except the `rear` fan**


* [ ] node_fan_speed_rpm{location!=”rear”}
* [ ] node_fan_speed_rpm<location!=”rear”>
* [ ] node_fan_speed_rpm{location~=”rear”}
* [ ] node_fan_speed_rpm{location!~”rear”}

**Correct answer:**
* [x] node_fan_speed_rpm{location!=”rear”}

**Explaination**: To match on all time series where location is not `rear` use the `negative equality matcher` `!=`

**22. Which of the following is not a valid time value to be used in a range selector?**


* [ ] 30s
* [ ]  25m
* [ ]  3hr 
* [ ] 2w 
* [ ] 80ms 
* [ ] 4y

**Correct answer:**
* [x]  3hr 

**Explaination**: the time unit for hours is just `h` not `hr`


**23. What query will return all the instances whose active memory bytes is less than 10000?**


* [ ] node_memory_Active_bytes{< 10000}
* [ ] node_memory_Active_bytes !< 10000
* [ ] node_memory_Active_bytes < 10000
* [ ] node_memory_Active_bytes > 10000

**Correct answer:**
* [x] node_memory_Active_bytes < 10000

**Explaination**: To return all time series less than 10000, use the `<` operator.

**24. With the following alertmanager configs, after a notification has been sent out,  a new alert comes in. How long will alertmanager wait before firing a new notification?**


* [ ] 60s 
* [ ] 15m 
* [ ] 12hr 
* [ ] 5m 
* [ ] 80ms 
* [ ] 4y

**Correct answer:**
* [x] 15m 

**Code**: 
route:
  receiver: staff
  group_by: ['severity']
  group_wait: 60s
  group_interval: 15m
  repeat_interval: 12h
  routes:
    - matches:
job: kubernetes
      receiver: infra
      group_by: ['severity']


**Explaination**: The group_interval property determines how long alertmanager will wait after sending a notification, before it sends a new notification for a group.


**25. For `metric_relabel_configs` and `relabel_configs`, when matching on multiple source labels, what is the default delimiter**


* [ ] ;
* [ ]  :
* [ ]  - 
* [ ] =
* [ ]  / 
* [ ] ?

**Correct answer:**
* [x] ;

**Explaination**: The default delimiter is `;`


**26. What does the following config do?**


* [ ] drops all targets with `env`, and `team` labels
* [ ] scrapes all targets with `env` and `team` labels
* [ ] drops all targets whose `env` label is set to `dev` and `team` label is set to `marketing`
* [ ] sets the `env` and team` label on all targets for this job

**Correct answer:**
* [x] drops all targets whose `env` label is set to `dev` and `team` label is set to `marketing`

**Code**: 
scrape_configs:
 - job_name: example
   relabel_configs:
    - source_labels: [env, team]
      regex: dev;marketing
      action: drop


**Explaination**: The source labels match on labels `env` and `team`. The regex field determines what values to match on for the labels. In this case it will match on env=dev and team=marketing. Since the action is drop, it means all targets with env=dev and team=marketing labels will be dropped. All other targets will get scraped


**27. Which of the following components is responsible for collecting metrics from an instance and exposing them in a format Prometheus expects?**


* [ ] exporters 	
* [ ] alertmanager 	
* [ ] pushgateway 	
* [ ] Grafana 	
* [ ] TSDB

**Correct answer:**
* [x] exporters 	

**28. The metric `node_filesystem_avail_bytes` reports the available bytes for each filesystem on a node. Which query will return all filesystems that has either less than 1000 available bytes or greater than 50000 bytes**


* [ ] node_filesystem_avail_bytes < 1000 || node_filesystem_avail_bytes > 50000
* [ ] node_filesystem_avail_bytes < 1000 or > 50000
* [ ] node_filesystem_avail_bytes < 1000 || > 50000
* [ ] node_filesystem_avail_bytes < 1000 or node_filesystem_avail_bytes > 50000

**Correct answer:**
* [x] node_filesystem_avail_bytes < 1000 or node_filesystem_avail_bytes > 50000

**Explaination**: To return all filesystems less than 1000 or greater than 50000, use the `or` operator.


**29. Which of the following is Prometheus’ built in dashboarding/visualization feature?**


* [ ] Go Templates 
* [ ] 	Grafana 	
* [ ] client libraries
* [ ] Console Templates

**Correct answer:**
* [x] Console Templates

**30. What does the following config do?**


* [ ] The label `fstype` will be dropped for all metrics
* [ ] All targets with the `fstype` label will not get scraped
* [ ] The metric `fstype` will have all of its labels dropped
* [ ] the metric `fstype` will get dropped

**Correct answer:**
* [x] The label `fstype` will be dropped for all metrics

**Code**: 
scrape_configs:
  - job_name: "demo"
    metric_relabel_configs:
      - regex: fstype
        action: labeldrop


**Explaination**: `metric_relabel_configs` runs after metrics are scraped. The `labeldrop` action will drop the label matched from the `regex` field.



**31. The following time series return values with a lot of decimal values. What query will return values rounded down to the closest integer node_cpu_seconds_total {cpu="0", mode="idle"} 115.12​ {cpu="0", mode="irq"} 87.4482​ {cpu="0", mode="steal"} 44.245**


* [ ] ceil(node_cpu_seconds_total)
* [ ] floor(node_cpu_seconds_total)
* [ ] abs(node_cpu_seconds_total)
* [ ] node_cpu_seconds_total @floor

**Correct answer:**
* [x] floor(node_cpu_seconds_total)

**Explaination**: To round the values down to nearest integer use the `floor` function

**32. The metric http_requests tracks the total number of requests across each endpoint and method. What query will return the total number of requests for each path**


* [ ] http_requests{path} 	
* [ ] http_requests on path
* [ ] sum by(path) (http_requests)
* [ ] sum(http_requests{by=path}

**Correct answer:**
* [x] sum by(path) (http_requests)

**Code**: 
http_requests{method="get", path="/auth"} 3​

http_requests{method="post", path="/auth"} 1​

http_requests{method="get", path="/user"} 4​

http_requests{method="post", path="/user"} 8​

http_requests{method="post", path="/upload"} 2​

http_requests{method="get", path="/tasks"} 4​

http_requests{method="put", path="/tasks"} 6​

http_requests{method="post", path="/tasks"} 1​

http_requests{method="get", path="/admin"} 3​

http_requests{method="post", path="/admin"} 9


**Explaination**: To get the total number of requests for each path, a `sum` aggregator must be performed and since we want to group it by path a `by` clause on `path` should by used


**33. Which of the following is not a valid method for reloading alertmanager configuration?**


* [ ] restart alertmanager process
* [ ] send a SIGHUP signal to alertmanager proces
* [ ] HTTP post to /-/reload endpoint
* [ ] hit the `reload config` button in alertmanager web-ui

**Correct answer:**
* [x] hit the `reload config` button in alertmanager web-ui

**34. Which statement is true regarding Prometheus rules?**


* [ ] Groups are run sequentially and rules within a group are run in parallel
* [ ] Groups are run in parallel and rules within a group are run sequentially
* [ ] Groups and rules within a group are run in parallel
* [ ] Groups and rules within a group are run sequentially 	

**Correct answer:**
* [x] Groups are run in parallel and rules within a group are run sequentially

**35. Groups and rules within a group are run sequentially**


* [ ] Both Alert `labels` and `annotations` can be used for routing on Alertmanager
* [ ] Both Alert `labels` and `annotations` are used purely for descriptive purposes
* [ ] Alert `labels` can be used as metadata so alertmanager can match on them and performing routing policies, `Annotations` should be used for cosmetic descriptions of the alerts
* [ ] Alert `annotations` can be used as metadata so alertmanager can match on them and performing routing policies, `labels` should be used for cosmetic description of the alerts

**Correct answer:**
* [x] Alert `labels` can be used as metadata so alertmanager can match on them and performing routing policies, `Annotations` should be used for cosmetic descriptions of the alerts

**36. Where are alert rules defined?**


* [ ] In the  Alertmanager.yml file
* [ ] In the PushGateway
* [ ] In the Prometheus.yml file
* [ ] In a separate rules file on the Prometheus server

**Correct answer:**
* [x] In a separate rules file on the Prometheus server

**Explaination**: Alert rules are defined on the prometheus server in a separate rules file


**37. What update needs to occur to add an annotation called `description` that prints out the message `redis server <insert instance name> is down!`**


* [ ] description: "redis server <.Labels.instance> is down!"
* [ ] description: "redis server $$.Labels.instance$$ is down!"
* [ ] description: "redis server {{up{job=”redis”}}} is down!"
* [ ] description: "redis server {{.Labels.instance}} is down!"

**Correct answer:**
* [x] description: "redis server {{.Labels.instance}} is down!"

**Code**: 
- name: redis-alerts
    rules:
      - alert: redis_down
        expr: up{job="redis"} == 0
        labels:
          org: kodekloud


**Explaination**: To create an annotation that will insert the instance name, the go templating language needs to be used. To access the instance label use `{{.Labels.instance}}`


**38. Analyze the alertmanager configs below  :  Based off the alert below, which receiver will send the notification for the alert 	 	alert 	labels: 		team: frontend**


* [ ] kodekloud 	
* [ ] apple 	
* [ ] general 	
* [ ] kodekloud-pager

**Correct answer:**
* [x] general 	

**Code**: 
route:
  group_wait: 20s
  receiver: general
  group_by: ['alertname']
  routes:
    - match:
        org: kodekloud
      receiver: kodekloud-pager
    - match:
        org: apple
      receiver: apple


**Explaination**: The alert has only one label `team: frontend` which does not match any of the routes, so it fallsback to the default receiver which is `general`


**39. What data type do Prometheus metric values use?**


* [ ] Decimals 	
* [ ] integers 	
* [ ] strings 	
* [ ] 64 bit floats

**Correct answer:**
* [x] 64 bit floats

**Explaination**: The alert has only one label `team: frontend` which does not match any of the routes, so it fallsback to the default receiver which is `general`


**40. The metric `health_consumed_calories` tracks how many calories a user has eaten and `health_burned_calories` tracks the number of calories burned while exercising. To calculate net calories for the day subtract `health_burned_calories` from `health_consumed_calories`. Based on the time series below, which expression successfully calculates net calories.  	 health_consumed_calories{job="health", meal="dinner"} 800 health_burned_calories{job="health", activity="cardio"} 200**


* [ ] health_consumed_calories - health_burned_calories
* [ ] health_consumed_calories - ignoring(meal, activity) health_burned_calories
* [ ] health_consumed_calories - ignoring(meal) health_burned_calories
* [ ] health_consumed_calories - ignoring(actvity) health_burned_calories

**Correct answer:**
* [x] health_consumed_calories - ignoring(meal, activity) health_burned_calories

**Explaination**: The `health_consumed_calories` has label `meal` that `health_burned_calories` does not have and `health_burned_calories` has the label `activity` that `health_consumed_calories` doesn’t have. To perform subtraction operation between the two, both labels will need to be ignored using the `ignoring` keyword



**41. Which component of the Prometheus architecture should be used to automatically discover all nodes in a Kubernetes cluster?**


* [ ] service discovery 
* [ ] 	exporters 	
* [ ] push gateway 	
* [ ] alertmanager

**Correct answer:**
* [x] service discovery 

**42. What is the purpose of Prometheus `scrape_interval`?**


* [ ] defines what targets to scrape
* [ ] Defines how long to wait for a scrape before timing out
* [ ] Defines how long Prometheus waits before clearing out the TSDB
* [ ] Defines how frequently to scrape a target

**Correct answer:**
* [x] Defines how frequently to scrape a target

**Explaination**: scrape_interval configs determine how often to scrape a target. If scrape_interval is set to 30s then each target will get scraped every 30s


**43. A car reports the number of miles it has been driven with the metric `car_total_miles` Which query returns what is the average rate of miles the car has driven the past 2 hours. Use a 4m sample range and a query interval of 1m.**


* [ ] avg_over_time(rate(car_total_miles[4h]))
* [ ] avg_over_time(rate(car_total_miles[4m])) [2h:1m]
* [ ] avg_over_time(rate(car_total_miles[4m]) [2h:1m])
* [ ] avg_over_time(rate(car_total_miles[4m]) [1m:2h])

**Correct answer:**
* [x] avg_over_time(rate(car_total_miles[4m]) [2h:1m])

**Explaination**: Since the question is asking for the what is the average `rate` of miles over the past 2 hours, rate function will need to be used:
rate(car_total_miles[4m])

To get the average over the past 2 hours use the avg_over_time function. The avg_over_time function requeries a range vector to be passed in, a subquery will need to be performed on the rate to get the range vector that contains the rate of errors for the past 2 hours. Since we need the average for the past 2hours, the first value in the subquery is going to be `2h` and the second number is the query interval. Thus the final query looks like:


avg_over_time(rate(car_total_miles[4m]) [2h:1m])


**44. Which query will return whether or not a target is currently able to be scraped?**


* [ ] status 	
* [ ] up 	
* [ ] scrape 	
* [ ] reachability

**Correct answer:**
* [x] up 	

**Explaination**: the `up` query will return a `1` if a target is able to be successfully scraped and a `0` if it is not


**45. Which statement is true about the rate/irate functions?**


* [ ] rate() calculates average rate over entire interval, irate() calculates the rate only between the last two datapoints in an interval
* [ ] rate() and irate() operate in the same exact way
* [ ] rate() calculates rate by using the first two datapoints over an interval, irate() calculates the rate only between the last two datapoints in an interval
* [ ] irate() calculates average rate over entire interval, rate() calculates the rate only between the last two datapoints in an interval

**Correct answer:**
* [x] rate() calculates average rate over entire interval, irate() calculates the rate only between the last two datapoints in an interval

**46. Which configuration in alertmanager will wait 2 minutes before firing off an alert to prevent unnecessary notifications getting sent?**


* [ ] group_wait: 2m 	
* [ ] group_interval: 2m 	
* [ ] repeat_interval: 2m
* [ ] group_cache: 2m

**Correct answer:**
* [x] group_wait: 2m 	

**Explaination**: When an alert arrives on alertmanager, alertmanager will wait the amount of time specified on `group_wait`, to wait for other alerts to arrive before firing off a notification.


**47. What does the following config do?**


* [ ] Targets with label `team` will get scraped, all other targets will be dropped
* [ ] renames the `organization` label to `team` and the value of the label will get prepended with `org-`
* [ ] the `team` label will get dropped
* [ ] renames the `team` label to `orginization` and the value of the label will get prepended with `org-`

**Correct answer:**
* [x] renames the `team` label to `orginization` and the value of the label will get prepended with `org-`

**Code**: 
scrape_configs:
  - job_name: "example"
    metric_relabel_configs:
      - source_labels: [team]
        regex: (.*)
        action: replace
        target_label: organization
        replacement: org-$1


**Explaination**:  The metric_relabel_configs takes place after scraping metrics. The `source_labels` matches `team` and the action is set to `replace. Which will replace the label name of `team` with the `target_label` that is specified. The value of the label will be changed to what is specified in the replacement field, which prepends `org-` to the value.


**48. The `node_cpu_seconds_total` metric tracks the number of seconds cpu has spent in a specific mode. The metric will break it down per cpu using the `cpu` label. Which query will return the total time all cpus on an instance spent in a mode that is not `idle`. Make sure to group the result on a per instance basis**


* [ ] sum by(instance) (node_cpu_seconds{mode!="idle"}
* [ ] sum(node_cpu_seconds{mode!="idle"}
* [ ] sum by(instance) (node_cpu_seconds{mode=~"idle"}

**Correct answer:**
* [x] sum by(instance) (node_cpu_seconds{mode!="idle"}

**Code**: 
node_cpu_seconds_total{cpu="0", instance="192.168.1.168:9100", job="test", mode="idle"}
node_cpu_seconds_total{cpu="0", instance="192.168.1.168:9100", job="test", mode="iowait"}
node_cpu_seconds_total{cpu="0", instance="192.168.1.168:9100", job="test", mode="irq"}
node_cpu_seconds_total{cpu="0", instance="192.168.1.168:9100", job="test", mode="nice"}
node_cpu_seconds_total{cpu="0", instance="192.168.1.168:9100", job="test", mode="softirq"}
node_cpu_seconds_total{cpu="0", instance="192.168.1.168:9100", job="test", mode="steal"}
node_cpu_seconds_total{cpu="0", instance="192.168.1.168:9100", job="test", mode="system"}
node_cpu_seconds_total{cpu="1", instance="192.168.1.168:9100", job="test", mode="idle"}
node_cpu_seconds_total{cpu="1", instance="192.168.1.168:9100", job="test", mode="iowait"}
node_cpu_seconds_total{cpu="1", instance="192.168.1.168:9100", job="test", mode="irq"}
node_cpu_seconds_total{cpu="1", instance="192.168.1.168:9100", job="test", mode="nice"}
node_cpu_seconds_total{cpu="1", instance="192.168.1.168:9100", job="test", mode="softirq"}
node_cpu_seconds_total{cpu="1", instance="192.168.1.168:9100", job="test", mode="steal"}
node_cpu_seconds_total{cpu="1", instance="192.168.1.168:9100", job="test", mode="system"}


**Explaination**:  To sum up all modes except `idle` use the negative equality matcher `mode!="idle"` and use the `sum` aggregator. To group by instance use the `by` keyword and pass the `instance` label



**49. A database backup service has an slo that states that 97% of all backup jobs will be completed within 60s. A histogram metric is configured to track the backup process time, which of the following bucket configurations is recommended for the desired slo**


* [ ] 1, 5, 10, 25, 35, 50, 80, 90
* [ ] 10, 25, 27, 30, 32, 35, 40, 50
* [ ] 35, 45, 55, 60, 65, 75, 100
* [ ] 1, 3, 8, 10, 12, 15, 17, 30, 40, 55, 65, 70

**Correct answer:**
* [x] 35, 45, 55, 60, 65, 75, 100

**Explaination**: Since histogram quantiles are approximations, to find out if a slo has been met, make sure that a bucket is specified at the desired slo value of 60s.


**50. What type of data should prometheus monitor?**


* [ ] Events 	
* [ ] numeric 	
* [ ] traces 	
* [ ] system logs

**Correct answer:**
* [x] numeric 	

**51. How is application instrumentation achieved?**


* [ ] pushgateway 	
* [ ] alertmanager 	
* [ ] Service discovery 	
* [ ] Client libraries 	
* [ ] grafana

**Correct answer:**
* [x] Client libraries 	

**Explaination**: Prometheus client libraries allow you to instrument applications.



**52. What is the purpose of repeat_interval in alertmanager?**


* [ ] how long to initially wait to send a group of alerts
* [ ] How often to fire alerts to alertmanager
* [ ] How long to wait before sending a notification again if it has already been sent successfully for an alert
* [ ] How often to space out notifications for each group of alerts

**Correct answer:**
* [x] How long to wait before sending a notification again if it has already been sent successfully for an alert

**Explaination**: Prometheus client libraries allow you to instrument applications.


**53. Analayze alertmanager configs below.      Based off the following alert which receiver will receive the notification  alertname: node_filesystem_full labels: 	team: frontend 	notification: pager**


* [ ] frontend-email 	
* [ ] frontend-pager 
* [ ] 	backend-email 
* [ ] general-email

**Correct answer:**
* [x] frontend-pager 

**Code**: 
route:
  receiver: general-email
  group_by: [alertname]
  routes:
    - receiver: frontend-email
      matchers:
        - team: frontend
      routes:
        - matchers:
            notification: pager
          receiver: frontend-pager
    - receiver: backend-email
      matchers:
        - team: backend
    - receiver: auth-email
      matchers:
        - team: auth


**Explaination**: Since the alert has the label `team: frontend` it will match the first route. The second label `notification: pager` will cause the alert to match the subroute and send it to frontend-pager


**54. What does the double underscore `__` before a label name signify?**


* [ ] The label was set by a pushgateway
* [ ] The label is a `reserved` label
* [ ] The label is a malformed label
* [ ] The label was set by a client library

**Correct answer:**
* [x] The label is a `reserved` label

**55. What type of data does Prometheus collect?**


* [ ] events
* [ ]  logs
* [ ]  traces 
* [ ] numeric

**Correct answer:**
* [x] numeric

**56. For a histogram metric, what are the different submetrics?**


* [ ] `_count`, `_bucket`
* [ ] `_bucket`
* [ ] `_count`, `_bucket`, `_sum`
* [ ] `_total`, `_sum`, `_bucket`

**Correct answer:**
* [x] `_count`, `_bucket`, `_sum`

**Explaination**: Histogram metrics have 3 submetrics:
		count: total number of observations
		sum: sum of all observations
		bucket: number of observations for a specific bucket


**57. What is the default path Prometheus will scrape to collect metrics?**


* [ ] /metrics 	
* [ ] /swagger-stats/metrics 	
* [ ] /prometheus 
* [ ] 	/stats

**Correct answer:**
* [x] /metrics 	

**58. The metric `http_errors_total` has 3 labels, `path`, `method`, `error`. Which of the following queries will give the total number of errors for a path of `/auth`, method of `POST`, and error code of `401`?**


* [ ] http_errors_total
* [ ] http_errors_total{path="/auth",method="POST",code="401"}
* [ ] http_errors_total{path="/auth",method="POST"}
* [ ] http_errors_total{path="/auth",method="PUT",code="401"}

**Correct answer:**
* [x] http_errors_total{path="/auth",method="POST",code="401"}

**59. In the prometheus configuration, what is the purpose of the `scheme` field?**


* [ ] Determines if prometheus uses push or pull based model
* [ ] Determines if prometheus will collect metrics or logs
* [ ] Determines if prometheus will use http or https
* [ ] Determines the format metrics are stored in the database 	

**Correct answer:**
* [x] Determines if prometheus will use http or https

**60. What does the following config do?**


* [ ] Drops all targets with the `docker_container_crash_total` metric
* [ ] renames the metric `docker_container_crash_total` to `docker_container_restart_total`
* [ ] Drops the `docker_container_crash_total` metric 
* [ ] Replaces all labels fro  `docker_container_crash_total`

**Correct answer:**
* [x] renames the metric `docker_container_crash_total` to `docker_container_restart_total`

**Code**: 
scrape_configs:
  - job_name: "demo"
    metric_relabel_configs:
      - source_labels: [__name__]
        regex: docker_container_crash_total
        action: replace
        target_label: __name__
        replacement: docker_container_restart_total


**Explaination**:  when the `source_label` matches on the label `__name__` this represents the metrics name. In this case the field `regex` represents the name of the metric you want to match on. The action is set to replace, and the value of `replacement` will be the new name of the metric. 


---


## Docker Engine Architecture

**1. By default, data stored inside the container is always persistent**


* [ ] True 
* [ ] False

**Correct answer:**
* [x] False

**2. What are the components of the Docker Engine?**


* [ ] REST API, Docker Daemon
* [ ]  Images, Containers, Volumes
* [ ]  Docker Cli, Docker Daemon, REST API

**Correct answer:**
* [x]  Docker Cli, Docker Daemon, REST API

**3. What component of the docker engine manages the images, containers, volumes and networks on a host?**


* [ ]  REST API
* [ ]  Docker Daemon
* [ ] Docker CLI

**Correct answer:**
* [x]  Docker Daemon

**4. Which component is responsible for keeping the containers alive when the Docker Daemon goes down?**


* [ ]  LibContainer
* [ ]  Runc
* [ ] Containerd
* [ ] Containerd-Shim

**Correct answer:**
* [x] Containerd-Shim

**5. We can run containers without installing Docker.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**6. By default, Docker is configured to look for images on Google Cloud Registry**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**7. What are the primary objects that Docker engine manages?**


* [ ] RunC
* [ ] LibContainer
* [ ]  Containerd
* [ ]  Images, Containers, Volumes, Networks

**Correct answer:**
* [x]  Images, Containers, Volumes, Networks

**8. What component of the docker architecture is responsible for managing containers on Linux on version 1.15 of Docker Engine?**


* [ ] LibContainer
* [ ]  Docker API
* [ ] LXC

**Correct answer:**
* [x] LibContainer

**9. What does OCI stand for?**


* [ ] Open Communication Initiative
* [ ]  Open Container Initiative
* [ ]  Open Command Interface
* [ ] Open Container Interface

**Correct answer:**
* [x]  Open Container Initiative

**10. Which component is a read-only template used for creating a Docker container?**


* [ ]  Docker Network
* [ ]  Docker images
* [ ] Container
* [ ] Docker volume

**Correct answer:**
* [x]  Docker images

**11. Which is the default data directory for Docker?**


* [ ] /var/lib/docker
* [ ] /var/log/docker
* [ ] /etc/docker
* [ ] /home/docker

**Correct answer:**
* [x] /var/lib/docker

**12. What are the 2 specifications from OCI?**


* [ ] container-spec
* [ ]  runtime-spec
* [ ] image-spec
* [ ] oci-spec
* [ ]  libcontainer-spec

**Correct answer:**
* [x]  runtime-spec
* [x] image-spec

**13. What is the command to view the version of docker engine installed?**


* [ ] docker --version
* [ ] docker version
* [ ] docker engine info
* [ ]  docker info engine

**Correct answer:**
* [x] docker version

---


## Docker Service Configuration

**1. What is the port conventionally used to configure un-encrypted traffic on TCP?**


* [ ]  2345 
* [ ]  2346  
* [ ] 2375  
* [ ] 2376

**Correct answer:**
* [x] 2375  

**2. What is the command used to list the running containers on the Docker Host?**


* [ ] docker container ls
* [ ] docker container start
* [ ] docker container stop
* [ ] None of the above

**Correct answer:**
* [x] docker container ls

**3. What file is used to configure the docker daemon?**


* [ ] /var/lib/docker/docker.conf
* [ ] /var/lib/docker/daemon.json
* [ ] /etc/docker/daemon.json
* [ ] /etc/docker/daemon.conf

**Correct answer:**
* [x] /etc/docker/daemon.json

**4. What flags are used to configure encryption on docker daemon?**


* [ ]  tlsverify, tlscert, tlskey
* [ ]  tlsverify, key, cert
* [ ]  key, cert, tls  
* [ ] host, key, cert, tls

**Correct answer:**
* [x]  tlsverify, tlscert, tlskey

**5. On what interfaces are the docker daemon made available by default?**


* [ ]  TCP socket
* [ ]  UDP socket
* [ ]  Unix Socket
* [ ]  192.168.1.10

**Correct answer:**
* [x]  Unix Socket

**6. What is the default network driver used when a container is created?**


* [ ] overlay  
* [ ] bridge 
* [ ]  none  
* [ ] host

**Correct answer:**
* [x] bridge 

**7. Which of the below commands create a container with nginx image and name nginx?**


* [ ]  docker container create nginx --name nginx
* [ ] docker container --name nginx nginx
* [ ] docker container run nginx
* [ ] docker container create --name nginx nginx

**Correct answer:**
* [x] docker container create --name nginx nginx

**8. What is the command to start docker daemon manually?**


* [ ] docker
* [ ] dockerd
* [ ] docker-engine
* [ ] docker --start-engine

**Correct answer:**
* [x] dockerd

**9. How to list all running and stopped containers and their status?**


* [ ]  docker container ls
* [ ] docker container ls -a
* [ ] docker container ls -aq  
* [ ] docker container ls -q

**Correct answer:**
* [x] docker container ls -a

**10. How to start a stopped Container?**


* [ ]  docker container rm nginx
* [ ] docker container start nginx
* [ ]  docker container create nginx
* [ ] docker container run nginx

**Correct answer:**
* [x] docker container start nginx

**11. What is the option used in docker run command to attach to the terminal of the container in an interactive mode?**


* [ ] -f 
* [ ]  -it 
* [ ]  -i  
* [ ] -t

**Correct answer:**
* [x]  -it 

**12. How do I get only the IDs of running containers?**


* [ ] docker container ls
* [ ] docker container ls -a
* [ ] docker container ls -aq
* [ ] docker container ls -q

**Correct answer:**
* [x] docker container ls -q

**13. You cannot start a killed container.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**14. What is the command to change the container name “httpd” to “webapp”?**


* [ ]  docker container rename httpd webapp
* [ ] docker container rename webapp httpd
* [ ] docker container replace --name httpd webapp
* [ ] docker container create --name webapp httpd

**Correct answer:**
* [x]  docker container rename httpd webapp

**15. What is the command to run a “nginx” container in a detached mode with name “webapp”?**


* [ ] docker container run -it --name webapp nginx
* [ ] docker container run -it --name nginx webapp
* [ ] docker container run -d --name webapp nginx
* [ ] docker container run -d --name nginx webapp

**Correct answer:**
* [x] docker container run -d --name webapp nginx

**16. Delete the stopped container named “webapp”.**


* [ ] docker container delete webapp
* [ ] docker container remove webapp
* [ ] docker container kill webapp
* [ ] docker container rm webapp

**Correct answer:**
* [x] docker container rm webapp

---


## Interacting with a Running Container

**1. Which combination of keys are used to escape from the shell and keep the container webapp running?**


* [ ]  Ctrl+c
* [ ] Ctrl+p+q
* [ ]  exit, Ctrl+p+q
* [ ] Ctrl+c, exit

**Correct answer:**
* [x] Ctrl+p+q

**2. You have a running container and want to execute a command inside it. Which command will you use?**


* [ ]  execute 
* [ ]  run  
* [ ] start 
* [ ]  exec

**Correct answer:**
* [x]  exec

**3. How to display the running processes inside the container?**


* [ ] docker container top container-name
* [ ] docker container stats container-name
* [ ] docker ps container-name
* [ ] docker container logs container-name

**Correct answer:**
* [x] docker container top container-name

**4. You have a webapp container and image httpd. Inspect the logs of the webapp container. Which command is used to get the stream logs of the webapp container so that you can view the logs live?**


* [ ] docker container log webapp
* [ ] docker container log -f webapp
* [ ] docker container logs webapp
* [ ] docker container logs -f webapp

**Correct answer:**
* [x] docker container logs -f webapp

**5. Which combination of keys are used to exit from the shell and stop the container webapp?**


* [ ]  Ctrl+c
* [ ]  Ctrl+p+q
* [ ]  Ctrl+p
* [ ]  Ctrl+z

**Correct answer:**
* [x]  Ctrl+c

**6. We deployed a container called webapp. Inspect this container to get the IPPrefixLen.**


* [ ]  docker container inspect webapp | grep IPPrefixLen
* [ ] docker container top webapp | grep IPPrefixLen
* [ ] docker container run webapp | grep IPPrefixLen
* [ ] docker container logs webapp | grep IPPrefixLen

**Correct answer:**
* [x]  docker container inspect webapp | grep IPPrefixLen

**7. We have deployed some containers. What command is used to get the container with the highest memory?**


* [ ] docker container stats
* [ ] docker container status
* [ ] docker container top
* [ ] docker container ls

**Correct answer:**
* [x] docker container stats

**8. Run a container called webapp with image nginx, and in an interactive mode.**


* [ ] docker container run -it nginx
* [ ] docker container run -it nginx --name webapp
* [ ] docker container run nginx
* [ ] docker container run -it --name webapp nginx

**Correct answer:**
* [x] docker container run -it --name webapp nginx

**9. Which command returns only new and/or live events?**


* [ ] docker system info
* [ ] docker container events
* [ ] docker container events -f
* [ ]  docker system events

**Correct answer:**
* [x]  docker system events

**10. Which command returns events since the past 30 minutes?**


* [ ] docker system events since 30m
* [ ] docker system events --since 30m
* [ ] docker container events --since 30m
* [ ] docker container events since 30m

**Correct answer:**
* [x] docker system events --since 30m

**11. Run a container named webapp with nginx image in detached mode. Select the right answer.**


* [ ] docker container run --detach --name=webapp nginx
* [ ] docker container run --detach --name=nginx webapp
* [ ] docker container create -d --name=nginx webapp
* [ ] docker container create -d nginx

**Correct answer:**
* [x] docker container run --detach --name=webapp nginx

**12. Delete the “webapp” Container. Select the right answer.**


* [ ] docker container delete webapp
* [ ] docker container remove webapp
* [ ] docker container kill webapp
* [ ]  docker container rm webapp

**Correct answer:**
* [x]  docker container rm webapp

**13. Stop the container named "nginx".**


* [ ] docker container halt nginx
* [ ] docker container stop nginx
* [ ] docker container rm nginx
* [ ] docker container pause nginx

**Correct answer:**
* [x] docker container stop nginx

**14. Which command is used to get the events of the container named "webapp"? (This one is for you to read the documentation)**


* [ ] docker system events since 10m
* [ ] docker system events --filter 'container=webapp'
* [ ]  docker system events --filter 'image=webapp'

**Correct answer:**
* [x] docker system events --filter 'container=webapp'

**15. How do you list running & stopped containers?**


* [ ] docker container ls -a
* [ ] docker container ls -q
* [ ] docker container ls
* [ ] docker container ls -q, docker container ls

**Correct answer:**
* [x] docker container ls -a

**16. Stop all running containers on the host. Select the right answer.**


* [ ] docker container stop $(docker container ls -a)
* [ ] docker container rm $(docker container ls -q)
* [ ] docker container stop $(docker container ls -q)
* [ ] docker container stop --all

**Correct answer:**
* [x] docker container stop $(docker container ls -q)

**17. Delete all running and stopped containers on the host. (Explore the documentation to identify an option to force remove running containers)**


* [ ] docker container stop $(docker container ls -q)
* [ ] docker container rm $(docker container ls -q)
* [ ] docker container stop $(docker container ps -q)
* [ ] docker container rm -f $(docker container ls -aq)

**Correct answer:**
* [x] docker container rm -f $(docker container ls -aq)

**18. What is the command to pause a running container?**


* [ ] docker container pause
* [ ] docker container --pause
* [ ] docker container halt
* [ ] docker container SIGSTOP

**Correct answer:**
* [x] docker container pause

**19. Which command is used to delete the stopped containers?**


* [ ] docker container remove $(docker container ls -aq)
* [ ] docker container rm $(docker container ls -aq)
* [ ] docker container prune
* [ ] docker container rm --all

**Correct answer:**
* [x] docker container prune

**20. What are the signals sent to a running container when the docker container stop command is executed?**


* [ ] SIGSTOP followed by SIGKILL
* [ ]  SIGTERM followed by SIGKILL
* [ ]  SIGKILL followed by SIGTERM
* [ ]  SIGKILL followed by SIGSTOP

**Correct answer:**
* [x]  SIGTERM followed by SIGKILL

---


## Restart Policies

**1. Run a container with image nginx, name nginx and hostname webapp.**


* [ ] docker container run -d --name webapp --hostname=webapp nginx
* [ ] docker container run -d --name nginx webapp
* [ ] docker container run -d --name nginx --hostname=webapp nginx
* [ ] docker container run -d --name webapp nginx

**Correct answer:**
* [x] docker container run -d --name nginx --hostname=webapp nginx

**2. Which policy would restart the containers even after the docker daemon is restarted?**


* [ ] unless-stopped 
* [ ]  on-failure
* [ ]  always  
* [ ] always, unless-stopped

**Correct answer:**
* [x]  always  

**3. What is the default restart policy?**


* [ ]  unless-stopped
* [ ]  on-failure
* [ ]  no  
* [ ] always

**Correct answer:**
* [x]  no  

**4. Which policy is used to restart a container unless it is explicitly stopped or Docker is restarted.**


* [ ] unless-stopped  
* [ ] on-failure
* [ ] no  
* [ ] always

**Correct answer:**
* [x] unless-stopped  

**5. Which command can be used to check the restart policy of webapp container?**


* [ ] docker container inspect webapp
* [ ]  docker container info webapp
* [ ] docker container check webapp 
* [ ] None of the above

**Correct answer:**
* [x] docker container inspect webapp

**6. Restart container unless it is explicitly stopped or Docker is restarted.**


* [ ]  unless-stopped  
* [ ] on-failure
* [ ]  no  
* [ ] always

**Correct answer:**
* [x]  unless-stopped  

**7. Which command should be used to update the httpd container with the always policy?**


* [ ] docker container update --restart always httpd
* [ ] docker container unpause --restart always httpd
* [ ] docker container upgrade --restart always httpd

**Correct answer:**
* [x] docker container update --restart always httpd

**8. What is the hostname set on the container when the following command is run : docker container run -d --name webapp httpd**


* [ ]  webapp
* [ ]   apache
* [ ]  httpd  
* [ ] containers unique id

**Correct answer:**
* [x] containers unique id

**9. Which command should be used to update all the running containers with unless-stopped policy?**


* [ ] docker container upgrade --restart unless-stopped $(docker container ls -q)
* [ ] docker container update --restart unless-stopped $(docker container ls -q)
* [ ] docker container upgrade --restart unless-stopped $(docker container ls -aq)
* [ ] docker container update --restart unless-stopped $(docker container ls -aq)

**Correct answer:**
* [x] docker container update --restart unless-stopped $(docker container ls -q)

**10. Which option is used to reduce container downtime due to daemon crashes, planned outages, or upgrades?**


* [ ] Restart Policy  
* [ ] Swarm
* [ ]  Live Restore  
* [ ] Restart Policy, Live Restore

**Correct answer:**
* [x]  Live Restore  

**11. How to enable the live restore setting to keep containers alive when the daemon becomes unavailable?**


* [ ] echo '{"live-restore": true}' >> /etc/docker/daemon.json
* [ ] echo '{"live-restore": true}' >> /var/lib/docker/daemon.json
* [ ] echo '{true: "live-restore"}' >> /etc/docker/daemon.json
* [ ] echo '{true: "live-restore"}' >> /var/lib/docker/daemon.json

**Correct answer:**
* [x] echo '{"live-restore": true}' >> /etc/docker/daemon.json

**12. What is the path file which is used to add the live restore?**


* [ ] /etc/docker/daemon.json
* [ ] /var/lib/docker/daemon.json
* [ ] /var/log/docker/daemon.json
* [ ]  /var/lib/docker

**Correct answer:**
* [x] /etc/docker/daemon.json

**13. Which of the below commands may be used to copy a file /web.conf from a container named webapp with id 89683681 to the /tmp directory on the host?**


* [ ] docker container cp /tmp/web.conf webapp:/etc/web.conf
* [ ] docker container cp webapp:/web.conf /webapp
* [ ] docker container cp 89683681:/web.conf /tmp/
* [ ] docker container cp webapp:/web.conf /tmp/

**Correct answer:**
* [x] docker container cp 89683681:/web.conf /tmp/
* [x] docker container cp webapp:/web.conf /tmp/

**14. Copy the /etc/nginx directory from the webapp container to the docker host under /tmp/.**


* [ ] docker container copy webapp:/etc/nginx /tmp/
* [ ] docker container cp webapp:/etc/nginx /tmp/
* [ ] docker container copy /tmp/ webapp:/etc/nginx
* [ ] docker container cp /tmp/ webapp:/etc/nginx

**Correct answer:**
* [x] docker container cp webapp:/etc/nginx /tmp/

**15. What is the command to copy the file /root/myfile.txt from the host to /root/ of the webapp container?**


* [ ] docker container copy /root/myfile.txt webapp:/root/
* [ ] docker container cp /root/myfile.txt webapp:/root/
* [ ] docker container copy webapp:/root/ /root/myfile.txt
* [ ] docker container cp webapp:/root/ /root/myfile.txt

**Correct answer:**
* [x] docker container cp /root/myfile.txt webapp:/root/

**16. We can copy a file from a stopped container.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**17. Data inside the container is persistent.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**18. You can run multiple instances of the same application on the docker host.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**19. You can map to the same port on the Docker host more than once.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**20. Which option could be used to expose a webapp container to the outside world?**


* [ ]  -p 
* [ ]  -P  
* [ ] –publish 
* [ ]  –expose

**Correct answer:**
* [x]  -p 
* [x]  -P  
* [x] –publish 

**21. Unless specified otherwise, docker publishes the exposed port on all network interfaces.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**22. Map UDP port 80 in the container to port 8080 on the Docker host.**


* [ ]  -p 8080:80/udp 
* [ ]  -p 80:8080/udp
* [ ] -P 8080:80/udp  
* [ ] None of the above

**Correct answer:**
* [x]  -p 8080:80/udp 

**23. Map TCP port 80 in the container to port 8080 on the Docker host for connections to host IP 192.168.1.10 . Select the all right answers**


* [ ]  -p 192.168.1.10:8080:80
* [ ]  -p 192.168.1.10:80:8080
* [ ]  -p 192.168.1.10:8080:80/tcp
* [ ]  -p 192.168.1.10:8080:8080

**Correct answer:**
* [x]  -p 192.168.1.10:8080:80
* [x]  -p 192.168.1.10:8080:80/tcp

**24. How does the -P option in the docker container run command know what ports to publish on the container?**


* [ ]  It identifies the ports listening inside the container using netstat command
* [ ]  It uses the ExposedPorts field set on the container or the EXPOSE instruction in the Dockerfile
* [ ]  It requires the –expose command line argument
* [ ]  It assigns random ports between 32768 and 61000

**Correct answer:**
* [x]  It uses the ExposedPorts field set on the container or the EXPOSE instruction in the Dockerfile

**25. How does docker map a port on a container to a port on the host?**


* [ ] Using an internal load balancer  
* [ ] FirewallD Rules
* [ ]  Using an external load balancer
* [ ]  IPTables Rules

**Correct answer:**
* [x]  IPTables Rules

**26. What IPTables chains does Docker modify to configure port mapping on a host?**


* [ ]  INPUT 
* [ ]  FORWARD  
* [ ] DOCKER 
* [ ]  OUTPUT

**Correct answer:**
* [x] DOCKER 

---


## Troubleshooting Docker Daemon

**1. Enable the debugging mode. Select the right answer**


* [ ] echo '{"debug": true}' > /etc/docker/daemon.json
* [ ] echo '{"debug"}' > /etc/docker/daemon.json
* [ ]  echo '{"debug": true}' > /var/lib/docker/daemon.json
* [ ] echo '{"debug"}' > /var/lib/docker/daemon.json

**Correct answer:**
* [x] echo '{"debug": true}' > /etc/docker/daemon.json

**2. Which environment variable will be used to connect a remote docker server?**


* [ ]  DOCKER_REMOTE  
* [ ] DOCKER_HOST
* [ ] DOCKER_CONFIG  
* [ ] None of the above

**Correct answer:**
* [x] DOCKER_HOST

**3. How to check the logs of the docker daemon?**


* [ ] journalctl -u docker.service
* [ ] less /var/log/messages
* [ ] less /var/log/daemon.log
* [ ]  /var/log/docker.log

**Correct answer:**
* [x] journalctl -u docker.service
* [x] less /var/log/messages
* [x] less /var/log/daemon.log
* [x]  /var/log/docker.log

**4. What may be the cause of this error: “unable to configure the Docker daemon with file /etc/docker/daemon.json: the following directives are specified both as a flag and in the configuration file: tls: (from flag: true, from file: false)”?**


* [ ]  The tls flag is set to true in daemon.json file and false in the command line
* [ ]  The tls flag is set to false in daemon.json file and true in the command line
* [ ] The tls flag is not set on the command line
* [ ]  The tls flag is not set in the daemon.json file

**Correct answer:**
* [x]  The tls flag is set to false in daemon.json file and true in the command line

**5. How to check if the docker service is running or not?**


* [ ] docker status
* [ ]  sudo systemctl status docker
* [ ]  sudo systemctl docker status
* [ ] sudo service status docker

**Correct answer:**
* [x]  sudo systemctl status docker

**6. Which command is used to check the default logging driver?**


* [ ] docker system df
* [ ] docker system events
* [ ] docker system prune
* [ ]  docker system info

**Correct answer:**
* [x]  docker system info

**7. Where is the log of the webapp container with id 78373635 on the Docker Host?**


* [ ] /var/lib/docker/containers/78373635/78373635.json
* [ ]  /var/log/docker/78373635.json
* [ ] /etc/docker/78373635.json
* [ ] /var/lib/docker/tmp/78373635/78373635.json

**Correct answer:**
* [x] /var/lib/docker/containers/78373635/78373635.json

**8. What is the default logging driver?**


* [ ]  json-file  
* [ ] syslog 
* [ ]  journald
* [ ] splunk

**Correct answer:**
* [x]  json-file  

**9. Run a webapp container, and make sure that no logs are configured for this container.**


* [ ] docker run -it --log-driver none webapp
* [ ]  docker run -it --logging-driver none webapp
* [ ] docker run -it webapp
* [ ] docker run -it --log none webapp

**Correct answer:**
* [x] docker run -it --log-driver none webapp

**10. How to change the default logging driver to syslog?**


* [ ] echo '{"log-driver": "syslog"}' > /etc/docker/daemon.json
* [ ] echo '{"syslog": "log-driver"}' > /etc/docker/daemon.json
* [ ] echo '{"log-driver": "syslog"}' > /var/lib/docker/daemon.json
* [ ] echo '{"syslog": "log-driver"}' > /var/lib/docker/daemon.json

**Correct answer:**
* [x] echo '{"log-driver": "syslog"}' > /etc/docker/daemon.json

---


## Docker Images

**1. What is the default tag if not specified when building an image with the name webapp?**


* [ ]  none  
* [ ] default
* [ ]   latest 
* [ ]  v1

**Correct answer:**
* [x]   latest 

**2. Run ubuntu container with the trusty tag.**


* [ ] docker run ubuntu
* [ ]  docker run ubuntu:latest
* [ ]  docker run ubuntu:trusty
* [ ] docker run ubuntu -t trusty

**Correct answer:**
* [x]  docker run ubuntu:trusty

**3. What is the default public registry for docker?**


* [ ]  Docker Hub  
* [ ] Amazon Container Registry
* [ ]  Google Container Registry
* [ ] Docker Trusted Registry

**Correct answer:**
* [x]  Docker Hub  

**4. List the full length image IDs. (Please explore documentation)**


* [ ] docker image ls --digests 
* [ ]  docker images --digests
* [ ] docker images --no-trunc
* [ ]  None of the above

**Correct answer:**
* [x] docker images --no-trunc

**5. What is the purpose of a private registry?**


* [ ]  tightly control where your images are being stored
* [ ]  fully own your images distribution pipeline
* [ ]  integrate image storage and distribution tightly into your in-house development workflow
* [ ] All of the above

**Correct answer:**
* [x] All of the above

**6. Select the right answer. Which command is used to list the local images?**


* [ ] docker image ls  
* [ ] docker images ls
* [ ] docker container image ls
* [ ] docker container images ls

**Correct answer:**
* [x] docker image ls  

**7. Display images with a name containing postgres, at least 12 stars.**


* [ ] docker find --filter=stars=12 postgres
* [ ] docker search --filter=stars=12 postgres
* [ ] docker find --limit=12 postgres
* [ ] docker search --limit=12 postgres

**Correct answer:**
* [x] docker search --filter=stars=12 postgres

**8. Download nginx image from the Google Container Registry hub registry.**


* [ ] docker image pull nginx
* [ ] docker image build nginx
* [ ] docker image load nginx
* [ ] docker pull gcr.io/kodekloud/nginx

**Correct answer:**
* [x] docker pull gcr.io/kodekloud/nginx

**9. You have an nginx:v1 image with size 100M. You’ve now created your own version of the image – nginx:v2 by retagging the first image, what is the total size of both?**


* [ ]  50M  
* [ ] 100M  
* [ ] 150M  
* [ ] 200M

**Correct answer:**
* [x] 100M  

**10. What is the command to inspect the "httpd:latest" image?**


* [ ] docker inspect image httpd:latest
* [ ] docker run inspect httpd:latest
* [ ] docker container inspect httpd:latest
* [ ] docker analysis image httpd:latest

**Correct answer:**
* [x] docker inspect image httpd:latest

**11. Display images with a name containing busybox, at least 3 stars and are official builds.**


* [ ] docker find --filter is-official=true --filter stars=3 busybox
* [ ] docker search --filter is-official=true --filter stars=3 busybox
* [ ] docker find --filter is-official=true --limit=3 busybox
* [ ] docker search --filter is-official=true --limit=3 busybox

**Correct answer:**
* [x] docker search --filter is-official=true --filter stars=3 busybox

**12. Which command should be used to get the total size consumed by all images on a host?**


* [ ] docker image list  
* [ ] docker image df
* [ ] docker system df  
* [ ] docker system list

**Correct answer:**
* [x] docker system df  

**13. In the output of the "docker system df" command what does the ACTIVE field indicate on the images row?**


* [ ]  Number of Images currently available on the system
* [ ]  Number of Images built on the system
* [ ]  Number of Images with containers
* [ ] Number of containers running on the system

**Correct answer:**
* [x]  Number of Images with containers

---


##  Image Addressing Convention Add-on

**1. What is the total space consumed by images on this system?**


* [ ]  355 MB  
* [ ] 505 MB  
* [ ] 405 MB  
* [ ] 455 MB

**Correct answer:**
* [x]  355 MB  

**2. What command might have generated the above output?**


* [ ]  docker container ps
* [ ] docker ps
* [ ] docker image ps  
* [ ] docker image list

**Correct answer:**
* [x] docker image list

**3. What image might the webapp image be made of?**


* [ ]  redis 
* [ ]  ubuntu 
* [ ]  alpine  
* [ ] nginx

**Correct answer:**
* [x] nginx

**4. When you run the docker image inspect ubuntu command it gives the error “No such image”. Why is that?**


* [ ]  Must run the command docker inspect ubuntu/ubuntu
* [ ]  Image Ubuntu does not have the latest tag
* [ ]  Must authenticate to docker hub first before running this command
* [ ]  Must run the command docker image history ubuntu

**Correct answer:**
* [x]  Image Ubuntu does not have the latest tag

---


## Authenticating to Registry

**1. Which command is used to remove webapp:v1 image locally?**


* [ ] docker image rm webapp
* [ ] docker image rm webapp:v1
* [ ] docker image remove webapp:v1
* [ ]  docker image del webapp:v1

**Correct answer:**
* [x] docker image rm webapp:v1
* [x] docker image remove webapp:v1

**2. While trying to delete image postgres, you got an error “conflict: unable to remove repository reference “postgres” (must force) – container 1a56b95e073c is using its referenced image adf2b126dda8″. What may be the cause of this error?**


* [ ] A container is using this image
* [ ]  Must use force option to delete an image
* [ ]  Another image is using layers from this image
* [ ] The image was built locally on this host

**Correct answer:**
* [x] A container is using this image

**3. When you log in to a registry, the command stores credentials in … (Please explore the documentation pages for this)**


* [ ] $HOME/.docker/config.json
* [ ] /etc/docker/.docker/config.json
* [ ] /var/lib/docker/.docker/config.json
* [ ] /var/lib/docker/containers/.docker/config.json

**Correct answer:**
* [x] $HOME/.docker/config.json

**4. What is the user/account and image/repository name for the image company/nginx?**


* [ ]  image=company, user=nginx
* [ ]  image=company, user=company
* [ ]  image=nginx, user=nginx
* [ ]  image=nginx, user=company

**Correct answer:**
* [x]  image=nginx, user=company

**5. You are required to store a copy of the official alpine image in your company’s internal docker registry. What would be your approach?**


* [ ]  Create a Dockerfile similar to the official image and build an image
* [ ] Pull the official image, tag it with the address of the internal docker registry and push to the internal docker registry

**Correct answer:**
* [x] Pull the official image, tag it with the address of the internal docker registry and push to the internal docker registry

**6. Choose the right command to pull ubuntu image from a private registry at gcr.io**


* [ ] docker pull ubuntu
* [ ] docker pull kk/ubuntu
* [ ] docker pull gcr.io/kk/ubuntu
* [ ] All of the above

**Correct answer:**
* [x] docker pull gcr.io/kk/ubuntu

**7. Remove all unused images on the Docker host**


* [ ]  docker image prune -a  
* [ ] docker image rm -a
* [ ] docker image delete -a
* [ ] None of the above

**Correct answer:**
* [x]  docker image prune -a  

**8. Which command is used to authenticate with azr.com registry which listens on port 5000?**


* [ ] docker auth azr.com:5000
* [ ]  docker login azr.com:5000

**Correct answer:**
* [x]  docker login azr.com:5000

**9. Which subcommand will be used to get more info about images?**


* [ ]  inspect 
* [ ]  load  
* [ ] import  
* [ ] ls

**Correct answer:**
* [x]  inspect 

**10. Which command can be used to get the ExposedPorts of a webapp image?**


* [ ] docker container ls
* [ ]  docker image inspect webapp  
* [ ] docker container inspect webapp
* [ ] docker image ls

**Correct answer:**
* [x]  docker image inspect webapp  

**11. Display all layers of httpd image along with the size on each layer.**


* [ ] docker image layers httpd
* [ ] docker image history httpd
* [ ] docker image inspect httpd
* [ ] docker images history httpd

**Correct answer:**
* [x] docker image history httpd

**12. How to get the Os field alone of the httpd image?**


* [ ] docker image inspect httpd -f '{{.Os}}'
* [ ] docker image ls | grep Os
* [ ] docker image history | grep Os
* [ ] docker image inspect httpd -f '{{.OperatingSystem}}'

**Correct answer:**
* [x] docker image inspect httpd -f '{{.Os}}'

**13. A government facility runs a secure data center with no internet connectivity. A new application requires access to docker images hosted on docker hub. What is the best approach to solve this?**


* [ ]  Get the Dockerfile of the image and build a local version from within the restricted environment.
* [ ] Establish a secure link between the host in the restricted environment and docker hub
* [ ]  Pull docker images from a host with access to docker hub, convert to a tarball using docker image save command, and copy to the restricted environment and extract the tarball
* [ ]  Pull docker images from a host with access to docker hub, then push to a registry hosted within the restricted environment.

**Correct answer:**
* [x]  Pull docker images from a host with access to docker hub, convert to a tarball using docker image save command, and copy to the restricted environment and extract the tarball

**14. Print the value of ‘Architecture’ and ‘Os’ for a 'webapp' image.**


* [ ] docker image inspect webapp -f '{{.Os}}' -f '{{.Architecture}}'
* [ ] docker image inspect webapp -f '{{.Os}} {{.Architecture}}'
* [ ] docker image inspect webapp -f '{{.Os}}', -f '{{.Architecture}}'
* [ ] docker image inspect webapp -f '{{.Os .Architecture}}'

**Correct answer:**
* [x] docker image inspect webapp -f '{{.Os}} {{.Architecture}}'

**15. Which command can be used to get a backup of image webapp?**


* [ ] docker image backup webapp -o webapp.tar
* [ ] docker image save webapp -o webapp.tar
* [ ] docker container save webapp -o webapp.tar
* [ ] docker container backup webapp -o webapp.tar

**Correct answer:**
* [x] docker image save webapp -o webapp.tar

**16. A tarfile – nginx.tar – has been created using the docker image save command. Which command can be used to extract it into your docker host.**


* [ ] docker image import -i nginx.tar
* [ ] docker image restore -i nginx.tar
* [ ]  docker container restore -i nginx.tar
* [ ] docker image load -i nginx.tar

**Correct answer:**
* [x] docker image load -i nginx.tar

**17. You have created a nginx container and customized it to create your own webpage. How can you create an image out of it to share with others?**


* [ ] docker image save 
* [ ]  docker image export
* [ ] docker export  
* [ ] You can only create an image using a Dockerfile

**Correct answer:**
* [x] docker export  

**18. How do you restore an image created from the docker export command?**


* [ ] docker container import  
* [ ] docker image import
* [ ] docker image load  
* [ ] docker image restore

**Correct answer:**
* [x] docker image import

**19. The “export” command works with Docker images.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**20. Export webapp container’s filesystem as a tar archive. Select the right answer**


* [ ] docker export webapp mywebapp.tar
* [ ] docker image export --output="mywebapp.tar" webapp
* [ ] docker image save -i mywebapp.tar
* [ ] docker container export webapp > mywebapp.tar

**Correct answer:**
* [x] docker container export webapp > mywebapp.tar

---


## Building a Custom Image

**1. Which method can be used to build an image using existing containers?**


* [ ] docker commit 
* [ ]  docker export  
* [ ] docker save 
* [ ]  docker load

**Correct answer:**
* [x] docker commit 
* [x]  docker export  

**2. The container being committed and its processes will be paused while the image is committed.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**3. The … is a text document that contains all the commands a user could call on the command line to assemble an image.**


* [ ] Docker Compose  
* [ ] .dockerignore  
* [ ] build context
* [ ]  Dockerfile

**Correct answer:**
* [x]  Dockerfile

**4. Which of the following is not an instruction supported in the Dockerfile? Select the all right answers.**


* [ ]  EXPOSE  
* [ ] ADD  
* [ ] WORKDIR  
* [ ] EXEC

**Correct answer:**
* [x] EXEC

**5. The docker container commit is the recommended approach for building a custom image.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**6. Which of the following commands is used to list the docker images on the Docker Host?**


* [ ] docker images  
* [ ] docker image ls
* [ ] docker image get 
* [ ]  docker ls image

**Correct answer:**
* [x] docker images  
* [x] docker image ls

**7. We have a running container named webapp with the nginx image. We added a custom html file to this container. How do we create an image named mynginx from this container?**


* [ ]  docker container commit webapp mynginx
* [ ]  docker container commit mynginx webapp
* [ ] docker container update webapp mynginx
* [ ]  None of the above

**Correct answer:**
* [x]  docker container commit webapp mynginx

**8. Which of the following commands used to match all images with the com.example.version label?**


* [ ]  docker images --label="com.example.version"
* [ ] docker images --filter "com.example.version"
* [ ]  docker images --filter "label=com.example.version"
* [ ] docker images --format "label=com.example.version"

**Correct answer:**
* [x]  docker images --filter "label=com.example.version"

**9. You are required to create an image from an existing image. What is the recommended approach?**


* [ ]  Use docker image export and docker image import command
* [ ]  Use docker container export and docker container import command
* [ ]  Use docker image save and docker image load command
* [ ]  Use docker container commit command

**Correct answer:**
* [x]  Use docker image save and docker image load command

**10. You are required to create an image from an existing container. What is the recommended approach?**


* [ ]  Use docker image export and docker image import command
* [ ]  Use docker container export and docker container import command
* [ ]  Use docker container commit command
* [ ] Use docker container export and docker image import command

**Correct answer:**
* [x] Use docker container export and docker image import command

---


## Building a Custom Image

**1. What is the port of the web application configured for the service to listen within the container?**


* [ ]  8080 
* [ ]  5000  
* [ ] 80  
* [ ] 0.0.0.0

**Correct answer:**
* [x]  8080 

**Code**: 
FROM python:3.6
RUN pip install flask
COPY . /opt/
EXPOSE 8080
WORKDIR /opt
ENTRYPOINT ["python", "app.py"]

**2. When a container is created using the image built with the following Dockerfile, what is the command used to RUN the application inside it.**


* [ ] pip install flask 
* [ ]  docker run app.py
* [ ] app.py 
* [ ]  python app.py

**Correct answer:**
* [x]  python app.py

**Code**: 
FROM python:3.6
RUN pip install flask
COPY . /opt/
EXPOSE 8080
WORKDIR /opt
ENTRYPOINT ["python", "app.py"]


**3. To what location within the container is the application code copied to?**


* [ ] /opt 
* [ ]  /app 
* [ ]  /root 
* [ ]  /var

**Correct answer:**
* [x] /opt 

**Code**: 
FROM python:3.6
RUN pip install flask
COPY . /opt/
EXPOSE 8080
WORKDIR /opt
ENTRYPOINT ["python", "app.py"]


**4. Refer to the below Dockerfile and answer the following questions: What is the parent image from which this application is created?**


* [ ] ubuntu:latest  
* [ ] python  
* [ ] centos:7 
* [ ]  python:3.6

**Correct answer:**
* [x]  python:3.6

**Code**: 

FROM python:3.6
RUN pip install flask
COPY . /opt/
EXPOSE 8080
WORKDIR /opt
ENTRYPOINT ["python", "app.py"]

---


## Build Contexts

**1. Whenever a build is initiated by running the Docker build command, the files under the build context are transferred to the Docker daemon, at a temporary directory under the docker’s filesystem. Which directory are these files stored in?**


* [ ] /var/lib/docker/tmp
* [ ] /var/lib/docker/image
* [ ] /var/lib/docker/volumes
* [ ] /var/lib/docker/plugins

**Correct answer:**
* [x] /var/lib/docker/tmp

**2. While building a docker image from code stored in a remote URL, which command will be used to build from a directory called docker in the branch dev?**


* [ ] docker build https://github.com/kk/dca.git#dev:docker
* [ ] docker build https://github.com/kk/dca.git#docker:dev
* [ ] docker build https://github.com/kk/dca.git:dev
* [ ] docker build https://github.com/kk/dca.gitdev:#docker

**Correct answer:**
* [x] docker build https://github.com/kk/dca.git#dev:docker

**3. Which of the below commands may be used to build an image with the Dockerfile filename?**


* [ ]  docker build . 
* [ ]  docker build -f Dockerfile .
* [ ] docker build -t Dockerfile2 .
* [ ] docker build -t .

**Correct answer:**
* [x]  docker build . 
* [x]  docker build -f Dockerfile .

**4. Choose the correct flag that is used to apply a tag to an image.**


* [ ]  -i
* [ ]   -p  
* [ ] -f  
* [ ] -t

**Correct answer:**
* [x] -t

**5. Build an image using a context build under path /tmp/docker and name it webapp.**


* [ ] docker build /tmp/docker
* [ ] docker build /tmp/docker -t webapp
* [ ] docker build webapp -t /tmp/docker
* [ ] docker pull -it /tmp/docker bash

**Correct answer:**
* [x] docker build /tmp/docker -t webapp

**6. If you do not specify a tag name, you can’t build the image.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**7. What is the default tag if not specified when building an image with the name webapp?**


* [ ]  none  
* [ ] default 
* [ ]  latest 
* [ ]  v1

**Correct answer:**
* [x]  latest 

**8. A build’s context is the set of files located in the specified PATH or URL, Which kind of resources can the URL parameter refer to ?**


* [ ]  Git repositories
* [ ] pre-packaged tarball contexts
* [ ]  Path to a local directory

**Correct answer:**
* [x]  Git repositories
* [x] pre-packaged tarball contexts
* [x]  Path to a local directory

**9. What is the command to build an image using a Dockerfile.dev file under path /opt/myapp with the name webapp. The current directory you are in is /tmp.**


* [ ] docker build Dockerfile.dev -t webapp /opt/myapp
* [ ] docker build -f /opt/myapp/Dockerfile.dev /opt/myapp -t webapp
* [ ] docker build -f Dockerfile.dev /opt/myapp -t webapp
* [ ] docker build -t Dockerfile.dev -name webapp -f /opt/myapp

**Correct answer:**
* [x] docker build -f /opt/myapp/Dockerfile.dev /opt/myapp -t webapp

**10. What is a recommended approach for installing packages and libraries while building an image?**


* [ ]  Download packages on the host and use ADD instructions to add them to the image.
* [ ]  Use the ADD instruction to provide a URL to the package on the remote host.
* [ ] Use the RUN instruction and have the apt-get update and apt-get install commands on the same instruction.
* [ ] Use the RUN instruction and have the apt-get update and apt-get install commands as separate instructions.

**Correct answer:**
* [x] Use the RUN instruction and have the apt-get update and apt-get install commands on the same instruction.

**11. What is the file used to exclude temporary files such as log files or builds from the context during a build?**


* [ ]  .git 
* [ ]  .gitignore
* [ ]  .dockerignore 
* [ ]  None of the above

**Correct answer:**
* [x]  .dockerignore 

**12. Using RUN apt-get update && apt-get install -y ensures your Dockerfile installs the latest package versions with no further coding or manual intervention. This technique is known as …..**


* [ ] Docker-stack  
* [ ] Cache busting 
* [ ]  Version pinning 
* [ ]  Build-context

**Correct answer:**
* [x] Cache busting 

**13. If the build fails at a particular stage, it repurposes the previous layers from the cache and does not really rebuild them.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**14. What is a best practice while installing multiple packages as part of the install instruction?**


* [ ]  Add them on the same line
* [ ]  Add them on separate lines separated by a slash in alphanumeric order
* [ ]  Add a separate instruction for each package
* [ ]  Add them on separate lines separated by a slash

**Correct answer:**
* [x]  Add them on separate lines separated by a slash in alphanumeric order

**15. Which among the following scenarios will lead to docker invalidating cache on a given layer?**


* [ ]  Change in instruction  
* [ ] Change in a file used with the ADD instruction
* [ ]  Addition of a new instruction at the end of the file
* [ ]  Release of a new version of a package installed with the RUN instruction

**Correct answer:**
* [x]  Change in instruction  
* [x] Change in a file used with the ADD instruction

**16. …… forces the build to install a particular version of package regardless of what’s in the cache. This technique can also reduce failures due to unanticipated changes in required packages.**


* [ ] Docker-stack 
* [ ]  Cache busting  
* [ ] Version pinning 
* [ ]  Build-context

**Correct answer:**
* [x] Version pinning 

**17. Which option can be used to disable the cache while building a docker image?**


* [ ] `--no-cache`
* [ ] `--force-rm=true`
* [ ] `--disable-cache`
* [ ] `--cache-from false`

**Correct answer:**
* [x] `--no-cache`

**Documentation Link**: https://docs.docker.com/engine/reference/commandline/build/#:~:text=%2D%2Dno%2Dcache,building%20the%20image

**18. What is a recommended approach to reduce build time while building docker images?**


* [ ]  Instructions likely to change more often must be at the top of the Dockerfile
* [ ]  Instructions likely to change more often must be at the bottom of the Dockerfile
* [ ] Instructions likely to change more often must be in the middle of the Dockerfile
* [ ] The order of the instructions within the Dockerfile doesn’t matter.

**Correct answer:**
* [x]  Instructions likely to change more often must be at the bottom of the Dockerfile

**19. A Dockerfile is built from the Ubuntu image as the base image. What would happen to the cache when a new version of the Ubuntu image is made available at Dockerhub?**


* [ ] Cache is invalidated and docker pulls the new image and recreates from scratch.
* [ ]  Cache is not invalidated and docker continues to use existing cache.

**Correct answer:**
* [x]  Cache is not invalidated and docker continues to use existing cache.

**20. COPY instruction has some features like local-only tar extraction and remote URL support.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**21. COPY instruction only supports the basic copying of local files into the container.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**22. Which instruction(s) can be used in the Dockerfile to copy content from the local filesystem into the containers?**


* [ ]  ADD  
* [ ] COPY  
* [ ] MOVE  
* [ ] RUN

**Correct answer:**
* [x]  ADD  
* [x] COPY  

**23. What is the right instruction to download a file from "https://file.tar.xz" and copy to "/testdir" in the image?**


* [ ] ADD https://file.tar.xz /testdir
* [ ] COPY https://file.tar.xz /testdir
* [ ]  RUN https://file.tar.xz /testdir  
* [ ] None of the above

**Correct answer:**
* [x] ADD https://file.tar.xz /testdir

---


## CMD-vs-Entrypoint

**1. Which of the following is the correct format for CMD instruction?**


* [ ] CMD ["executable","param1","param2"]
* [ ] CMD ["param1","param2"]
* [ ] CMD command param1 param2
* [ ] CMD command,param1,param2

**Correct answer:**
* [x] CMD ["executable","param1","param2"]
* [x] CMD ["param1","param2"]
* [x] CMD command param1 param2

**2. What is the output of the following Dockerfile snippet when container runs as `docker run -it <image> kk`?**


* [ ]  Hello kk  
* [ ] Hello  
* [ ] World kk  
* [ ] kk Hello

**Correct answer:**
* [x]  Hello kk  

**Code**: 
ENTRYPOINT ["/bin/echo", "Hello"]
CMD ["World"]

**3. If you list more than one CMD instruction in the Dockerfile then only the last CMD will take effect.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**4. What is the output of the following Dockerfile snippet when container runs as `docker run -it <image>`?**


* [ ] Hello world  
* [ ] Hello  
* [ ] world  
* [ ] world Hello

**Correct answer:**
* [x] Hello world  

**Code**: 
ENTRYPOINT ["/bin/echo", "Hello"]
CMD ["world"]

**5. Choose the correct instruction to add the echo "Hello World" command in the Dockerfile.**


* [ ]  CMD [echo "Hello World"]  
* [ ] CMD ["echo", "Hello World"]
* [ ] CMD ["Hello World"]
* [ ] None of the above

**Correct answer:**
* [x] CMD ["echo", "Hello World"]

**6. If CMD is used to provide default arguments for the ENTRYPOINT instruction, both the CMD and ENTRYPOINT instructions should be specified.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**7. A parent image is the image that your image is based on. It refers to the contents of the FROM directive in the Dockerfile.**


* [ ] True 
* [ ] False

**Correct answer:**
* [x] True 

**8. When a user runs the command `docker run my-custom-image sleep 1000`.**


* [ ]  docker overrides the ENTRYPOINT instruction with "sleep 1000"
* [ ] docker overrides the CMD instruction with "sleep 1000"
* [ ]  docker override ENTRYPOINT instruction with "sleep" and CMD instruction with "1000"

**Correct answer:**
* [x] docker overrides the CMD instruction with "sleep 1000"

**9. A parent image has FROM scratch in its Dockerfile.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**10. While building an image, You have one base image, but there could be multiple parent images.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**11. How do you identify if a Docker file is configured to use multi-stage builds?**


* [ ]  The Dockerfile has the tag multi-stage at the top
* [ ] The Dockerfile has multiple FROM instructions
* [ ]  The Dockerfile has multiple RUN instructions
* [ ]  The Dockerfile is built from the scratch image

**Correct answer:**
* [x] The Dockerfile has multiple FROM instructions

**12. You are developing an e-commerce application. The application must store cart details of users temporarily as long as the user’s session is active. What is the recommended approach to storing the cart details with the application deployed as a docker container?**


* [ ] Store the cart details in the /tmp directory of the container
* [ ]  Store the cart details in the memory of the container
* [ ]  Store the cart details in a volume backed by a in-memory cache service like redis

**Correct answer:**
* [x]  Store the cart details in a volume backed by a in-memory cache service like redis

**13. By default, the stages are not named, and you refer to them by their integer number, starting with 1 for the first FROM instruction in the multi-stage build.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**14. The "--from=0" in the following Dockerfile instruction line refers to:**


* [ ]  The base image specified in the FROM instruction of the first set of instructions.
* [ ]  The base image specified in the FROM instruction of the second set of instructions.
* [ ] The image built using the first set of instructions in the Dockerfile.
* [ ] The image built using the last set of instructions in the Dockerfile

**Correct answer:**
* [x] The image built using the first set of instructions in the Dockerfile.

**Code**: 
"COPY --from=0 /go/src/github.com/alexellis/href-counter/app ."

**15. Name the stage which uses nginx as a base image to builder in the Dockerfile.**


* [ ] FROM nginx  
* [ ] FROM nginx AS builder
* [ ]  The last image build
* [ ] FROM node AS builder

**Correct answer:**
* [x] FROM nginx AS builder

**16. What instruction is used to copy a file from an external image named `redis` not part of any stage in the multi-stage build process?**


* [ ]  –from=redis 
* [ ]  –from=0  
* [ ] –copy-from=redis  
* [ ] –copy-from=0

**Correct answer:**
* [x]  –from=redis 

**17. It’s recommended to avoid sending unwanted files to the build context by using .gitignore file to exclude those files.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**18. Which is the recommended approach to install packages following the best practices in Dockerfile?**


* [ ]  RUN apt-get update && apt-get install -y git httpd
* [ ] RUN apt-get update && apt-get install -y \        git \        httpd
* [ ] RUN apt-get update \  RUN apt-get install -y git \  RUN apt-get install -y httpd

**Correct answer:**
* [x] RUN apt-get update && apt-get install -y \        git \        httpd

**19. An application you are developing requires an httpd server as frontend, a python application as the backend API server, a MongoDB database and a worker developed in Python. What is the recommended approach in building images for these containers?**


* [ ] Build httpd, python API server, MongoDB database and Python worker into a single image to allow ease of deployment
* [ ]  Build httpd into an image, MongoDB database to another and Python API and worker together into a single image
* [ ]  Build separate images for each component of the application

**Correct answer:**
* [x]  Build separate images for each component of the application

**20. Which of the below steps can help minimize the build time of images?**


* [ ]  Only install necessary packages within the image
* [ ] Avoid sending unwanted files to the build context using .dockerignore
* [ ]  Combine multiple dependent instructions into a single one and cleanup temporary files
* [ ] Move the instructions that are likely to change most frequently to the bottom of the Dockerfile
* [ ]  Use multi-stage builds

**Correct answer:**
* [x] Avoid sending unwanted files to the build context using .dockerignore
* [x] Move the instructions that are likely to change most frequently to the bottom of the Dockerfile

**21. Which of the below can help minimize the image size?**


* [ ] Only install necessary packages within the image
* [ ]  Avoid sending unwanted files to the build context using .dockerignore
* [ ]  Combine multiple dependent instructions into a single one and cleanup temporary files
* [ ]  Move the instructions that are likely to change most frequently to the bottom of the Dockerfile
* [ ]  Use multi-stage builds

**Correct answer:**
* [x] Only install necessary packages within the image
* [x]  Combine multiple dependent instructions into a single one and cleanup temporary files
* [x]  Use multi-stage builds

---


## Docker Networking

**1. What is the command to connect a running container with name myapp to the existing bridge network my-net?**


* [ ] docker container connect myapp my-net
* [ ]  docker container attach myapp my-net
* [ ]  docker network connect my-net myapp
* [ ]  docker network connect myapp my-net

**Correct answer:**
* [x]  docker network connect my-net myapp

**2. Which command is used to see the network settings and IP address assigned to a container with id c164825bb3d3 that uses the myapp image?**


* [ ] docker inspect myapp 
* [ ]  docker container ls myapp
* [ ]  docker container ls c164825bb3d3
* [ ]  docker inspect c164825bb3d3

**Correct answer:**
* [x]  docker inspect c164825bb3d3

**3. Which command is used to list the default available networks?**


* [ ]  docker network --filter  
* [ ] docker network get  
* [ ] docker network ls
* [ ] None of the above

**Correct answer:**
* [x] docker network ls

**4. Overlay networks connect multiple Docker daemons together and enable swarm services to communicate with each other.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**5. Which of the following commands would create a user-defined bridge network called my-net?**


* [ ] docker network create my-net
* [ ] docker create network my-net
* [ ]  docker network create -d bridge my-net
* [ ] docker network create --type bridge my-net
* [ ] docker network create --driver bridge my-net

**Correct answer:**
* [x] docker network create my-net
* [x]  docker network create -d bridge my-net
* [x] docker network create --driver bridge my-net

**6. What is the default network driver used on a container if you haven’t specified one?**


* [ ]  host  
* [ ] bridge 
* [ ]  overlay  
* [ ] Macvlan

**Correct answer:**
* [x] bridge 

**7. How to get the subnet, gateway of the network c0a0b59a3807?**


* [ ] docker info c0a0b59a3807
* [ ] docker container inspect c0a0b59a3807
* [ ]  docker network inspect c0a0b59a3807
* [ ] docker inspect c0a0b59a3807

**Correct answer:**
* [x]  docker network inspect c0a0b59a3807

**8. If you use the …… network mode for a container, that container’s network stack is not isolated from the Docker host (the container shares the host’s networking namespace), and the container does not get its own IP-address allocated.**


* [ ]  host  
* [ ] bridge 
* [ ]  overlay  
* [ ] Macvlan

**Correct answer:**
* [x]  host  

**9. What is the command to remove the my-net network?**


* [ ] docker network create my-net 
* [ ]  docker network rm my-net
* [ ]  docker network connect my-net
* [ ] None of the above

**Correct answer:**
* [x]  docker network rm my-net

**10. What is the command to remove all unused networks?**


* [ ]  docker network create my-net 
* [ ]  docker network rm my-net
* [ ] docker network prune
* [ ] docker network rm --all

**Correct answer:**
* [x] docker network prune

---


## Docker Storage

**1. The volumes are mounted as “readonly” by default inside the container if no options are specified.**


* [ ] True 
* [ ] False

**Correct answer:**
* [x] False

**2. What is the command to remove unused volumes?**


* [ ] docker container rm my-vol
* [ ] docker volume rm my-vol
* [ ] docker volume prune
* [ ] docker volume rm --all

**Correct answer:**
* [x] docker volume prune

**3. By default, all files created inside a container are stored on a writable container layer.**


* [ ] True 
* [ ] False

**Correct answer:**
* [x] True 

**4. Volumes are the preferred mechanism for persisting data generated by and used by Docker containers.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**5. Which command is used to remove the my-vol volume?**


* [ ] docker volume del my-vol 
* [ ]  docker volume remove my-vol
* [ ] docker volume prune my-vol
* [ ] docker volume rm my-vol

**Correct answer:**
* [x]  docker volume remove my-vol
* [x] docker volume rm my-vol

**6. What is the command to create a volume with the name my-vol?**


* [ ] docker volume create my-vol
* [ ] docker create volume my-vol
* [ ] docker volume prune
* [ ] docker volume rm all

**Correct answer:**
* [x] docker volume create my-vol

**7. What is the command to list volumes?**


* [ ] docker volume ls 
* [ ]  docker volume prune
* [ ] docker volume get
* [ ]  None of the above

**Correct answer:**
* [x] docker volume ls 

**8. What is the command to get details of the volume my-vol such as the driver, mountpoint, volumename, ..etc?**


* [ ] docker volume inspect my-vol
* [ ] docker volume fetch my-vol
* [ ]  docker volume get my-vol
* [ ] docker volume ls my-vol

**Correct answer:**
* [x] docker volume inspect my-vol

**9. Which option is used to mount a volume ?**


* [ ]  -v  
* [ ] –volume-mount
* [ ] --mount
* [ ] --volume

**Correct answer:**
* [x]  -v  
* [x] --volume
* [x] --mount

**10. You can remove a vol1 which is in use by a container using the command docker volume rm --force vol1.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**11. Which among the below is a correct command to start a webapp container with the volume vol2, mounted to the destination directory /app?**


* [ ] docker run -d --name webapp --mount source=vol2,target=/app httpd
* [ ] docker run -d --name webapp -v vol2:/app httpd
* [ ] docker run -d --name webapp --volume vol2:/app httpd

**Correct answer:**
* [x] docker run -d --name webapp --mount source=vol2,target=/app httpd
* [x] docker run -d --name webapp -v vol2:/app httpd
* [x] docker run -d --name webapp --volume vol2:/app httpd

**12. By default, all files inside an image are in a writable layer.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**13. Which among the below is a correct command to start a webapp container with the volume vol3, mounted to the destination directory /opt in readonly mode?**


* [ ] docker run -d --name webapp --mount source=vol3,target=/opt,readonly httpd
* [ ] docker run -d --name webapp -v vol3:/opt:ro httpd
* [ ] docker run -d --name webapp -v vol3:/opt:readonly httpd
* [ ] docker run -d --name webapp --volume vol3:/opt:ro httpd
* [ ] docker run -d --name webapp --mount source=vol3,target=/opt,ro httpd

**Correct answer:**
* [x] docker run -d --name webapp --mount source=vol3,target=/opt,readonly httpd
* [x] docker run -d --name webapp -v vol3:/opt:ro httpd
* [x] docker run -d --name webapp --volume vol3:/opt:ro httpd
* [x] docker run -d --name webapp --mount source=vol3,target=/opt,ro httpd

---


## Docker Compose

**1. …….. is a YAML file which contains details about the services, networks, and volumes for setting up a Docker application.**


* [ ] Dockerfile  
* [ ] Docker Compose  
* [ ] .dockerignore 
* [ ]  .env

**Correct answer:**
* [x] Docker Compose  

**2. Which command can be used to create and start containers in foreground using the existing docker-compose.yml?**


* [ ] docker-compose up 
* [ ]  docker-compose ps
* [ ] docker-compose logs
* [ ] docker-compose stop

**Correct answer:**
* [x] docker-compose up 

**3. Using …… we can configure containers and communication between them in a declarative way.**


* [ ] Docker Compose  
* [ ] Dockerfile  
* [ ] Device Mapper  
* [ ] Build-context

**Correct answer:**
* [x] Docker Compose  

**4. …… is the command to list the containers created by compose file.**


* [ ] docker-compose ls  
* [ ] docker-compose ps  
* [ ] docker-compose list

**Correct answer:**
* [x] docker-compose ps  

**5. …… is the command to check the logs for the whole stack defined inside compose file.**


* [ ] docker-compose up  
* [ ] docker-compose ps
* [ ] docker-compose logs  
* [ ] docker-compose up -d

**Correct answer:**
* [x] docker-compose logs  

**6. Which command can be used to create and start containers in background or in detached mode in compose using the existing docker-compose.yml?**


* [ ] docker-compose up
* [ ]   docker-compose up --background
* [ ] docker-compose up --detach
* [ ] docker-compose up -d

**Correct answer:**
* [x] docker-compose up --detach
* [x] docker-compose up -d

**7. Which command can be used to stop (only and not delete) the whole stack of containers created by compose file?**


* [ ] docker-compose down  
* [ ] docker-compose stop
* [ ] docker-compose destroy 
* [ ]  docker-compose halt

**Correct answer:**
* [x] docker-compose stop

**8. docker-compose stop command stops and removes the whole stack of container created by compose file.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**9. Select the right answer. Which command can be used to delete the application stack created using compose file?**


* [ ] docker-compose rm  
* [ ] docker-compose stop
* [ ] docker-compose down 
* [ ]  docker-compose destroy

**Correct answer:**
* [x] docker-compose down 

**10. Compose files that doesn’t declare a version are considered “version 0”.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**11. With the docker-compose up command, we can run containers on multiple docker hosts.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**12. Compose files using the version 2 and version 3 syntax must indicate the version number at the root of the document.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

---


## Docker Compose-Add On

**1. What is the host port on which the web application will be exposed on?**


* [ ] 80  
* [ ] 8080  
* [ ] Version 1  
* [ ] foobar.com

**Correct answer:**
* [x] 8080  

**2. What kind of volume mount is configured on the web application for the /var/log directory inside the container?**


* [ ] Volume mount  
* [ ] Bind mount

**Correct answer:**
* [x] Volume mount  

**3. What kind of volume mount is configured on the web application for the /code directory inside the container?**


* [ ]  Volume Mount
* [ ]   Bind Mount

**Correct answer:**
* [x]   Bind Mount

**4. How can the web application address redis?**


* [ ]  Using the container ID generated by redis
* [ ]  Using the name redis
* [ ] Using the internal IP address of the redis container
* [ ]  By exposing port 6379 of redis container on the host and then using hosts IP

**Correct answer:**
* [x]  Using the name redis

**5. Which of the following statements are true?**


* [ ]  All of the web, redis and db images will be built before deploying containers.
* [ ]  The redis image will be built and the web image will be pulled from Dockerhub if it doesn’t already exist on the host.
* [ ] The web image will be built and the redis image will be pulled from Dockerhub if it doesn’t already exist on the host.
* [ ]  All images will be pulled from Dockerhub.

**Correct answer:**
* [x] The web image will be built and the redis image will be pulled from Dockerhub if it doesn’t already exist on the host.

**6. Which is the correct statement referring to the following Compose file?**


* [ ] The depends_on configuration is not supported in Compose version 3
* [ ]  db and redis services will be started before web service
* [ ]  web service will be started before db and redis services
* [ ]  None of the above

**Correct answer:**
* [x]  db and redis services will be started before web service

---


## Swarm Architecture and Setup

**1. Swarm nodes can be physical or virtual, on the cloud or on prem that have Docker engine installed on it.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**2. A swarm cluster consists of at least one manager node and one or more worker nodes.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**3. What are the advantages of container orchestration?**


* [ ] High availability  
* [ ] Auto Scaling  
* [ ] Self healing  
* [ ] Declarative

**Correct answer:**
* [x] High availability  
* [x] Auto Scaling  
* [x] Self healing  
* [x] Declarative

**4. What technologies can be used to group multiple machines together into a single cluster to run applications in the form of containers?**


* [ ]  Swarm  
* [ ] Kubernetes  
* [ ] Mesos  
* [ ] Openshift

**Correct answer:**
* [x]  Swarm  
* [x] Kubernetes  
* [x] Mesos  
* [x] Openshift

**5. Who is responsible for maintaining the desired state of the swarm cluster and taking necessary actions if a node was to fail or a new node was added to the cluster?**


* [ ]  manager node  
* [ ] worker node  
* [ ] slave node  
* [ ] worker, slave nodes

**Correct answer:**
* [x]  manager node  

**6. You can promote a worker node to a manager node.**


* [ ] True
* [ ] False 

**Correct answer:**
* [x] True

**7. Manager nodes are dedicated to management tasks only and cannot run workloads.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**8. The manager in the swarm cluster receives instructions or tasks from the worker node and runs containers.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**9. The communication between the nodes in the swarm cluster are not secured by default.**


* [ ] True
* [ ] Flase

**Correct answer:**
* [x] Flase

**10. The command docker node promote can be executed on any node “manager or worker”.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**11. Is it possible to promote a worker node to manager in swarm?**


* [ ] Yes
* [ ] No

**Correct answer:**
* [x] Yes

**12. What does the Reachable status of a node indicate in docker swarm?**


* [ ]  The node is a worker node and is reachable
* [ ]  The node is a manager node and is reachable
* [ ]  The node is a manager node and is reachable and is not the leader
* [ ] The node is a manager node and is the leader

**Correct answer:**
* [x]  The node is a manager node and is reachable and is not the leader

**13. If you have one manager in your swarm cluster, is it possible to demote it to a worker node?**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**14. What feature of swarm closely relates to this use case – “If an instance of an application crashes, it is immediately replaced by a new one”?**


* [ ] Rolling updates  
* [ ] Self healing 
* [ ]  Scaling  
* [ ] Load Balancing

**Correct answer:**
* [x] Self healing 

**15. Promote worker1 to a manager node . Select the right answer.**


* [ ] docker promote node worker1
* [ ] docker node promote worker1
* [ ] docker swarm node promote worker1
* [ ] docker swarm promote node worker1

**Correct answer:**
* [x] docker node promote worker1

**16. Change manager1 to a worker node . Select the right answer.**


* [ ]  docker swarm node demote manager1 docker swarm node demote manager1
* [ ] docker demote node manager1
* [ ] docker node demote manager1
* [ ] docker node demote manager1 worker

**Correct answer:**
* [x] docker node demote manager1

**17. Which command is used to check the status of manager/worker nodes?**


* [ ] docker swarm ls  
* [ ] docker node ps
* [ ] docker node ls
* [ ] docker swarm show nodes

**Correct answer:**
* [x] docker node ls

**18. We want to perform maintenance tasks on node – worker1 – for performing patching and updates. Select the best way to achieve this.**


* [ ] docker node update --availability drain worker1
* [ ]  docker node update --availability active worker1
* [ ] docker node rm worker1
* [ ]  None of the above

**Correct answer:**
* [x] docker node update --availability drain worker1

**19. We have a single manager 2 worker node swarm cluster. All three nodes are hosting workload. What is the sequence of activities to remove the manager node from the swarm cluster?**


* [ ]  Drain the node, and run docker swarm leave.
* [ ]  Demote to a worker node, drain the node and run docker swarm leave.
* [ ]  Promote a worker node to manager, demote manager to worker, drain the node and run docker swarm leave.
* [ ] Add a new worker node, drain the manager node, and run docker swarm leave.

**Correct answer:**
* [x]  Promote a worker node to manager, demote manager to worker, drain the node and run docker swarm leave.

**20. When a worker node becomes active again after draining, the old containers will go back to this node.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**21. It is recommended to have one manager in your cluster?**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**22. …. is responsible for making sure that all the manager nodes that are in charge of managing and scheduling tasks in the cluster, are storing the same consistent state.**


* [ ]  CFS  
* [ ] Scheduler  
* [ ] Raft consensus 
* [ ]  Leader

**Correct answer:**
* [x] Raft consensus 

**23. The manager node is responsible for maintaining the cluster state, distributing and ensuring the state of containers and services across all workers.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**24. Which of the below statements are true when you have more than 1 manager nodes?**


* [ ]  All decisions are made by all the managers at once.
* [ ]  All decisions are made by each manager turn by turn in a round robin fashion.
* [ ]  All decisions are made by 1 manager who is the leader.

**Correct answer:**
* [x]  All decisions are made by 1 manager who is the leader.

**25. Out of a total of 3 masters if one node was to fail or was not responding at that moment and only two nodes were available,the decision to add the new worker can still be made with an agreement between the two available nodes.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**26. …. is defined as the minimum number of managers required to be present for carrying out cluster management tasks.**


* [ ]  Majority  
* [ ] Fault tolerance  
* [ ] Quorum  
* [ ] Single failure

**Correct answer:**
* [x] Quorum  

**27. What is the maximum number of managers recommended by Docker in a swarm cluster?**


* [ ] 3  
* [ ] 5  
* [ ] 7  
* [ ] No limit

**Correct answer:**
* [x] 7  

**28. Which formula can be used to calculate the Quorum of N nodes?**


* [ ]  N + 1  
* [ ] N+1 / 2  
* [ ] N-1/2 
* [ ]  (N /2) +1

**Correct answer:**
* [x]  (N /2) +1

**29. Which formula can be used to calculate the fault tolerance of N nodes?**


* [ ]  N + 1  
* [ ] N+1 / 2  
* [ ] N / 2 +1  
* [ ] (N-1)/2

**Correct answer:**
* [x] (N-1)/2

**30. What is the maximum number of managers possible in a swarm cluster?**


* [ ]  3 
* [ ]  5  
* [ ] 7  
* [ ] No limit

**Correct answer:**
* [x] No limit

**31. It is recommended to have an even number of master nodes.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**32. Assume that you have 3 managers in your cluster, what will happen if 2 managers fail at the same time? Select the all right answers.**


* [ ]  The services hosted on the available worker nodes will continue to run.
* [ ] The services hosted on the available worker nodes will stop running.
* [ ] New services/workers can be created or added.
* [ ] New services/workers can’t be created or added.

**Correct answer:**
* [x]  The services hosted on the available worker nodes will continue to run.
* [x] New services/workers can’t be created or added.

**33. How many manager nodes must be online in a cluster with 7 manager nodes for the swarm cluster to continue to operate?**


* [ ] 3 
* [ ]  1 
* [ ]  4 
* [ ]  5

**Correct answer:**
* [x]  4 

**34. How many manager nodes must be online in a cluster with 13 manager nodes for the swarm cluster to continue to operate?**


* [ ]  3  
* [ ] 1  
* [ ] 6 
* [ ]  7

**Correct answer:**
* [x]  7

**35. Among the below what is the recommended number of manager nodes as per best practices?**


* [ ]  9  
* [ ] 7
* [ ]   1  
* [ ] 4

**Correct answer:**
* [x] 7

**36. You have 3 data centers and 13 managers. How best should you distribute the managers between them to withstand site wide disruptions?**


* [ ] 5-5-3 
* [ ]  6-6-1  
* [ ] 7-5-1  
* [ ] 7-3-3

**Correct answer:**
* [x] 5-5-3 

**37. You have 3 data centers and 7 managers. How best should you distribute the managers between them to withstand site wide disruptions?**


* [ ]  4-2-1  
* [ ] 3-2-3 
* [ ]  3-2-2  
* [ ] 3-1-3

**Correct answer:**
* [x]  3-2-2  

**38. You have 3 data centers and 11 managers. How best should you distribute the managers between them to withstand site wide disruptions?**


* [ ]  4-3-3 
* [ ]  4-4-3 
* [ ]  4-5-2 
* [ ]  9-0-2

**Correct answer:**
* [x]  4-4-3 

**39. You have 3 data centers and 9 managers. How best should you distribute the managers between them to withstand site wide disruptions?**


* [ ]  3-3-3  
* [ ] 9-0-0  
* [ ] 4-4-1  
* [ ] 4-3-2

**Correct answer:**
* [x]  3-3-3  

**40. Which of the below statements are true?**


* [ ]  By default, manager nodes host workloads. You must explicitly configure it not to.
* [ ]  By default, manager nodes do not host workloads. You must explicitly configure it to host workloads.

**Correct answer:**
* [x]  By default, manager nodes host workloads. You must explicitly configure it not to.

---


## Auto Lock

**1. After restarting the docker service and trying to run docker service ls, you get an error “Error response from daemon: Swarm is encrypted and needs to be unlocked before it can be used. How can you solve this error?**


* [ ] docker swarm leave  
* [ ] docker swarm update
* [ ] docker swarm lock
* [ ]  docker swarm unlock

**Correct answer:**
* [x]  docker swarm unlock

**2. Which command can be used to return the current key which is used inside the cluster ?**


* [ ] docker swarm lock-key
* [ ] docker swarm lock --autolock=true
* [ ] docker swarm unlock --autolock=true
* [ ] docker swarm unlock-key

**Correct answer:**
* [x] docker swarm unlock-key

**3. …. are one or more instances of a single application that runs across the Swarm Cluster.**


* [ ] docker stack  
* [ ] services  
* [ ] pods  
* [ ] None of the above

**Correct answer:**
* [x] services  

**4. Which command can be used to run an instance on swarm?**


* [ ] docker container run -d webapp
* [ ] docker container create webapp
* [ ] docker service create webapp
* [ ] docker swarm service create webapp

**Correct answer:**
* [x] docker service create webapp

**5. Which command can be used to enable auto lock on an existing swarm?**


* [ ] docker swarm update --autolock=true
* [ ] docker swarm lock --autolock=true
* [ ] docker swarm set --autolock=true
* [ ] docker swarm unlock-key

**Correct answer:**
* [x] docker swarm update --autolock=true

**6. The RAFT logs are stored on disk and not protected.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**7. The RAFT logs are stored in memory on the manager nodes.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**8. The default behaviour requires you to unlock the swarm when a new node joins the swarm cluster.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**9. What is the command to run 3 instances of httpd on a swarm cluster?**


* [ ] docker swarm service create --instances=3 httpd
* [ ]  docker swarm service create --replicas=3 httpd
* [ ] docker service create --instances=3 httpd
* [ ] docker service create --replicas=3 httpd

**Correct answer:**
* [x] docker service create --replicas=3 httpd

**10. What component is responsible for creating tasks in swarm?**


* [ ]  scheduler  
* [ ] dispatcher 
* [ ]  orchestrator  
* [ ] allocater

**Correct answer:**
* [x]  orchestrator  

**11. The …. assigns tasks to nodes in swarm.**


* [ ] scheduler  
* [ ] dispatcher  
* [ ] orchestrator  
* [ ] allocater

**Correct answer:**
* [x] dispatcher  

**12. The …. is used to allocate IP addresses to tasks in swarm.**


* [ ]  scheduler  
* [ ] dispatcher  
* [ ] orchestrator 
* [ ]  allocater

**Correct answer:**
* [x]  allocater

**13. Which command can be used to list the running service inside the swarm?**


* [ ] docker service ps  
* [ ] docker service ls
* [ ] docker container ps 
* [ ]  docker container ls

**Correct answer:**
* [x] docker service ls

**14. What component is responsible for instructing a worker to run a task?**


* [ ]  scheduler  
* [ ] dispatcher  
* [ ] orchestrator 
* [ ]  allocater

**Correct answer:**
* [x]  scheduler  

**15. Create a swarm service webapp with image httpd and expose port 8080 on host to port 80 in container.**


* [ ] docker container run --name=webapp -p 8080:80 httpd
* [ ]  docker service create --name=webapp -p 8080:80 httpd
* [ ] docker service create --name=webapp -p 8800:80 --instances=3 httpd
* [ ] docker service create --replicas=3 httpd

**Correct answer:**
* [x]  docker service create --name=webapp -p 8080:80 httpd

**16. Which command can be used to list the tasks that are running as part of a specified service?**


* [ ] docker service ps SERVICE-NAME 
* [ ]  docker service ls
* [ ] <code>docker container ps SERVICE-NAME</code>
* [ ]  <code>docker container ls</code>

**Correct answer:**
* [x] docker service ps SERVICE-NAME 

**17. List more details about each service with a human readable format.**


* [ ]  docker service ps SERVICE-NAME --pretty
* [ ] docker container ps SERVICE-NAME --pretty
* [ ] docker service inspect SERVICE-NAME --pretty
* [ ]  None of the above

**Correct answer:**
* [x] docker service inspect SERVICE-NAME --pretty

**18. Which command can be used to get the logs of a swarm service?**


* [ ]  docker container logs SERVICE-NAME
* [ ] docker service logs SERVICE-NAME
* [ ] docker swarm log SERVICE-NAME
* [ ] docker swarm logs SERVICE-NAME

**Correct answer:**
* [x] docker service logs SERVICE-NAME

**19. Delete a webapp service from your cluster.**


* [ ]  docker swarm leave webapp
* [ ] docker service rm webapp
* [ ] docker service rollback webapp
* [ ] docker service del webapp

**Correct answer:**
* [x] docker service rm webapp

**20. How many replicas does a service created with the command – 'docker service create webapp' have?**


* [ ] 0
* [ ] 1
* [ ] 2
* [ ] 3

**Correct answer:**
* [x] 1

**21. The webapp:v1 had some bugs and we fixed them in webapp:v2. We want to apply webapp:v2 to webapp service. Select the right answer.**


* [ ] docker service update --image=webapp:v1 webapp
* [ ] docker service update --image=webapp:v2 webapp
* [ ] docker service update webapp webapp:v1
* [ ] docker service update webapp webapp:v2

**Correct answer:**
* [x] docker service update --image=webapp:v2 webapp

**22. Which command can be used to increase the number of replicas from 2 to 4 of webapp? Select all the right answer.**


* [ ] docker service update --replicas=4 webapp
* [ ]  docker service update --replicas=2 webapp
* [ ] docker service scale webapp=2
* [ ] docker service scale webapp=4

**Correct answer:**
* [x] docker service update --replicas=4 webapp
* [x] docker service scale webapp=4

**23. Which option of the docker service command can be used to update 4 replicas of mywebapp service at a time?**


* [ ] --update-delay 4 
* [ ]  --update-parallelism 4
* [ ] --placement-pref-add 4
* [ ] --replicas 4

**Correct answer:**
* [x]  --update-parallelism 4

**24. An image update operation in a swarm service happens all at once by default.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**25. If at any time during an update a task returns FAILED, the default behaviour of the scheduler is to rollback the changes unless specified otherwise.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**26. What option may be used to change the default behaviour of a failed task during an update in swarm?**


* [ ] --update-failure-action 
* [ ]  --update-parallelism
* [ ] --update-delay
* [ ] --placement-pref-add

**Correct answer:**
* [x] --update-failure-action 

**27. What are the actions which can be used with <code>–update-failure-action</code> ?**


* [ ]  pause  
* [ ] continue  
* [ ] stop
* [ ]   rollback  
* [ ] rolling-update

**Correct answer:**
* [x]  pause  
* [x] continue  
* [x]   rollback  

**28. After an update we realized that something is wrong with the new version and we want to revert back to the old version. How can we achieve that?**


* [ ] docker service update rollback webapp
* [ ]  docker service rollback webapp
* [ ]  docker service rm webapp
* [ ] docker service leave webapp

**Correct answer:**
* [x]  docker service rollback webapp

**29. The default type of a service in docker swarm is global.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**30. With a global service, you can specify a minimum number of replicas for the service.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**31. With a global service if a node is removed from the cluster, then that instance is removed as well and is rescheduled on another available node.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**32. A global service will always deploy exactly one instance of the application on all the nodes in the cluster.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**33. Create a replicated service webapp with 2 replicas. Select the all right answer.**


* [ ]  docker service create --replicas=2 webapp
* [ ] docker service create --mode=replicated --replicas=2 webapp
* [ ] docker service create --mode=global --replicas=2 webapp
* [ ] docker service create --replicas=2 web

**Correct answer:**
* [x]  docker service create --replicas=2 webapp
* [x] docker service create --mode=replicated --replicas=2 webapp

**34. Deploy exactly one instance of the application on all the nodes in the cluster.**


* [ ] docker service create --replicas=1 webapp
* [ ] docker service create --mode=replicated --replicas=1 webapp
* [ ] docker service create --mode=global --replicas=1 webapp
* [ ] docker service create --mode=global webapp

**Correct answer:**
* [x] docker service create --mode=global webapp

**35. How to get the service type of webapp service? Select the all right answer.**


* [ ] docker container inspect webapp
* [ ] docker service inspect webapp
* [ ] docker container ls  
* [ ] docker service ls

**Correct answer:**
* [x] docker service inspect webapp
* [x] docker service ls

**36. You are required to deploy an agent of splunk on all nodes in the swarm cluster to monitor the health of the nodes and gather logs. What is the best approach to achieve this?**


* [ ]  Deploy the agent as a docker container on each node in the cluster. Use a cron job to set this up.
* [ ]  Deploy the agent as a global service in the swarm cluster.
* [ ]  Deploy the agent as a replicated service with the replica count equal to the number of worker nodes in the swarm cluster.

**Correct answer:**
* [x]  Deploy the agent as a global service in the swarm cluster.

**37. By default, when we create a service, the tasks may be assigned to any of these nodes without any additional considerations as long as they have sufficient hardware resources.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**38. If you specify multiple placement constraints, the service only deploys onto nodes where they are all met.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**39. What is the command to deploy a service named webapp on a node which has a 'type=cpu-optimized' label.**


* [ ]  docker service create --constraint=node.labels.type==cpu-optimized webapp
* [ ] docker service create --labels type==cpu-optimized webapp
* [ ] docker service create --container-label type==cpu-optimized webapp
* [ ]  None of the above

**Correct answer:**
* [x]  docker service create --constraint=node.labels.type==cpu-optimized webapp

**40. Which option can be used to control the workload placements with the help of labels and constraints?**


* [ ]  replicated services  
* [ ] global services  
* [ ] placement constraints 
* [ ]  resource restrictions

**Correct answer:**
* [x] placement constraints 

**41. What is the command to apply <code>’disk=ssd'</code> label to <code>worker1</code> in a swarm cluster.**


* [ ] docker node update --label-add disk=ssd worker1
* [ ] docker node update --label-rm disk=ssd worker1
* [ ] docker service update --labels disk=ssd worker1
* [ ] docker service update --container-label disk=ssd worker1

**Correct answer:**
* [x] docker node update --label-add disk=ssd worker1

---


## Docker Overlay Network

**1. If you use the host network mode for a container, that container’s network stack is not isolated from the Docker host as the container shares the host’s networking namespace.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**2. When you create a swarm service and do not connect it to a user-defined overlay network, it connects to the …. network by default.**


* [ ]  host 
* [ ]  bridge  
* [ ] macvlan 
* [ ]  ingress

**Correct answer:**
* [x]  ingress

**3. The …. network connects multiple Docker daemons together and enables swarm services to communicate with each other.**


* [ ]  host  
* [ ] overlay  
* [ ] bridge  
* [ ] none

**Correct answer:**
* [x] overlay  

**4. When you initialize a Docker Swarm cluster it creates a new network of type overlay which is an internal private network that spans across all the nodes participating in the swarm cluster.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**5. Which network will be created when you initialize a swarm or join a Docker host to an existing swarm?**


* [ ]  host  
* [ ] bridge 
* [ ]  macvlan  
* [ ] ingress

**Correct answer:**
* [x] bridge 
* [x] ingress

**6. Bridge is the default network a container gets attached to**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**7. The …. network disables all networking. Usually used in conjunction with a custom network driver.**


* [ ] host  
* [ ] bridge  
* [ ] overlay  
* [ ] none

**Correct answer:**
* [x] none

**8. What is the command to create an overlay network driver called my-overlay?**


* [ ] docker network create my-overlay
* [ ] docker create network my-overlay
* [ ] docker network create -d overlay my-overlay
* [ ] docker network create overlay my-overlay

**Correct answer:**
* [x] docker network create -d overlay my-overlay

**9. Remove the existing ingress network using a docker command.**


* [ ] docker network inspect ingress
* [ ] docker network -d rm ingress
* [ ] docker network rm ingress
* [ ] docker network create ingress

**Correct answer:**
* [x] docker network rm ingress

**10. Which command is used to list available networks in the swarm cluster?**


* [ ] docker network --filter
* [ ] docker network get
* [ ] docker network ls
* [ ]  None of the above

**Correct answer:**
* [x] docker network ls

**11. Encrypt the application data and enable IPSEC encryption while creating the overlay network called my-overlay-network using a docker command.**


* [ ] docker network create --driver overlay --opt encrypted my-overlay-network
* [ ] docker network create --driver overlay -o encrypted=true my-overlay-network
* [ ] docker network create -d overlay --encrypted my-overlay

**Correct answer:**
* [x] docker network create --driver overlay --opt encrypted my-overlay-network
* [x] docker network create --driver overlay -o encrypted=true my-overlay-network

**12. Create an overlay network that can also be connected by standalone containers that were not created as part of a swarm service.**


* [ ] docker network create --driver overlay --attachable my-overlay-network
* [ ] docker network create --driver overlay --subnet 10.15.0.0/16 my-overlay-network
* [ ] docker network create --driver overlay --opt encrypted my-overlay-network
* [ ] docker network create --driver overlay my-overlay-network

**Correct answer:**
* [x] docker network create --driver overlay --attachable my-overlay-network

**13. Create an overlay network driver called my-overlay with subnet 10.15.0.0/16 using a docker command.**


* [ ] docker network create my-overlay
* [ ] docker network create --driver overlay --subnet 10.15.0.0/16 my-overlay
* [ ] docker network create -d overlay -subnet 10.15.0.0/16
* [ ] docker network create overlay my-overlay

**Correct answer:**
* [x] docker network create --driver overlay --subnet 10.15.0.0/16 my-overlay

**14. Delete an overlay network driver called my-overlay using a docker command.**


* [ ]  docker network rm my-overlay  
* [ ] docker create network -d my-overlay
* [ ] docker rm network my-overlay
* [ ] docker network rm -d my-overlay

**Correct answer:**
* [x]  docker network rm my-overlay  

**15. By default, all swarm service management traffic is encrypted using …. algorithm.**


* [ ]  TKIP  
* [ ] DES  
* [ ] AES  
* [ ] RSA

**Correct answer:**
* [x] AES  

**16. When you create an overlay network, only containers created as part of a swarm service can attach to it by default.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**17. Which port should be opened to allow the overlay and ingress network traffic?**


* [ ] 2377 
* [ ]  7946  
* [ ] 4789  
* [ ] 4946

**Correct answer:**
* [x] 4789  

**18. Remove all of the unused networks using a docker command.**


* [ ] docker network rm my-overlay-network  
* [ ] docker network prune
* [ ] docker rm network my-overlay-network
* [ ] docker network rm -all

**Correct answer:**
* [x] docker network prune

**19. You have a legacy application which monitors network traffic, and expects to be directly connected to the physical network. What network driver would you recommend?**


* [ ]  host  
* [ ] bridge 
* [ ]  macvlan 
* [ ]  ingress

**Correct answer:**
* [x]  macvlan 

**20. The …. port should be opened to allow communication among nodes/Container Network Discovery.**


* [ ]  2377  
* [ ] 7946  
* [ ] 4789
* [ ]   4946

**Correct answer:**
* [x] 7946  

**21. Map UDP port 80 in the container to port 5000 on the overlay network using the my-web-server image.**


* [ ] docker service create -p 80:5000/udp my-web-server
* [ ]  docker service create --publish published=80,target=5000,protocol=udp my-web-server
* [ ] docker service create -p 5000:80/udp my-web-server
* [ ] docker service create --publish published=5000,target=80,protocol=udp my-web-server

**Correct answer:**
* [x] docker service create -p 5000:80/udp my-web-server
* [x] docker service create --publish published=5000,target=80,protocol=udp my-web-server

**22. Which among the below networks are used to establish connectivity between containers on different hosts?**


* [ ]  Bridge 
* [ ]  MACVlan 
* [ ]  None  
* [ ] Overlay  
* [ ] Host  
* [ ] IPVlan

**Correct answer:**
* [x]  MACVlan 
* [x] Overlay  
* [x] IPVlan

**23. Macvlan network driver assigns a MAC address to each container’s virtual network interface, making it appear to be a physical network interface directly connected to the physical network.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**24. The routing mesh enables each node in the swarm to accept connections on published ports for any service running in the swarm, even if there’s no task running on the node.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**25. Docker requires an external DNS server to be configured during installation to help the containers resolve each other using the container name.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**26. The built-in DNS server in Docker always runs at IP address ….**


* [ ] 127.0.0.11  
* [ ] 127.0.0.1 
* [ ]  172.17.0.3  
* [ ] 172.17.0.1

**Correct answer:**
* [x] 127.0.0.11  

**27. The services only under the same network will get resolved with their names so all of the micro-service component should be under the same network so that they can resolve each other.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**28. Attach the application my-web-server to a service so that we can access it using its name with the existing overlay network driver my-overlay.**


* [ ] docker service create --name=my-service my-web-server
* [ ] docker service create --name=my-web-server my-web-server
* [ ] docker service create --name=my-web-server --network=my-overlay my-web-server
* [ ] docker service create --publish published=5000,target=80,protocol=udp my-web-server

**Correct answer:**
* [x] docker service create --name=my-web-server --network=my-overlay my-web-server

**29. Service Discovery allows containers and services to locate and communicate with each other with their names.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**30. Attach the application my-web-server to a service so that we can access it using its name with the existing overlay network driver my-overlay and the custom DNS 8.8.8.8**


* [ ] docker service create --name=my-service --dns=8.8.8.8 my-web-server
* [ ] docker service create --name=my-web-server --dns=8.8.8.8 my-web-server
* [ ] docker service create --name=my-web-server --dns=8.8.8.8 --network=my-overlay my-web-server
* [ ] docker service create --name=my-web-server --network=my-overlay my-web-server

**Correct answer:**
* [x] docker service create --name=my-web-server --dns=8.8.8.8 --network=my-overlay my-web-server

---


## Docker Stack

**1. The ….. is a group of interrelated services that together form an entire application.**


* [ ]  stack  
* [ ] service  
* [ ] docker image
* [ ]   container

**Correct answer:**
* [x]  stack  

**2. To list the services created by a stack, run …**


* [ ] docker stack deploy  
* [ ] docker stack ls  
* [ ] docker stack services 
* [ ]  docker stack ps

**Correct answer:**
* [x] docker stack services 

**3. Which command can be used to list the tasks in a stack named <code>webapp</code>?**


* [ ] docker stack deploy webapp  
* [ ] docker stack ls webapp
* [ ] docker stack services webapp
* [ ] docker stack ps webapp

**Correct answer:**
* [x] docker stack ps webapp

**4. We must use version “2” and above of docker-compose file for stack configurations as Version “3” comes with a support for a new property called deploy which is used by docker swarm for stack related configurations.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**5. To list stacks, run …**


* [ ] docker stack deploy 
* [ ]  docker stack ls  
* [ ] docker stack services  
* [ ] docker stack ps

**Correct answer:**
* [x]  docker stack ls  

**6. The ….. is one or multiple instances of the same type of container that runs on a single node or across multiple nodes in a swarm cluster.**


* [ ]  stack  
* [ ] service 
* [ ]  docker image
* [ ]   container

**Correct answer:**
* [x] service 

**7. The ….. is a packaged form of an application that has its own dependencies and runs in its own isolated environment.**


* [ ]  stack  
* [ ] service 
* [ ]  docker image 
* [ ]  container

**Correct answer:**
* [x]  container

**8. Which command can be used to remove a webapp stack?**


* [ ] docker stack deploy webapp 
* [ ]  docker stack ls webapp
* [ ] docker stack services webapp
* [ ] docker stack rm webapp

**Correct answer:**
* [x] docker stack rm webapp

**9. You could restrict the resources assigned to a service using the …. property in the docker stack.**


* [ ]  resources 
* [ ]  replicas 
* [ ]  constrains  
* [ ] restrict

**Correct answer:**
* [x]  resources 

**10. Which command can be used to deploy the STACKDEMO stack from a compose file? Select all the right answers.**


* [ ] docker stack deploy --compose-file docker-compose.yml STACKDEMO
* [ ] cat docker-compose.yml | docker stack deploy --compose-file - STACKDEMO
* [ ] docker stack services --compose-file docker-compose.yml STACKDEMO
* [ ] docker stack ps --compose-file docker-compose.yml STACKDEMO

**Correct answer:**
* [x] docker stack deploy --compose-file docker-compose.yml STACKDEMO
* [x] cat docker-compose.yml | docker stack deploy --compose-file - STACKDEMO

---


## Docker Stack Add On

**1. Which of the below statements are true?**


* [ ] The redis container will always be deployed on the manager node
* [ ] The postgres container will only be deployed on the manager node
* [ ]  The web container may be deployed on any node – manager or worker
* [ ]  The redis container will not be deployed on the manager node

**Correct answer:**
* [x] The postgres container will only be deployed on the manager node
* [x]  The web container may be deployed on any node – manager or worker

**2. What does the timeout value in the health check stand for?**


* [ ]  The time after the container starts and before the first health check starts
* [ ] The time between each health check trigger
* [ ] The time the health check test – in this case curl – waits to receive a successful response
* [ ]  The number of times a check is performed before marking the container failed

**Correct answer:**
* [x] The time the health check test – in this case curl – waits to receive a successful response

**3. The health check on the web service is configured to run at an interval of every 30 seconds. What would happen if the web server takes 45 seconds to boot up the first time?**


* [ ] The web server container will be killed and restarted after 30 seconds
* [ ] The health checks only start after 2 minutes, so the web server has sufficient time to boot up
* [ ] The health checks runs every 5 seconds and will mark the container as failed after 5 attempts
* [ ]  The web service will go into an infinite loop

**Correct answer:**
* [x] The health checks only start after 2 minutes, so the web server has sufficient time to boot up

**4. How many containers would be created in total for all services together?**


* [ ] 3
* [ ] 9
* [ ] 5
* [ ] 1

**Correct answer:**
* [x] 9

**5. What does the retries value in the health check configuration stand for?**


* [ ]  The time after the container starts and before the first health check starts
* [ ]  The time between each health check trigger
* [ ] The time the health check test – in this case curl – waits to receive a successful response
* [ ]  The number of times a check is performed before marking the container failed
* [ ]  The number of times the container restarts after the health check fails

**Correct answer:**
* [x]  The number of times a check is performed before marking the container failed

---


## Kubernetes Architecture

**1. The health check on the web service is configured to run at an interval of every 30 seconds. What would happen if the web server takes 45 seconds to boot up the first time?**


* [ ] The web server container will be killed and restarted after 30 seconds
* [ ] The health checks only start after 2 minutes, so the web server has sufficient time to boot up
* [ ] The health checks runs every 5 seconds and will mark the container as failed after 5 attempts
* [ ]  The web service will go into an infinite loop

**Correct answer:**
* [x] The health checks only start after 2 minutes, so the web server has sufficient time to boot up

**2. Which statement best describes a Kubernetes node? (Choose 3)**


* [ ] A machine part of the Kubernetes cluster that runs workloads
* [ ]  A Virtual Machine that hosts workloads part of a Kubernetes cluster
* [ ]  A Physical Machine that hosts workloads part of a Kubernetes cluster
* [ ]  A machine that automatically schedules the pods across the nodes in the cluster.
* [ ]  A tool to start a Kubernetes cluster.

**Correct answer:**
* [x] A machine part of the Kubernetes cluster that runs workloads
* [x]  A Virtual Machine that hosts workloads part of a Kubernetes cluster
* [x]  A Physical Machine that hosts workloads part of a Kubernetes cluster

**3. Which statement best describes the Worker Node component?**


* [ ]  kubelet and container runtime are the worker node components
* [ ]  kube-proxy is one of the worker node component
* [ ]  kube-scheduler is one of the worker node component
* [ ]  kube-apiserver is one of the worker node component

**Correct answer:**
* [x]  kubelet and container runtime are the worker node components
* [x]  kube-proxy is one of the worker node component

**4. What are the features of Kubernetes?**


* [ ] Self-healing & Batch execution
* [ ]  Secrets & configuration management
* [ ]  Container Image Management  
* [ ] Automated rollouts and rollbacks

**Correct answer:**
* [x] Self-healing & Batch execution
* [x]  Secrets & configuration management
* [x] Automated rollouts and rollbacks

**5. Kubernetes was originally developed by?**


* [ ]  Microsoft 
* [ ]  Redhat  
* [ ] Google 
* [ ]  IBM

**Correct answer:**
* [x] Google 

**6. Which statement best describes kubectl in Kubernetes?**


* [ ]  kubectl is an agent that runs on Kubernetes nodes 
* [ ]  kubectl is used to bring up the Kubernetes cluster
* [ ]  The Kubernetes command-line tool
* [ ]  kubectl is a tool that lets you run Kubernetes locally

**Correct answer:**
* [x]  The Kubernetes command-line tool

**7. Which statement best describes a control plane component?**


* [ ] The control plane's components decides how workloads are placed across the nodes in the cluster
* [ ]  kube-proxy is one of the control plane component
* [ ]  kube-scheduler is one of the control plane component
* [ ] kube-controller is one of the control plane component

**Correct answer:**
* [x] The control plane's components decides how workloads are placed across the nodes in the cluster
* [x]  kube-scheduler is one of the control plane component
* [x] kube-controller is one of the control plane component

**8. Kubernetes is now maintained by? Select the correct answer**


* [ ]  Google Technology Company  
* [ ] Cloud Native Computing Foundation (CNCF)
* [ ]  Open Container Initiative (OCI)
* [ ]  Apache Software Foundations (ASF)

**Correct answer:**
* [x] Cloud Native Computing Foundation (CNCF)

**9. ETCD by default listens on port 2780.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**10. Which of the following statements best describes ETCD? Select the correct answer**


* [ ]  Etcd serves as the backing datastore for Kubernetes cluster data
* [ ]  ETCD is a worker node component
* [ ]  ETCD is a distributed reliable key-value store
* [ ]  None of the above

**Correct answer:**
* [x]  Etcd serves as the backing datastore for Kubernetes cluster data
* [x]  ETCD is a distributed reliable key-value store

**11. Which of the following are components deployed only on a Master Node in a Kubernetes cluster?**


* [ ] Kube Scheduler  
* [ ] Kube Controller Manager  
* [ ] Kube Api-server  
* [ ] Kubelet  
* [ ] Kube-Proxy

**Correct answer:**
* [x] Kube Scheduler  
* [x] Kube Controller Manager  
* [x] Kube Api-server  

**12. Which of the following is the etcd command line tool?**


* [ ]  etcd  
* [ ] etcdctl
* [ ]   kubectl  
* [ ] etcdcli

**Correct answer:**
* [x] etcdctl

**13. Which of the following component watches for newly created pods and selects a node for them to run on?**


* [ ] kube-proxy  
* [ ] kube-node-controller  
* [ ] kube-scheduler 
* [ ]  kubelet Agent

**Correct answer:**
* [x] kube-scheduler 

**14. Which of the below comes under Kubernetes Hosted Solutions?**


* [ ] Google Compute Engine (GCE)  
* [ ] Google Kubernetes Engine (GKE)
* [ ] Azure Kubernetes Service (AKS)
* [ ] Amazon EC2 Service

**Correct answer:**
* [x] Google Kubernetes Engine (GKE)
* [x] Azure Kubernetes Service (AKS)

**15. What is a component of the Kubernetes control plane that allows external users or services to manage the Kubernetes cluster?**


* [ ]  Kubernetes Scheduler 
* [ ]  ETCDCTL  
* [ ] Kube API Server  
* [ ] Kube Proxy

**Correct answer:**
* [x] Kube API Server  

**16. What is the purpose of the replication controller?**


* [ ]  Responsible for noticing and responding when nodes go down.
* [ ]  An agent that runs on each node in the cluster. It makes sure that containers are running in a Pod.
* [ ] Responsible for maintaining the correct number of replicas of PODs at all times.
* [ ]  Replication controller makes sure that a pod or a homogeneous set of pods is always up and available

**Correct answer:**
* [x] Responsible for maintaining the correct number of replicas of PODs at all times.
* [x]  Replication controller makes sure that a pod or a homogeneous set of pods is always up and available

**17. Which of the following are the container runtimes that Kubernetes supports.**


* [ ]  Docker  
* [ ] Containerd  
* [ ] CRI-O
* [ ]   LXC

**Correct answer:**
* [x]  Docker  
* [x] Containerd  
* [x] CRI-O

**18. Which component on the worker node is responsible for maintaining network rules on nodes?**


* [ ] kube-controller-manager  
* [ ] kube-proxy 
* [ ]  kubelet 
* [ ]  kube-apiserver

**Correct answer:**
* [x] kube-proxy 

**19. Which of the following statements best describes kube-scheduler?**


* [ ]  The kube-scheduler is only responsible for deciding which pod goes on which node.
* [ ] It places the pod on the nodes  
* [ ] Kube-scheduler is a worker node component
* [ ]  All of the above

**Correct answer:**
* [x]  The kube-scheduler is only responsible for deciding which pod goes on which node.

**20. What is the command to deploy a nginx pod?**


* [ ]  kubectl deploy nginx --image nginx  
* [ ] kubectl run nginx --image nginx
* [ ] kubectl start -it nginx bash  
* [ ] kubelet run nginx --image nginx

**Correct answer:**
* [x] kubectl run nginx --image nginx

**21. Which statement best describes Multi-Container POD? Select all the answers that apply.**


* [ ]  Multi-container Pods can share resources and dependencies, communicate with one another, and coordinate when and how they are terminated
* [ ]  A single pod can have multiple containers
* [ ]  A single pod can have multiple containers of the same kind to scale up.
* [ ]  It is recommended to always use multi-container pods to improve performance of applications.

**Correct answer:**
* [x]  Multi-container Pods can share resources and dependencies, communicate with one another, and coordinate when and how they are terminated
* [x]  A single pod can have multiple containers

**22. Which statements best describe a POD in Kubernetes?**


* [ ]  Kubernetes deploys applications in the form of Pods  
* [ ] A Pod can contain only one container
* [ ]  To scale up an application, increase the number of containers in a Pod.
* [ ]  Every container in the pod gets its own hostname and IP address

**Correct answer:**
* [x]  Kubernetes deploys applications in the form of Pods  

**23. Which of the following are the types of controllers in Kubernetes?**


* [ ]  Node-Controller  
* [ ] Replication-Controller  
* [ ] Endpoint-Controller  
* [ ] Deployment-Controller

**Correct answer:**
* [x]  Node-Controller  
* [x] Deployment-Controller
* [x] Replication-Controller  
* [x] Endpoint-Controller  

**24. What is the command to list all the pods that are in a default namespace? Select all the answers that apply.**


* [ ] kubectl list pods -n default  
* [ ] kubectl get pods
* [ ] kubectl list pods
* [ ] kubectl get pods -n default

**Correct answer:**
* [x] kubectl get pods
* [x] kubectl get pods -n default

**25. Which of the following statement is correct? Select all the answers that apply.**


* [ ]  Pods can only be created via kubectl commands
* [ ]  Pods can be created with kubectl commands as well as via API calls.
* [ ] Pods can only be created via API calls.
* [ ]  None of the above

**Correct answer:**
* [x]  Pods can be created with kubectl commands as well as via API calls.

**26. What is the command to check which nodes are the pods placed on? Select all the answers that apply.**


* [ ]  kubectl get pods 
* [ ]  kubectl get pods -o wide
* [ ] kubectl describe pod  
* [ ] kubectl get nodes

**Correct answer:**
* [x]  kubectl get pods -o wide
* [x] kubectl describe pod  

**27. What is the command to delete the pod?**


* [ ] kubectl pod delete  
* [ ] kubectl delete 
* [ ]  kubectl delete pod 
* [ ]  kubectl pod --delete

**Correct answer:**
* [x]  kubectl delete pod 

**28. What are the possible ways to update the pod image? Select all the answers that apply.**


* [ ]  You cannot update a pod image once a pod is created.
* [ ]  Update the pod-definition file and use kubectl apply command.
* [ ]  Use kubectl edit pod command and specify the new image
* [ ] None of the above

**Correct answer:**
* [x]  Update the pod-definition file and use kubectl apply command.
* [x]  Use kubectl edit pod command and specify the new image

**29. How do you specify image names in a pod definition YAML file?**


* [ ] containers.image  
* [ ] spec.containers.image
* [ ] template.containers.image  
* [ ] kind.containers.image

**Correct answer:**
* [x] spec.containers.image

**30. What are the 4 top level fields a Kubernetes definition file for POD contains?**


* [ ] apiVersion
* [ ]   templates  
* [ ] metadata
* [ ] labels  
* [ ] kind  
* [ ] spec
* [ ] namespaces 
* [ ]  containers

**Correct answer:**
* [x] metadata
* [x] apiVersion
* [x] kind  
* [x] spec

**31. What is the command to create a pod with the pod-definition.yaml file?**


* [ ] kubectl run -f pod-definition.yaml
* [ ] kubectl pod -f pod-definition.yaml
* [ ]  kubectl create -f pod-definition.yaml
* [ ] kubectl apply -f pod-definition.yaml

**Correct answer:**
* [x]  kubectl create -f pod-definition.yaml
* [x] kubectl apply -f pod-definition.yaml

**32. How do you add labels to a pod in a pod definition YAML file?**


* [ ] labels  
* [ ] spec.labels  
* [ ] spec.containers.labels  
* [ ] metadata.labels

**Correct answer:**
* [x] metadata.labels

**33. What is the command to delete a pod via a pod-definition file?**


* [ ] kubectl remove -f pod-definition.yaml
* [ ] kubectl rm -f pod-definition.yaml
* [ ] kubectl delete -f pod-definition.yaml
* [ ] kubectl del -f pod-definition.yaml

**Correct answer:**
* [x] kubectl delete -f pod-definition.yaml

---


## Pods with Yaml Add-On

**1. How many IP addresses are consumed by the pod when it’s created?**


* [ ] 1
* [ ] 2
* [ ] 3
* [ ] 4

**Correct answer:**
* [x] 1

**Code**: 
YAML
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
spec:
  containers: 
   - name: nginx-container
     image: nginx
   - name: agent
     image: agent

**2. How many containers are created when this pod is created?**


* [ ] 1
* [ ] 2
* [ ] 3
* [ ] 4

**Correct answer:**
* [x] 2

**Code**: 
YAML
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
spec:
  containers: 
   - name: nginx-container
     image: nginx
   - name: agent
     image: agent

---


## Replication Controllers and Replicasets

**1. The label selector is the core grouping primitive in Kubernetes. What kind of selectors are supported?**


* [ ]  Equality-Based  
* [ ] Value-Based  
* [ ] Operator-Based 
* [ ]  Set-Based

**Correct answer:**
* [x]  Equality-Based  
* [x]  Set-Based

**2. Which of the following commands are used to list all the ReplicaSets? Select all the answers that apply.**


* [ ] <code>kubectl get services</code> 
* [ ]  <code>kubectl get rs</code>
* [ ]  <code>kubectl get replicaset</code>
* [ ]  <code>kubectl get pods</code>

**Correct answer:**
* [x]  <code>kubectl get rs</code>
* [x]  <code>kubectl get replicaset</code>

**3. A ReplicaSet is one of the Kubernetes controllers?**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**4. What is the command to delete a replication controller nginx?**


* [ ] kubectl get rc nginx 
* [ ]  kubectl remove rc nginx
* [ ] kubectl rm rc nginx
* [ ] kubectl delete rc nginx

**Correct answer:**
* [x] kubectl delete rc nginx

**5. Which statements best describe replication controllers and replica sets? Select all answers that apply.**


* [ ]  Replication Controller is the older technology that is being replaced by a ReplicaSet.
* [ ]  There is no difference between Replication controller and ReplicaSet.
* [ ]  The replication controller supports equality based selectors whereas the replica set supports equality based as well as set based selectors.
* [ ] ReplicaSet is the new way to set up replication.

**Correct answer:**
* [x]  Replication Controller is the older technology that is being replaced by a ReplicaSet.
* [x]  The replication controller supports equality based selectors whereas the replica set supports equality based as well as set based selectors.
* [x] ReplicaSet is the new way to set up replication.

**6. What is the command to list all the labels of a ReplicaSet?**


* [ ] kubectl get rs --show-labels
* [ ] kubectl get rs --labels
* [ ]  kubectl get rs -l
* [ ] kubectl get rs --details

**Correct answer:**
* [x] kubectl get rs --show-labels

**7. What is the command to delete a ReplicaSets triage?**


* [ ] kubectl get rs triage  
* [ ] kubectl remove rs triage
* [ ] kubectl rm rs triage  
* [ ] kubectl delete rs triage

**Correct answer:**
* [x] kubectl delete rs triage

**8. What is a Label in Kubernetes?**


* [ ]  A way to expose traffic 
* [ ]  A type of Deployment
* [ ] A way to group related things using key/value pairs
* [ ]  None of the above

**Correct answer:**
* [x] A way to group related things using key/value pairs

**9. How do you scale replica sets? Select all the answers that apply.**


* [ ]  Update the number of replicas in the replicaset-definition.yaml definition file and apply.
* [ ]  Update using the kubectl scale command.
* [ ]  Delete and recreate a replica set.
* [ ]  Create a new replica set with the desired number of pods and delete the old replica set.

**Correct answer:**
* [x]  Update the number of replicas in the replicaset-definition.yaml definition file and apply.
* [x]  Update using the kubectl scale command.

**10. What is the command to get the list of deployments. Select all the answers that apply.**


* [ ] kubectl get deploy  
* [ ] kubectl get deployment  
* [ ] kubectl get deployments
* [ ] kubectl get deployments.apps

**Correct answer:**
* [x] kubectl get deploy  
* [x] kubectl get deployment  
* [x] kubectl get deployments
* [x] kubectl get deployments.apps

**11. You are required to deploy an application in the form of containers that can easily scale up or down and supports upgrade of applications by maintaining information about different revisions. What is the recommended approach to deploying the application?**


* [ ]  Create a POD  
* [ ] Create a ReplicaSet
* [ ]  Create a Replication Controller  
* [ ] Create a Deployment

**Correct answer:**
* [x] Create a Deployment

**12. What command would you use to create a Deployment? Select the correct answer**


* [ ] kubectl get deployments  
* [ ] kubectl get nodes
* [ ] kubectl create  
* [ ] kubectl run

**Correct answer:**
* [x] kubectl create  

**13. What is the flag that you use along with "kubectl create" to scale a deployment in Kubernetes?**


* [ ] --image 
* [ ]  --label 
* [ ]  --replicas 
* [ ]  --scale

**Correct answer:**
* [x]  --replicas 

**14. Which of the following subcommands of kubectl can be used to get additional details of an object?**


* [ ] kubectl details 
* [ ]  kubectl info 
* [ ]  kubectl check
* [ ] kubectl describe

**Correct answer:**
* [x] kubectl describe

**15. What is the command to delete a deployment?**


* [ ]  kubectl deployment delete deployment-name
* [ ] kubectl delete deployment deployment-name
* [ ] kubectl deployment-name delete deployment
* [ ] kubectl deployment-name deployment delete

**Correct answer:**
* [x] kubectl delete deployment deployment-name

**16. What is the command to create the deployment using the deployment definition file?**


* [ ]  kubectl deployment -f deploy-definition.yaml
* [ ]  kubectl create -f deploy-definition.yaml
* [ ]  kubectl deploy -f deploy-definition.yaml
* [ ] kubectl get -f deploy-definition.yaml

**Correct answer:**
* [x]  kubectl create -f deploy-definition.yaml

**17. Which statement best describes deployment in Kubernetes? Select all the answers that apply.**


* [ ]  Deployments create PODs and not ReplicaSets.
* [ ] Deployments create ReplicaSets that create PODs.
* [ ] Deployments support rolling updates and roll backs of applications.
* [ ]  Deployments support rolling updates but not roll backs.

**Correct answer:**
* [x] Deployments create ReplicaSets that create PODs.
* [x] Deployments support rolling updates and roll backs of applications.

**18. What is the command to update the deployment in Kubernetes? Let’s update the nginx Pods to use the nginx:1.16.1 image instead of the nginx:1.14.2 image.**


* [ ] kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.16.1
* [ ]  kubectl set image deployment/nginx-deployment nginx=nginx:1.16.1
* [ ] kubectl set --image=deployment/nginx-deployment nginx=nginx:1.16.1
* [ ] kubectl edit deployment.v1.apps/nginx-deployment

**Correct answer:**
* [x] kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.16.1
* [x]  kubectl set image deployment/nginx-deployment nginx=nginx:1.16.1
* [x] kubectl edit deployment.v1.apps/nginx-deployment

**19. Where do you configure the selector labels in the deployment YAML file?**


* [ ] metadata.selector 
* [ ]  spec.selector
* [ ] spec.template.selector
* [ ] spec.template.metadata.selector

**Correct answer:**
* [x]  spec.selector

**20. Where do you configure the pod images in the deployment YAML file?**


* [ ]  metadata.image  
* [ ] spec.containers.image
* [ ] spec.template.spec.containers.image
* [ ] spec.template.containers.image

**Correct answer:**
* [x] spec.template.spec.containers.image

**21. Which of the following statements about Kubernetes deployments are correct?**


* [ ]  You describe a desired state in a Deployment, and the Deployment Controller changes the actual state to the desired state at a controlled rate.
* [ ] You can define Deployments to create new ReplicaSets, or to remove existing Deployments and adopt all their resources with new Deployments.
* [ ] You may manually update the ReplicaSets owned by a Deployment.
* [ ]  You should not manually update the ReplicaSets owned by a Deployment.

**Correct answer:**
* [x]  You describe a desired state in a Deployment, and the Deployment Controller changes the actual state to the desired state at a controlled rate.
* [x] You can define Deployments to create new ReplicaSets, or to remove existing Deployments and adopt all their resources with new Deployments.
* [x]  You should not manually update the ReplicaSets owned by a Deployment.

**22. Rolling updates allows deployments to update with zero downtime ?**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**23. What kubectl command can be used to perform a Deployment update?**


* [ ]  kubectl set image 
* [ ]  kubectl rollout update
* [ ] kubectl rolling-update
* [ ] kubectl update

**Correct answer:**
* [x]  kubectl set image 

**24. What is the apiVersion for Kubernetes deployment?**


* [ ]  v1 
* [ ]  apps/v1  
* [ ] app/v1  
* [ ] apps/v

**Correct answer:**
* [x]  apps/v1  

**25. What is the command to check the status of a deployment rollout named nginx-deploy?**


* [ ] kubectl rollout status deployment/nginx-deploy
* [ ] kubectl rollout undo deployment/nginx-deploy
* [ ] kubectl rollout update deployment/nginx-deploy
* [ ] kubectl deployment status nginx-deploy

**Correct answer:**
* [x] kubectl rollout status deployment/nginx-deploy

**26. What is the command used to rollback to the previous deployment?**


* [ ]  <code>kubectl set image</code>
* [ ]  <code>kubectl rollout undo</code>
* [ ]  <code>kubectl rollout status</code>
* [ ]  <code>kubectl rollout start</code>

**Correct answer:**
* [x]  <code>kubectl rollout undo</code>

**27. What is the command used to view previous rollout revisions and configurations?**


* [ ] kubectl rollout status  
* [ ] kubectl rollout history 
* [ ]  kubectl rollout undo
* [ ] kubectl rollout pause

**Correct answer:**
* [x] kubectl rollout history 

**28. You performed an upgrade of images on a deployment recently. You’d like to check what command was run during the last update. However the output of the rollout history command does not show the command. What may be the cause?**


* [ ]  The upgrade was done using a kubectl apply command
* [ ] The command run to upgrade did not use the –record flag.
* [ ] The kubectl set command was used to perform the upgrade
* [ ] The API server was down when the upgrade was performed

**Correct answer:**
* [x] The command run to upgrade did not use the –record flag.

**29. If .spec.strategy.type is set to Recreate, then all existing pods are killed before new ones are created.**


* [ ] True
* [ ] False 

**Correct answer:**
* [x] True

**30. If .spec.strategy.type is set to RollingUpdate, then all new PODs are created first and then all existing pods are killed at once.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**31. Which of the following are the deployment strategy types in Kubernetes?**


* [ ]  RollingUpdate 
* [ ]  BlueGreen  
* [ ] Canary 
* [ ]  Recreate

**Correct answer:**
* [x]  RollingUpdate 
* [x]  Recreate

**32. Which of the following is the default deployment strategy in Kubernetes deployments?**


* [ ]  Recreate  
* [ ] RollingUpdate  
* [ ] Redeploy  
* [ ] BlueGreen

**Correct answer:**
* [x] RollingUpdate  

---


## Deployments Update and Rollback Add-on

**1. How many IP addresses would be consumed when the deployment is created?**


* [ ] 3
* [ ] 6
* [ ] 9
* [ ] 1

**Correct answer:**
* [x] 3

**2. Which of the below statements are true?**


* [ ]  This is an invalid configuration because the selector matchLabel nginx does not match the label web set on the deployment
* [ ]  This is an invalid configuration because there are more than 1 containers configured in the template
* [ ]  This is an invalid configuration because the selector field must come under the template section and not directly under spec
* [ ]  This is an invalid configuration because the API version is not set correctly
* [ ] This is a valid configuration

**Correct answer:**
* [x] This is a valid configuration

**3. How many containers would be created in total when this deployment is created (excluding the PAUSE containers)?**


* [ ] 3
* [ ] 6
* [ ] 9
* [ ] 1

**Correct answer:**
* [x] 9

---


## Networking in Kubernetes

**1. What is the command to delete a Kubernetes service?**


* [ ] kubectl delete svc SERVICE-NAME  
* [ ] kubectl rm service SERVICE-NAME
* [ ] kubectl del services SERVICE-NAME
* [ ]  kubectl delete services SERVICE-NAME

**Correct answer:**
* [x] kubectl delete svc SERVICE-NAME  
* [x]  kubectl delete services SERVICE-NAME

**2. Each container inside a POD gets its own IP address assigned**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**3. ow many IP addresses are consumed by 3 PODs each with 2 containers?**


* [ ] 3
* [ ] 9
* [ ] 2
* [ ] 6

**Correct answer:**
* [x] 3

**4. If the service type is NodePort, then Kubernetes will allocate a port on every worker node. .**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**5. Which of the following statements are correct about NodePort? Select all the answers that apply.**


* [ ]  NodePort exposes a service on the same port as that of the exposed port on containers in the PODs.
* [ ]  NodePort exposes a service internally within the hosts only.
* [ ] NodePort exposes a service to make it externally accessible on a port on the nodes.
* [ ]  None of the Above

**Correct answer:**
* [x] NodePort exposes a service to make it externally accessible on a port on the nodes.

**6. Which of the following are valid service types in Kubernetes?**


* [ ] NodePort  
* [ ] ClusterIP  
* [ ] LoadBalancer
* [ ]  ExternalName  
* [ ] ElasticLoadBalancer

**Correct answer:**
* [x] NodePort  
* [x] ClusterIP  
* [x] LoadBalancer
* [x]  ExternalName  

**7. What is the default range of ports that Kubernetes uses for NodePort if one is not specified?**


* [ ] 32767-64000  
* [ ] 30000-32767 
* [ ]  32000-32767  
* [ ] 80-8080

**Correct answer:**
* [x] 30000-32767 

**8. What is the command to list the Kubernetes services? Select all the answers that apply.**


* [ ] kubectl get svc
* [ ] kubectl list services
* [ ] kubectl get services
* [ ]  kubectl list svc

**Correct answer:**
* [x] kubectl get svc
* [x] kubectl get services

**9. A NodePort service exposes a deployment only on the nodes on which the PODs of that deployment are running.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**10. An application has 2 tiers – a web service that must be externally accessible to users and a database service that must be accessible within the cluster only. What service types should be configured?**


* [ ]  Web – NodePort, Database – LoadBalancer  
* [ ] Web – ClusterIP, Database – ClusterIP
* [ ]  Web – NodePort, Database – ClusterIP
* [ ]  Web – ClusterIP, Database – NodePort

**Correct answer:**
* [x]  Web – NodePort, Database – ClusterIP

**11. ClusterIP is the default service type for Kubernetes service.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

---


## Services ClusterIP Add-on

**1. For this service to discover the web service, what must be the label set on the PODs hosting the web service?**


* [ ] obj: web-service  
* [ ] app: web  
* [ ] app: web-service  
* [ ] obj: web

**Correct answer:**
* [x] app: web  

**2. What port on the PODs is the web service most likely exposed on?**


* [ ]  80  
* [ ] 9376  
* [ ] 8080  
* [ ] 39376

**Correct answer:**
* [x] 9376  

**3. Which of the below statements are correct?**


* [ ] Traffic to port 39376 on the node hosting the pod in the cluster is routed to port 9376 on a POD with the label app web on the same node
* [ ]  Traffic to port 39376 on all nodes in the cluster is routed to port 9376 on a random POD with the label app web
* [ ]  Traffic to port 80 on the service is routed to port 9376 on a random POD with the label app web
* [ ]  Traffic to port 80 on the node is routed to port 9376 on the service

**Correct answer:**
* [x]  Traffic to port 39376 on all nodes in the cluster is routed to port 9376 on a random POD with the label app web
* [x]  Traffic to port 80 on the service is routed to port 9376 on a random POD with the label app web

**4. A user is trying to access the application using the Nodes IP and Port number. What port must the user try to connect to?**


* [ ]  80  
* [ ] 9376  
* [ ] 8080  
* [ ] 39376

**Correct answer:**
* [x] 39376

---


## Commands and Arguments in Kubernetes

**1. How do you set environment variables in a pod definition file?**


* [ ]  Using environment section  
* [ ] Using env section
* [ ]  Using env_var section
* [ ]  Using variables section

**Correct answer:**
* [x] Using env section

**2. Which field of Kubernetes pod definition file corresponds to the entrypoint instruction in the Dockerfile?**


* [ ] ENTRYPOINT instruction in Dockerfile corresponds to command in kubernetes definition file
* [ ]  ENTRYPOINT instruction in Dockerfile corresponds to args in kubernetes definition file
* [ ] CMD instruction in Dockerfile corresponds to args in kubernetes definition file
* [ ] CMD instruction in Dockerfile corresponds to command in kubernetes definition file

**Correct answer:**
* [x] ENTRYPOINT instruction in Dockerfile corresponds to command in kubernetes definition file
* [x] CMD instruction in Dockerfile corresponds to args in kubernetes definition file

**3. Which of the following statements is true about configuring commands and arguments in Kubernetes? Select all the answers that apply.**


* [ ] To define a command, include the command field in the configuration file.
* [ ]  To define a command, include the args field in the configuration file.
* [ ] To define arguments for the command, include the command field in the configuration file.
* [ ]  To define arguments for the command, include the args field in the configuration file.

**Correct answer:**
* [x] To define a command, include the command field in the configuration file.
* [x]  To define arguments for the command, include the args field in the configuration file.

**4. Which of the following flags can be used to pass an environment variable while creating a pod with docker run command?**


* [ ] docker run --environment APP_COLOR=pink simple-webapp-color
* [ ]  docker run --env APP_COLOR=pink simple-webapp-color
* [ ] docker run -e APP_COLOR=pink simple-webapp-color
* [ ] docker run -v APP_COLOR=pink simple-webapp-color

**Correct answer:**
* [x]  docker run --env APP_COLOR=pink simple-webapp-color
* [x] docker run -e APP_COLOR=pink simple-webapp-color

**5. The command and arguments that you define in the configuration file override the default command and arguments configured in the container image.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**6. What are the different ways of setting up environment variables in Kubernetes? Select all the answers that apply.**


* [ ]  plain key-value pair 
* [ ]  configmap 
* [ ]  from disk  
* [ ] secrets

**Correct answer:**
* [x] secrets
* [x]  configmap 
* [x]  plain key-value pair 

**7. Which of the below are valid instructions to set environment variables in a Dockerfile?**


* [ ] ENVIRONMENT name=value  
* [ ] ENV name=value  
* [ ] ENV name value
* [ ] VAR name value

**Correct answer:**
* [x] ENV name=value  
* [x] ENV name value

**8. Where is the env instruction set in a Kubernetes pod definition file?**


* [ ] spec.containers.env  
* [ ] spec.env
* [ ] spec.template.spec.env
* [ ] spec.template.env

**Correct answer:**
* [x] spec.containers.env  

**9. What is the command to display details of the ConfigMap?**


* [ ]  kubectl get configmap CONFIGMAP-NAME  
* [ ] kubectl describe configmap CONFIGMAP-NAME
* [ ] kubectl list configmap CONFIGMAP-NAME
* [ ] kubectl get configmap CONFIGMAP-NAME --details

**Correct answer:**
* [x] kubectl describe configmap CONFIGMAP-NAME

**10. What is the command to create config maps? Select all the answers that apply.**


* [ ] kubectl create configmap CONFIGMAP-NAME --from-literal=KEY1=VALUE1 --from-literal=KEY2=VALUE2
* [ ] kubectl create configmap CONFIGMAP-NAME --from-file=/tmp/env
* [ ] kubectl create configmap CONFIGMAP-NAME --file=/tmp/env
* [ ]  kubectl create configmap CONFIGMAP-NAME --literal=KEY1=VALUE1 KEY2=VALUE2

**Correct answer:**
* [x] kubectl create configmap CONFIGMAP-NAME --from-literal=KEY1=VALUE1 --from-literal=KEY2=VALUE2
* [x] kubectl create configmap CONFIGMAP-NAME --from-file=/tmp/env

**11. You can pass in the --from-file argument multiple times to create a ConfigMap from multiple data sources.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**12. What is the command to list configmaps? Select all the answers that apply.**


* [ ] kubectl get pods 
* [ ]  kubectl get cm 
* [ ]  kubectl get configmap
* [ ] kubectl get maps

**Correct answer:**
* [x]  kubectl get configmap
* [x]  kubectl get cm 

**13. Which statements best describe configmaps?**


* [ ]  ConfigMap is an API object mainly used to store confidential data in key-value pairs.
* [ ]  ConfigMap is an API object mainly used to store non-confidential data in key-value pairs.
* [ ]  Pods can consume ConfigMaps as environment variables, command-line arguments, or as configuration files in a volume.
* [ ]  ConfigMap provides secrecy or encryption

**Correct answer:**
* [x]  ConfigMap is an API object mainly used to store non-confidential data in key-value pairs.
* [x]  Pods can consume ConfigMaps as environment variables, command-line arguments, or as configuration files in a volume.

**14. What is the flag that we can use to define a literal value from the command line?**


* [ ] --env 
* [ ]  --from-literal 
* [ ]  --literal 
* [ ]  --text

**Correct answer:**
* [x]  --from-literal 

**15. How do you inject configmap into a pod?**


* [ ]  Using envFrom and configMapRef
* [ ]  Using env and configMapRef
* [ ] Using envFrom and configMap
* [ ]  Using env and configMap

**Correct answer:**
* [x]  Using envFrom and configMapRef

**16. Where do you configure the configMapKeyRef in a pod to use environment variables defined in a ConfigMap?**


* [ ] spec.containers.env 
* [ ]  spec.env.valueFrom  
* [ ] spec.containers.valueFrom
* [ ] spec.containers.env.valueFrom

**Correct answer:**
* [x] spec.containers.env.valueFrom

**17. What is the recommended approach to load a set of configurations into the pod in the form of a file to /var/configs?**


* [ ]  Add a separate env parameter for each config and use a startup script to write to a file
* [ ]  Create a ConfigMap with the required configurations, configure it as a volume in the pod definition file and then mount the volume as a file at /var/configs
* [ ] Create a ConfigMap with the required configurations, configure it as an env variable in the pod definition file and use a startup script to write to a file

**Correct answer:**
* [x]  Create a ConfigMap with the required configurations, configure it as a volume in the pod definition file and then mount the volume as a file at /var/configs

**18. What is the command to display details of the secret?**


* [ ] kubectl get secret SECRET-NAME 
* [ ]  kubectl describe secret SECRET-NAME
* [ ] kubectl list secret SECRET-NAME
* [ ] kubectl get secret SECRET-NAME --details

**Correct answer:**
* [x]  kubectl describe secret SECRET-NAME

**19. What is the command to list the Kubernetes secrets?**


* [ ] kubectl list secrets  
* [ ] kubectl get secrets
* [ ] kubectl secrets  
* [ ] kubectl secrets --list

**Correct answer:**
* [x] kubectl get secrets

**20. What is the command to create a secret using the "kubectl create secret" command?**


* [ ] kubectl create secret test-secret --from-literal='username=my-app' --from-literal='password=39528$vdg7Jb'
* [ ] kubectl create secret opaque test-secret --from-literal='username=my-app' --from-literal='password=39528$vdg7Jb'
* [ ] kubectl create secret credentials test-secret --from-literal='username=my-app' --from-literal='password=39528$vdg7Jb'
* [ ] kubectl create secret generic test-secret --from-literal='username=my-app' --from-literal='password=39528$vdg7Jb'

**Correct answer:**
* [x] kubectl create secret generic test-secret --from-literal='username=my-app' --from-literal='password=39528$vdg7Jb'

**21. How do you configure all key-value pairs in a Secret as container environment variables?**


* [ ] env.secreRef  
* [ ] envFrom.secret  
* [ ] envFrom.secretRef  
* [ ] envFrom.secretRefKey

**Correct answer:**
* [x] envFrom.secretRef  

**22. Which statements best describe Kubernetes secrets?**


* [ ]  Kubernetes secrets let you store and manage sensitive information, such as passwords, OAuth tokens, and ssh keys.
* [ ]  Storing confidential information in a Secret is safer.
* [ ]  Users can create Secrets and the system also creates some Secrets.
* [ ]  It is safe to check in secrets into source code repositories.

**Correct answer:**
* [x]  Kubernetes secrets let you store and manage sensitive information, such as passwords, OAuth tokens, and ssh keys.
* [x]  Storing confidential information in a Secret is safer.
* [x]  Users can create Secrets and the system also creates some Secrets.

**23. You can pass in the --from-file argument multiple times to create a secret from multiple data sources.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**24. Secrets store sensitive information in an encrypted format.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**25. what is the default Secret type if omitted from a Secret configuration file?**


* [ ] kubernetes.io/tls  
* [ ] kubernetes.io/ssh-auth
* [ ] Opaque 
* [ ]  kubernetes.io/dockercfg

**Correct answer:**
* [x] Opaque 

---


## Readiness Probes

**1. Readiness probes are configured similarly to liveness probes. The only difference is that you use the readinessProbe field instead of the livenessProbe field.**


* [ ] False
* [ ] True

**Correct answer:**
* [x] True

**2. What are the different types of probes?**


* [ ]  Command  
* [ ] HTTP  
* [ ] TCP  
* [ ] CURL

**Correct answer:**
* [x]  Command  
* [x] HTTP  
* [x] TCP  

**3. Which of the following would be the result/state of a probe? Select the all right answers**


* [ ]  SUCCESS  
* [ ] FAILURE  
* [ ] UNKNOWN  
* [ ] PENDING

**Correct answer:**
* [x]  SUCCESS  
* [x] FAILURE  
* [x] UNKNOWN  

**4. If a readiness probe starts to fail, Kubernetes stops sending traffic to the pod until it passes.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**5. The kubelet uses liveness probes to know when a container is ready to start accepting traffic.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**6. If a Container does not provide a liveness probe, the default state is Failure.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**7. Which statement best describes the readiness probe?**


* [ ]  The kubelet uses readiness probes to know when a container is ready to start accepting traffic.
* [ ]  The kubelet uses readiness probes to know when to restart a container
* [ ] The readiness probes run on the container during it's entire lifecycle.

**Correct answer:**
* [x]  The kubelet uses readiness probes to know when a container is ready to start accepting traffic.
* [x] The readiness probes run on the container during it's entire lifecycle.

**8. Which statement best describes the liveness probe?**


* [ ]  The kubelet uses liveness probes to know when a container is ready to start accepting traffic.
* [ ] The kubelet uses liveness probes to know when to restart a container
* [ ]  The liveness probes may be configured with an HTTP test to check if a container is live.
* [ ]  The liveness probe runs before the readiness probe is run on the container

**Correct answer:**
* [x] The kubelet uses liveness probes to know when to restart a container
* [x]  The liveness probes may be configured with an HTTP test to check if a container is live.

**9. If the liveness probe fails, the kubelet kills the container, and the container is subjected to its restart policy.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**10. Liveness probes let Kubernetes know if your app is alive or stuck/dead.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**11. The traffic from a web server fetching data from a database server may be categorized as**


* [ ]  Ingress  
* [ ] Egress

**Correct answer:**
* [x] Egress

**12. What is the default traffic flow configuration between pods in a Kubernetes cluster?**


* [ ]  All traffic is allowed between different pods in the cluster
* [ ]  All traffic is denied between different pods in the cluster
* [ ]  Traffic between different pods must be explicitly allowed using rules

**Correct answer:**
* [x]  All traffic is allowed between different pods in the cluster

**13. Which of the following statements best describes Kubernetes network policies?**


* [ ]  If you want to control traffic flow at the IP address or port level, then you might consider using Kubernetes NetworkPolicies.
* [ ] NetworkPolicies are an application-centric construct which allow you to specify how a pod is allowed to communicate with various network "entities" over the network
* [ ]  Network Policies are implemented by the network plugin
* [ ]  Pods become isolated by having a NetworkPolicy that selects them

**Correct answer:**
* [x]  If you want to control traffic flow at the IP address or port level, then you might consider using Kubernetes NetworkPolicies.
* [x] NetworkPolicies are an application-centric construct which allow you to specify how a pod is allowed to communicate with various network "entities" over the network
* [x]  Network Policies are implemented by the network plugin
* [x]  Pods become isolated by having a NetworkPolicy that selects them

**14. Which of the following solutions support network policies?**


* [ ] kube-router 
* [ ]  Calico  
* [ ] Flannel  
* [ ] Weave-Net

**Correct answer:**
* [x] kube-router 
* [x]  Calico  
* [x] Weave-Net

**15. By default, pods are isolated; they block traffic from any source.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**16. Kubernetes Network Policies can control traffic flow at the OSI layer 3 or 4.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**17. Which among the following statements are true without any change made to the default behaviour of network policies in the namespace?**


* [ ]  As soon as a network policy is associated with a POD traffic between all PODs in the namespace is denied
* [ ] As soon as a network policy is associated with a POD all ingress and egress traffic to that POD are denied except allowed by the network policy
* [ ]  As soon as a network policy is associated with a POD all ingress and egress traffic to that POD are allowed except for the the ones blocked by the network policy

**Correct answer:**
* [x] As soon as a network policy is associated with a POD all ingress and egress traffic to that POD are denied except allowed by the network policy

---


## Volume Driver Plugins in Docker

**1. Which statement best describes docker volume plugin?**


* [ ]  Docker Engine volume plugins enables Engine deployments to be integrated with external storage systems such as Amazon EBS
* [ ] The local volume plugin helps to create a volume on Docker host and store its data under the /var/lib/docker/volumes/ directory.
* [ ] ZFS, BTRFS and Device Mapper are some of the supported volume drivers
* [ ]  Volume plugins should not write data to the /var/lib/docker/ directory, including /var/lib/docker/volumes.

**Correct answer:**
* [x]  Docker Engine volume plugins enables Engine deployments to be integrated with external storage systems such as Amazon EBS
* [x] The local volume plugin helps to create a volume on Docker host and store its data under the /var/lib/docker/volumes/ directory.
* [x]  Volume plugins should not write data to the /var/lib/docker/ directory, including /var/lib/docker/volumes.

**2. Which statements best describe Persistent Volume in Kubernetes?**


* [ ]  A PersistentVolume (PV) is a piece of storage in the cluster that has been provisioned by an administrator or dynamically provisioned using Storage Class
* [ ]  It is a resource in the cluster just like a node is a cluster resource.
* [ ]  PVs are volume plugins like Volumes
* [ ] PVs are not volume plugins

**Correct answer:**
* [x]  A PersistentVolume (PV) is a piece of storage in the cluster that has been provisioned by an administrator or dynamically provisioned using Storage Class
* [x]  It is a resource in the cluster just like a node is a cluster resource.
* [x]  PVs are volume plugins like Volumes

**3. What are the types of volumes that Kubernetes supports?**


* [ ] hostPath  
* [ ] configMap  
* [ ] emptyDir 
* [ ]  local

**Correct answer:**
* [x]  local
* [x] emptyDir 
* [x] configMap  
* [x] hostPath  

**4. Which statements best describe hostPath volume type?**


* [ ] A hostPath volume mounts a file or directory from the host node's file system into your Pod.
* [ ] Running a container that needs access to Docker internals, use a hostPath of /var/lib/docker
* [ ] You either need to run your process as root in a privileged Container or modify the file permissions on the host to be able to write to a hostPath
* [ ] The hostPath volume type is initially empty

**Correct answer:**
* [x] You either need to run your process as root in a privileged Container or modify the file permissions on the host to be able to write to a hostPath

**5. What is the command to list the persistent volumes?**


* [ ] kubectl list pv  
* [ ] kubectl get pv  
* [ ] kubectl get persistentvolume
* [ ] kubectl list persistentvolume

**Correct answer:**
* [x] kubectl get pv  
* [x] kubectl get persistentvolume

**6. Which statements best describe emptyDir volume type?**


* [ ]  An emptyDir volume is first created when a Pod is assigned to a node, and still exists after a pod termination.
* [ ]  An emptyDir volume is first created when a Pod is assigned to a node, and exists as long as that Pod is running on that node.
* [ ]  The emptyDir volume is initially empty
* [ ]  When a Pod is removed from a node for any reason, the data in the emptyDir is deleted permanently

**Correct answer:**
* [x]  An emptyDir volume is first created when a Pod is assigned to a node, and exists as long as that Pod is running on that node.
* [x]  The emptyDir volume is initially empty
* [x]  When a Pod is removed from a node for any reason, the data in the emptyDir is deleted permanently

**7. Which of the following is the default volume driver plugin used in Kubernetes?**


* [ ]  BlockBridge  
* [ ] local  
* [ ] DRBD  
* [ ] Flocker

**Correct answer:**
* [x] local  

**8. A Persistent Volume is a cluster-wide pool of storage volumes.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**9. What are the different access modes configurable on a persistent volume?**


* [ ] ReadOnlyMany  
* [ ] ReadWrite
* [ ] ReadWriteMany 
* [ ]  ReadOnly
* [ ] ReadWriteOnce

**Correct answer:**
* [x] ReadOnlyMany  
* [x] ReadWriteMany 
* [x] ReadWriteOnce

**10. What is the status of a volume when it is associated with a claim?**


* [ ]  Available  
* [ ] Bound  
* [ ] Released  
* [ ] Failed

**Correct answer:**
* [x] Bound  

**11. What is the command to list the PersistentVolumeClaim?**


* [ ] kubectl list pvc 
* [ ]  kubectl get pvc
* [ ] kubectl get persistentvolumeclaim
* [ ] kubectl list persistentvolumeclaim

**Correct answer:**
* [x]  kubectl get pvc
* [x] kubectl get persistentvolumeclaim

**12. Once the Persistent Volume Claim is created, you need to manually bind the persistent volumes to claim.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**13. Which statements best describe a PersistentVolumeClaim?**


* [ ]  A PersistentVolumeClaim (PVC) is a request for storage by a user.
* [ ]  A PVC will be automatically bound to a PV on creation when a PV is available
* [ ] Claims can request specific size and access modes
* [ ]  A PVC will not automatically bound to a PV on creation of a PV

**Correct answer:**
* [x]  A PersistentVolumeClaim (PVC) is a request for storage by a user.
* [x]  A PVC will be automatically bound to a PV on creation when a PV is available
* [x] Claims can request specific size and access modes

**14. What is the command to delete the PersistentVolumeClaim?**


* [ ] kubectl delete pvc PV-NAME  
* [ ] kubectl del pvc PV-NAME 
* [ ]  kubectl rm pvc PV-NAME
* [ ] kubectl erase pvc PV-NAME

**Correct answer:**
* [x] kubectl delete pvc PV-NAME  

**15. What is the command to delete the persistent volumes?**


* [ ] kubectl delete pv PV-NAME  
* [ ] kubectl del pv PV-NAME
* [ ] kubectl rm pv PV-NAME
* [ ]  kubectl erase pv PV-NAME

**Correct answer:**
* [x] kubectl delete pv PV-NAME  

**16. What is the status of a volume after it is created but not yet bound to a claim?**


* [ ]  Available  
* [ ] Bound  
* [ ] Released  
* [ ] Failed

**Correct answer:**
* [x]  Available  

**17. A PV of 100 GB is in an available state. A PVC with a requirement of 50 GB storage is created. What would happen if there are no other PVs or PVCs created?**


* [ ]  The PVC would bind to the PV with 100 GB
* [ ]  The PVC will be in a pending state as there is no PV with the same amount of storage

**Correct answer:**
* [x]  The PVC would bind to the PV with 100 GB

**18. What is the kubectl command to list the storage classes in kubectl?**


* [ ] kubectl list sc 
* [ ]  kubectl get sc
* [ ] kubectl get storageclass
* [ ] kubectl list storageclass

**Correct answer:**
* [x]  kubectl get sc
* [x] kubectl get storageclass

**19. What happens to the PV by default when the associated PVC is deleted?**


* [ ]  The PV is deleted automatically.
* [ ] The PV is left as is until it is manually deleted by an administrator
* [ ]  The data in the PV is scrubbed and the PV is made available for other PVCs

**Correct answer:**
* [x] The PV is left as is until it is manually deleted by an administrator

**20. Which statement best describes a Kubernetes Storage Class?**


* [ ] A StorageClass provides a way for administrators to describe the "classes" of storage they offer
* [ ]  Each StorageClass contains the fields provisioner, parameters, and reclaimPolicy.
* [ ] Any user can set the name and other parameters of a class when first creating StorageClass objects
* [ ] The StorageClass objects can use a provisioner that can dynamically provision storage on supported storage providers.

**Correct answer:**
* [x] A StorageClass provides a way for administrators to describe the "classes" of storage they offer
* [x]  Each StorageClass contains the fields provisioner, parameters, and reclaimPolicy.
* [x] The StorageClass objects can use a provisioner that can dynamically provision storage on supported storage providers.

**21. What is the sequence of operations to be followed while configuring a storage class for an application?**


* [ ]  Create a storage class with a provisioner, create a persistent volume with definition using the storage class, create a PVC and then use the PVC in the volumes section in the pod definition file
* [ ]  Create a storage class with a provisioner, create a PVC with the storage class, and then use the PVC in the volumes section in the pod definition file
* [ ] Create a storage class, and use it directly in the volumes section in the pod definition file

**Correct answer:**
* [x]  Create a storage class with a provisioner, create a PVC with the storage class, and then use the PVC in the volumes section in the pod definition file

**22. What is the command to delete the Storage Class?**


* [ ] kubectl delete sc SC-NAME 
* [ ]  kubectl del sc SC-NAME
* [ ]  kubectl delete storageclass SC-NAME
* [ ] kubectl erase sc SC-NAME

**Correct answer:**
* [x] kubectl delete sc SC-NAME 
* [x]  kubectl delete storageclass SC-NAME

---


## Docker UCP Setup

**1. Which of the following are the features of Docker Enterprise Edition?**


* [ ]  Security & Access Control  
* [ ] Universal Control Plane & Trusted Registry
* [ ] Source Code Management
* [ ]  Docker Swarm Service
* [ ]  Kubernetes Service

**Correct answer:**
* [x]  Security & Access Control  
* [x] Universal Control Plane & Trusted Registry
* [x]  Docker Swarm Service

**2. Docker Engine Enterprise is a hardened and secure version of Docker Engine.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**3. …. is the enterprise-grade cluster management platform from Docker. You install it on-premises or in your virtual private cloud, and it helps you manage your Docker cluster and applications through a single interface.**


* [ ] Universal Control Plane (UCP)  
* [ ] Docker Enterprise Edition  
* [ ] Docker Community Edition
* [ ] Docker Trusted Registry (DTR)

**Correct answer:**
* [x] Universal Control Plane (UCP)  

**4. You can download your Docker EE license by navigating to the “My Content” page on Docker Store.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**5. Docker Enterprise cluster can only be provisioned on public cloud platforms.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**6. … is a combination of multiple enterprise grade tool sets which include Docker Engine Enterprise as well as container orchestration and registry tools.**


* [ ]  Universal Control Plane (UCP) 
* [ ]  Docker Enterprise Edition
* [ ]  Docker Community Edition
* [ ]  Docker Trusted Registry (DTR)

**Correct answer:**
* [x]  Docker Enterprise Edition

**7. Which of the below statements are true?**


* [ ]  Docker CE or Docker Community Edition is the open source and free version of Docker.
* [ ]  Docker CE needs a license key (or) file to activate it.
* [ ]  Both Docker Community Edition and Docker Enterprise Edition have the same core features and functions. Docker Enterprise Edition comes with additional support for IT teams to build, share and run business critical applications at scale.
* [ ] The only difference between Docker CE and Docker EE is that Docker EE comes with a dedicated support team

**Correct answer:**
* [x]  Docker CE or Docker Community Edition is the open source and free version of Docker.
* [x]  Both Docker Community Edition and Docker Enterprise Edition have the same core features and functions. Docker Enterprise Edition comes with additional support for IT teams to build, share and run business critical applications at scale.

**8. Which statement best describes Docker Enterprise?**


* [ ] Docker Enterprise provides a consistent and secure end-to-end application pipeline, choice of tools and languages, and globally consistent Kubernetes environments that run in any cloud.
* [ ]  Docker Enterprise enables deploying highly available workloads using Docker Swarm only.
* [ ] Docker Enterprise automates many of the tasks that orchestration requires, like provisioning pods, containers, and cluster resources.
* [ ]  Self-healing components ensure that Docker Enterprise clusters remain highly available.

**Correct answer:**
* [x] Docker Enterprise provides a consistent and secure end-to-end application pipeline, choice of tools and languages, and globally consistent Kubernetes environments that run in any cloud.
* [x] Docker Enterprise automates many of the tasks that orchestration requires, like provisioning pods, containers, and cluster resources.

**9. … is the enterprise-grade image storage solution from Docker. You install it behind your firewall so that you can securely store and manage the Docker images you use in your applications.**


* [ ]  Universal Control Plane (UCP)  
* [ ] Docker Enterprise Edition
* [ ]  Docker Community Edition  
* [ ] Docker Trusted Registry (DTR)

**Correct answer:**
* [x] Docker Trusted Registry (DTR)

**10. Docker EE license subscription includes …. product.**


* [ ] Universal Control Plane (UCP)  
* [ ] Docker Trusted Registry (DTR)
* [ ]  Docker Engine – Enterprise with enterprise-grade support
* [ ] Docker Engine – Community Edition

**Correct answer:**
* [x] Universal Control Plane (UCP)  
* [x] Docker Trusted Registry (DTR)
* [x]  Docker Engine – Enterprise with enterprise-grade support

**11. …. is one of the system requirements for UCP.**


* [ ] The nodes must be Linux Kernel version 3.10 or higher
* [ ] Each node must be configured with a static ip address
* [ ]  User namespaces should not be configured on any node
* [ ]  all nodes must have Docker Engine Enterprise installed

**Correct answer:**
* [x] The nodes must be Linux Kernel version 3.10 or higher
* [x] Each node must be configured with a static ip address
* [x]  User namespaces should not be configured on any node
* [x]  all nodes must have Docker Engine Enterprise installed

**12. User namespaces should not be configured on any node as they are not currently supported in UCP.**


* [ ] True
* [ ] false

**Correct answer:**
* [x] True

**13. DTR is highly available through the use of multiple replicas of all containers and metadata such that if a machine fails, DTR continues to operate and can be repaired.**


* [ ] False
* [ ] True

**Correct answer:**
* [x] True

**14. Which command can be used to verify the installed docker is a CE or EE?**


* [ ] docker container ls  
* [ ] docker version
* [ ] docker --version  
* [ ] None of the above

**Correct answer:**
* [x] docker version

**15. With …, you can manage all of the computing resources you have available, like nodes, volumes, and networks from a centralized place.**


* [ ] Universal Control Plane (UCP)  
* [ ] Docker Enterprise Edition
* [ ]   Docker Community Edition   
* [ ] Docker Trusted Registry (DTR)

**Correct answer:**
* [x] Universal Control Plane (UCP)  

**16. Docker enterprise is the only platform that supports both Docker swarm and Kubernetes on the same cluster.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**17. Which of the following need to be installed in order to provision the Docker EE infrastructure?**


* [ ]  Docker Enterprise Engine should be installed on every node inside your Docker EE setup
* [ ]  A MongoDB database should be pre-provisioned prior to installing Docker EE
* [ ]  Install the Universal Control Plane (UCP)
* [ ]  Install the Docker Trusted Registry (DTR)

**Correct answer:**
* [x]  Docker Enterprise Engine should be installed on every node inside your Docker EE setup
* [x]  Install the Universal Control Plane (UCP)
* [x]  Install the Docker Trusted Registry (DTR)

**18. Docker Engine – Enterprise can only be installed on linux distros.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**19. Which of the following are the major components of Docker Engine – Enterprise?**


* [ ]  A server which is a type of long-running program called a daemon process (the dockerd command).
* [ ]  A REST API which specifies interfaces that programs can use to talk to the daemon and instruct it what to do.
* [ ]  A command line interface (CLI) client (the docker command).

**Correct answer:**
* [x]  A server which is a type of long-running program called a daemon process (the dockerd command).
* [x]  A REST API which specifies interfaces that programs can use to talk to the daemon and instruct it what to do.
* [x]  A command line interface (CLI) client (the docker command).

**20. To install Docker Enterprise, you will need the URL of the Docker Enterprise repository associated with your trial or subscription.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**21. UCP provides a simple and easy to use GUI to manage your applications.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**22. What is the command to show the installed version of “Docker Enterprise” Server and Client?**


* [ ] docker version 
* [ ]  docker --version
* [ ] docker -V 
* [ ]  docker -v

**Correct answer:**
* [x] docker version 

**23. Which command can be used to start the docker ee service on a systemctl configured system?**


* [ ] sudo systemctl start docker-ee
* [ ]  sudo systemctl start docker
* [ ] sudo systemctl docker start
* [ ] sudo systemctl docker-ee start

**Correct answer:**
* [x]  sudo systemctl start docker

**24. Which of the below statements best describe the Universal Control Plane (UCP)?**


* [ ]  UCP manages your Docker cluster and applications through a single interface.
* [ ]  Universal Control Plane (UCP) is the enterprise-grade cluster management solution from Docker.
* [ ]  UCP stores and manages images used by applications
* [ ]  UCP has its own built-in authentication mechanism and integrates with LDAP and AD services.

**Correct answer:**
* [x]  UCP manages your Docker cluster and applications through a single interface.
* [x]  Universal Control Plane (UCP) is the enterprise-grade cluster management solution from Docker.
* [x]  UCP has its own built-in authentication mechanism and integrates with LDAP and AD services.

**25. Which of the following features does UCP provide?**


* [ ]  Centralized Cluster Management  
* [ ] Deploy, manage and monitor workload
* [ ]  Built-in security and access control  
* [ ] Issue Tracker
* [ ]  Source Code Management

**Correct answer:**
* [x]  Centralized Cluster Management  
* [x] Deploy, manage and monitor workload
* [x] Issue Tracker

**26. What are the minimum hardware requirements to install UCP?**


* [ ] 4GB RAM, 2vCPUs and 10GB disk space for the /var partition for manager nodes, 2GB RAM and 500MB disk space for the /var partition for worker nodes
* [ ]  8GB RAM, 2vCPUs and 10GB disk space for the /var partition for manager nodes, 4GB RAM and 500MB disk space for the /var partition for worker nodes
* [ ]  8GB RAM, 2vCPUs and 10GB disk space for the /var/lib/docker partition for manager nodes, 4GB RAM and 500MB disk space for the /var/lib/docker partition for worker nodes
* [ ]  4GB RAM, 2vCPUs and 10GB disk space for the /var/lib/docker partition for manager nodes, 2GB RAM and 500MB disk space for the /var/lib/docker partition for worker nodes

**Correct answer:**
* [x]  8GB RAM, 2vCPUs and 10GB disk space for the /var partition for manager nodes, 4GB RAM and 500MB disk space for the /var partition for worker nodes

**27. What are the recommended hardware requirements to install UCP for manager nodes?**


* [ ] 8GB RAM, 4vCPUs and 25-100GB disk space for the /var partition for manager nodes
* [ ]  8GB RAM, 4vCPUs and 10GB disk space for the /var/lib/docker partition for manager nodes
* [ ] 16GB RAM, 4vCPUs and 25GB-100GB disk space for /var partition for manager nodes
* [ ]  4GB RAM, 2vCPUs and 10GB disk space for the /var/lib/docker partition for manager nodes

**Correct answer:**
* [x] 16GB RAM, 4vCPUs and 25GB-100GB disk space for /var partition for manager nodes

**28. Which of the following are the prerequisites to install UCP for linux?**


* [ ] The nodes must be Linux Kernel version 3.10 or higher.
* [ ]  Each node must be configured with a static ip address.
* [ ] Ensure they are configured with an NTP server to sync time.
* [ ]  User namespaces should be configured on every node.
* [ ]  All nodes must have Docker Engine Enterprise installed.

**Correct answer:**
* [x] The nodes must be Linux Kernel version 3.10 or higher.
* [x]  Each node must be configured with a static ip address.
* [x] Ensure they are configured with an NTP server to sync time.
* [x]  All nodes must have Docker Engine Enterprise installed.

**29. Which of the following are the components deployed by UCP on worker nodes?**


* [ ]  ucp-agent 
* [ ]  ucp-metric 
* [ ]  ucp-proxy 
* [ ]  ucp-auth-api

**Correct answer:**
* [x]  ucp-agent 
* [x]  ucp-proxy 

**30. UCP works with Docker swarm under the hoods.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**31. Which of the following are the components deployed on the manager node by UCP?**


* [ ] ucp-controller 
* [ ]  ucp-metric 
* [ ]  proxy  
* [ ] ucp-auth-api 
* [ ]  agent

**Correct answer:**
* [x] ucp-controller 
* [x]  ucp-metric 
* [x] ucp-auth-api 

**32. Which of the following steps are required to add a worker node to a UCP cluster?**


* [ ]  Make sure Docker EE is up and running and Pull the UCP image from the registry
* [ ] Set the Admin Username and Password for UCP Console
* [ ]  Login to the Browser and provide the downloaded Docker EE License
* [ ] Add more managers and workers as per requirement

**Correct answer:**
* [x]  Make sure Docker EE is up and running and Pull the UCP image from the registry
* [x] Set the Admin Username and Password for UCP Console
* [x]  Login to the Browser and provide the downloaded Docker EE License
* [x] Add more managers and workers as per requirement

**33. When a new node is added to the cluster the ucp-agent is deployed there and it automatically configures the node to work with UCP.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**34. UCP works with Docker swarm under the hoods.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**35. When a new node is added to the cluster the ucp-agent is deployed there and it automatically configures the node to work with UCP.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**36. UCP has its own built-in authentication mechanism and integrates with LDAP and AD services.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**37. When a new node is added to the cluster, an ucp-agent is deployed there.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**38. When UCP is deployed it creates a replicated service called ucpagent.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**39. Which component is responsible to serve the UCP components such as the web ui, the authentication api,metrics server, proxy and data stores used by UCP in the form of containers?**


* [ ]  UCP Agent  
* [ ] Docker Enterprise Edition  
* [ ] Docker Community Edition  
* [ ] Docker Trusted Registry (DTR)

**Correct answer:**
* [x]  UCP Agent  

**40. We can interact with UCP from the GUI only.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**41. One of the prerequisites to install the UCP is make sure that the Docker CE is up and running.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**42. The ucp-agent installs the necessary components on the worker node automatically after a new node joins the cluster.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**43. Which of the following steps are required to add a worker node?**


* [ ] Provision a node and Install Docker enterprise engine on it.
* [ ]  Run the docker swarm join command to join the new node to the cluster.
* [ ]  Deploy an instance of the ucp-agent on the new node.
* [ ]  ucp-agent then installs the necessary components on the worker node.

**Correct answer:**
* [x] Provision a node and Install Docker enterprise engine on it.
* [x]  Run the docker swarm join command to join the new node to the cluster.
* [x]  ucp-agent then installs the necessary components on the worker node.

**44. ucp-agent is configured as a global service.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

---


## Docker Trusted Registry Setup

**1. What is a recommended approach to deploying DTR?**


* [ ] Deploy a single instance of DTR.
* [ ]  Deploy at least 2 instances of DTR to support high availability.
* [ ] Deploy at least 3 instances of DTR to support high availability.

**Correct answer:**
* [x] Deploy at least 3 instances of DTR to support high availability.

**2. Which of the following statements best describe Docker Trusted Registry (DTR)?**


* [ ]  Docker Trusted Registry (DTR) is Mirantis’s enterprise-grade image storage solution.
* [ ]  Images stored on DTR can only be accessed from within the Docker EE Cluster
* [ ]  DTR provides a secure environment on which users can store and manage Docker images.
* [ ] DTR should be installed only on a worker node that is managed by UCP

**Correct answer:**
* [x]  Docker Trusted Registry (DTR) is Mirantis’s enterprise-grade image storage solution.
* [x]  DTR provides a secure environment on which users can store and manage Docker images.
* [x] DTR should be installed only on a worker node that is managed by UCP

**3. Docker Trusted Private Registry stores Docker Images in a highly secure manner with additional features like Image signing, and Image scans etc.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**4. Docker Trusted Registry (DTR) is a containerized application that runs on a Docker Universal Control Plane cluster.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**5. What are the features of Docker Trusted Registry (DTR)?**


* [ ] Built-in Access Control 
* [ ]  Image and Job Management
* [ ]  Automated image builds
* [ ]  Security Scanning  
* [ ] Dockerfile management in SCM
* [ ]  Image Signing

**Correct answer:**
* [x] Built-in Access Control 
* [x]  Image and Job Management
* [x]  Security Scanning  
* [x]  Image Signing

**6. To install DTR, all nodes must _______ and ____________.**


* [ ]  Be a worker node managed by UCP  
* [ ] Be a manager node managed by UCP
* [ ] Have a fixed hostname  
* [ ] Have atleast 200 GB disk space

**Correct answer:**
* [x]  Be a worker node managed by UCP  
* [x] Have a fixed hostname  

**7. What are the production hardware requirements to install DTR?**


* [ ]  4GB RAM, 2vCPUs and 200GB of free disk space.
* [ ]  8GB RAM, 2vCPUs and 200GB of free disk space.
* [ ] 16GB RAM, 4vCPUs and 25GB of free disk space.
* [ ]  8GB RAM, 2vCPUs and 10GB of free disk space.

**Correct answer:**
* [x] 16GB RAM, 4vCPUs and 25GB of free disk space.

**8. DTR is able to reduce the bandwidth used when pulling Docker images by caching images closer to users.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**9. DTR also supports RBAC for access control.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**10. … is a private image registry/repository that can be deployed within your organization to store images securely.**


* [ ] DTR  
* [ ] UCP  
* [ ] UCP Agent  
* [ ] None of the above

**Correct answer:**
* [x] DTR  

**11. DTR can only be installed on-premises, and not on virtual private cloud.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**12. What are the recommended hardware requirements to install DTR?**


* [ ]  16GB RAM, 2vCPUs and 100GB of free disk space.
* [ ]  16GB RAM, 4vCPUs and 25-100GB of free disk space.
* [ ] 8GB RAM, 4vCPUs and 25-100GB of free disk space.
* [ ]  8GB RAM, 2vCPUs and 100GB of free disk space.

**Correct answer:**
* [x]  16GB RAM, 4vCPUs and 25-100GB of free disk space.

**13. DTR has a web user interface that allows authorized users in your organization to browse Docker images and review repository events.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**14. DTR and UCP must be installed on the same node.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**15. What is the type and the name of the network of the DTR deployment?**


* [ ]  overlay/dtr  
* [ ] overlay/dtr-ol  
* [ ] bridge/dtr  
* [ ] bridge/dtr-ol

**Correct answer:**
* [x] overlay/dtr-ol  

**16. Which image is used to deploy the dtr?**


* [ ]  dtr  
* [ ] docker/dtr  
* [ ] ucp  
* [ ] docker/ucp

**Correct answer:**
* [x] docker/dtr  

**17. By default, the docker registry stores the images you push to it in an NFS shared storage.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

---


## Deployment in Docker EE

**1. You can interact with the UCP either through the UCP GUI Console or through the CLI.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**2. What environment variables must be set to allow client to communicate with UCP via CLI?**


* [ ]  DOCKER  
* [ ] DOCKER_HOST  
* [ ] DOCKER_CERT_PATH  
* [ ] DOCKER_PATH

**Correct answer:**
* [x] DOCKER_HOST  
* [x] DOCKER_CERT_PATH  

**3. Which of the following statements are true about deploying workload via GUI on UCP Cluster?**


* [ ]  The Docker swarm section in the UI helps create swam services by specifying the image details, scheduling, network, environment, resource and logging configuration.
* [ ]  UCP only supports deploying workloads on Docker Swarm
* [ ]  UCP only supports deploying workloads on External Kubernetes Clusters
* [ ]  The Kubernetes section in the UI helps create Kubernetes objects such as PODs, services, ingress, controllers such as replicasets, deployments, daemonsets, statefulsets, job or cron jobs as well as service accounts or storage.

**Correct answer:**
* [x]  The Docker swarm section in the UI helps create swam services by specifying the image details, scheduling, network, environment, resource and logging configuration.
* [x]  The Kubernetes section in the UI helps create Kubernetes objects such as PODs, services, ingress, controllers such as replicasets, deployments, daemonsets, statefulsets, job or cron jobs as well as service accounts or storage.

**4. You can interact with the UCP through the CLI.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**5. To authorize access to cluster resources across your organization, which of the following high-level steps must UCP administrators take?**


* [ ] Configure subjects (users, teams, and service accounts).
* [ ]  Define custom roles (or use defaults) by adding permitted operations per type of resource.
* [ ] Configure resource sets of Swarm collections or Kubernetes namespaces.
* [ ]  Create grants by combining subject + role + resource set

**Correct answer:**
* [x] Configure subjects (users, teams, and service accounts).
* [x]  Define custom roles (or use defaults) by adding permitted operations per type of resource.
* [x] Configure resource sets of Swarm collections or Kubernetes namespaces.
* [x]  Create grants by combining subject + role + resource set

**6. What is the command line interface used to interact with UCP from a shell?**


* [ ]  docker-ucp  
* [ ] docker  
* [ ] docker-ee 
* [ ]  docker-ucp-cli

**Correct answer:**
* [x] docker  

**7. Universal Control Plane (UCP), lets you authorize users to view, edit, and use cluster resources by granting role-based permissions against resource sets.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**8. Which of the following statements are true about deploying workload via CLI on UCP Cluster?**


* [ ]  With CLI you may use the docker command line interface to interact with the UCP cluster.
* [ ]  CLI access doesn't require authentication to the UCP Cluster.
* [ ]  CLI access requires authentication to the UCP Cluster.
* [ ] Download the certificate from UCP Console and copy this over to the server from where you’d like to access and extract it to a path.

**Correct answer:**
* [x]  With CLI you may use the docker command line interface to interact with the UCP cluster.
* [x]  CLI access requires authentication to the UCP Cluster.
* [x] Download the certificate from UCP Console and copy this over to the server from where you’d like to access and extract it to a path.

**9. Which of the statements best describes “Subjects” in the Access Control Model?**


* [ ]  A subject represents a user, team, organization
* [ ]  A subject does not represent a service account.
* [ ]  A subject can be granted a role that defines permitted operations against one or more resource sets.
* [ ]  A subject represents a service account.

**Correct answer:**
* [x]  A subject represents a user, team, organization
* [x]  A subject can be granted a role that defines permitted operations against one or more resource sets.
* [x]  A subject represents a service account.

**10. Which of the statements best describe “Roles” in the Access Control Model?**


* [ ]  Roles define what operations are allowed on a resource.
* [ ]  A role is a set of permitted operations against a type of resource, like a container or volume, which can only be assigned to individual users.
* [ ]  Most organizations use multiple roles to fine-tune appropriate access to users and teams.
* [ ]  All of the above

**Correct answer:**
* [x]  Roles define what operations are allowed on a resource.
* [x]  Most organizations use multiple roles to fine-tune appropriate access to users and teams.

**11. Which of the statements best describe “Grants” in the Access Control Model?**


* [ ]  Grants define which users can access what resources in what way.
* [ ]  A grant is made up of a role and a resource set.
* [ ]  A grant is made up of a subject, a role, and a resource set.
* [ ]  Grants are effectively Access Control Lists (ACLs) which provide comprehensive access policies for an entire organization when grouped together.

**Correct answer:**
* [x]  Grants define which users can access what resources in what way.
* [x]  A grant is made up of a subject, a role, and a resource set.
* [x]  Grants are effectively Access Control Lists (ACLs) which provide comprehensive access policies for an entire organization when grouped together.

**12. Which of the following is a common workflow for RBAC in Docker EE is**


* [ ]  Create users, teams and organization 
* [ ]  Create custom roles with set of permissions
* [ ]  Combine resources sets using collection

**Correct answer:**
* [x]  Create users, teams and organization 
* [x]  Create custom roles with set of permissions
* [x]  Combine resources sets using collection

**13. Which of the statements best describe “Resource sets” in Access Control Model?**


* [ ]  A collection of resources in Docker Swarm 
* [ ]  A collection in Kubernetes
* [ ]  A namespace in Kubernetes
* [ ] A namespace in Docker Swarm

**Correct answer:**
* [x]  A collection of resources in Docker Swarm 
* [x]  A namespace in Kubernetes

**14. Only an administrator can manage grants, subjects, roles, and access to resources.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**15. A group of teams that share a specific set of permissions forms a collection.**


* [ ] False
* [ ] True

**Correct answer:**
* [x] False

**16. Docker Enterprise Edition provides … , where in we can create users and group them into teams which are nothing but group of users and tie them up with an organization.**


* [ ]  DTR  
* [ ] UCP  
* [ ] UCP Agent  
* [ ] RBAC

**Correct answer:**
* [x] RBAC

**17. The … allows you to authorize a remote Docker engine to a specific user account managed in Docker EE, absorbing all associated RBAC controls in the process**


* [ ]  DTR  
* [ ] UCP  
* [ ] Client bundle  
* [ ] RBAC

**Correct answer:**
* [x] Client bundle  

**18. Using …. in Docker EE we can control who can access and make changes to your cluster and applications.**


* [ ] DTR 
* [ ]  UCP
* [ ]   Client bundle  
* [ ] RBAC

**Correct answer:**
* [x] RBAC

**19. A client bundle is a group of certificates downloadable directly from the Docker Trusted Registry (DTR) user interface within the admin section for “My Profile”**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

---


## Research Questions Docker Trusted Registry Operations

**1. If we do not specify a registry information then it is assumed to be the default registry at docker hub at the address docker.io.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**2. What is the command to pull the docker repository owned by an organization?**


* [ ] docker get DTR-DOMAIN-NAME/ORG/REPOSITORY:TAG
* [ ]  docker pull DTR-DOMAIN-NAME/ORG/REPOSITORY:TAG
* [ ] docker download DTR-DOMAIN-NAME/ORG/REPOSITORY:TAG
* [ ] docker fetch DTR-DOMAIN-NAME/ORG/REPOSITORY:TAG

**Correct answer:**
* [x]  docker pull DTR-DOMAIN-NAME/ORG/REPOSITORY:TAG

**3. When a user creates a repository, by default other users will also have permissions to make changes to the repository.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**4. DTR only supports creating private repositories.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**5. When using the built-in authentication mechanism, you can create users to grant them fine-grained permissions. Which of the following statements best describes managing users in DTR?**


* [ ]  Users are shared across UCP and DTR.
* [ ]  When you create a new user in UCP, that user becomes available in DTR and vice versa.
* [ ]  Check the Trusted Registry admin option, if you want to grant permissions for the user to be a UCP and DTR administrator.
* [ ]  Users are not shared across UCP and DTR

**Correct answer:**
* [x]  Users are shared across UCP and DTR.
* [x]  When you create a new user in UCP, that user becomes available in DTR and vice versa.
* [x]  Check the Trusted Registry admin option, if you want to grant permissions for the user to be a UCP and DTR administrator.

**6. By default, DTR has one organization called 'docker-datacenter', that is shared between DTR and UCP.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**7. By default, when pushing an image to DTR, it automatically creates a new repository if one does not already exist by that name.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**8. Which of the following is the docker image addressing convention?**


* [ ]  Registry-Address/Image-or-Repository-Name/User-Or-Account-Name
* [ ] Registry-Address/User-Or-Account-Name/Image-or-Repository-Name
* [ ] User-Or-Account-Name/Image-or-Repository-Name/Registry-Address
* [ ] Image-or-Repository-Name/User-Or-Account-Name/Registry-Address

**Correct answer:**
* [x] Registry-Address/User-Or-Account-Name/Image-or-Repository-Name

**9. You cannot configure DTR to allow pushing to repositories that don’t exist yet.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**10. We can use the CLI to enable pushing to repositories that don’t exist yet.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

---


## Design for authentication and authorization

**1. What should Vendetta Corp do to ensure employees have the correct permissions for their job role?**


* [ ] Require access review 
* [ ] Review role based access control permissions manually
* [ ] Create Azure Policy
* [ ] Create conditional access policy

**Correct answer:**
* [x] Require access review 

**Explaination**: An access review would give managers an opportunity to validate the employees access.


**2. What should Vendetta Corp do to give access to the partner developers?**


* [ ] Use Azure AD Connect to synchronize identities of developers 
* [ ] Setup Azure B2C tenant for developers to sign in
* [ ] Invite developers to Vendetta’s directory
* [ ] Create cloud identities for the developers

**Correct answer:**
* [x] Invite developers to Vendetta’s directory

**Explaination**: In Business-to-Business scenarios guest user accounts are created by inviting them. You can then apply the appropriate permissions once the account is created.


**3. What solution would be best for the user sign-in attempts requirement?**


* [ ] Create user risk policy 
* [ ] Create conditional access
* [ ] Create sign-in risk policy
* [ ] Create MFA registration policy

**Correct answer:**
* [x] Create sign-in risk policy

**Explaination**: A sign-in risk policy can identify anonymous IP and atypical locations. Secondary multifactor authentication can then be required.


**4. How can Vendetta Corp ensure that employees at the company's retail stores can access company applications only from approved tablet devices?**


* [ ] SSO 
* [ ]  MFA
* [ ]  Identity Protection
* [ ] Conditional Access

**Correct answer:**
* [x] Conditional Access

**Explaination**: Conditional Access enables you to require users to access your applications only from approved, or managed, devices.


---


## Design a governance solution

**1. Which governance tool should Vendetta Corp use for the ISO 27001 requirements?**


* [ ] Azure Blueprints
* [ ]  Azure Management Groups 
* [ ] Azure RBAC
* [ ]  Azure Resource Groups

**Correct answer:**
* [x] Azure Blueprints

**Explaination**: Azure blueprints will deploy all the artifacts for ISO 27001 compliance.


**2. How can Vendetta Corp ensure applications use geo-redundancy to create highly available storage applications?**


* [ ] Add a resource tag to each storage account for geo-redundant storage
* [ ] Add a resource group to contain all storage accounts that require geo-redundancy
* [ ] Add a subscription to contain all geo-redundant storage accounts
* [ ] Create a policy that requires geo-redundant storage

**Correct answer:**
* [x] Create a policy that requires geo-redundant storage

**Explaination**: An Azure policy can enforce different rules over your resource configurations


**3. How can Vendetta Corp to ensure policies are implemented across multiple subscriptions?**


* [ ] Add a resource group and apply the relevant policies
* [ ] Add a resource tag that includes the policy definition
* [ ] Create a management group which includes all the required subscription and apply policies
* [ ] Create a policy initiative

**Correct answer:**
* [x] Create a management group which includes all the required subscription and apply policies

**Explaination**: A management group could include all the subscriptions. Then a policy could be scoped to the management group and applied to all the subscriptions.


**4. How can Vendetta Corp report all the costs associated with a new product?**


* [ ] Create a resource tag to identify which is used by resources for new product
* [ ] Move all resources to a single resource group
* [ ] Download the cost data and filter
* [ ] Create an Azure Policy to calculate the cost

**Correct answer:**
* [x] Download the cost data and filter

---


## Design a network architecture

**1. Which security solution is recommended?**


* [ ] Azure Bastion 
* [ ] Azure NSG 
* [ ] Azure WAF 
* [ ] Azure Firewall

**Correct answer:**
* [x] Azure Firewall

**Explaination**: Azure Firewall can filter HTTP(S) traffic from Azure to on-premises and outbound to the internet


**2. What network topology is recommended based on the architecture requirement?**


* [ ] Mesh network 
* [ ] Virtual WAN network topology
* [ ] Traditional hub-spoke architecture
* [ ] Single virtual network

**Correct answer:**
* [x] Virtual WAN network topology

**Explaination**: A Virtual WAN topology meets the requirements for deploying resources across several Azure regions and enables global connectivity between VNets in these Azure regions and multiple on-premises locations.

**3. Which load-balancing solution is recommended?**


* [ ] Azure Load Balancer 
* [ ] Azure Application Gateway 
* [ ] Azure Front Door 
* [ ] Azure Traffic Manager

**Correct answer:**
* [x] Azure Application Gateway 

**Explaination**: Application Gateway can be used for both internal and external application making it the best choice for a web application that is not internet facing.


---


## Design a compute solution

**1. Which type of virtual machine is best for the data center migration requirement?**


* [ ] General purpose 
* [ ] Compute optimized 
* [ ] Memory optimized 
* [ ] GPU enabled VMs

**Correct answer:**
* [x] Memory optimized 

**Explaination**: Memory optimized VMs are designed to have a high memory-to-CPU ratio. Great for relational database servers, medium to large caches, and in-memory analytics.


**2. What compute solution is best for hosting the company’s data processing app?**


* [ ] Azure Virtual Machines 
* [ ] Azure Container Instances
* [ ] Azure Functions 
* [ ] Azure Logic Apps

**Correct answer:**
* [x] Azure Container Instances

**Explaination**: This is an ideal use for containers. The company will achieve significant cost saving through per-second billing.


**3. Which compute option is best to support the company’s real-time inventory tracking requirement?**


* [ ] Azure VMs 
* [ ] Azure Batch 
* [ ] Azure Logic Apps 
* [ ] Azure Functions

**Correct answer:**
* [x] Azure Functions

**Explaination**: Because the Vendetta Corp developers’ team has already written the logic in C#, it would make sense to copy the relevant C# code from the Windows service and port it to an Azure function. The developers would bind the function to trigger each time a new message appears on a specific queue.


---


## Design a nonrelational data storage solution

**1. What type of storage should Vendetta Corp use for their photos and videos?**


* [ ] Blob Storage
* [ ]  File Storage 
* [ ] Disk Storage 
* [ ] Queue Storage

**Correct answer:**
* [x] Blob Storage

**Explaination**: Blob storage is best for their photos


**2. What is the best way to provide the developer access to the ecommerce HTML files?**


* [ ] Enable secure transfer 
* [ ] Enable firewall policies
* [ ] Enable immutable policies
* [ ] Share access signatures

**Correct answer:**
* [x] Share access signatures

**Explaination**: Shared access signatures provide secure delegated access. This functionality can be used to define permissions and how long access is allowed.


**3. Which access tier should be used for the older versions of the product catalog?**


* [ ] Hot 
* [ ] Cool 
* [ ] Archive
* [ ]  Premium

**Correct answer:**
* [x] Cool 

**Explaination**: The cool access tier is for content that would not be viewed frequently but must be available immediately if accessed.


**4. What is the best way for Vendetta Corp to protect their warranty information?**


* [ ] Legal hold policy 
* [ ] Time based retention policy
* [ ]  Private endpoint 
* [ ] Service endpoint

**Correct answer:**
* [x] Time based retention policy

**Explaination**: With a time-based retention policy, users can set policies to store data for a specified interval. When a time-based retention policy is in place, objects can be created and read, but not modified or deleted.

---


## Design a relational data storage solution

**1. Which Azure SQL deployment option will be easiest to use for the Inventory application?**


* [ ] SQL Server on VM
* [ ] Azure SQL Managed Instance
* [ ] Azure SQL Database – Single Database
* [ ] Azure SQL Database – Elastic Pool

**Correct answer:**
* [x] SQL Server on VM

**Explaination**: SQL Server in an Azure VM allows for Windows authentication

**2. Which Azure SQL deployment option should be used for the HR application?**


* [ ] SQL Server on VM 
* [ ] Azure SQL Managed Instance
* [ ] Azure SQL Database – Single Database
* [ ] Azure SQL Database – Elastic Pool

**Correct answer:**
* [x] Azure SQL Managed Instance

**Explaination**: Azure SQL Managed Instance is the only PaaS service that supports instance-scoped features like CLR and Service Broker.


**3. What database service tier should be selected for the databases being migrated?**


* [ ] Standard 
* [ ] General Purpose 
* [ ] Business Critical 
* [ ] Hyperscale

**Correct answer:**
* [x] Hyperscale

**Explaination**: Hyper scale can support up to 100 TB


---


## Design a data integration solution

**1. Based on the manufacturing data requirement what solution meets the requirements?**


* [ ] Azure Data Factory 
* [ ] Azure Databricks
* [ ]  Azure Data Lake 
* [ ] Azure HDInsight

**Correct answer:**
* [x] Azure Data Factory 

**Explaination**: Azure Data Factory can connect different data sources and run stored procedures on the data.


**2. Based on the historical company data requirements what solution meets the requirements?**


* [ ] Hot 
* [ ] Cold 
* [ ] Archive 
* [ ] Warm

**Correct answer:**
* [x] Cold 

**Explaination**: Cold storage is the appropriate solution for data that is rarely used.


**3. Based on the real time data requirements what solution provides real-time data ingestion and storage of multiple data sources?**


* [ ] Azure Data Factory 
* [ ] Azure Databricks 
* [ ] Azure Data Lake 
* [ ] Azure HDInsight

**Correct answer:**
* [x] Azure Data Lake 

**Explaination**: Azure Data Lake can ingest real-time data directly from multiple sources.


---


## Design a migration solution

**1. What product would be best to migrate the web apps to Azure?**


* [ ] Azure App Service Migration Assistant 
* [ ] Azure Resource Mover
* [ ] Database Migration Services
* [ ] Database Migration Protection

**Correct answer:**
* [x] Azure App Service Migration Assistant 

**Explaination**: The Azure App Service Migration Assistant will help assess and migrate your web apps


**2. What product would be best to estimate monthly costs before migrating?**


* [ ] Azure Cost Management 
* [ ] Service Map
* [ ] Azure TCO Calculator
* [ ] Azure Advisor

**Correct answer:**
* [x] Azure TCO Calculator

**Explaination**: The Azure TCO Calculator provides an estimate of monthly running costs in Azure, which enables a comparison with on-premises costs.


**3. What product would be best to migrate the on-premises SQL Server databases?**


* [ ] Azure Resource Mover 
* [ ] Server Migration 
* [ ] Database Migration Service
* [ ] Azure Storage Migration Service

**Correct answer:**
* [x] Database Migration Service

**Explaination**: The Data Migration Assistant helps detects compatibility issues that can impact database functionality in a new version of SQL Server or Azure SQL Database.


**4. What product would be best for the large data transfer requirement?**


* [ ] Azure Import/Export Service 
* [ ] Azure Data Box
* [ ] Azure Migrate
* [ ] Azure Storage Explorer

**Correct answer:**
* [x] Azure Data Box

**Explaination**: For Azure Data Box Microsoft provides the hardware appliance and handles the shipping.


**5. What product would be best for the migrating to new hardware requirements?**


* [ ] AzCopy 
* [ ] Azure Migrate
* [ ] Azure Storage Migration Service
* [ ] Azure Import/Export

**Correct answer:**
* [x] Azure Storage Migration Service

**Explaination**: The Storage Migration service is used if you have one or more servers that you want to migrate to newer hardware or virtual machines. The product provides an inventory and then can rapidly transfer files.


---


## Design a business continuity solution

**1. What backup solution is best for the on-premises virtual machines?**


* [ ] Geo-replication 
* [ ] Failover groups
* [ ] Azure Site Recovery
* [ ] Azure Backup

**Correct answer:**
* [x] Azure Backup

**Explaination**: Azure Backup can protect on-premises virtual machines



**2. Which storage redundancy option is best for the unstructured data?**


* [ ] LRS 
* [ ] ZRS 
* [ ] RA-GZRS

**Correct answer:**
* [x] RA-GZRS

**Explaination**: RA-GZRS provides read access in a secondary region if the primary region becomes unavailable.



**3. To address the company’s concern with accidental data deletion, which of these solutions is best?**


* [ ] Disk caching 
* [ ] Resource lock to storage account
* [ ] Soft delete
* [ ] Blob backup

**Correct answer:**
* [x] Soft delete

**Explaination**: With container or blob soft delete you can specify a retention period. The data is retained during the retention period and can be recovered.


**4. What replication option would be best for the Azure virtual machine backups?**


* [ ] Failover groups 
* [ ] Azure Site Recovery 
* [ ] Azure Backup
* [ ] Geo-replication

**Correct answer:**
* [x] Azure Site Recovery 

**Explaination**: Azure Site Recovery is designed to provide continuous replication to a secondary region.


---


## Design an app architecture solution

**1. Which of the following solutions should be used for the update management requirement?**


* [ ] ARM templates 
* [ ] Azure Automation 
* [ ] Bicep
* [ ] Terraform

**Correct answer:**
* [x] Azure Automation 

**Explaination**: Azure Automation gives you complete control of update management.


**2. Which of the following solutions should be used for the event handling requirement?**


* [ ] Event Hub 
* [ ] Event Grid 
* [ ] IoT Hub Service 
* [ ] Bus queues

**Correct answer:**
* [x] Event Hub 

**Explaination**: Azure Event Hub can handle millions of events with low latency. Azure Event Hub can stream events to blob storage.


**3. Which the following solutions should be used for the transaction processing requirement?**


* [ ] Queues 
* [ ] Service Bus queues 
* [ ] Event Hub 
* [ ] Service Bus topics

**Correct answer:**
* [x] Service Bus queues 

**Explaination**: Azure Service Bus queues provide advanced message handing. For example, the ability to group messages into a transaction.


---


## Design a logging and monitoring solution

**1. Which Log Analytics Workspace deployment model best supports the company’s needs to host all logs in a single location?**


* [ ] Centralized 
* [ ] Hybrid 
* [ ] Decentralized 
* [ ] Decoupled

**Correct answer:**
* [x] Centralized 

**Explaination**: The centralize model meets their needs


**2. What solution should be used to log user sign-in activity?**


* [ ] VM Insights 
* [ ] Azure AD Audit logs
* [ ] Activity log 
* [ ] Azure Alerts

**Correct answer:**
* [x] Azure AD Audit logs

**Explaination**: Azure Active Directory Audit logs contains the history of sign-in activity and audit trail of changes made within a particular tenant


**3. What monitoring tool should be used to measure user experience and analyze users' behavior for all external facing applications?**


* [ ] Azure Container Insights
* [ ]  VM Insights
* [ ] Application Gateway Insights
* [ ] Application Insights

**Correct answer:**
* [x] Application Insights

**Explaination**: Application Insights can measure user experience and analyze users' behavior


---


## Mock Exam Quiz 1

**1. You have an Azure subscription. The subscription has a blob container that contains multiple blobs. Five users in the Marketing department of your company plan to access the blobs during the month of April. You need to recommend a solution to enable access to the blobs during the month of April only.   Which security solution should you include in the recommendation?**


* [ ] Shared Access Signature 
* [ ] Access keys
* [ ] Multi-factor authentication
* [ ] Managed identities

**Correct answer:**
* [x] Shared Access Signature 

**Explaination**: Shared Access Signatures (SAS) helps you to provide fine grained access by defining the permissions, start date, end date, protocol, IP address etc.


**Documentation Link**: https://learn.microsoft.com/en-us/azure/storage/common/storage-sas-overview

**2. You have an Azure Active Directory (Azure AD) tenant named vendetta.com that has a security group named ‘Application Developers’. Group has an assigned permissions with 25 users including 8 guest users. You need to recommend a solution for evaluating the membership of the group.  The solution must meet the following requirements:  • The evaluation must be repeated automatically every three months  • Every member must be able to report whether they need to be in the group  • Users who report that they do not need to be in group must be removed from group automatically  • Users who do not report whether they need to be in the group must be removed from the group automatically.  What is the recommended solution for this?**


* [ ] Conditional Access Policies 
* [ ] Access Reviews
* [ ] Privileged Identity Management
* [ ] Change the group membership to dynamic

**Correct answer:**
* [x] Access Reviews

**Explaination**: Access reviews in Azure Active Directory (Azure AD), enable organizations to efficiently manage group memberships, access to enterprise applications, and role assignments. 

**Documentation Link**: https://learn.microsoft.com/en-us/azure/active-directory/governance/access-reviews-overview

**3. You have an Azure subscription that contains a custom application named Application was developed by an external company named Vendetta Corp were assigned role-based access control (RBAC) permissions to the application components. All users are licensed for the Microsoft 365 E5 plan. You need to recommend a solution to verify whether the Vendetta developers still require permissions to Application1. The solution must the following requirements. * To the manager of the developers, send a monthly email message that lists the access permissions to Application1. * If the manager does not verify access permission, automatically revoke that permission. * Minimize development effort. What should you recommend?**


* [ ] Enable advanced auditing in Azure AD
* [ ] Send the role assignments information to Log Analytics and review from there.
* [ ] Enable user sign-in risk policy in Identity Protection
* [ ] Create access review in Azure AD

**Correct answer:**
* [x] Create access review in Azure AD

**Explaination**: Access reviews helps in reviewing user role assignments and revoke if not needed.


**Documentation Link**: https://learn.microsoft.com/en-us/azure/active-directory/governance/access-reviews-overview

**4. You are designing a large Azure environment that will contain many subscriptions. You plan to use Azure Policy as part of a governance solution. To which three scopes can you assign Azure Policy definitions? Select all that apply.**


* [ ] Management groups 
* [ ] Subscriptions
* [ ]  Azure AD tenant
* [ ] Resource Groups
* [ ] Resources

**Correct answer:**
* [x] Management groups 
* [x] Subscriptions
* [x] Resource Groups

**Explaination**: Azure Policy can be assigned to Management Groups, Subscriptions and Resource Groups.


**5. You need to recommend a solution to generate a monthly report of all the new Azure Resource Manager resource deployment in your subscription. What should you include in the recommendation?**


* [ ] Azure Monitor Workbooks 
* [ ] Azure Monitor Metrics
* [ ] Activity Logs
* [ ] VM Insights

**Correct answer:**
* [x] Activity Logs

**Explaination**: The Azure Monitor activity log is a platform log in Azure that provides insight into subscription-level events. The activity log includes information like when a resource is modified, or a virtual machine is started. You can view the activity log in the Azure portal or retrieve entries with PowerShell and the Azure CLI. 


**Documentation Link**: https://learn.microsoft.com/en-us/azure/azure-monitor/essentials/activity-log?tabs=powershell

**6. You have an Azure Active Directory (Azure AD) tenant that syncs with an on-premises Active Directory domain. You have an internal web app named WebApp1 that is hosted on-premises. WebApp1 uses Integrated Windows authentication. Some users work remotely and do not have VPN access to the on-premises network. You need to provide the remote users with single sign-on (SSO) access to WebApp1. You already have Conditional Access setup for remote access from internet.  What is the other service that you need to use?**


* [ ] Privileged Identity Management
* [ ] Azure AD Enterprise Applications
* [ ] Azure AD Application Proxy
* [ ] Azure Application Gateway

**Correct answer:**
* [x] Azure AD Application Proxy

**Explaination**: Azure Active Directory's Application Proxy provides secure remote access to on-premises web applications. After a single sign-on to Azure AD, users can access both cloud and on-premises applications through an external URL or an internal application portal. 

**Documentation Link**: https://learn.microsoft.com/en-us/azure/active-directory/app-proxy/application-proxy

**7. You have an Azure subscription that contains an Azure Blob storage account named blobstore1. You have an on-premises file server named FileServer1 that runs Windows Server 2016. FileServer1 stores 500 GB of organizational files. You need to store a copy of the organization’s files from FileServer1 in blobStore1.   Which two possible Azure services achieve this goal?**


* [ ] Azure Power BI
* [ ]  Azure Data Gateway
* [ ] Azure Data Factory
* [ ] Azure Import/Export 

**Correct answer:**
* [x] Azure Data Factory
* [x] Azure Import/Export 

**Explaination**: Import/Export service can be used to export files from on-premises to blob storage or file storage. Similarly, by connecting Azure Data Factory with the on-premises file server and using pipelines, we can retrieve the data and copy to blob storage.


**Documentation Link**: https://learn.microsoft.com/en-us/azure/import-export/storage-import-export-service https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-blob-storage?tabs=data-factory

**8. You have 100 servers that run Windows Server 2012 R2 and host Microsoft SQL Server 2012 R2 instances. The instances host databases that have the following characteristics:  The largest database is currently 3 TB, none of the databases will ever exceed 4 TB.  Stored procedures are implemented by using CLR.  You plan to move all the data from SQL Server to Azure. You need to recommend an Azure service to host the databases. The solution must meet the following requirements:  Whenever possible, minimize management overhead for the migrated databases  Ensure that users can authenticate by using their Active Directory credentials.  What should you include in the recommendation?**


* [ ] Azure SQL on VM 
* [ ] Azure SQL Databases
* [ ] Azure SQL Elastic Pools
* [ ] Azure SQL Managed Instances

**Correct answer:**
* [x] Azure SQL Managed Instances

**Explaination**: For instance, scope features (stored procedures using CLR for example) and easy migration, you can go for Azure SQL Managed Instance. Though, SQL Database also a PaaS solution like SQL Managed Instance, it doesn’t support CLR. 


**Documentation Link**: https://learn.microsoft.com/en-us/azure/azure-sql/database/features-comparison?view=azuresql

**9. An application will host video files that range from 50 MB to 12 GB. The application will use certificate-based authentication and will be available to users on the internet. You need to recommend a storage option for the video files. The solution must provide the fastest read performance and must minimize storage costs. What should you recommend?**


* [ ] Azure Cosmos DB 
* [ ] Azure Data Lake Storage Gen2
* [ ] Azure File Storage
* [ ] Azure Blob Storage

**Correct answer:**
* [x] Azure Blob Storage

**Explaination**: For storing unstructured data like videos, music, and documents, Azure blob storage would be the best choice. The stored data can be accessed anywhere over HTTP or HTTPS connection making it ideal for sharing content across multiple users. The maximum blob is 4.77 TB and our application file size ranges from 50 MB to 12 GB


**Documentation Link**: https://learn.microsoft.com/en-us/azure/storage/blobs/storage-blobs-introduction

**10. You have an Azure subscription that contains two applications named WebApp1 and WebApp2. WebApp1 is a sales processing application. When a transaction in WebApp1 requires shipping, a message is added to an Azure Storage account queue, and then WebApp2 listens to the queue for relevant transactions. In the future, additional applications will be added that will process some of the shipping requests based on the specific details of the transactions. You need to recommend a replacement for the storage account queue to ensure that each additional application will be able to read the relevant transactions. What should you recommend?**


* [ ] Azure Cosmos DB 
* [ ] Azure Service Bus queue
* [ ] Azure Service Bus topics
* [ ] Azure Event Grid

**Correct answer:**
* [x] Azure Service Bus topics

**Explaination**: Queues are for processing of message by a single consumer. If you want the same message to be processed by multiple consumers, then you need to use the publish-subscribe pattern. So, the right service for this is Service Bus topics.


**Documentation Link**: https://learn.microsoft.com/en-us/azure/service-bus-messaging/service-bus-queues-topics-subscriptions#topics-and-subscriptions

**11. You are designing a SQL database solution. The solution will include 15 databases that will be 20 GB each and have varying usage patterns. You need to recommend a database platform to host the databases. The solution must meet the following requirements:  The compute resources allocated to the databases must scale dynamically.  The solution must meet an SLA of 99.99% uptime. The solution must have reserved capacity.  Compute charges must be minimized.  What should you include in the recommendation?**


* [ ] Host all 10 databases in a single SQL Database
* [ ] Create 10 instances of SQL DB serverless
* [ ] Create an elastic pool with 10 databases
* [ ] Create Azure SQL VM on VM with Always on Availability Group and add 10 databases

**Correct answer:**
* [x] Create an elastic pool with 10 databases

**Explaination**: Azure SQL Database elastic pools are a simple, cost-effective solution for managing and scaling multiple databases that have varying and unpredictable usage demands with a guaranteed SLA of 99.99%. The databases in an elastic pool are on a single server and share a set number of resources at a set price. Elastic pools in Azure SQL Database enable SaaS developers to optimize the price performance for a group of databases within a prescribed budget while delivering performance elasticity for each database. Since we need dedicated reserved capacity, we cannot go for serverless. 


**Documentation Link**: https://learn.microsoft.com/en-us/azure/azure-sql/database/elastic-pool-overview?view=azuresql

**12. You have SQL Server on an Azure virtual machine. The databases are written every day at 4:00 AM as part of scheduled task. You need to recommend a disaster recovery solution for the data. The solution must meet the following requirements:  Provide the ability to recover in the event of a regional outage.  Support a recovery time objective (RTO) of 15 minutes.  Support a recovery point objective (RPO) of 24 hours.  Minimize costs.  What should you include in the recommendation?**


* [ ] Azure VM Backup 
* [ ] Azure Availability Set
* [ ] SQL Always-on Availability Group
* [ ] Azure Site Recovery

**Correct answer:**
* [x] Azure Site Recovery

**Explaination**: As the SQL database is using SQL on VM offering, we don’t have geo-replication like the PaaS databases. Since this is hosted on a VM, we can use Azure Site Recovery which offers RPO and RTO values matching the requirements.


**Documentation Link**: https://learn.microsoft.com/en-us/azure/site-recovery/site-recovery-sql

**13. You are designing an application that will aggregate content for users. You need to recommend a database solution for the application. The solution must meet the following requirements: Support SQL commands.  Support multi-region writes.  Guarantee low latency read operations.  What should you include in the recommendation?**


* [ ] Azure Cosmos DB SQL API
* [ ] Azure SQL Database with geo-replication
* [ ] Azure SQL Managed Instance
* [ ] Azure Data Explorer

**Correct answer:**
* [x] Azure Cosmos DB SQL API

**Explaination**: Azure Cosmos DB supports SQL API where developers can use SQL-like commands to query the data. It also supports multi-region read and write operation with guaranteed low latency operations. SQL Database geo-replication doesn’t allow multi-region write operation; it simply replicates the data to the secondary region once it’s committed in the primary region.


**Documentation Link**: https://learn.microsoft.com/en-us/azure/cosmos-db/introduction

**14. You plan to move a web application named WebApp1 from an on-premises data center to Azure. WebApp1 depends on a custom COM component that is installed on the host server. You need to recommend a solution to host WebApp1 in Azure. The solution must meet the following requirements:  App1 must be available to users if an Azure data center becomes unavailable. Costs must be minimized.**


* [ ] Deploy the app to App Services in two regions and use Azure Traffic Manager
* [ ] Deploy the app to Virtual Machine Scale sets in two regions and use Load Balancer
* [ ] Deploy the app to Virtual Machine Scale sets in two regions and use Azure Traffic Manager
* [ ] Deploy the app in Virtual Machine Scale sets that uses Azure Availability zones and use Azure Load Balancer

**Correct answer:**
* [x] Deploy the app in Virtual Machine Scale sets that uses Azure Availability zones and use Azure Load Balancer

**Explaination**: Since the app requires custom COM component to be installed on the host server, we cannot use PaaS solutions and should focus on IaaS solution like Virtual Machine Scale Set. To minimize the cost and to ensure app availability during datacenter unavailability, we can go for availability zones with load balancing done using Azure Load Balancer.


**15. You need to deploy resources to host a stateless web app in an Azure subscription. The solution must meet the following requirements: Provide access to the full .NET framework.  Provide redundancy if an Azure region fails.  Grant administrators access to the operating system to install custom application dependencies. Which of the following combinations should you use to meet the requirements?**


* [ ] Use Azure VMs and Application Gateway
* [ ] Use Azure App Services and Traffic Manager
* [ ] Use Azure VMs and Traffic Manager
* [ ] Use Azure VMs and Basic Load Balancer

**Correct answer:**
* [x] Use Azure App Services and Traffic Manager

**Explaination**: As the administrators requires access to the operating system to install custom application dependencies, the application can be hosted on VMs only. Since we need regional redundancy, we need to use Azure Traffic Manager.


**Documentation Link**: https://learn.microsoft.com/en-us/azure/traffic-manager/traffic-manager-monitoring

**16. You are planning an Azure IoT Hub solution that will include 70,000 IoT devices. Each device will stream data, including temperature, device ID, and time data. Approximately 70,000 records will be written every second. The data will be visualized in near real time. You need to recommend a service to store and query the data. Which two services can you recommend?**


* [ ] Azure Data Explorer 
* [ ] Azure Table Storage 
* [ ] Azure Cosmos NoSQL
* [ ] Azure Event Hub

**Correct answer:**
* [x] Azure Data Explorer 
* [x] Azure Cosmos NoSQL

**Explaination**: Event Hubs are used for data ingestion, they cannot be used for storing or querying data. The data can be stored in the Cosmos DB due to high data ingestion and throughput requirement. Once the data is stored it can be analyzed or queried using Azure Data Explorer.


**Documentation Link**: https://learn.microsoft.com/en-us/azure/architecture/solution-ideas/articles/iot-using-cosmos-db https://learn.microsoft.com/en-us/azure/data-explorer/time-series-analysis

**17. You have an Azure subscription that contains a Basic Azure virtual WAN named VWAN1 and VWAN2 and they are deployed in East US and West US respectively. You have an ExpressRoute circuit in the East US region. You need to create an ExpressRoute association to VWAN1. What should you do first?**


* [ ] Create ExpressRoute Gateway in East US network
* [ ] Create a new virtual network in East US 
* [ ] Enable ExpressRoute Premium Add-on for regional connectivity
* [ ] Upgrade the Virtual WAN to Standard

**Correct answer:**
* [x] Upgrade the Virtual WAN to Standard

**Explaination**: Basic VWAN only supports Site-to-Site connection, to have ExpressRoute you need to use the Standard tier of Virtual WAN.


**Documentation Link**: https://learn.microsoft.com/en-us/azure/virtual-wan/virtual-wan-about#basicstandard

**18. Your environment in Azure has a subscription called ‘Subscription A’ and 20 web apps. In on-premises, you have an Active Directory domain controller which synchronizes the identities with Azure AD using Azure AD Connect. You also have a Linux server on-premises which runs an application that uses LDAP queries to verify user identities in the on-premises Active Directory domain. You are planning to migrate the Linux server to Azure. Due to the organizational policies, resources in Azure cannot communicate to on-premises. You need to suggest a solution to let the Linux server work after migration. The solution should meet the organizational policy.**


* [ ] Deploy Azure Application Proxy 
* [ ] Deploy VPN Gateway to connect to on-premises
* [ ] Deploy Azure AD Active Directory Domain Services
* [ ] Deploy Application Connector

**Correct answer:**
* [x] Deploy Azure AD Active Directory Domain Services

**Explaination**: Active Directory Domain Services (Azure AD DS) provides managed domain services such as domain join, group policy, lightweight directory access protocol (LDAP), and Kerberos/NTLM authentication. This one could work since AAD DS will bring in the existing accounts from Azure AD which in turn are synchronized from on-premises AD over AD connect. However, you would probably need to reconfigure the app and update the LDAP connection Azure Active Directory (Azure AD) supports LDAP Authentication via Azure AD Domain Services (AD DS). 


**Documentation Link**: https://learn.microsoft.com/en-us/azure/active-directory/fundamentals/auth-ldap

**19. You have an Azure subscription that contains a storage account. An application sometimes writes duplicate files to the storage account. You have a PowerShell script that identifies and deletes duplicate files in the storage account. Currently, the script is run manually after approval from the operations manager. You need to recommend a serverless solution that performs the following actions:  Runs the script once an hour to identify whether duplicate files exist  Sends an email notification to the operations manager requesting approval to delete the duplicate files  Processes an email response from the operations manager specifying whether the deletion was approved  Runs the script if the deletion was approved  What should you include in the recommendation?**


* [ ] Azure Event Grid and Function App 
* [ ] Azure Logic Apps and Function App
* [ ] Azure Logic Apps and Azure Batch
* [ ] Azure Service Bus and Azure Function

**Correct answer:**
* [x] Azure Logic Apps and Function App

**Explaination**: We will trigger the function when a file is added to the storage account and with the help of script it will check if there are duplicates. If the script detects any duplicate, that will be used to invoke the Logic App via HTTP trigger and send an email to the Operations Manager seeking approval. Once the Operations Manager Approves the deletion, the Logic App invokes the Function App to execute the script to delete the files.


**20. You have an on-premises environment and an Azure subscription. The on-premises environment has several branch offices. A branch office in Singapore contains a virtual machine named VM01 running Windows Server 2016 that is configured as a file server. Users access the shared files on VM01 from all the offices. You need to recommend a solution to ensure that the users can access the shares files as quickly as possible if the Singapore branch office is inaccessible. What should you include in the recommendation?**


* [ ] Create snapshots of your file share in all branch offices
* [ ] Use Recovery Services Vault to replicate the file server
* [ ] Create Azure File Share and use File Sync
* [ ] Crate Azure File Share and keep copies in Blob Storage

**Correct answer:**
* [x] Create Azure File Share and use File Sync

**Explaination**: Azure File Sync enables centralizing your organization's file shares in Azure Files, while keeping the flexibility, performance, and compatibility of a Windows file server. While some users may opt to keep a full copy of their data locally, Azure File Sync additionally could transform Windows Server into a quick cache of your Azure file share. You can use any protocol that's available on Windows Server to access your data locally, including SMB, NFS, and FTPS. You can have as many caches as you need across the world.


**Documentation Link**: https://learn.microsoft.com/en-us/azure/storage/file-sync/file-sync-introduction

**21. You have .NET web service named webservice01 that has the following requirements.  Must read and write to the local file system.  Must write to the Windows Application event log.  You need to recommend a solution to host Service1 in Azure. The solution must meet the following requirements:  Minimize maintenance overhead.  Minimize costs.  What should you include in the recommendation?**


* [ ] Azure Virtual Machine Scale Set 
* [ ] Azure App Service Environment
* [ ] Azure Function App
* [ ] Azure App Service

**Correct answer:**
* [x] Azure App Service

**Explaination**: To avoid maintenance overhead, the best approach would be to go for PaaS solution. Since our application requires to read and write to the local file system and write to the Windows Application Event Log, the right solution is Azure App Service.


**22. You are designing a microservices architecture comprising different organizational APIs that will be hosted in an Azure Kubernetes Service (AKS) cluster. Apps that will consume the microservices will be hosted on Azure virtual machines. The virtual machines and the AKS cluster will reside on the same virtual network. You need to design a solution to expose the microservices to the consumer apps. The solution must meet the following requirements:  Ingress access to the microservices must be restricted to a single private IP address and protected by using mutual TLS authentication.  The number of incoming microservice calls must be rate limited. What will you suggest?**


* [ ] Use Azure Application Gateway and expose the web applications
* [ ] Deploy API Management in the same virtual network
* [ ] Use Azure Front Door Service
* [ ] Use Azure Application Gateway with Web Application Firewall

**Correct answer:**
* [x] Deploy API Management in the same virtual network

**Explaination**: One option is to deploy APIM (API Management) Premium tier inside the cluster VNet. The AKS cluster and the applications that consume the microservices might reside within the same VNet, hence there is no reason to expose the cluster publicly as all API traffic will remain within the VNet. For these scenarios, you can deploy API Management into the cluster VNet. API Management Premium tier supports VNet deployment. 


**Documentation Link**: https://learn.microsoft.com/en-us/azure/api-management/api-management-kubernetes

**23. You need to design a solution that will execute custom C# code in response to an event routed to Azure Event Grid. The solution must meet the following requirements:  The executed code must be able to access the private IP address of a Microsoft SQL Server instance that runs on an Azure virtual machine.  Costs must be minimized.  What should you include in the solution?**


* [ ] Azure Logic Apps in the integrated service environment
* [ ] Azure Functions in the Dedicated plan and the Basic Azure App Service plan
* [ ] Azure Logic Apps in the Consumption plan
* [ ] Azure Functions in the Consumption plan

**Correct answer:**
* [x] Azure Functions in the Consumption plan

**Explaination**: With the consumption plan, you don’t have to provision the infrastructure and the cost will be based on number of executions.


**Documentation Link**: https://learn.microsoft.com/en-us/azure/azure-functions/consumption-plan

**24. Your company has an app named App1 that uses data from the on-premises Microsoft SQL Server databases shown in the following table.  ![question24](https://res.cloudinary.com/dezmljkdo/image/upload/v1680790236/az104/24_ed6qat.png) <br> <br> App1 retrieves the data very rarely twice or thrice in a month. The data is not expected to grow more than 2% each year. The company is migrating App1 as an Azure web app and plans to migrate all the data to Azure. You need to migrate the data to Azure SQL Database. The solution must minimize costs. Which service tier should you use?**


* [ ] vCore-based Business Critical 
* [ ] vCore-based General Purpose
* [ ] DTU-based Standard 
* [ ] DTU-based Basic

**Correct answer:**
* [x] DTU-based Standard 

**Explaination**: Since we want to minimize the cost, we can only choose the DTU tier. The total amount is data is around 700 GB and this is beyond the size of Basic tier. So, the best option would be to choose the Standard tier.


**Documentation Link**: https://learn.microsoft.com/en-us/azure/azure-sql/database/service-tiers-dtu?view=azuresql#compare-service-tiers

**25. You are developing a sales application that will contain several Azure cloud services and will handle different components of a transaction. Different cloud services will process customer orders, billing, payment, inventory, and shipping. You need to recommend a solution to enable the cloud services to asynchronously communicate transaction information by using REST messages. What should you include in the recommendation?**


* [ ] Azure Blob Storage 
* [ ] Azure Synapse Analytics 
* [ ] Azure Databricks
* [ ] Azure Service Bus

**Correct answer:**
* [x] Azure Service Bus

**Explaination**: Service Bus is a transactional message broker and ensures transactional integrity for all internal operations against its message stores. All transfers of messages inside of Service Bus, such as moving messages to a dead-letter queue or automatic forwarding of messages between entities, are transactional.


**Documentation Link**: https://learn.microsoft.com/en-us/azure/service-bus-messaging/service-bus-transactions

**26. Your company has 300 virtual machines hosted in a VMware environment. The virtual machines vary in size and have various utilization levels. You plan to move all the virtual machines to Azure. You need to recommend how many and what size Azure virtual machines will be required to move the current workloads to Azure. The solution must minimize administrative effort. What should you use to make the recommendation?**


* [ ] Azure Pricing Calculator 
* [ ] Azure TCO Calculator 
* [ ] Azure Cost Management
* [ ] Azure Migrate

**Correct answer:**
* [x] Azure Migrate

**Explaination**: With the help of assessment tools in the Azure Migrate, you will be able to assess the on-premises environment and recommend how many and what size Azure virtual machines will be required based on current environment. 


**Documentation Link**: https://learn.microsoft.com/en-us/azure/migrate/concepts-assessment-calculation

**27. You plan provision a High Performance Computing (HPC) cluster in Azure. You need to recommend a solution to provision and manage the HPC cluster node. What should you include in the recommendation?**


* [ ] Azure Batch  
* [ ] Azure Virtual Machine Scale Set
* [ ] Azure Kubernetes Service
* [ ]  Azure Service Fabric

**Correct answer:**
* [x] Azure Batch  

**Explaination**: Use Azure Batch to run large-scale parallel and high-performance computing (HPC) batch jobs efficiently in Azure. Azure Batch creates and manages a pool of compute nodes (virtual machines), installs the applications you want to run, and schedules jobs to run on the nodes.




**Documentation Link**: https://learn.microsoft.com/en-us/azure/batch/batch-technical-overview

**28. Your company, named Vendetta Corp, implements several Azure logic apps that have HTTP triggers. The logic apps provide access to an on-premises web service. Vendetta establishes a partnership with another company named FinDepth. FinDepth does not have an existing Azure Active Directory (Azure AD) tenant and uses third-party OAuth 2.0 identity management to authenticate its users. I Developers at FinDepth plan to use a subset of the logic apps to build applications that will integrate with the on-premises web service of Vendetta. You need to design a solution to provide the FinDepth developers with access to the logic apps. The solution must meet the following requirements: Requests to the logic apps from the developers must be limited to lower rates than the requests from the users at Vendetta. The developers must be able to rely on their existing OAuth 2.0 provider to gain access to the logic apps. The solution must NOT require changes to the logic apps.  The solution must NOT use Azure AD guest accounts.  What should you include in the solution?**


* [ ] Azure AD B2B 
* [ ] Azure AD B2C
* [ ]  Azure AD Application Proxy
* [ ] Azure API Management

**Correct answer:**
* [x] Azure API Management

**Explaination**: API Management helps organizations publish APIs to external, partner, and internal developers to unlock the potential of their data and services. You can secure API Management using the OAuth 2.0 client credentials flow.


**Documentation Link**: https://learn.microsoft.com/en-us/azure/api-management/api-management-key-concepts

**29. Your company plans to deploy various Azure App Service instances that will use Azure SQL databases. The App Service instances will be deployed at the same time as the Azure SQL databases. The company has a regulatory requirement to deploy the App Service instances only to specific Azure regions. The resources for the App Service instances must reside in the same region. You need to recommend a solution to meet the regulatory requirement.**


* [ ] Azure Blueprints 
* [ ] Azure Role Based Access Control
* [ ] Azure Policy
* [ ] Azure Governance Checker

**Correct answer:**
* [x] Azure Policy

**Explaination**: With the help of Azure Policy, we can define the set of allowed locations are the resources can be deployed only to those regions unless there is an exception provided by the organization.



**Documentation Link**: https://learn.microsoft.com/en-us/azure/governance/policy/overview

**30. You have an Azure subscription. You need to deploy an Azure Kubernetes Service (AKS) solution that will use Windows Server 2019 nodes. The solution must meet the following requirements:  Minimize the time it takes to provision compute resources during scale-out operations.   Support autoscaling of Windows Server containers.  Which scaling option should you recommend?**


* [ ] Virtual Kubelet 
* [ ] Cluster Autoscaler 
* [ ] Virtual Nodes
* [ ] Horizontal Pod Autoscaler

**Correct answer:**
* [x] Cluster Autoscaler 

**Explaination**: Since these are Windows nodes, we cannot use Virtual Nodes. So, the only option is to make use of the Cluster Autoscaler to increase the node count.


**Documentation Link**: https://learn.microsoft.com/en-us/azure/aks/cluster-autoscaler

**31. You have an Azure subscription. You need to recommend an Azure Kubernetes service (AKS) solution that will use Linux nodes. The solution must meet the following requirements: Minimize the time it takes to provision compute resources during scale-out operations.  Support autoscaling of Linux containers. Minimize administrative effort.  Which scaling option should you recommend?**


* [ ] Virtual Kubelet 
* [ ] Cluster Autoscaler
* [ ]  Virtual Nodes
* [ ] Horizontal Pod Autoscaler

**Correct answer:**
* [x]  Virtual Nodes

**Explaination**: To rapidly scale application workloads in an AKS cluster, you can use virtual nodes. With virtual nodes, you have quick provisioning of pods, and only pay per second for their execution time. You don't need to wait for Kubernetes cluster autoscaler to deploy VM compute nodes to run the additional pods. Virtual nodes are only supported with Linux pods and nodes.


**Documentation Link**: https://learn.microsoft.com/en-us/azure/aks/virtual-nodes

**32. You are designing an order processing system in Azure that will contain the Azure resources shown in the following table.<br>![question 32](https://res.cloudinary.com/dezmljkdo/image/upload/v1680786941/az104/az104-mockq1_llvlpx.png) <br> <br> The order processing system will have the following transaction flow:<br><ul><li>A customer will place an order by using App1.</li> <li>When the order is received, App1 will generate a message to check for product availability at vendor 1 and vendor 2.</li><li>An integration component will process the message, and then trigger either Function1 or Function2 depending on the type of order.</li><li>Once a vendor confirms the product availability, a status message for App1 will be generated by Function1 or Function2.</li><li>All the steps of the transaction will be logged to storage1.</li></ul> <br>Which type of resource should you recommend for the integration component?**


* [ ] Azure Service Bus queue 
* [ ] Azure Data Factory pipeline
* [ ] Azure Event Grid domain
* [ ] Azure Event Hubs capture

**Correct answer:**
* [x] Azure Service Bus queue 

**Explaination**: Answer:
A data factory can have one or more pipelines. A pipeline is a logical grouping of activities that together perform a task. The activities in a pipeline define actions to perform on your data. Data Factory has three groupings of activities: data movement activities, data transformation activities, and control activities. Azure Functions is now integrated with Azure Data Factory, allowing you to run an Azure function as a step in your data factory pipelines.



**Documentation Link**: https://learn.microsoft.com/en-us/azure/data-factory/concepts-pipelines-activities?tabs=data-factory

**33. You plan to deploy 10 applications to Azure. The applications will be deployed to two Azure Kubernetes Service (AKS) clusters. Each cluster will be deployed to a separate Azure region. The application deployment must meet the following requirements:  Ensure that the applications remain available if a single AKS cluster fails.  Ensure that the connection traffic over the internet is encrypted by using SSL without having to configure SSL on each container.  Which service should you include in the recommendation?**


* [ ] Azure Front Door 
* [ ] AKS Ingress Controller
* [ ]  Azure Load Balancer
* [ ] Azure Traffic Manager

**Correct answer:**
* [x] Azure Front Door 

**Explaination**: Azure Front Door, which focuses on global load-balancing and site acceleration, and Azure CDN Standard, which offers static content caching and acceleration. The new Azure Front Door brings together security with CDN technology for a cloud based CDN with threat protection and additional capabilities.


**Documentation Link**: https://learn.microsoft.com/en-us/azure/frontdoor/front-door-overview

**34. You plan to deploy an application named App1 that will run on five Azure virtual machines. Additional virtual machines will be deployed later to run App1. You need to recommend a solution to meet the following requirements for the virtual machines that will run App1:  Ensure that the virtual machines can authenticate to Azure Active Directory (Azure AD) to gain access to an Azure key vault, Azure Logic Apps instances, and an Azure SQL database.  Avoid assigning new roles and permissions for Azure services when you deploy additional virtual machines.  Avoid storing secrets and certificates on the virtual machines.  Which type of identity should you include in the recommendation?**


* [ ] Service principal configured to use a client secret
* [ ] Service principal configured to use client certificate
* [ ] System-assigned managed identity
* [ ] User-assigned managed identity

**Correct answer:**
* [x] User-assigned managed identity

**Explaination**: Managed identities for Azure resources eliminate the need to manage credentials in code. You can use them to get an Azure Active Directory (Azure AD) token for your applications. User-assigned managed identities can be used on multiple resources.

**Documentation Link**: https://learn.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/how-manage-user-assigned-managed-identities?pivots=identity-mi-methods-azp

**35. You have an Azure subscription. You need to recommend a solution to provide developers with the ability to provision Azure virtual machines. The solution must meet the following requirements:  Only allow the creation of the virtual machines in specific regions.  Only allow the creation of specific sizes of virtual machines.  What should you include in the recommendation?**


* [ ] Azure Role Based Access Control 
* [ ] Azure Policy
* [ ] Azure Resource Manager templates
* [ ] Conditional Access policies

**Correct answer:**
* [x] Azure Policy

**Explaination**: An Azure Policy definition, created in Azure Policy, is a rule about specific security conditions that you want controlled. Built in definitions include things like controlling what type of resources can be deployed or enforcing the use of tags on all resources. You can also create your own custom policy definitions.



**Documentation Link**: https://learn.microsoft.com/en-us/azure/defender-for-cloud/security-policy-concept

---


## Mock Exam Quiz 2

**1. After you migrate App1 to Azure App Service, you need to ensure that data modification and deletion is restricted to meet the security and compliance requirements. What should you do?**


* [ ] Create an access policy for the blob service. 
* [ ]  Implement Azure resource locks.
* [ ] Create Azure RBAC assignments. 
* [ ] Modify the access level of the blob service.

**Correct answer:**
* [x]  Implement Azure resource locks.

**Explaination**: With the help of resource locks, we can avoid data modification and deletion


**Documentation Link**: https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/lock-resources?tabs=json

**2. You have 320 GB of data in SQL Database, and you need to recommend a solution to meet the database retention requirements. What should you recommend?**


* [ ] Configure differential backup 
* [ ] Configure log transaction backup
* [ ] Configure long-term retention backup
* [ ] Configure Azure Backup

**Correct answer:**
* [x] Configure long-term retention backup

**Explaination**: Long-term backup retention (LTR) leverages the full database backups that are automatically created to enable point in time restore (PITR). If an LTR policy is configured, these backups are copied to different blobs for long-term storage.


**Documentation Link**: https://learn.microsoft.com/en-us/azure/azure-sql/database/long-term-retention-overview?view=azuresql

**3. You need to have a role that lets you manage networks, but not access to them. What is the minimum number of role assignments required to achieve this? Ensure that you follow principle of least privilege.**


* [ ] Owner 
* [ ] Virtual Network Contributor 
* [ ] Contributor
* [ ] Network Contributor

**Correct answer:**
* [x] Network Contributor

**Documentation Link**: https://learn.microsoft.com/en-us/azure/role-based-access-control/built-in-roles#network-contributor

**4. You plan to create an Azure Storage account that will host file shares. The shares will be accessed from on-premises applications that are transaction-intensive. You need to recommend a solution to minimize latency when accessing the file shares. The solution must provide the highest-level of resiliency for the selected storage tier. What should you include in the recommendation?**


* [ ] Transaction optimized with GRS 
* [ ] Premium with GRS
* [ ] Hot with ZRS
* [ ] Transaction optimized with ZRS

**Correct answer:**
* [x] Premium with GRS

**Explaination**: Premium file shares are backed by solid-state drives (SSDs) and provide consistent high performance and low latency, within single-digit milliseconds for most IO operations, for IO-intensive workloads. With ZRS, three copies of each file stored, however these copies are physically isolated in three distinct storage clusters in different Azure availability zones.


**5. You have an Azure subscription. You need to recommend a solution to provide developers with the ability to provision Azure virtual machines. The solution must meet the following requirements: Only allow the creation of the virtual machines in specific regions. Only allow the creation of specific sizes of virtual machines.  What should you include in the recommendation?**


* [ ] Conditional Access Policies 
* [ ] Role based access control
* [ ]  Azure Policy
* [ ] Azure Governance Map

**Correct answer:**
* [x]  Azure Policy

**Explaination**: Azure Policy helps to enforce organizational standards and to assess compliance at-scale.


**Documentation Link**: https://learn.microsoft.com/en-us/azure/governance/policy/overview

**6. You plan to migrate App1 to Azure. You need to recommend a private network connectivity solution for the Azure Storage account that will host the App1 data. The solution must meet the security and compliance requirements. What should you include in the recommendation?**


* [ ] A private endpoint 
* [ ]  A service endpoint that has a service endpoint policy 
* [ ] Azure public peering for an ExpressRoute circuit 
* [ ] Microsoft peering for an ExpressRoute circuit

**Correct answer:**
* [x] A private endpoint 

**Explaination**: Private Endpoint securely connect to storage accounts from on-premises networks that connect to the VNet using VPN or ExpressRoute with private-peering. Private Endpoint also secure your storage account by configuring the storage firewall to block all connections on the public endpoint for the storage service.


**Documentation Link**: https://learn.microsoft.com/en-us/azure/private-link/private-endpoint-overview

**7. You are planning to migrate 50 VMs that are hosted in Hyper-V to Azure. Before migrating, you need to estimate the cost savings you can realize by migrating your workloads to Azure. What is your recommendation?**


* [ ] Azure Pricing Calculator 
* [ ] Azure TCO Calculator
* [ ]  Azure Cost Management
* [ ] Azure Advisor

**Correct answer:**
* [x] Azure TCO Calculator

**Documentation Link**: https://azure.microsoft.com/en-us/pricing/tco/calculator/

**8. ou are designing an application that will use Azure Linux virtual machines to analyze video files. The files will be uploaded from corporate offices that connect to Azure by using ExpressRoute. You plan to provision an Azure Storage account to host the files. You need to ensure that the storage account meets the following requirements:  Supports video files of up to 7 TB  Provides the highest availability possible Ensures that storage is optimized for the large video files. How should you configure the storage account?**


* [ ] Premium page blobs with ZRS
* [ ]  Premium file shares with ZRS
* [ ] GPv2 with GZRS
* [ ]  GPv2 with GRS

**Correct answer:**
* [x]  Premium file shares with ZRS

**Explaination**: Azure Files ZRS premium tier should be considered for managed file services where performance and regional availability are critical for the business. ZRS provides high availability by synchronously writing three replicas of your data across three different Azure Availability Zones, thereby protecting your data from cluster, datacenter, or entire zone outage. Zonal redundancy enables you to read and write data even if one of the availability zones is unavailable.


**9. You have a web app deployed on-premises that is communicating with SQL Database deployed in Azure over ExpressRoute connection. The database connection string is stored in Azure Key Vault. Recommend solutions by which the app can access the Key Vault and retrieve the key. Select all that apply.**


* [ ] System assigned managed identity
* [ ]  User assigned managed identity
* [ ] Service principal with client secret 
* [ ] Service principal with client certificate

**Correct answer:**
* [x] Service principal with client secret 
* [x] Service principal with client certificate

**Explaination**: Managed identities can be only used supported Azure services, since the webapp is deployed on-premises we need to use service principal with secret or certificate to connect to the key vault securely for retrieving the connection string from Key Vault.


**10. You are designing a data storage solution to support reporting. The solution will ingest high volumes of data in the JSON format by using Azure Event Hubs. As the data arrives, Event Hubs will write the data to storage. The solution must meet the following requirements:  Organize data in directories by date and time. Allow stored data to be queried directly, transformed into summarized tables, and then stored in a data warehouse. Ensure that the data warehouse can store 50 TB of relational data and support between 200 and 300 concurrent read operations. Which service should you recommend for each type of data store?**


* [ ] Azure Blob Storage with Cosmos DB Cassandra API 
* [ ] ADLS Gen2 with Cosmos DB SQL API
* [ ] ADLS Gen2 with SQL Database Hyperscale
* [ ] ADLS Gen2 with Apache Spark

**Correct answer:**
* [x] ADLS Gen2 with SQL Database Hyperscale

**Explaination**: Azure Data Explorer integrates with Azure Blob Storage and Azure Data Lake Storage (Gen1 and Gen2), providing fast, cached, and indexed access to data stored in external storage. You can analyze and query data without prior ingestion into Azure Data Explorer. You can also query across ingested and un-ingested external data simultaneously. Azure Data Lake Storage is optimized storage for big data analytics workloads. Azure SQL Database Hyperscale is optimized for OLTP and high throughput analytics workloads with storage up to 100TB.
A Hyperscale database supports up to 100 TB of data and provides high throughput and performance, as well as rapid scaling to adapt to the workload requirements. Connectivity, query processing, database engine features, etc. work like any other database in Azure SQL Database.


**11. You have an app named App1 that uses two on-premises Microsoft SQL Server databases named DB1 and DB2. You plan to migrate DB1 and DB2 to Azure. You need to recommend an Azure solution to host DB1 and DB2. The solution must meet the following requirements: Support server-side transactions across DB1 and DB2. Minimize administrative effort to update the solution. What should you recommend?**


* [ ] Two Azure SQL databases in an Elastic Pool 
* [ ] Two Managed instances
* [ ] Two databases on the same managed instance
* [ ] Two databases on the same SQL Database

**Correct answer:**
* [x] Two databases on the same managed instance

**Explaination**: Since we need instance scoped features and easy lift and shift, managed instance would be the right choice. Now to minimize the cost and administrative effort, it’s better to have both the databases deployed on a single managed instance rather than going for individual instances.


**12. You need share the monthly recommendations from your Azure environment with team members to optimize your cloud environment. From which service you can retrieve the recommendations?**


* [ ] Azure Pricing Calculator 
* [ ] Azure Advisor 
* [ ] Azure TCO Calculator
* [ ] Azure Resource Manager

**Correct answer:**
* [x] Azure Advisor 

**Explaination**: Advisor is a personalized cloud consultant that helps you follow best practices to optimize your Azure deployments. It analyzes your resource configuration and usage telemetry and then recommends solutions that can help you improve the cost effectiveness, performance, Reliability (formerly called High availability), and security of your Azure resources.


**Documentation Link**: https://learn.microsoft.com/en-us/azure/advisor/advisor-overview

**13. You need to design a highly available Azure SQL database that meets the following requirements: Failover between replicas of the database must occur without any data loss. The database must remain available in the event of a zone outage. Costs must be minimized. Which deployment option should you use?**


* [ ] SQL Database Premium 
* [ ]  SQL Database Hyperscale 
* [ ] SQL Database Standard
* [ ] SQL Database General Purpose

**Correct answer:**
* [x] SQL Database Premium 

**Explaination**: Azure SQL Database Premium tier supports multiple redundant replicas for each database that are automatically provisioned in the same datacenter within a region. This design leverages the SQL Server Availability Groups technology and provides resilience to server failures with 99.99% availability SLA and RPO=0. With the introduction of Azure Availability Zones, we are happy to announce that SQL Database now offers built-in support of Availability Zones in its Premium service tier. Hyper scale is expensive than premium and standard / GP doesn’t support zone redundancy.


**14. You have an Azure subscription that contains 300 virtual machines that run Windows Server 2019. You need to centrally monitor all warning events in the System logs of the virtual machines. What should you include in the solution?**


* [ ] Install Azure Monitoring Agent and configure to send data to Azure Log Analytics
* [ ] Create Event Hub to send data to Azure Log Analytics
* [ ] Create CI/CD pipeline to send the data to Azure Monitor
* [ ] Install Dependency agent and configure to send data to Azure Log Analytics

**Correct answer:**
* [x] Install Azure Monitoring Agent and configure to send data to Azure Log Analytics

**Explaination**: Azure Monitoring Agent (AMA) is responsible for the collection of data from the virtual machines. AMA will be further configured to ingest the data to the Azure Log Analytics.


**15. You need to design an architecture to capture the creation of users and the assignment of roles. The captured data must be stored in Azure Cosmos DB. Which services should you include in the design?**


* [ ] A: Event Grid, B: Azure Log Analytics 
* [ ] A: Event Hub, B: Azure Functions
* [ ] A: Event Grid, B: Azure Functions
* [ ] A: Service Bus Topics, B: Azure Log Analytics

**Correct answer:**
* [x] A: Event Hub, B: Azure Functions

**Code**: 
   Azure AD Audit log → A? → B? → Cosmos DB

**Explaination**: Event Hub is used for streaming Azure Active Directory (Azure AD) activity logs to several endpoints for long term retention and data insights. The streamed data will be used by Azure Functions along with Cosmos DB change feed and further stored to Cosmos DB


**16. You are deploying web app in multiple regions. You need to recommend a solution for load balancing that is fast, reliable, and secure access between your users and your applications’ static and dynamic web content across the globe. What is your suggestion?**


* [ ] Azure Traffic Manager 
* [ ] Azure Load Balancer 
* [ ] Azure Application Gateway
* [ ] Azure Front Door

**Correct answer:**
* [x] Azure Front Door

**Explaination**: Azure Front Door delivers your content using the Microsoft’s global edge network with hundreds of global and local points of presence (PoPs) distributed around the world close to both your enterprise and consumer end users.


**Documentation Link**: https://learn.microsoft.com/en-us/azure/frontdoor/front-door-overview

**17. The number of applications that you can deploy to an App Service depends on the tier of the App Service Plan. True or False.**


* [ ] False 
* [ ] True

**Correct answer:**
* [x] True

**18. A company has a hybrid ASP.NET Web API application that is based on a software as a service (SaaS) offering. Users report general issues with the data. You advise the company to implement live monitoring and use ad hoc queries on stored JSON data. You also advise the company to set up smart alerting to detect anomalies in the data. You need to recommend a solution to set up smart alerting. What should you recommend?**


* [ ] Azure Data Bricks
* [ ]  Azure Data Lake Analytics 
* [ ] Azure Stream Analytics
* [ ] Azure Application Insights

**Correct answer:**
* [x] Azure Application Insights

**Explaination**: Application Insights, a feature of Azure Monitor, is an extensible Application Performance Management (APM) service for developers and DevOps professionals.
Use it to monitor your live applications. It will automatically detect performance anomalies, and includes powerful analytics tools to help you diagnose issues and to understand what users do with your app.


**Documentation Link**: https://learn.microsoft.com/en-us/azure/azure-monitor/app/app-insights-overview?tabs=net

**19. You deployed a firewall to a virtual network to ensure that all incoming and outgoing traffic from the virtual network is inspected. After deploying, you still see the resources in the same virtual network are directly communicating to the internet. What should you do?**


* [ ] By default, firewall allows all traffic, add deny rules in the firewall
* [ ] Enable peering between the AzureFirewallSubnet and workload subnets
* [ ] Force UDR to the subnets to send the traffic to firewall
* [ ] Deploy a gateway to send traffic to firewall

**Correct answer:**
* [x] Force UDR to the subnets to send the traffic to firewall

**Explaination**: With the help of UDR, we can force the traffic to be routed to the firewall by setting the IP address of the next hop as firewall IP address in the route table.


**20. You have an Azure subscription that is linked to an Azure Active Directory (Azure AD) tenant. The subscription contains 10 resource groups, one for each department at your company. Each department has a specific spending limit for its Azure resources. You need to ensure that when a department reaches its spending limit, the compute resources of the department shut down automatically. Which two features should you include in the solution?**


* [ ] Azure Logic Apps 
* [ ] Azure Function Apps
* [ ] Azure Cost Management  
* [ ] Azure Automation Runbook

**Correct answer:**
* [x] Azure Cost Management  
* [x] Azure Automation Runbook

**Explaination**: With the budget feature in Azure Cost Management, we can set up budget and execute Azure Runbooks to stop the resources if needed.


**Documentation Link**: https://learn.microsoft.com/en-us/azure/cost-management-billing/costs/manage-automation

**21. Your company uses Microsoft System Center Service Manager on its on-premises network. You plan to deploy several services to Azure. You need to recommend a solution to push Azure service health alerts to Service Manager. What should you include in the recommendation?**


* [ ] Azure Event Hubs
* [ ]  Azure Application Insights 
* [ ] Azure Notification Hubs
* [ ] ITSM connector

**Correct answer:**
* [x] ITSM connector

**Explaination**: With the help of ITSM connector we can push Azure service health alerts to Service Manager


**22. You have an Azure Active Directory (Azure AD) tenant named kodekloud.com that contains two administrative user accounts named Admin1 and Admin2. You create two Azure virtual machines named VM1 and VM2. You need to ensure that Admin1 and Admin2 are notified when more than five events are added to the security log of VM1 or VM2 during a period of 120 seconds. The solution must minimize administrative tasks. What should you create?**


* [ ] two action groups and two alert rules
* [ ]  one action group and one alert rule
* [ ] five action groups and one alert rule
* [ ] two action groups and one alert rule

**Correct answer:**
* [x]  one action group and one alert rule

**Explaination**: Action groups are notification preferences that can be used with multiple alert rules


**23. You have an Azure subscription that contains web apps in three Azure regions. You need to implement Azure Key Vault to meet the following requirements: ✑ In the event of a regional outage, all keys must be readable. ✑ All the web apps in the subscription must be able to access Key Vault. ✑ The number of Key Vault resources to be deployed and managed must be minimized. How many instances of Key Vault should you implement?**


* [ ] 6
* [ ] 4
* [ ] 2
* [ ] 1

**Correct answer:**
* [x] 1

**Explaination**: The contents of your key vault are replicated within the region and to a secondary region at least 150 miles away but within the same geography. This maintains high durability of your keys and secrets. See the Azure paired regions document for details on specific region pairs.
Example: Secrets that must be shared by your application in both Europe West and Europe North. Minimize these as much as you can. Put these in a key vault in either of the two regions. Use the same URI from both regions. Microsoft will fail over the Key Vault service internally.


**24. You need to ensure none of the administrators (except the break glass account) has permanent permissions assigned to them. All administrators should activate their admin role before they could perform any admin tasks. What solution should you use?**


* [ ] Azure AD Identity Protection 
* [ ] Azure AD Access reviews 
* [ ] Azure AD MFA
* [ ] Azure AD Privileged Identity Management

**Correct answer:**
* [x] Azure AD Privileged Identity Management

**Explaination**: Privileged Identity Management (PIM) is a service in Azure Active Directory (Azure AD) that enables you to manage, control, and monitor access to important resources in your organization.


**25. You have an application that is used by 6,000 users to validate their vacation requests. The application manages its own credential, users must enter a username and password to access the application. The application does NOT support identity providers. You plan to upgrade the application to use single sign-on (SSO) authentication by using an Azure Active Directory (Azure AD) application registration. Which SSO method should you use?**


* [ ] Open ID connect 
* [ ] SAML 
* [ ] Password-based 
* [ ] XML

**Correct answer:**
* [x] Password-based 

**Documentation Link**: https://learn.microsoft.com/en-us/azure/active-directory/fundamentals/auth-password-based-sso

**26. You have data files in Azure Blob Storage. You plan to transform the files and move them to Azure Data Lake Storage. You need to transform the data by using mapping data flow. Which service should you use?**


* [ ] Azure Data Factory 
* [ ] Azure Databricks 
* [ ] Azure File Sync
* [ ] Azure Synapse Analytics

**Correct answer:**
* [x] Azure Data Factory 

**Explaination**: You can use Copy Activity in Azure Data Factory to copy data from and to Azure Data Lake Storage Gen2, and use Data Flow to transform data in Azure Data Lake Storage Gen2


**Documentation Link**: https://learn.microsoft.com/en-us/azure/data-factory/solution-template-move-files

**27. You plan to deploy an app that will use an Azure Storage account. You need to deploy the storage account. The solution must meet the following requirements:  Store the data of multiple users.  Encrypt each user's data by using a separate key. Encrypt all the data in the storage account by using Microsoft keys or customer-managed keys.  What should you deploy?**


* [ ] File storage in GPv2 storage 
* [ ] Blobs storage in GPv2 storage
* [ ] Premium file share
* [ ] Blobs in ADLS Gen2

**Correct answer:**
* [x] Blobs storage in GPv2 storage

**Explaination**: Blobs in GPv2 as we need Customer Managed Keys. CMK is only supported for blobs and files.


**Documentation Link**: https://learn.microsoft.com/en-us/azure/storage/common/storage-service-encryption?toc=%2Fazure%2Fstorage%2Fblobs%2Ftoc.json&bc=%2Fazure%2Fstorage%2Fblobs%2Fbreadcrumb%2Ftoc.json

**28. You store web access logs data in Azure Blob storage. You plan to generate monthly reports from the access logs. You need to recommend an automated process to upload the data to Azure SQL Database every month. What should you include in the recommendation?**


* [ ] Database Migration Assistant 
* [ ] Azure Database Migration Service
* [ ] AzCopy
* [ ]  Azure Data Factory

**Correct answer:**
* [x]  Azure Data Factory

**Explaination**:  Azure Data Factory is the platform that solves such data scenarios. It is the cloud-based ETL and data integration service that allows you to create data-driven workflows for orchestrating data movement and transforming data at scale. Using Azure Data Factory, you can create and schedule data-driven workflows (called pipelines) that can ingest data from disparate data stores. You can build complex ETL processes that transform data visually with data flows or by using compute services such as Azure HDInsight Hadoop, Azure Databricks, and Azure SQL Database.


**Documentation Link**: https://learn.microsoft.com/en-gb/azure/data-factory/introduction

**29. You plan to deploy an Azure SQL database that will store Personally Identifiable Information (PII). You need to ensure that only privileged users can view the PII. What should you include in the solution?**


* [ ] Transparent Data Encryption (TDE) 
* [ ] Data Labelling 
* [ ] Data masking
* [ ] Role based access control (RBAC)

**Correct answer:**
* [x] Data masking

**Explaination**: Dynamic data masking helps prevent unauthorized access to sensitive data by enabling customers to designate how much of the sensitive data to reveal with minimal impact on the application layer


**Documentation Link**: https://learn.microsoft.com/en-us/azure/azure-sql/database/dynamic-data-masking-overview?view=azuresql

**30. You plan to deploy Azure Databricks to support a machine learning application. Data engineers will mount an Azure Data Lake Storage account to the Databricks file system. Permissions to folders are granted directly to the data engineers. You need to recommend a design for the planned Databrick deployment. The solution must meet the following requirements:  Ensure that the data engineers can only access folders to which they have permissions. Minimize development effort.  Minimize costs.  What should you include in the recommendation?**


* [ ] Premium tier with Credential Passthrough 
* [ ] Standard tier with Credential Passthrough
* [ ] Premium tier with MLFlow 
* [ ] Standard tier with Secret scope

**Correct answer:**
* [x] Standard tier with Credential Passthrough

**Explaination**: When you enable Azure Data Lake Storage credential passthrough for your cluster, commands that you run on that cluster can read and write data in Azure Data Lake Storage without requiring you to configure service principal credentials for access to storage. Since we want to minimize cost, Standard tier is the right choice.


**Documentation Link**: https://learn.microsoft.com/en-us/azure/databricks/data-governance/credential-passthrough/adls-passthrough

**31. You have the resources shown in the following table: Name Type AS1 Azure Synapse Analytics instance CDB1 Azure Cosmos DB SQL API account   CDB1 hosts a container that stores continuously updated operational data. You are designing a solution that will use AS1 to analyze the operational data daily. You need to recommend a solution to analyze the data without affecting the performance of the operational data store. What should you include in the recommendation?**


* [ ] Azure Cosmos DB consistency levels
* [ ] Azure Data Factory with Azure Cosmos DB and Azure Synapse Analytics connectors
* [ ] Azure Synapse Link for Azure Cosmos DB
* [ ] Azure Synapse Analytics with PolyBase data loading 

**Correct answer:**
* [x] Azure Synapse Link for Azure Cosmos DB

**Explaination**: PolyBase enables your SQL Server instance to query data with T-SQL directly from SQL Server, Oracle, Teradata, MongoDB, Hadoop clusters, Cosmos DB, and S3-compatible object storage without separately installing client connection software. You can also use the generic ODBC connector to connect to additional providers using third-party ODBC drivers.


**Documentation Link**: https://learn.microsoft.com/en-us/sql/relational-databases/polybase/polybase-guide?view=sql-server-ver16

**32. You plan to deploy an Azure web app named App1 that will use Azure Active Directory (Azure AD) authentication. App1 will be accessed from the internet by the users at your company. All the users have computers that run Windows 10 and are joined to Azure AD. You need to recommend a solution to ensure that the users can connect to App1 without being prompted for authentication and can access App1 only from company-owned computers. What should you recommend for each requirement?**


* [ ] Azure AD App registration with Conditional Access
* [ ] Azure AD managed identity with Conditional Access
* [ ] Azure AD app registration with Azure Policy
* [ ] Azure AD managed identity with Azure AD Administrative Units

**Correct answer:**
* [x] Azure AD App registration with Conditional Access

**Explaination**: Azure active directory (AD) provides cloud based directory and identity management services. You can use Azure AD to manage users of your application and authenticate access to your applications using azure active directory. You register your application with Azure active directory tenant. Conditional Access policies at their simplest are if-then statements, if a user wants to access a resource, then they must complete an action. By using Conditional Access policies, you can apply the right access controls when needed to keep your organization secure and stay out of your user's way when not needed.


**Documentation Link**: https://learn.microsoft.com/en-us/azure/active-directory/conditional-access/overview

**33. You have an Azure subscription. The subscription contains Azure virtual machines that run Windows Server 2016 and Linux. You need to use Azure Monitor to design an alerting strategy for security-related events. Which Azure Monitor Logs tables should you query?**


* [ ] EventLogs for Windows and Syslog for Linux 
* [ ] Event for Windows and LinuxLogs for Linux
* [ ] Event for Windows and Syslog for Linux
* [ ] AzureDiagnostics for both Windows and Linux

**Correct answer:**
* [x] Event for Windows and Syslog for Linux

**Explaination**: Event logs will be ingested to Event table and Linux logs will be ingested to Syslog table in Log Analytics Service.


**Documentation Link**: https://learn.microsoft.com/en-us/azure/azure-monitor/reference/tables/syslog https://learn.microsoft.com/en-us/azure/azure-monitor/reference/tables/event

**34. Your on-premises network contains a server named Server1 that runs an ASP.NET application named App1. You have a hybrid deployment of Azure Active Directory (Azure AD). You need to recommend a solution to ensure that users sign in by using their Azure AD account and Azure Multi-Factor Authentication (MFA) when they connect to App1 from the internet. Which three features should you recommend be deployed and configured in sequence?**


* [ ] Azure AD Application Proxy, Azure Managed Identity, and Azure AD Conditional Access Policies
* [ ] Azure AD Application Proxy, Azure AD enterprise application, and Azure AD Conditional Access Policy
* [ ] Azure AD enterprise application, Azure AD Application Proxy, Azure AD Conditional Access
* [ ] Azure AD managed identity, Azure AD Application Proxy, Azure AD enterprise application

**Correct answer:**
* [x] Azure AD Application Proxy, Azure AD enterprise application, and Azure AD Conditional Access Policy

**Documentation Link**: https://learn.microsoft.com/en-us/azure/active-directory/app-proxy/application-proxy

---


## Docker Engine Architecture

**1. What file is used to configure the docker daemon?**


* [ ]  /var/lib/docker/docker.conf
* [ ] /var/lib/docker/daemon.json
* [ ] /etc/docker/daemon.json
* [ ] /etc/docker/daemon.conf

**Correct answer:**
* [x] /etc/docker/daemon.json

**2. Run a container called apps with image nginx, and in an interactive mode.**


* [ ] docker container run -it nginx
* [ ] docker container run -it nginx --name apps
* [ ] docker container run nginx
* [ ] docker container run -it --name apps nginx

**Correct answer:**
* [x] docker container run -it --name apps nginx

**3. What is the port used to configure encrypted traffic on TCP?**


* [ ] 2345 
* [ ]  2346  
* [ ] 2375  
* [ ] 2376

**Correct answer:**
* [x] 2376

**4. How to display the running processes inside the container?**


* [ ]  docker container top container-name
* [ ] docker container stats container-name
* [ ]  docker ps container-name
* [ ] docker container logs container-name

**Correct answer:**
* [x]  docker container top container-name

**5. We have deployed some containers. What command is used to get the container with the highest memory?**


* [ ]  docker container stats
* [ ]  docker container status
* [ ] docker container top  
* [ ] docker container ls

**Correct answer:**
* [x]  docker container stats

**6. Delete the stopped container named db.**


* [ ] docker container delete db
* [ ] docker container remove db
* [ ]  docker container kill db
* [ ] docker container rm db

**Correct answer:**
* [x] docker container rm db

**7. Which of the below commands create a container with redis image and name redis?**


* [ ]  docker container create redis --name redis
* [ ] docker container --name redis redis
* [ ]  docker container run redis
* [ ] docker container create --name redis redis

**Correct answer:**
* [x] docker container create --name redis redis

**8. Delete all running and stopped containers on the host.**


* [ ]  docker container stop $(docker container ls -q)
* [ ]  docker container rm $(docker container ls -q)
* [ ] docker container stop $(docker container ps -q)
* [ ]  docker container rm -f $(docker container ls -aq)

**Correct answer:**
* [x]  docker container rm -f $(docker container ls -aq)

**9. Which policy should be used to restart the container at all times except when it is stopped manually by a user?**


* [ ]  unless-stopped
* [ ]   on-failure
* [ ]   no
* [ ]   always

**Correct answer:**
* [x]  unless-stopped

**10. Which command should be used to update the restart policy of the httpd container with always?**


* [ ]  docker container update --restart always httpd
* [ ] docker container unpause --restart always httpd
* [ ]  docker container upgrade --restart always httpd
* [ ]  None of the above

**Correct answer:**
* [x]  docker container update --restart always httpd

**11. What is the path to the file which is used to add the live restore option?**


* [ ]  /etc/docker/daemon.json 
* [ ]  /var/lib/docker/daemon.json
* [ ] /var/log/docker/daemon.json
* [ ]  /var/lib/docker

**Correct answer:**
* [x]  /etc/docker/daemon.json 

**12. Which environment variable will be used to connect to a remote docker server?**


* [ ]  DOCKER_REMOTE  
* [ ] DOCKER_HOST  
* [ ] DOCKER_SERVER  
* [ ] None of the above

**Correct answer:**
* [x] DOCKER_HOST  

**13. You can map multiple containers to the same port on the Docker host.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**14. Which command is used to check the default logging driver?**


* [ ]  docker system df  
* [ ] docker system events
* [ ] docker system prune
* [ ]  docker system info

**Correct answer:**
* [x]  docker system info

**15. Run a dev container, and make sure that no logs are configured for this container.**


* [ ]  docker run -it --log-driver none dev
* [ ] docker run -it --logging-driver none dev
* [ ] docker run -it dev
* [ ] docker run -it --log none dev

**Correct answer:**
* [x]  docker run -it --log-driver none dev

**16. Copy the /etc/nginx directory from the webapp container to the docker host at the path /tmp/.**


* [ ] docker container copy webapp:/etc/nginx /tmp/
* [ ]  docker container cp webapp:/etc/nginx /tmp/
* [ ] docker container copy /tmp/ webapp:/etc/nginx
* [ ] docker container cp /tmp/ webapp:/etc/nginx

**Correct answer:**
* [x]  docker container cp webapp:/etc/nginx /tmp/

**17. What is the default logging driver?**


* [ ]  json-file  
* [ ] syslog 
* [ ]  journald  
* [ ] splunk

**Correct answer:**
* [x]  json-file  

**18. How to check if the docker service is running?**


* [ ]  docker status  
* [ ] sudo systemctl status docker
* [ ] sudo systemctl docker status
* [ ]  docker service status

**Correct answer:**
* [x] sudo systemctl status docker

**19. Unless specified otherwise, docker publishes the exposed port on all network interfaces.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**20. How does docker map a port on a container to a port on the host?**


* [ ] Using an internal load balancer  
* [ ] Using FirewallD Rules
* [ ]  Using an external load balancer
* [ ]  Using IPTables Rules

**Correct answer:**
* [x]  Using IPTables Rules

**21. What option is used in the docker command to map UDP port 80 in the container to port 9595 on the Docker host?**


* [ ]  -p 9595:80/udp 
* [ ]  -p 80:9595/udp 
* [ ]  -P 9595:80/udp
* [ ]  -p 9595:80 -protocol udp

**Correct answer:**
* [x]  -p 9595:80/udp 

**22. What IPTables chain does Docker modify to configure port mapping on a host?**


* [ ]  INPUT 
* [ ]  FORWARD  
* [ ] DOCKER 
* [ ]  OUTPUT

**Correct answer:**
* [x] DOCKER 

**23. You can run multiple instances of the same application on the docker host.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**24. Data inside the container is deleted when the container is destroyed.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**25. What is the command to copy the file /root/myconfig.txt from the host to /root/ of the data container?**


* [ ]  docker container copy /root/myconfig.txt data:/root/
* [ ]  docker container cp /root/myconfig.txt data:/root/
* [ ] docker container copy data:/root/ /root/myconfig.txt
* [ ] docker container cp data:/root/ /root/myconfig.txt

**Correct answer:**
* [x]  docker container cp /root/myconfig.txt data:/root/

**26. Which option is used to reduce container downtime due to daemon crashes, planned outages, or upgrades?**


* [ ]  Restart Policy  
* [ ] Swarm 
* [ ]  Live Restore 
* [ ]  Restart Policy, Live Restore

**Correct answer:**
* [x]  Live Restore 

**27. Which command is used to update all running containers with the <code>unless-stopped</code> restart policy?**


* [ ]  docker container upgrade --restart unless-stopped $(docker container ls -q)
* [ ] docker container update --restart unless-stopped $(docker container ls -q)
* [ ] docker container upgrade --restart unless-stopped $(docker container ls -aq)
* [ ]  docker container update --restart unless-stopped $(docker container ls -aq)

**Correct answer:**
* [x] docker container update --restart unless-stopped $(docker container ls -q)

**28. Which command is used to get the events of the container named dev-apps?**


* [ ] docker system events since 10m --container=dev-apps
* [ ]  docker system events --filter 'container=dev-apps'
* [ ] docker system events --filter 'image=dev-apps'

**Correct answer:**
* [x]  docker system events --filter 'container=dev-apps'

**29. Which command can be used to check the restart policy of a webapp container?**


* [ ] docker container inspect webapp
* [ ] docker container info webapp
* [ ] docker container check webapp
* [ ]  None of the above

**Correct answer:**
* [x] docker container inspect webapp

**30. Which command is used to get the stream logs of the web-app container so that you can view the logs live?**


* [ ] docker container log web-app  
* [ ] docker container log -f web-app
* [ ]  docker container logs web-app
* [ ]  docker container logs -f web-app

**Correct answer:**
* [x]  docker container logs -f web-app

**31. What is the command to pause a running container?**


* [ ]  docker container pause 
* [ ]  docker container --pause
* [ ] docker container halt
* [ ]  docker container SIGSTOP

**Correct answer:**
* [x]  docker container pause 

**32. Run a container with image nginx, name lab and hostname testing.**


* [ ] docker container run -d lab --hostname=testing nginx
* [ ]  docker container run -d --name lab testing
* [ ]  docker container run -d --name lab --hostname=testing nginx
* [ ] docker container run -d --name testing nginx

**Correct answer:**
* [x]  docker container run -d --name lab --hostname=testing nginx

---


## Practice Test Docker Engine Images

**1. What is the command to change the tag of busybox:latest to busybox:v1?**


* [ ] docker image retag busybox:v1 busybox:latest
* [ ]  docker container tag busybox:latest busybox:v1
* [ ] docker image retag busybox:latest busybox:v1
* [ ]  docker image tag busybox:latest busybox:v1

**Correct answer:**
* [x]  docker image tag busybox:latest busybox:v1

**2. What is the command to remove all unused images on the Docker host?**


* [ ] docker image prune -a 
* [ ]  docker image rm -a 
* [ ]  docker image delete -a
* [ ]  None of the above

**Correct answer:**
* [x] docker image prune -a 

**3. Build an image using a context build under path /opt/docker and name it pyapp.**


* [ ] docker build /opt/docker 
* [ ]  docker build /opt/docker -t pyapp
* [ ]  docker build pyapp -t /opt/docker
* [ ] docker build . -f /opt/docker -t pyapp

**Correct answer:**
* [x]  docker build /opt/docker -t pyapp

**4. If you list more than one CMD instruction in the Dockerfile then the second instruction will be appended to the first.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**5. What is the command to list the images in Docker host?**


* [ ]  docker container ps  d
* [ ] ocker image ls 
* [ ]  docker list image
* [ ]  docker image ps
* [ ] docker image list

**Correct answer:**
* [x] ocker image ls 
* [x]  docker image ps
* [x] docker image list

**6. Which command is used to remove kodekloud:v1 image locally?**


* [ ] docker image rm kodekloud  
* [ ] docker image rm kodekloud:v1
* [ ] docker image remove kodekloud:v1
* [ ] docker image del kodekloud:v1

**Correct answer:**
* [x] docker image rm kodekloud:v1
* [x] docker image remove kodekloud:v1

**7. Which of the following tag image will get when creating a redis container with image redis? docker run -itd --name redis redis**


* [ ]  none 
* [ ]  default
* [ ]   v1 
* [ ]  latest

**Correct answer:**
* [x]  latest

**8. Name the stage which uses alpine as a base image to builder in the Dockerfile.**


* [ ]  FROM alpine 
* [ ]  FROM alpine AS builder
* [ ] FROM alpine TO builder
* [ ] FROM builder AS alpine

**Correct answer:**
* [x]  FROM alpine AS builder

**9. A parent image is the image that your image is based on. It refers to the contents of the FROM directive in the Dockerfile.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**10. Which instruction(s) can be used in the Dockerfile to copy content from the local filesystem into the containers?**


* [ ]  ADD
* [ ]   CP 
* [ ]  MOVE 
* [ ]  RUN

**Correct answer:**
* [x]  ADD

**11. While building an image, You have one parent image but there could be multiple base images.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**12. Choose the correct instruction to add the sleep "1000" command in the Dockerfile.**


* [ ] CMD [sleep "1000"]  
* [ ] CMD ["sleep", "1000"]
* [ ] CMD ["sleep 1000"]
* [ ]  None of the above

**Correct answer:**
* [x] CMD ["sleep", "1000"]

**13. Which command is used to list the full length image IDs?**


* [ ]  docker image ls –digests  
* [ ] docker images --digests  
* [ ] docker images --no-trunc
* [ ]  None of the above

**Correct answer:**
* [x] docker images --no-trunc

**14. Which command is used to download the redis image from Docker Hub?**


* [ ] docker image pull redis  
* [ ] docker image build redis 
* [ ]  docker image load redis
* [ ]  None of the above

**Correct answer:**
* [x] docker image pull redis  

**15. The COPY /tmp/file.tar.gz copies and extracts the tar file into the image.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**16. What is the command to build an image with the name webapp using a Dockerfile file under path /opt/myapp . The current directory you are in is /tmp.**


* [ ] docker build /opt/myapp -t webapp 
* [ ]  docker build -f /opt/myapp/Dockerfile /opt/myapp -t webapp
* [ ] docker build -f Dockerfile /opt/myapp -t webapp
* [ ] docker build -t Dockerfile -name webapp -f /opt/myapp

**Correct answer:**
* [x]  docker build -f /opt/myapp/Dockerfile /opt/myapp -t webapp

**17. A build’s context is the set of files located in the specified PATH or URL. Which kind of resources can the URL parameter refer to ?**


* [ ]  Git Repositories 
* [ ]  Pre-packaged tarball contexts 
* [ ]  Path to a local directory

**Correct answer:**
* [x]  Git Repositories 
* [x]  Pre-packaged tarball contexts 
* [x]  Path to a local directory

**18. To disable the cache while building a docker image, Which option can be used?**


* [ ]  –no-cache=true  
* [ ] –force-rm=true  
* [ ] –cache-from true  
* [ ] None of the above

**Correct answer:**
* [x]  –no-cache=true  

**19. Which of the below steps can help minimize the build time of images?**


* [ ]  Only install necessary packages within the image. 
* [ ]  Avoid sending unwanted files to the build context using .dockerignore.
* [ ]  Combine multiple dependent instructions into a single one and cleanup temporary files.
* [ ]  Move the instructions that are likely to change most frequently to the bottom of the Dockerfile.
* [ ]  Use multi-stage builds.

**Correct answer:**
* [x]  Avoid sending unwanted files to the build context using .dockerignore.
* [x]  Move the instructions that are likely to change most frequently to the bottom of the Dockerfile.

**20. When you log in to a registry, the command stores credentials in which location path?**


* [ ] $HOME/.docker/config.json 
* [ ]  /etc/docker/.docker/config.json
* [ ]  /var/lib/docker/.docker/config.json
* [ ] /var/lib/docker/containers/.docker/config.json

**Correct answer:**
* [x] $HOME/.docker/config.json 

**21. Which method can be used to build an image using existing containers?**


* [ ] docker commit  
* [ ] docker build
* [ ]   docker save
* [ ]   docker load

**Correct answer:**
* [x] docker commit  

**22. You are required to generate a report of the total amount of space consumed by docker images, containers and build cache on a docker host. What would be your approach?**


* [ ]  Run the command df -h 
* [ ]  Develop a script to query the Docker API
* [ ]  Run the command docker system df
* [ ]  Run the command docker system list

**Correct answer:**
* [x]  Run the command docker system df

**23. Which command can be used to get the Exposed Ports of an image by the name kodekloud?**


* [ ] docker container ls  
* [ ] docker image inspect kodekloud
* [ ] docker container inspect kodekloud
* [ ]   docker image ls

**Correct answer:**
* [x] docker image inspect kodekloud

**24. Choose the correct option to pull alpine image from the kk organization hosted at a private Registry at gcr.io.**


* [ ] docker pull alpine  
* [ ] docker pull kk/alpine 
* [ ]  docker pull gcr.io/kk/alpine
* [ ]  All of the above

**Correct answer:**
* [x]  docker pull gcr.io/kk/alpine

**25. Which command can be used to get a backup of image mysql?**


* [ ]  docker image backup mysql -o mysql.tar
* [ ]   docker image save mysql -o mysql.tar
* [ ]  docker container save mysql -o mysql.tar
* [ ] docker container backup mysql -o mysql.tar

**Correct answer:**
* [x]   docker image save mysql -o mysql.tar

**26. What is a best practice while installing multiple packages as part of the install instruction?**


* [ ]  Add them on the same line  
* [ ] Add them on separate lines separated by a slash in alphanumeric order
* [ ]  Add a separate instruction for each package

**Correct answer:**
* [x] Add them on separate lines separated by a slash in alphanumeric order

**27. Whenever a build is initiated by running the Docker build command, the files under the build context are transferred to the Docker daemon, at a temporary directory under the docker’s filesystem. Which directory are these files stored in?**


* [ ] /var/lib/docker/tmp 
* [ ]  /var/lib/docker/builds 
* [ ]  /var/lib/tmp
* [ ]  /tmp//docker/builds

**Correct answer:**
* [x] /var/lib/docker/tmp 

**28. You have created an nginx container and customized it to create your own webpage. How can you create an image out of it to share with others?**


* [ ]  docker image save  
* [ ] docker image export 
* [ ]  docker export
* [ ]  You can only create an image using a Dockerfile

**Correct answer:**
* [x]  docker export

**29. While trying to delete image mariadb, you got an error “conflict: unable to remove repository reference mariadb (must force) – container 1a56b95e073c is using its referenced image adf2b126dda8”. What may be the cause of this error?**


* [ ]  A container is using this image  
* [ ] Must use force option to delete an image 
* [ ]  Another image is using layers from this image
* [ ]  The image was built locally on this host

**Correct answer:**
* [x]  A container is using this image  

**30. Which of the below can help minimize the image size?**


* [ ] Only install necessary packages within the image
* [ ]  Avoid sending unwanted files to the build context using .dockerignore
* [ ] Combine multiple dependent instructions into a single one and cleanup temporary files
* [ ]  Move the instructions that are likely to change most frequently to the bottom of the Dockerfile
* [ ] Use multi-stage builds

**Correct answer:**
* [x] Only install necessary packages within the image
* [x] Combine multiple dependent instructions into a single one and cleanup temporary files
* [x] Use multi-stage builds

---


## Docker Engine Security

**1. What is a linux feature that prevents a process within the container to access raw sockets?**


* [ ]  Control Groups (CGroups)  
* [ ] Namespaces  
* [ ] Kernel Capabilities
* [ ]  Network Namespaces

**Correct answer:**
* [x] Kernel Capabilities

**2. Each container gets a CPU share of …. assigned by default.**


* [ ]  256 
* [ ]  512 
* [ ]  1024  
* [ ] 2048

**Correct answer:**
* [x]  1024  

**3. Which of the below statements are true:**


* [ ]  By default a container runs with 1 vCPU and 500 MB of memory  
* [ ] By default a container runs with unlimited CPU and Memory resources
* [ ] By default a container runs with 0.5 vCPU and 500 MB of memory
* [ ]  By default a container runs with unlimited vCPU and 500 MB of memory

**Correct answer:**
* [x] By default a container runs with unlimited CPU and Memory resources

**4. By default, all containers get the same share of CPU cycles. How to modify the shares?**


* [ ]  docker container run --cpu-shares=512 nginx  
* [ ] docker container run --cpuset-cups=512 nginx
* [ ] docker container run --cpu-quota=512 nginx
* [ ] docker container run --cpus=512 nginx

**Correct answer:**
* [x]  docker container run --cpu-shares=512 nginx  

**5. What will happen if the --memory-swap is set to -1?**


* [ ]  the container does not have access to swap. 
* [ ]  the setting is ignored, and the value is treated as unset.
* [ ]  the container is allowed to use unlimited swap.
* [ ]  None of the above

**Correct answer:**
* [x]  the container is allowed to use unlimited swap.

---


## Docker Engine Networking

**1. Which of the following commands would create a user-defined bridge network called dev-net?**


* [ ]  docker network create dev-net 
* [ ]  docker create network dev-net
* [ ] docker network create -d bridge dev-net
* [ ] docker network create --type bridge dev-net
* [ ] docker network create --driver bridge dev-net

**Correct answer:**
* [x]  docker network create dev-net 
* [x] docker network create -d bridge dev-net
* [x] docker network create --driver bridge dev-net

**2. What is the command to delete the network named connector?**


* [ ] docker network prune connector  
* [ ] docker network rm connector
* [ ] docker network delete connector  
* [ ] None of the above

**Correct answer:**
* [x] docker network rm connector

**3. What is the command to remove all unused networks from the Docker host?**


* [ ] docker network create cisco-net 
* [ ]  docker network rm web-net
* [ ] docker network prune
* [ ]  docker network rm –all

**Correct answer:**
* [x] docker network prune

**4. What is the command to connect a running container with name myapp to the existing bridge network dev-net?**


* [ ]  docker container connect myapp dev-net 
* [ ]  docker container attach myapp dev-net
* [ ] docker network connect dev-net myapp
* [ ] docker network connect myapp dev-net

**Correct answer:**
* [x] docker network connect dev-net myapp

**5. What is the command to list all available networks?**


* [ ]  docker network ls 
* [ ]  docker network show 
* [ ]  docker network display  
* [ ] docker get network

**Correct answer:**
* [x]  docker network ls 

**6. If you use the … network mode for a container, that container’s network stack is not isolated from the Docker host (the container shares the host’s networking namespace), and the container does not get its own IP-address allocated.**


* [ ] bridge  
* [ ] overlay  
* [ ] host  
* [ ] none

**Correct answer:**
* [x] host  

**7. What is the default network driver used on a container if you haven’t specified one?**


* [ ]  host
* [ ]   bridge 
* [ ]  overlay 
* [ ]  All of the above

**Correct answer:**
* [x]   bridge 

**8. Which command is used to see the IP address and other network settings assigned to a container with id 33373b1ccc3f that uses the wordpress image?**


* [ ]  docker inspect wordpress 
* [ ]  docker container ls wordpress
* [ ] docker container ls 33373b1ccc3f
* [ ]  docker inspect 33373b1ccc3f

**Correct answer:**
* [x]  docker inspect 33373b1ccc3f

**9. Which command is used to disconnect the my-net network from the redis container?**


* [ ] docker network rm redis my-net 
* [ ]  docker network disconnect redis my-net
* [ ] docker network disconnect my-net redis
* [ ] docker network disconnect redis

**Correct answer:**
* [x] docker network disconnect my-net redis

**10. Which command is used to see the details of the subnet and gateway of network id ce982a9edf65?**


* [ ] docker info ce982a9edf65  
* [ ] docker container inspect ce982a9edf65
* [ ]  docker show ce982a9edf65
* [ ]  docker network inspect ce982a9edf65

**Correct answer:**
* [x]  docker network inspect ce982a9edf65

---


## Docker Engine Storage

**1. What is the command to create a volume with the name first-volume?**


* [ ] docker volume create first-volume 
* [ ]  docker create volume first-volume 
* [ ]  docker volume add first-volume
* [ ] docker volume prune first-volume

**Correct answer:**
* [x] docker volume create first-volume 

**2. Which option is used to mount a volume ?**


* [ ]  -v 
* [ ]  --volume 
* [ ]  --mount 
* [ ]  --storage

**Correct answer:**
* [x]  -v 
* [x]  --volume 
* [x]  --mount 

**3. Which among the below is a correct command to start an appstore container with the volume vol1, mounted to the destination directory /opt in read-only mode?**


* [ ] docker run -d --name appstore --mount source=vol1,target=/opt,readonly httpd
* [ ] docker run -d --name appstore -v vol1:/opt:ro httpd
* [ ]  docker run -d --name appstore -v vol1:/opt:readonly httpd
* [ ] docker run -d --name appstore --volume vol1:/opt:ro httpd

**Correct answer:**
* [x] docker run -d --name appstore --mount source=vol1,target=/opt,readonly httpd
* [x] docker run -d --name appstore -v vol1:/opt:ro httpd
* [x] docker run -d --name appstore --volume vol1:/opt:ro httpd

**4. What is the command to get details of the volume data-volume?**


* [ ]  docker volume inspect data-volume 
* [ ]  docker volume fetch data-volume
* [ ]  docker volume get data-volume  
* [ ] docker volume ls data-volume

**Correct answer:**
* [x]  docker volume inspect data-volume 

**5. We have created multiple docker volumes, which command is used to list all the available volumes in the Docker host?**


* [ ]  docker volume ls  
* [ ] docker volume show 
* [ ]  docker volume get
* [ ] docker volume list

**Correct answer:**
* [x]  docker volume ls  

**6. All files inside an image are in a writable layer.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**7. Volumes are the preferred mechanism for persisting data generated by and used by Docker containers.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

---


## Practice Test Docker Compose

**1. We have created a docker compose file for an application. Choose the correct option to deploy the application and run it in the background.**


* [ ] docker-compose up -d  
* [ ] docker-compose up -b 
* [ ]  docker-compose up --detach
* [ ] docker-compose start --background

**Correct answer:**
* [x] docker-compose up -d  
* [x]  docker-compose up --detach

**2. Compose files that doesn’t specifically declare a version are considered “version 0”.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**3. Which command is used to check the logs for the whole stack defined inside docker compose file?**


* [ ] docker logs  
* [ ] docker-compose ps  
* [ ] docker-compose logs 
* [ ]  docker-stack logs

**Correct answer:**
* [x] docker-compose logs 

**4. Which command can be used to delete the application stack created using a compose file?**


* [ ] docker-compose rm  
* [ ] docker-compose stop  
* [ ] docker-compose down  
* [ ] docker-compose destroy

**Correct answer:**
* [x] docker-compose rm  
* [x] docker-compose down  

**5. What is the command to see the running process inside of containers created by compose file?**


* [ ] docker-compose top 
* [ ]  docker-compose stats 
* [ ]  docker top  
* [ ] docker stats

**Correct answer:**
* [x] docker-compose top 

**6. Which command is used to list the containers created by compose file?**


* [ ] docker-compose ps  
* [ ] docker-compose ls  
* [ ] docker-compose list  
* [ ] docker-compose get

**Correct answer:**
* [x] docker-compose ps  

**7. Compose files using the version ‘2’ and version ‘3’ syntax must indicate the version number at the root of the document.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**8. …. is a YAML file which contains details about the services, networks, and volumes for setting up a Docker application.**


* [ ]  Dockerfile  
* [ ] Docker Compose File  
* [ ] dockerignore  
* [ ] env

**Correct answer:**
* [x] Docker Compose File  

**9. Using Docker Compose, we can configure containers and the communication between them in a declarative way.**


* [ ] False
* [ ] True

**Correct answer:**
* [x] True

---


## Research Questions Image Scanning

**1. DTR is a vulnerability scanner that analyzes container images for security vulnerabilities triggered by a manual request only.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**2. In which service does DTR image scanning occur?**


* [ ] A service known as the dtr-jobrunner container  
* [ ] A service known as the dtr-registry container
* [ ]  A service known as the dtr-api container  
* [ ] A service known as the dtr-runner container

**Correct answer:**
* [x] A service known as the dtr-jobrunner container  

**3. With Docker Trusted Registry, we need to rebuild the image in each stage to promote to different environments (e.g. Dev, Test, Stage, and Prod)**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**4. In which of the following will image scanning look for known vulnerabilities**


* [ ]  OS packages  
* [ ] Suspicious user accounts 
* [ ]  Libraries
* [ ]  IP Tables rules that are not required
* [ ]  Other dependencies that are defined in a container image
* [ ]  All of the above

**Correct answer:**
* [x]  OS packages  
* [x]  Libraries
* [x]  Other dependencies that are defined in a container image

**5. To scan an image, DTR ________________.**


* [ ]  Extracts a copy of the image layers from backend storage.  
* [ ] Extracts the files from the layer into a working directory inside the dtr-jobrunner container.
* [ ]  Executes the scanner against the files in this working directory, collecting a series of scanning data.
* [ ]  Once the scanning data is collected, the working directory for the layer will remain on the job-runner until garbage collection is initiated.
* [ ] All of the above

**Correct answer:**
* [x]  Extracts a copy of the image layers from backend storage.  
* [x] Extracts the files from the layer into a working directory inside the dtr-jobrunner container.
* [x]  Executes the scanner against the files in this working directory, collecting a series of scanning data.

**6. With Docker Trusted Registry you can promote an existing image, based on a policy, to be pushed to a new environment.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**7. You may also configure DTR to initiate scans automatically when an image is pushed.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**8. Once the scan is complete, a report shows all the vulnerabilities detected categorized as __________**


* [ ]  Major 
* [ ]  Minor  
* [ ] Warning  
* [ ] Critical  
* [ ] INFO  
* [ ] All of the above

**Correct answer:**
* [x]  Major 
* [x]  Minor  
* [x] Critical  

**9. A promotion can only be configured to another repository within the same registry.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**10. Which statement best describes Garbage Collection in DTR?**


* [ ]  Automatically removes unused image layers to save disk space at a scheduled interval.
* [ ]  Garbage Collection setting is available under the system -> garbage collection section.
* [ ] By default, garbage collection is enabled.
* [ ]  All of the above

**Correct answer:**
* [x]  Automatically removes unused image layers to save disk space at a scheduled interval.
* [x]  Garbage Collection setting is available under the system -> garbage collection section.

**11. DTR ships with Notary built-in so that you can use Docker Content Trust (DCT) to sign and verify images.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**12. You may configure garbage collection to run at a specific interval.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**13. DCT is integrated with the Docker CLI, and allows you to _____________________.**


* [ ]  Configure repositories  
* [ ] Add signers  
* [ ] Sign images using the docker trust command

**Correct answer:**
* [x]  Configure repositories  
* [x] Add signers  
* [x] Sign images using the docker trust command

**14. Under the hood, each image stored in DTR is made up of multiple files, what are they?**


* [ ]  A list of image layers that are unioned which represents the image filesystem
* [ ]  A configuration file that contains the architecture of the image and other metadata
* [ ]  A manifest file containing the list of all layers and configuration file for an image

**Correct answer:**
* [x]  A list of image layers that are unioned which represents the image filesystem
* [x]  A configuration file that contains the architecture of the image and other metadata
* [x]  A manifest file containing the list of all layers and configuration file for an image

**15. Which statements best describe Notary?**


* [ ]  Notary is a tool for publishing and managing trusted collections of content. 
* [ ]  The official Docker Hub Notary servers are located at https://docker.io
* [ ]  With Notary anyone can provide trust over arbitrary collections of data.
* [ ]  Notary uses Globally Unique Names (GUNs) to identify trust collections.

**Correct answer:**
* [x]  Notary is a tool for publishing and managing trusted collections of content. 
* [x]  With Notary anyone can provide trust over arbitrary collections of data.
* [x]  Notary uses Globally Unique Names (GUNs) to identify trust collections.

**16. What are the key components of Docker Trusted Registry (DTR) for signing an image?**


* [ ] Notary Server 
* [ ]  Notary Signer  
* [ ] Docker Hub  
* [ ] Universal Control Plane (UCP)

**Correct answer:**
* [x] Notary Server 
* [x]  Notary Signer  

**17. You are required to configure your environment to prevent untrusted images from being deployed on the cluster. What approach would you choose to ensure images deployed in the cluster are secure and trusted?**


* [ ] Configure RBAC and provide access to repositories to privileged users only
* [ ]  Enable vulnerability scanning on images on push
* [ ]  Configure UCP to Run only signed images. And enforce image signing for all images using DCT

**Correct answer:**
* [x]  Configure UCP to Run only signed images. And enforce image signing for all images using DCT

---


## Practice Test Replication Controllers and Replicasets

**1. What is a Label in Kubernetes?**


* [ ]  A way to expose traffic 
* [ ]  A type of Deployment  
* [ ] A way to group related things using key/value pairs 
* [ ]  None of the above

**Correct answer:**
* [x] A way to group related things using key/value pairs 

**2. A ReplicaSet is one of the Kubernetes controllers?**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**3. How do you add labels to a pod in a pod definition YAML file?**


* [ ] labels 
* [ ]  spec.labels 
* [ ]  spec.containers.labels  
* [ ] metadata.labels

**Correct answer:**
* [x] metadata.labels

**4. What is the flag that you use along with the kubectl create command to deploy multiple instances of an application in Kubernetes?**


* [ ]  --image 
* [ ]  --label 
* [ ]  --replicas 
* [ ]  --scale

**Correct answer:**
* [x]  --replicas 

**5. What is the command to delete the pod busybox?**


* [ ] kubectl pod delete busybox 
* [ ]  kubectl delete busybox 
* [ ]  kubectl delete pod/busybox 
* [ ]  kubectl pod busybox --delete

**Correct answer:**
* [x]  kubectl delete pod/busybox 

**6. What are the 4 top level fields of a Kubernetes definition file for ConfigMap?**


* [ ] apiVersion  
* [ ] templates
* [ ]   metadata 
* [ ]  data  
* [ ] kind 
* [ ]  spec 
* [ ]  containers

**Correct answer:**
* [x] apiVersion  
* [x]   metadata 
* [x]  data  
* [x] kind 

**7. What is the command to delete a replication controller nginx?**


* [ ] kubectl get rc nginx
* [ ]   kubectl remove rc nginx  
* [ ] kubectl rm rc nginx 
* [ ]  kubectl delete rc nginx

**Correct answer:**
* [x]  kubectl delete rc nginx

**8. Where do you configure the selector labels in the deployment YAML file?**


* [ ]  metadata.selector  
* [ ] spec.selector  
* [ ] spec.template.selector 
* [ ]  spec.template.metadata.selector

**Correct answer:**
* [x] spec.selector  

**9. Which of the following are the container runtimes that Kubernetes supports?**


* [ ]  Docker  
* [ ] Containerd  
* [ ] CRI-O
* [ ]   LXC

**Correct answer:**
* [x]  Docker  
* [x] Containerd  
* [x] CRI-O

**10. What is a component of the Kubernetes control plane that allows external users or services to manage the Kubernetes cluster?**


* [ ]  Kubernetes Scheduler  
* [ ] ETCDCTL  
* [ ] Kube API Server  
* [ ] Kube Proxy

**Correct answer:**
* [x] Kube API Server  

**11. What is the command to deploy a pod with the name jenkins and image jenkins?**


* [ ] kubectl deploy jenkins --image jenkins 
* [ ]  kubectl run jenkins --image jenkins
* [ ] kubectl start -it jenkins sh
* [ ] kubelet run jenkins --image jenkins

**Correct answer:**
* [x]  kubectl run jenkins --image jenkins

**12. Which of the following are components deployed only on a Master Node in a Kubernetes cluster?**


* [ ]  Kube Scheduler  
* [ ] Kube Controller Manager 
* [ ]  Kube Api-server
* [ ] Kubelet  
* [ ] Kube-Proxy

**Correct answer:**
* [x]  Kube Scheduler  
* [x] Kube Controller Manager 
* [x]  Kube Api-server

**13. ETCD by default listens on port 2780.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**14. Which statement best describes the Worker Node component?**


* [ ]  kubelet and container runtime are the worker node components  
* [ ] kube-proxy is one of the worker node component  
* [ ] kube-scheduler is one of the worker node component
* [ ] All of the above

**Correct answer:**
* [x]  kubelet and container runtime are the worker node components  
* [x] kube-proxy is one of the worker node component  

**15. Which of the below are the container orchestration tools?**


* [ ]  Kubernetes  
* [ ] Docker Swarm  
* [ ] Google Compute Engine
* [ ]  Apache Mesos  
* [ ] ETCD

**Correct answer:**
* [x]  Kubernetes  
* [x] Docker Swarm  
* [x]  Apache Mesos  

**16. What is the command to list all the pods that are in a netpol namespace? Select all the answers that apply.**


* [ ] kubectl list pods -n netpol  
* [ ] kubectl get pods 
* [ ]  kubectl list pods -n netpol
* [ ] kubectl get pods -n netpol

**Correct answer:**
* [x] kubectl get pods -n netpol

**17. What kubectl command can be used to perform a Deployment update?**


* [ ] kubectl set image  
* [ ] kubectl rollout update 
* [ ]  kubectl rolling-update 
* [ ]  kubectl update

**Correct answer:**
* [x] kubectl set image  

**18. Which of the following are the deployment strategy types in Kubernetes?**


* [ ]  RollingUpdate
* [ ]   BlueGreen  
* [ ] Canary  
* [ ] Recreate

**Correct answer:**
* [x] Recreate
* [x]  RollingUpdate

**19. Which statement best describes deployment in Kubernetes? Select all the answers that apply.**


* [ ]  Deployments create PODs and not ReplicaSets.  
* [ ] Deployments create ReplicaSets that create PODs.
* [ ] Deployments support rolling updates and roll backs of applications.
* [ ]  Deployments support rolling updates but not roll backs.

**Correct answer:**
* [x] Deployments create ReplicaSets that create PODs.
* [x] Deployments support rolling updates and roll backs of applications.

**20. Where do you configure the pod images in the deployment YAML file?**


* [ ] metadata.image 
* [ ]  spec.containers.image  
* [ ] spec.template.spec.containers.image
* [ ]  spec.template.containers.image

**Correct answer:**
* [x] spec.template.spec.containers.image

**21. The command and arguments that you define in the Kubernetes definition file override the default command and arguments configured in the container image.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**22. Which of the following statements are correct about ClusterIP?**


* [ ] ClusterIP exposes a service on the same port as that of the exposed port on containers in the PODs..
* [ ]  ClusterIP exposes a service internally within the hosts only.
* [ ]  ClusterIP exposes a service to make it externally accessible on a port on the nodes.
* [ ]  None of the Above

**Correct answer:**
* [x]  ClusterIP exposes a service internally within the hosts only.

**23. Each container inside a POD does not get its own IP address assigned. All containers inside a POD share a single IP address.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**24. Which among the following statements are true without any change made to the default behaviour of network policies in the namespace?**


* [ ]  As soon as a network policy is associated with a POD traffic between all PODs in the namespace is denied
* [ ]  As soon as a network policy is associated with a POD all ingress and egress traffic to that POD are denied except those allowed by the network policy
* [ ] As soon as a network policy is associated with a POD all ingress and egress traffic to that POD are allowed except for the the ones blocked by the network policy

**Correct answer:**
* [x]  As soon as a network policy is associated with a POD all ingress and egress traffic to that POD are denied except those allowed by the network policy

**25. What is the command to list configmaps? Select all the answers that apply.**


* [ ] kubectl get pods  
* [ ] kubectl get cm  
* [ ] kubectl get configmap 
* [ ]  kubectl get maps

**Correct answer:**
* [x] kubectl get cm  
* [x] kubectl get configmap 

**26. You can pass in the --from-file argument multiple times to create a ConfigMap from multiple data sources.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**27. How do you set environment variables in a pod definition file?**


* [ ]  Using environment section  
* [ ] Using env section  
* [ ] Using env_var section
* [ ]  Using variables section

**Correct answer:**
* [x] Using env section  

**28. What is the command to display details of the secret user-list?**


* [ ]  kubectl get secret user-list  
* [ ] kubectl describe secret user-list 
* [ ]  kubectl list secret user-list  
* [ ] kubectl get secret user-list --details

**Correct answer:**
* [x] kubectl describe secret user-list 

**29. Which command is used to make some changes into the already existing PersistentVolumeClaim mysql-pvc?**


* [ ] kubectl describe pvc mysql-pvc 
* [ ]  kubectl get pvc mysql-pvc
* [ ] kubectl pvc edit mysql-pvc
* [ ] kubectl edit persistentvolumeclaim mysql-pvc

**Correct answer:**
* [x] kubectl edit persistentvolumeclaim mysql-pvc

**30. Which statement best describes the readiness probe?**


* [ ] The kubelet uses readiness probes to know when a container is ready to start accepting traffic.
* [ ]  The kubelet uses readiness probes to know when to restart a container
* [ ] The Readiness probes run on the container during its whole lifecycle.
* [ ] All of the above

**Correct answer:**
* [x] The kubelet uses readiness probes to know when a container is ready to start accepting traffic.
* [x] The Readiness probes run on the container during its whole lifecycle.

**31. The kubelet uses liveness probes to know when a container is ready to start accepting traffic.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**32. Liveness probes let Kubernetes know if your app is alive or stuck/dead.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

---


## Practice Test Docker Engine Enterprise

**1. Which of the following features does UCP provide?**


* [ ]  Centralized Cluster Management 
* [ ]  Deploy, manage and monitor workload
* [ ] Built-in security and access control
* [ ]  Issue Tracker
* [ ]  Source Code Management

**Correct answer:**
* [x]  Centralized Cluster Management 
* [x]  Deploy, manage and monitor workload
* [x] Built-in security and access control

**2. Which of the following are the features of Docker Enterprise Edition?**


* [ ]  Security & Access Control  
* [ ] Universal Control Plane & Trusted Registry
* [ ]  Source Code Management
* [ ]  Docker Swarm Service  
* [ ] Kubernetes Service

**Correct answer:**
* [x]  Security & Access Control  
* [x] Universal Control Plane & Trusted Registry
* [x]  Docker Swarm Service  
* [x] Kubernetes Service

**3. Which of the following need to be installed in order to provision the Docker EE infrastructure?**


* [ ] Docker Enterprise Engine should be installed on every node inside your Docker EE setup
* [ ] A MongoDB database should be pre-provisioned prior to installing Docker EE
* [ ]  Install the Universal Control Plane (UCP)
* [ ]  Install the Docker Trusted Registry (DTR)

**Correct answer:**
* [x] Docker Enterprise Engine should be installed on every node inside your Docker EE setup
* [x]  Install the Universal Control Plane (UCP)
* [x]  Install the Docker Trusted Registry (DTR)

**4. Which of the following need to be installed in order to provision the Docker EE infrastructure?**


* [ ]  Docker Enterprise Engine should be installed on every node inside your Docker EE setup
* [ ]  A MongoDB database should be pre-provisioned prior to installing Docker EE
* [ ] Install the Universal Control Plane (UCP)
* [ ]  Install the Docker Trusted Registry (DTR)

**Correct answer:**
* [x]  Docker Enterprise Engine should be installed on every node inside your Docker EE setup
* [x] Install the Universal Control Plane (UCP)
* [x]  Install the Docker Trusted Registry (DTR)

**5. Docker Engine – Enterprise can only be installed on Linux distribution.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**6. Docker enterprise is the only platform that supports both Docker swarm and Kubernetes on the same cluster.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**7. The ucp-agent installs the necessary components on the worker node automatically after a new node joins the cluster.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**8. We can interact with UCP from the GUI only.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**9. ucp-agent is configured as a global service.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**10. What are the minimum hardware requirements to install UCP?**


* [ ]  4GB RAM, 2vCPUs and 10GB disk space for the /var partition for manager nodes, 2GB RAM and 500MB disk space for the /var partition for worker nodes
* [ ] 8GB RAM, 2vCPUs and 10GB disk space for the /var partition for manager nodes, 4GB RAM and 500MB disk space for the /var partition for worker nodes
* [ ]  8GB RAM, 2vCPUs and 10GB disk space for the /var/lib/docker partition for manager nodes, 4GB RAM and 500MB disk space for the /var/lib/docker partition for worker nodes
* [ ]  4GB RAM, 2vCPUs and 10GB disk space for the /var/lib/docker partition for manager nodes, 2GB RAM and 500MB disk space for the /var/lib/docker partition for worker nodes

**Correct answer:**
* [x] 8GB RAM, 2vCPUs and 10GB disk space for the /var partition for manager nodes, 4GB RAM and 500MB disk space for the /var partition for worker nodes

**11. …. is the enterprise-grade cluster management platform from Docker. You install it on-premises or in your virtual private cloud, and it helps you manage your Docker cluster and applications through a single interface.**


* [ ]  Universal Control Plane (UCP) 
* [ ]  Docker Enterprise Edition 
* [ ]  Docker Community Edition  
* [ ] Docker Trusted Registry (DTR)

**Correct answer:**
* [x]  Universal Control Plane (UCP) 

**12. User namespaces should not be configured on any node as they are not currently supported in UCP.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**13. UCP works with Docker swarm under the hoods.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**14. To authorize access to cluster resources across your organization, which of the following high-level steps must UCP administrators take?**


* [ ]  Configure subjects (users, teams, and service accounts).
* [ ]  Define custom roles (or use defaults) by adding permitted operations per type of resource.
* [ ]  Configure resource sets of Swarm collections or Kubernetes namespaces.
* [ ]  Create grants by combining subject + role + resource set

**Correct answer:**
* [x]  Configure subjects (users, teams, and service accounts).
* [x]  Define custom roles (or use defaults) by adding permitted operations per type of resource.
* [x]  Configure resource sets of Swarm collections or Kubernetes namespaces.
* [x]  Create grants by combining subject + role + resource set

**15. What is the command line interface used to interact with UCP from a shell?**


* [ ]  docker-ucp  
* [ ] docker  
* [ ] docker-ee 
* [ ]  docker-ucp-cli

**Correct answer:**
* [x] docker  

**16. Which of the statements best describes “Subjects” in the Access Control Model?**


* [ ]  A subject represents a user, team, organization  
* [ ] A subject does not represent a service account.
* [ ] A subject can be granted a role that defines permitted operations against one or more resource sets.
* [ ]  All of the above

**Correct answer:**
* [x]  A subject represents a user, team, organization  
* [x] A subject can be granted a role that defines permitted operations against one or more resource sets.

**17. What environment variables must be set to allow client to communicate with UCP via CLI?**


* [ ] DOCKER  
* [ ] DOCKER_HOST  
* [ ] DOCKER_CERT_PATH  
* [ ] DOCKER_PATH

**Correct answer:**
* [x] DOCKER_HOST  
* [x] DOCKER_CERT_PATH  

**18. A group of teams that share a specific set of permissions forms a collection.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**19. What are the features of Docker Trusted Registry (DTR)?**


* [ ] Built-in Access Control 
* [ ]  Image and Job Management  
* [ ] Automated image builds
* [ ]  Security Scanning  
* [ ] Dockerfile management in SCM
* [ ]  Image Signing

**Correct answer:**
* [x] Built-in Access Control 
* [x]  Image and Job Management  
* [x]  Security Scanning  
* [x]  Image Signing

**20. What is the type and the name of the network of the DTR deployment?**


* [ ] overlay/dtr  
* [ ] overlay/dtr-ol
* [ ] bridge/dtr 
* [ ]  bridge/dtr-ol

**Correct answer:**
* [x] overlay/dtr-ol

---


## Practice Test Docker Trusted Registry

**1. What is the command to pull the docker repository owned by an organization?**


* [ ] docker get DTR-DOMAIN-NAME/ORG/REPOSITORY:TAG 
* [ ]  docker pull DTR-DOMAIN-NAME/ORG/REPOSITORY:TAG
* [ ] docker download DTR-DOMAIN-NAME/ORG/REPOSITORY:TAG
* [ ] docker fetch DTR-DOMAIN-NAME/ORG/REPOSITORY:TAG

**Correct answer:**
* [x]  docker pull DTR-DOMAIN-NAME/ORG/REPOSITORY:TAG

**2. You may configure garbage collection to run at a specific interval.**


* [ ] True 
* [ ] False

**Correct answer:**
* [x] True 

**3. When a user creates a repository, by default other users will also have permissions to make changes to the repository.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**4. In DTR, the promotion of an image can only be configured to another repository within the same DTR registry**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**5. What are the key components of Docker Trusted Registry (DTR) for signing an image?**


* [ ]  Notary Server  
* [ ] Notary Signer  
* [ ] Docker Hub  
* [ ] Universal Control Plane (UCP)

**Correct answer:**
* [x]  Notary Server  
* [x] Notary Signer  

**6. By default, DTR has one organization called docker-datacenter, that is shared between DTR and UCP.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**7. DCT is integrated with the Docker CLI, and allows you to _____________________.**


* [ ]  Configure repositories 
* [ ]  Add signers  
* [ ] Sign images using the docker trust command

**Correct answer:**
* [x]  Configure repositories 
* [x]  Add signers  
* [x] Sign images using the docker trust command

**8. In which service does DTR image scanning occur?**


* [ ]  A service known as the dtr-jobrunner container 
* [ ]  A service known as the dtr-registry container
* [ ]  A service known as the dtr-api container
* [ ]  A service known as the dtr-runner container

**Correct answer:**
* [x]  A service known as the dtr-jobrunner container 

**9. By default, when pushing an image to DTR, it automatically creates a new repository if one does not already exist by that name.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**10. In which of the following will image scanning look for known vulnerabilities.**


* [ ] OS packages  
* [ ] Suspicious user accounts
* [ ]   Libraries
* [ ] IP Tables rules that are not required
* [ ]  Other dependencies that are defined in a container image
* [ ]  All of the above

**Correct answer:**
* [x] OS packages  
* [x]   Libraries
* [x]  Other dependencies that are defined in a container image

**11. Once the scan is complete, a report shows all the vulnerabilities detected categorized as __________.**


* [ ]  Major  
* [ ] Minor  
* [ ]  Critical  
* [ ] INFO 
* [ ]  All of the above
* [ ]  Warning

**Correct answer:**
* [x]  Major  
* [x] Minor  
* [x]  Critical  

**12. DTR ships with Notary built-in so that you can use Docker Content Trust(DCT) to sign and verify images.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

---


## AWS-Lambda-Networking

**1. Where does the lambda function operate by default?**


* [ ] AWS managed Lambda service VPC
* [ ] Private subnet
* [ ] Public subnet
* [ ] Customer managed VPC

**Correct answer:**
* [x] AWS managed Lambda service VPC

**Explaination**: Lambda service operates in AWS managed Lambda service VPC

**2. What is the service to be configured for the private subnet of VPC to have access to the internet?**


* [ ] NAT Gateway
* [ ] Internet Endpoint
* [ ] Internet Gateway
* [ ] VPC Endpoint

**Correct answer:**
* [x] NAT Gateway

**Explaination**: NAT Gateway - has to be configured in public subnet to access internet.

VPC Endpoint - for services in AWS cloud VPC to connect to other services with in AWS private network.


**3. Default Lambda functions have access to the public internet.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**Explaination**: Yes, lambda functions have access to the public internet.

**4. Lambda function connected to private subnet of VPC have direct access to internet?**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**Explaination**: Lambda function doesn't have direct access internet they has to connect through NAT gateway in Public subnet.

**5. Lambda functions in VPC will have access to other AWS services.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**Explaination**: False, user has to provide Endpoint or has to route traffic through NAT Gateway to provide access.

**6. Lambda function in the private subnet of VPC communicates with other AWS services through?**


* [ ] NAT Gateway
* [ ] Internet Endpoint
* [ ] Internet Gateway
* [ ] VPC Endpoint

**Correct answer:**
* [x] VPC Endpoint

**Explaination**: VPC Endpoint - for services in AWS cloud VPC to connect to other services with in AWS private network.

**7. High availability of a lambda function that is connected to VPC is to be managed by ?**


* [ ] Amazon Web Service
* [ ] Third-party service
* [ ] User
* [ ] None of the options

**Correct answer:**
* [x] User

**Explaination**: Lambda functions has to be connected to VPC subnets based on requirements by Customer or User.user has to select subnets of different regions for higher availability.


**8. What is the managed network resource that the Lambda service creates while connecting to VPC ?**


* [ ] Elastic Network Interface (ENI)
* [ ] VPC Endpoint
* [ ] Internet Gateway
* [ ] NAT Gateway

**Correct answer:**
* [x] Elastic Network Interface (ENI)

**Explaination**: The ENI is a managed network resource that the Lambda service creates and manages.

**9. How does the default Lambda function communicate with resources in the private subnet of VPC?**


* [ ] Interface Endpoint
* [ ] Gateway Endpoint
* [ ] AWS Direct Connect
* [ ] None of the options

**Correct answer:**
* [x] Interface Endpoint

---


## AWS-Events & Access Permissions

**1. Which Invocation model type allows you to designate destinations for successful and unsuccessful queueing?**


* [ ] Streams
* [ ] Queues
* [ ] Synchronous
* [ ] Asynchronous

**Correct answer:**
* [x] Asynchronous

**Explaination**: After invoking a function asynchronously, one don't wait for a response from the function code destinations will be configured for success or failure of execution.

**2. Number of times maximum a lambda function will retry for asynchronous invocation failure ?**


* [ ] 1
* [ ] 2
* [ ] 3
* [ ] 4

**Correct answer:**
* [x] 2

**Explaination**: The number of times Lambda retries when the function returns an error is between 0 and 2 based up on configuration.

**3. Identify the AWS service trigger for synchronous invocation source ?**


* [ ] Amazon SNS
* [ ] Amazon S3
* [ ] Amazon Event Bridge
* [ ] Amazon CloudFront

**Correct answer:**
* [x] Amazon CloudFront

**Explaination**: Amazon SNS, Amazon S3 & Amazon Event Bridge are Asynchronous invocation model sources.
Amazon CloudFront is Synchronous model source.

**4. Identify the AWS services that can be configured as destinations for asynchronous invocation ?**


* [ ] Amazon SQS
* [ ] Amazon SNS
* [ ] AWS Lambda
* [ ] Amazon EventBridge
* [ ] All options are correct

**Correct answer:**
* [x] All options are correct

**5. Which lambda event model allows sources to directly trigger a function ?**


* [ ] Push
* [ ] Pull
* [ ] All options are correct
* [ ] None of the options are correct

**Correct answer:**
* [x] Push

**Explaination**: PUSH invocation model can directly trigger a lambda function

**6. Identify the two PUSH model types of invocations ?**


* [ ] Synchronous
* [ ] Asynchronous
* [ ] Both
* [ ] None

**Correct answer:**
* [x] Both

**Explaination**: Based on the invocation model Synchronous and Asynchronous invocations are categorized as PUSH type.

**7. Identify the AWS service trigger for Asynchronous invocation source ?**


* [ ] Amazon SNS
* [ ] Amazon CloudFront
* [ ] AWS CloudFormation
* [ ] None of the options

**Correct answer:**
* [x] Amazon SNS

**Explaination**: Amazon CloudFront and Amazon CloudFormation are synchronous invocation model sources.
where Amazon SNS is a Asynchronous Invocation model.

**8. Which invocation model type returns a response back to the source ?**


* [ ] Streams
* [ ] Queues
* [ ] Synchronous
* [ ] Asynchronous

**Correct answer:**
* [x] Synchronous

**Explaination**: After invoking a function synchronously, Lambda runs the function and waits for a response.

**9. Amazon S3 is an example of which Event Source type? Select all that apply ?**


* [ ] Push & Asynchronous
* [ ] Pull & Asynchronous
* [ ] Push & Synchronous
* [ ] Pull & Synchronous

**Correct answer:**
* [x] Push & Asynchronous

**Explaination**: S3 can directly Invoke lambda function and don't wait for response code.

**10. In which model information flows as streams or polls in a queue for lambda function?**


* [ ] Push
* [ ] Pull
* [ ] Both
* [ ] None of the above

**Correct answer:**
* [x] Pull

**Explaination**: Lambda polls for information form streams and queues.

**11. Execution role grants the function permission to access AWS services and resources**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**Explaination**: Lambda function's execution role is an AWS Identity and Access Management (IAM) role that grants the function permission to access AWS services and resources.

**12. Resource based policy allows an AWS service to invoke function.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**Explaination**: Yes, resource-based policy allows an AWS service to invoke function. It can provide cross account access also.

---


## Disaster Recovery Docker Swarm

**1. What is the command to rebalance the docker swarm cluster workloads if absolutely necessary?**


* [ ] docker service update SERVICE-NAME  
* [ ] docker service update --force SERVICE-NAME
* [ ]  docker update service SERVICE-NAME
* [ ] docker update service --force SERVICE-NAME

**Correct answer:**
* [x] docker service update --force SERVICE-NAME

**2. In a Docker swarm cluster, when a failed node is brought back online it is ready to accept new workloads and existing workloads are automatically rebalanced.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**3. We could add a new node to the cluster as a manager but we cannot promote an existing worker node to be the manager.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**4. Which statement best describes Quorum?**


* [ ]  Quorum is the minimum number of nodes that must be available for the cluster to function properly.
* [ ]  In case of 3 manager nodes, the quorum is 3
* [ ]  It is recommended to maintain an odd number of managers to withstand network wide outages.
* [ ]  In case of 5 manager nodes, the quorum is 3

**Correct answer:**
* [x]  Quorum is the minimum number of nodes that must be available for the cluster to function properly.
* [x]  It is recommended to maintain an odd number of managers to withstand network wide outages.
* [x]  In case of 5 manager nodes, the quorum is 3

**5. For any given number of N nodes, What is the quorum value?**


* [ ]  Total number of nodes divided by 3 + 1 (Quorum = (N/3)+1)
* [ ] Total number of nodes divided by 2 + 1 (Quorum = (N/2)+1)
* [ ] Total number of nodes divided by 2 – 1 (Quorum = (N/2)-1)
* [ ]  Total number of nodes divided by 3 – 1 (Quorum = (N/3)-1)

**Correct answer:**
* [x] Total number of nodes divided by 2 + 1 (Quorum = (N/2)+1)

**6. A swarm cluster runs with 5 manager and 5 worker nodes with 10 replicas of an application running across all worker nodes. Which of the below statements are true when 3 manager nodes do go down at the same time.**


* [ ]  Since 2 manager nodes are available the cluster continues to operate normally
* [ ]  Cluster operates in a degraded mode with no management functionalities
* [ ]  The applications continue to work as normal without impacting users
* [ ]  Applications are killed and users are impacted

**Correct answer:**
* [x]  Cluster operates in a degraded mode with no management functionalities
* [x]  The applications continue to work as normal without impacting users

**7. Which of the below configurations can tolerate 3 manager node failures?**


* [ ] 4 Manager 2 Worker Node Cluster 
* [ ]  5 Manager 5 Worker Node Cluster  
* [ ] 6 Manager 5 Worker Node Cluster
* [ ]  7 Manager 3 Worker Node Cluster
* [ ] 7 Manager 5 Worker Node Cluster
* [ ] 8 Manager 6 Worker Node Cluster
* [ ] 8 Manager 2 Worker Node Cluster

**Correct answer:**
* [x]  7 Manager 3 Worker Node Cluster
* [x] 8 Manager 6 Worker Node Cluster
* [x] 8 Manager 2 Worker Node Cluster

**8. You should have at least 3 managers in the swarm cluster to support manager node failures.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**9. What is the command to forcefully create a cluster from its current state?**


* [ ] docker swarm init  
* [ ] docker swarm init --force
* [ ] docker swarm init --force-cluster
* [ ] docker swarm init --force-new-cluster

**Correct answer:**
* [x] docker swarm init --force-new-cluster

**10. What is the command to promote a node to manager in docker swarm cluster?**


* [ ]  docker promote node NODENAME  
* [ ] docker node promote NODENAME
* [ ] docker promote worker node NODENAME
* [ ] docker node promote worker NODENAME

**Correct answer:**
* [x] docker node promote NODENAME

**11. Which of the following statements are true? Select all the answers that apply.**


* [ ]  On every docker host, docker stores data about the object it manages under the /var/lib/docker directory.
* [ ] On a swarm manager node, it stores data about the swarm cluster in the /var/lib/docker/swarm directory.
* [ ]  On every docker host, docker stores data about the object it manages under the /var/run/docker directory.
* [ ]  On a swarm manager node, it stores data about the swarm cluster in the /var/run/docker/swarm directory.

**Correct answer:**
* [x]  On every docker host, docker stores data about the object it manages under the /var/lib/docker directory.
* [x] On a swarm manager node, it stores data about the swarm cluster in the /var/lib/docker/swarm directory.

**12. What is the command to enable automatic locking of managers with an encryption key?**


* [ ] docker swarm init --lock=true  
* [ ] docker swarm init --autolock=true
* [ ] docker swarm init --autounlock=false  
* [ ] docker swarm init --unlock=false

**Correct answer:**
* [x] docker swarm init --autolock=true

**13. The RAFT DB helps in restoring the services and any other configuration in a swarm cluster.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**14. It is recommended to perform a backup on the swarm leader node.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**15. What is the command to disable auto lock for a docker swarm cluster that has it enabled already?**


* [ ] docker swarm update --autolock=false  
* [ ] docker update swarm --autolock=false
* [ ] docker swarm update --auto-unlock=true
* [ ] docker update swarm --auto-unlock=true

**Correct answer:**
* [x] docker swarm update --autolock=false  

**16. What are the steps that we need to follow to backup the swarm database?**


* [ ]  Create a tar backup of the swarm data at /var/lib/docker/swarm and restart the docker service.
* [ ]  Stop docker service, create a tar backup of the swarm data at /var/lib/docker/swarm, start the docker.
* [ ]   Stop docker service, create a tar backup of the docker data at /var/lib/docker, start the docker
* [ ]  None of the above

**Correct answer:**
* [x]  Stop docker service, create a tar backup of the swarm data at /var/lib/docker/swarm, start the docker.

**17. The auto lock key is backed up along with the Swarm backup.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**18. The auto lock key is required when the cluster is restored, so it must be kept safe in an external password manager.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**19. What are the prerequisites for restoring swarm?**


* [ ]  You must use the same IP as the node from which you made the backup.
* [ ]  You must restore the backup on the same Docker Engine version.
* [ ]  If auto-lock was enabled on the old Swarm, the unlock key is required to perform the restore.
* [ ] You can find the list of manager IP addresses in state.json in the zip file

**Correct answer:**
* [x]  You must use the same IP as the node from which you made the backup.
* [x]  You must restore the backup on the same Docker Engine version.
* [x]  If auto-lock was enabled on the old Swarm, the unlock key is required to perform the restore.
* [x] You can find the list of manager IP addresses in state.json in the zip file

**20. Which of the following steps are required on each manager node to restore data to a new swarm?**


* [ ] Shut down the Docker Engine on the node you selected for the restore
* [ ]   Uninstall Docker on the node
* [ ]  Remove the /var/lib/docker directory on the new Swarm if it exists.
* [ ] Remove the contents of the /var/lib/docker/swarm directory on the new Swarm if it exists.
* [ ]  Restore the /var/lib/docker/swarm directory with the contents of the backup
* [ ]  Install Docker on the node
* [ ] Start Docker on the new node. Unlock the swarm if necessary
* [ ]  Re-initialize the swarm so that the node does not attempt to connect to nodes that were part of the old swarm, and presumably no longer exist.

**Correct answer:**
* [x] Shut down the Docker Engine on the node you selected for the restore
* [x] Remove the contents of the /var/lib/docker/swarm directory on the new Swarm if it exists.
* [x]  Restore the /var/lib/docker/swarm directory with the contents of the backup
* [x] Start Docker on the new node. Unlock the swarm if necessary
* [x]  Re-initialize the swarm so that the node does not attempt to connect to nodes that were part of the old swarm, and presumably no longer exist.

---


## Disaster Recovery UCP

**1. To take a backup of UCP, which docker image would you need to run with the backup command?**


* [ ]  docker/ucp-backup  
* [ ] docker/ucp  
* [ ] docker/backup 
* [ ]  docker/backup-ucp

**Correct answer:**
* [x] docker/ucp  

**2. To restore an existing UCP installation from a backup, you need to uninstall UCP from the swarm by using the uninstall-ucp command.**


* [ ] True
* [ ] false

**Correct answer:**
* [x] True

**3. You can only take backup of UCP via CLI.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**4. Which of the following statements are true about UCP backup?**


* [ ] Backups can be utilized for restoring clusters on a cluster with a newer version of Docker Enterprise.
* [ ]  More than one backup at the same time is supported.
* [ ]  For crashed clusters, backup capability is not guaranteed.
* [ ]  UCP backup includes swarm workloads.
* [ ] UCP backup includes Kubernetes workloads.

**Correct answer:**
* [x]  For crashed clusters, backup capability is not guaranteed.
* [x] UCP backup includes Kubernetes workloads.

**5. Which of the following ways a UCP backup can be created?**


* [ ]  CLI 
* [ ]  GUI  
* [ ] API

**Correct answer:**
* [x]  CLI 
* [x]  GUI  
* [x] API

**6. Which of the following data does Docker Trusted Registry maintain?**


* [ ]  Configurations  
* [ ] Notary Data 
* [ ]  Certificates and Keys
* [ ] Access Control to repos and Images

**Correct answer:**
* [x]  Configurations  
* [x] Notary Data 
* [x]  Certificates and Keys
* [x] Access Control to repos and Images

**7. In order to take backup of UCP, you need to backup each UCP manager node.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**8. Which of the following are included in a UCP backup?**


* [ ] User, Team and Organization details  
* [ ] Docker Swarm Services 
* [ ]  Kubernetes Namespaces
* [ ] Certificates and Keys
* [ ] Access Control Details
* [ ]  Overlay Networks  
* [ ] Docker Images
* [ ] Docker Swarm Secrets

**Correct answer:**
* [x] User, Team and Organization details  
* [x]  Kubernetes Namespaces
* [x] Certificates and Keys
* [x] Access Control Details

**9. What is the command to perform a backup of DTR node?**


* [ ]  Run the docker/dtr backup command
* [ ]  Run the docker/dtr-backup command
* [ ] Run the docker/backup-dtr command
* [ ]  Run the docker/backup dtr command

**Correct answer:**
* [x]  Run the docker/dtr backup command

**10. To create a backup of DTR, you don’t need to backup the DTR metadata, only backing up image content is enough.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**11. What is the recommended approach of taking a backup of images stored by Docker Trusted Registry?**


* [ ]  Store image data on local disk and backup image and DTR metadata together into a tarball
* [ ]  Store image data on a shared network storage and use supported backup mechanisms available for that network storage

**Correct answer:**
* [x]  Store image data on a shared network storage and use supported backup mechanisms available for that network storage

**12. Since you need your DTR replica ID during a backup, which of the following covers a few ways for you to determine your replica ID?**


* [ ]  UCP web interface  
* [ ] UCP client bundle 
* [ ]  SSH Access

**Correct answer:**
* [x]  UCP web interface  
* [x] UCP client bundle 
* [x]  SSH Access

**13. What is the command to restore the DTR from a backup tar (e.g dtr-metadata-backup.tar) ?**


* [ ] docker run -i --rm docker/dtr-restore < dtr-metadata-backup.tar
* [ ]  docker run -i --rm docker/dtr restore < dtr-metadata-backup.tar
* [ ] docker run -i --rm docker/restore-dtr < dtr-metadata-backup.tar
* [ ]  docker run -i --rm docker/restore dtr < dtr-metadata-backup.tar

**Correct answer:**
* [x]  docker run -i --rm docker/dtr restore < dtr-metadata-backup.tar

---


## Disaster Recovery

**1. You should have at least 3 managers in the swarm cluster to support manager node failures.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**2. What is the command to restore the DTR from a backup tar (e.g dtr-metadata-backup.tar) ?**


* [ ] docker run -i --rm docker/dtr-restore < dtr-metadata-backup.tar
* [ ] docker run -i --rm docker/dtr restore < dtr-metadata-backup.tar
* [ ] docker run -i --rm docker/restore-dtr < dtr-metadata-backup.tar
* [ ]  docker run -i --rm docker/restore dtr < dtr-metadata-backup.tar

**Correct answer:**
* [x] docker run -i --rm docker/dtr restore < dtr-metadata-backup.tar

**3. To create a backup of DTR, you don’t need to backup the DTR metadata, only backing up image content is enough.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**4. To take a backup of UCP, which docker image would you need to run with the backup command?**


* [ ]  docker/ucp-backup  
* [ ] docker/ucp 
* [ ]  docker/backup
* [ ]  docker/backup-ucp

**Correct answer:**
* [x] docker/ucp 

**5. What is the command to promote a node to manager in docker swarm cluster?**


* [ ]  docker promote node NODENAME 
* [ ]  docker node promote NODENAME
* [ ] docker promote worker node NODENAME
* [ ] docker node promote worker NODENAME

**Correct answer:**
* [x]  docker node promote NODENAME

**6. Which of the following are included in a UCP backup?**


* [ ]  User, Team and Organization details 
* [ ]  Docker Swarm Services
* [ ] Kubernetes Namespaces
* [ ] Certificates and Keys 
* [ ]  Access Control Details
* [ ]  Docker Images

**Correct answer:**
* [x]  User, Team and Organization details 
* [x] Kubernetes Namespaces
* [x] Certificates and Keys 
* [x]  Access Control Details

**7. The auto lock key is required when the cluster is restored, so it must be kept safe in an external password manager.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**8. Which of the below configurations can tolerate 3 manager node failures?**


* [ ]  4 Manager 2 Worker Node Cluster
* [ ]   5 Manager 5 Worker Node Cluster
* [ ]  6 Manager 5 Worker Node Cluster
* [ ]  7 Manager 3 Worker Node Cluster
* [ ] 7 Manager 5 Worker Node Cluster
* [ ] 8 Manager 6 Worker Node Cluster
* [ ] 8 Manager 2 Worker Node Cluster

**Correct answer:**
* [x]  7 Manager 3 Worker Node Cluster
* [x] 7 Manager 5 Worker Node Cluster
* [x] 8 Manager 6 Worker Node Cluster
* [x] 8 Manager 2 Worker Node Cluster

**9. What are the prerequisites for restoring swarm?**


* [ ]  You must use the same IP as the node from which you made the backup.
* [ ]  You must restore the backup on the same Docker Engine version.
* [ ] If auto-lock was enabled on the old Swarm, the unlock key is required to perform the restore.

**Correct answer:**
* [x]  You must use the same IP as the node from which you made the backup.
* [x]  You must restore the backup on the same Docker Engine version.
* [x] If auto-lock was enabled on the old Swarm, the unlock key is required to perform the restore.

**10. Which of the following statements are true? Select all the answers that apply.**


* [ ] On every docker host, docker stores data about the object it manages under the /var/lib/docker directory.
* [ ]  On a swarm manager node, it stores data about the swarm cluster in the /var/lib/docker/swarm directory.
* [ ]  On every docker host, docker stores data about the object it manages under the /var/run/docker directory.
* [ ] On a swarm manager node, it stores data about the swarm cluster in the /var/run/docker/swarm directory.

**Correct answer:**
* [x] On every docker host, docker stores data about the object it manages under the /var/lib/docker directory.
* [x]  On a swarm manager node, it stores data about the swarm cluster in the /var/lib/docker/swarm directory.

**11. What is the command to forcefully create a cluster from its current state?**


* [ ]  docker swarm init  
* [ ] docker swarm init --force
* [ ]  docker swarm init --force-cluster
* [ ]  docker swarm init --force-new-cluster

**Correct answer:**
* [x]  docker swarm init --force-new-cluster

---


## Building a Custom Image-Add on

**1. To what location within the container is the application code copied to?**


* [ ] /opt 
* [ ]  /app 
* [ ]  /root 
* [ ]  /var

**Correct answer:**
* [x] /opt 

**Code**: 
FROM python:3.6
RUN pip install flask
COPY . /opt/
EXPOSE 8080
WORKDIR /opt
ENTRYPOINT ["python", "app.py"]


**2. What is the port of the web application configured for the service to listen within the container?**


* [ ]  8080 
* [ ]  5000  
* [ ] 80  
* [ ] 0.0.0.0

**Correct answer:**
* [x]  8080 

**Code**: 
FROM python:3.6
RUN pip install flask
COPY . /opt/
EXPOSE 8080
WORKDIR /opt
ENTRYPOINT ["python", "app.py"]

**3. When a container is created using the image built with the following Dockerfile, what is the command used to RUN the application inside it.**


* [ ] pip install flask 
* [ ]  docker run app.py
* [ ] app.py 
* [ ]  python app.py

**Correct answer:**
* [x]  python app.py

**Code**: 
FROM python:3.6
RUN pip install flask
COPY . /opt/
EXPOSE 8080
WORKDIR /opt
ENTRYPOINT ["python", "app.py"]


**4. Refer to the below Dockerfile and answer the following questions: What is the parent image from which this application is created?**


* [ ] ubuntu:latest  
* [ ] python  
* [ ] centos:7 
* [ ]  python:3.6

**Correct answer:**
* [x]  python:3.6

**Code**: 

FROM python:3.6
RUN pip install flask
COPY . /opt/
EXPOSE 8080
WORKDIR /opt
ENTRYPOINT ["python", "app.py"]

---


## Understanding Lambda

**1. Which of the following are Pull Source Model types? Select all that apply.**


* [ ] Streams
* [ ] Queues
* [ ] Synchronous
* [ ] Asynchronous

**Correct answer:**
* [x] Streams
* [x] Queues

**Explaination**: Streams and Queues are the pull model source types.

**2. Which Push Model type allows you to designate destinations for successful and unsuccessful?**


* [ ] Streams
* [ ] Queues
* [ ] Synchronous
* [ ] Asynchronous

**Correct answer:**
* [x] Asynchronous

**Explaination**: In Asynchronous type we can designate destinations depending on successful and unsuccessful invocations.

**3. Which Lambda Event model allows sources to directly trigger a function?**


* [ ] Push
* [ ] Pull
* [ ] All of the above
* [ ] None of the above

**Correct answer:**
* [x] Push

**Explaination**: Correct answer is Push type event model.

**4. Which Push Event Model Type returns a response back to the source?**


* [ ] Streams
* [ ] Queues
* [ ] Synchronous
* [ ] Asynchronous

**Correct answer:**
* [x] Synchronous

**Explaination**: In synchronous invocation Lambda runs the function and waits for a response.

**Documentation Link**: https://docs.aws.amazon.com/lambda/latest/dg/invocation-sync.html

**5. Which of the following languages does Lambda support?**


* [ ] Python
* [ ] Java
* [ ] Node.js
* [ ] All of the above

**Correct answer:**
* [x] All of the above

**Explaination**: Java, Go, PowerShell, Node.js, C#, Python, and Ruby etc. are supported by AWS Lambda

**Documentation Link**: https://aws.amazon.com/lambda/faqs/#:~:text=AWS%20Lambda%20natively%20supports%20Java,our%20documentation%20on%20using%20Node.

**6. Which type of access permission allows an AWS Service to start a Lambda function?**


* [ ] Execution role
* [ ] Invocation permission
* [ ] Trust policy
* [ ] Service Control policy

**Correct answer:**
* [x] Invocation permission

**Explaination**: Invocation permissions can be give via resource based policies.

**Documentation Link**: https://docs.aws.amazon.com/lambda/latest/dg/access-control-resource-based.html

**7. What does Lambda require to take an action on another AWS Service?**


* [ ] Execution role
* [ ] Invocation permission
* [ ] Function handler
* [ ] Service Control policy

**Correct answer:**
* [x] Execution role

**Explaination**: one must include necessary permissions in the execution role's policy of lambda function to take action on other AWS services.

**Documentation Link**: https://docs.aws.amazon.com/lambda/latest/dg/lambda-permissions.html

**8. Amazon S3 is an example of which Event Source type? Select all that apply.**


* [ ] Push
* [ ] Pull
* [ ] Synchronous
* [ ] Asynchronous

**Correct answer:**
* [x] Push
* [x] Asynchronous

**Explaination**: AWS services, such as Amazon S3 and Amazon SNS invoke functions asynchronously to process events.

**Documentation Link**: https://docs.aws.amazon.com/lambda/latest/dg/invocation-async.html

**9. What benefits does the Lambda free tier provide per month?**


* [ ] 1 million free requests per month and 400,000 Gigabit-seconds
* [ ] 1 million free requests per year and 40,000 Gigabit-seconds
* [ ] 10 million free requests per month and 400,000 Gigabit minutes
* [ ] 10 million free requests per month and 40000 Gigabit minutes

**Correct answer:**
* [x] 1 million free requests per month and 400,000 Gigabit-seconds

**Explaination**: Lambda free tier includes one million free requests per month and 400,000 GB-seconds of compute time per month.

**Documentation Link**: https://aws.amazon.com/lambda/pricing/

---


## Configuring Lambda

**1. What is the maximum RAM that you can configure to a Lambda function?**


* [ ] 1 GB
* [ ] 10 GB
* [ ] 20 GB
* [ ] 512 MB

**Correct answer:**
* [x] 10 GB

**2. Which of these is a default Lambda metric when using AWS CloudWatch?**


* [ ] Number of requests
* [ ] Invocation duration per request
* [ ] Number of requests with errors
* [ ] Invocation request per user

**Correct answer:**
* [x] Number of requests
* [x] Invocation duration per request
* [x] Number of requests with errors

**3. What is the maximum runtime for a Lambda function?**


* [ ] 5 minutes
* [ ] 15 minutes
* [ ] 30 minutes
* [ ] 1 hour

**Correct answer:**
* [x] 15 minutes

**4. How many Lambda functions can run concurrently by default?**


* [ ] 1 million
* [ ] 10 million
* [ ] 10,000
* [ ] 1000

**Correct answer:**
* [x] 1000

**5. Which AWS service can provide a visual map of the application path for Lambda?**


* [ ] X-Ray
* [ ] Lambda Insights
* [ ] IAM
* [ ] CloudTrail

**Correct answer:**
* [x] X-Ray

**6. Which of these can you view using the Lambda Insights extension for CloudWatch?**


* [ ] A single Lambda function
* [ ] An aggregate view of all functions in an account
* [ ] An aggregate view of all functions in a region

**Correct answer:**
* [x] A single Lambda function
* [x] An aggregate view of all functions in an account
* [x] An aggregate view of all functions in a region

**7. How do you increase the concurrency limit?**


* [ ] Use the Advanced Configuration options to Increase the setting.
* [ ] Increase the amount of RAM configuration setting.
* [ ] Decrease the amount of RAM configuration setting.
* [ ] Open a quota increase case in the Service Quotas dashboard.

**Correct answer:**
* [x] Open a quota increase case in the Service Quotas dashboard.

---


## Lambda Advanced Topics

**1. What do you need to add so as to provide internet connectivity to your functions after moving Lambda to your private VPC?**


* [ ] Execution role
* [ ] NAT gateway
* [ ] Reserved concurrency
* [ ] Virtual Private Network

**Correct answer:**
* [x] NAT gateway

**Explaination**: To give your function access to the internet, route outbound traffic to a NAT gateway in a public subnet. The NAT gateway has a public IP address and can connect to the internet.

**Documentation Link**: https://docs.aws.amazon.com/lambda/latest/dg/foundation-networking.html#:~:text=To%20give%20your%20function%20access%20to%20the%20internet%2C%20route%20outbound%20traffic%20to%20a%20NAT%20gateway%20in%20a%20public%20subnet.%20The%20NAT%20gateway%20has%20a%20public%20IP%20address%20and%20can%20connect%20to%20the%20internet%20through%20the%20VPC%27s%20internet%20gateway.

**2. Which of the following results in increased costs?**


* [ ] Interface Endpoints
* [ ] Provisioned Concurrency
* [ ] Reserved Concurrency

**Correct answer:**
* [x] Interface Endpoints
* [x] Provisioned Concurrency

**Explaination**: Provisioned concurrency initializes a requested number of execution environments so that they are prepared to respond immediately to your function's invocations. Configuring provisioned concurrency incurs charges to your AWS account.

Standard pricing for AWS PrivateLink applies to interface endpoints for Lambda. Your AWS account is billed for every hour an interface endpoint is provisioned in each Availability Zone and for data processed through the interface endpoint.

**3. What must be done to provide High Availability after you move Lambda to your private VPC?**


* [ ] Include multiple subnets in multiple availability zones
* [ ] Configure IAM permissions and policies
* [ ] Push the code to an Elastic Container Registry repository
* [ ] Add a NAT Gateway to VPC

**Correct answer:**
* [x] Include multiple subnets in multiple availability zones

**Explaination**: The high availability of the Lambda service depends upon access to multiple Availability Zones within the Region where your code runs. When you create a Lambda function without a VPC configuration, it’s automatically available in all Availability Zones within the Region. When you set up VPC access, you choose which Availability Zones the Lambda function can use. As a result, to provide continued high availability, ensure that the function has access to at least two Availability Zones.

**Documentation Link**: https://docs.aws.amazon.com/lambda/latest/operatorguide/networking-vpc.html#:~:text=The%20high%20availability,two%20Availability%20Zones.

**4. Which of these AWS components allows you to connect your private VPC to the Lambda services VPC?**


* [ ] Elastic Container Registry
* [ ] Availability Zones
* [ ] Interface Endpoint
* [ ] Virtual Private Network

**Correct answer:**
* [x] Interface Endpoint

**Explaination**: An interface VPC endpoint allows you to privately connect your Amazon VPC to AWS Lambda services VPC.

**Documentation Link**: https://docs.aws.amazon.com/lambda/latest/dg/configuration-vpc-endpoints.html#:~:text=To%20establish%20a%20private%20connection%20between%20your%20VPC%20and%20Lambda%2C%20create%20an%20interface%20VPC%20endpoint.

**5. Which of these services provides repositories for containers in AWS?**


* [ ] Elastic Container Service
* [ ] Elastic Container Registry
* [ ] Simple Storage Service
* [ ] DynamoDB

**Correct answer:**
* [x] Elastic Container Registry

**Explaination**: Amazon Elastic Container Registry (Amazon ECR) is an AWS managed container image registry service that is secure, scalable, and reliable.

**Documentation Link**: https://docs.aws.amazon.com/AmazonECR/latest/userguide/what-is-ecr.html

**6. How do you increase the Unreserved Concurrency limit indefinitely?**


* [ ] Move your Lambda function to your private VPC
* [ ] Use the Concurrency settings in the Configuration tab
* [ ] Open a support case to submit a request
* [ ] Do nothing. Unreserved Concurrency is automatically increased when you need it

**Correct answer:**
* [x] Open a support case to submit a request

**Explaination**: To increase your Lambda function's concurrency limit, you must open a quota increase support case in the Service Quotas dashboard.

**Documentation Link**: https://aws.amazon.com/premiumsupport/knowledge-center/lambda-concurrency-limit-increase/

**7. How do you prevent execution latency caused by Lambda function cold start delays?**


* [ ] Increase the number of functions
* [ ] Increase the Unreserved Concurrency limit for the function
* [ ] Increase the Reserved Concurrency limit for the function
* [ ] Increase the Provisioned Concurrency limit for the function

**Correct answer:**
* [x] Increase the Provisioned Concurrency limit for the function

**Explaination**:  Provisioned concurrency initializes a requested number of execution environments so that they are prepared to respond immediately to your function's invocations. 

**Documentation Link**: https://docs.aws.amazon.com/lambda/latest/dg/provisioned-concurrency.html

**8. How does increasing the Reserved Concurrency limit affect the Unreserved Concurrency limit?**


* [ ] The Reserved Concurrency limit is subtracted from the Unreserved Concurrency limit.
* [ ] The Reserved Concurrency limit is added to the Unreserved Concurrency limit.
* [ ] The Reserved Concurrency limit is divided by the Unreserved Concurrency limit.
* [ ] The Reserved Concurrency limit has no impact on the Unreserved Concurrency limit.

**Correct answer:**
* [x] The Reserved Concurrency limit is subtracted from the Unreserved Concurrency limit.

**Explaination**: Reserved concurrency guarantees the maximum number of concurrent instances for the function. The reserved concurrency is deducted from the overall capacity for the AWS account in a given Region. 

**Documentation Link**: https://docs.aws.amazon.com/lambda/latest/operatorguide/reserved-concurrency.html

**9. What is the maximum file size for containers deployed to Lambda?**


* [ ] 512MB
* [ ] 250MB
* [ ] 1GB
* [ ] 10GB

**Correct answer:**
* [x] 10GB

**Explaination**: Lambda supports a maximum uncompressed image size of 10 GB, including all layers.

**Documentation Link**: https://docs.aws.amazon.com/lambda/latest/dg/images-create.html#:~:text=The%20container%20image%20must%20be%20able%20to%20run%20on%20a%20read%2Donly%20file%20system.%20Your%20function%20code%20can%20access%20a%20writable%20/tmp%20directory%20with%20between%20512%20MB%20and%2010%2C240%20MB%2C%20in%201%2DMB%20increments%2C%20of%20storage.

**10. Which of these application deployment methods are supported by Lambda?**


* [ ] Package in zip files
* [ ] Package in containers
* [ ] Both Package in zip files and Package in containers

**Correct answer:**
* [x] Both Package in zip files and Package in containers

**Explaination**: Lambda supports two types of deployment packages: container images and . zip file archives

**Documentation Link**: https://docs.aws.amazon.com/lambda/latest/dg/gettingstarted-package.html#:~:text=Your%20AWS%20Lambda%20function%27s%20code%20consists%20of%20scripts%20or%20compiled%20programs%20and%20their%20dependencies.%20You%20use%20a%20deployment%20package%20to%20deploy%20your%20function%20code%20to%20Lambda.%20Lambda%20supports%20two%20types%20of%20deployment%20packages%3A%20container%20images%20and%20.zip%20file%20archives.

---


## Practice Test Docker Swarm

**1. The communication between the nodes in the swarm cluster are not secured by default.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**2. You could restrict the resources assigned to a service using the … property in the docker stack.**


* [ ]  resources  
* [ ] replicas  
* [ ] constrains 
* [ ]  restrict

**Correct answer:**
* [x]  resources  

**3. Which command can be used to remove a kubeapp stack?**


* [ ] docker stack deploy kubeapp 
* [ ]  docker stack ls kubeapp 
* [ ]  docker stack services kubeapp
* [ ] docker stack rm kubeapp

**Correct answer:**
* [x] docker stack rm kubeapp

**4. Which command can be used to promote worker2 to a manager node? Select the right answer.**


* [ ] docker promote node worker2  
* [ ] docker node promote worker2  
* [ ] docker swarm node promote worker2  
* [ ] docker swarm promote node worker2

**Correct answer:**
* [x] docker node promote worker2  

**5. A global service will always deploy exactly one instance of the application on all the nodes in the cluster.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**6. The RAFT logs are stored in memory on the manager nodes.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**7. Which command can be used to return the current key which is used inside the cluster ?**


* [ ]  docker swarm lock-key  
* [ ] docker swarm lock --autolock=true
* [ ] docker swarm unlock --autolock=true  
* [ ] docker swarm unlock-key

**Correct answer:**
* [x] docker swarm unlock-key

**8. We want to perform maintenance tasks on node – worker1 – for performing patching and updates. Select the best way to achieve this.**


* [ ] docker node update --availability drain worker1 
* [ ]  docker node update --availability active worker1
* [ ] docker node rm worker1  
* [ ] None of the above

**Correct answer:**
* [x] docker node update --availability drain worker1 

**9. The built-in DNS server in Docker always runs at IP address …**


* [ ]  127.0.0.11 
* [ ]  127.0.0.1 
* [ ]  172.17.0.3  
* [ ] 172.17.0.1

**Correct answer:**
* [x]  127.0.0.11 

**10. What is the command to list the stacks in the Docker host?**


* [ ] docker stack deploy 
* [ ]  docker stack ls  
* [ ] docker stack services 
* [ ]  docker stack ps

**Correct answer:**
* [x]  docker stack ls  

**11. What technologies can be used to group multiple machines together into a single cluster to run applications in the form of containers?**


* [ ]  Swarm  
* [ ] Kubernetes 
* [ ]  Mesos  
* [ ] None of the above

**Correct answer:**
* [x]  Mesos  
* [x] Kubernetes 
* [x]  Swarm  

**12. Which command can be used to increase the number of replicas from 2 to 4 of webapp?**


* [ ] docker service update --from 2 --replicas=4 webapp  
* [ ] docker service update --replicas=4 webapp
* [ ] docker service update --replicas=2 webapp
* [ ] docker service scale --mix=2 --max=4 webapp

**Correct answer:**
* [x] docker service update --replicas=4 webapp

**13. Which command can be used to run an instance on swarm?**


* [ ] docker container run -d cloud 
* [ ]  docker container create cloud
* [ ] docker service create cloud  
* [ ] docker swarm service create cloud

**Correct answer:**
* [x] docker service create cloud  

**14. The … network connects multiple Docker daemons together and enables swarm services to communicate with each other.**


* [ ]  host 
* [ ]  bridge  
* [ ] overlay  
* [ ] none

**Correct answer:**
* [x] overlay  

**15. A swarm cluster consists of at least one manager node and multiple worker nodes.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**16. Create a swarm service redisapp with image redis and expose port 8080 on host to port 6379 in container.**


* [ ] docker container run --name=redisapp -p 8080:6379 redis  
* [ ] docker service create --name=redisapp -p 8080:6379 redis
* [ ] docker service create --name=redisapp -p 8800:6379 --instances=3 redis
* [ ] docker service create --replicas=3 redis

**Correct answer:**
* [x] docker service create --name=redisapp -p 8080:6379 redis

**17. What is the command to remove the existing ingress network using a docker command?**


* [ ] docker network inspect ingress 
* [ ]  docker network -d rm ingress  
* [ ] docker network rm ingress 
* [ ]  docker network create ingress

**Correct answer:**
* [x] docker network rm ingress 

**18. When you create a swarm service and do not connect it to a user-defined overlay network, it connects to the … network by default.**


* [ ]  host 
* [ ]  bridge  
* [ ] macvlan  
* [ ] ingress

**Correct answer:**
* [x] ingress

**19. What is the command to create an overlay network driver called `secure-network`?**


* [ ] docker network create secure-network
* [ ] docker create network secure-network
* [ ] docker network create -d overlay secure-network
* [ ] docker network create overlay secure-network

**Correct answer:**
* [x] docker network create -d overlay secure-network

**20. Map TCP port 80 in the container to port 9595 on the overlay network using the `web-server` image.**


* [ ] docker service create --publish published=9595,target=8080,protocol=tcp web-server
* [ ] docker service create -p 80:9595/tcp web-server
* [ ] docker service create --publish published=80,target=9595,protocol=udp web-server
* [ ] docker service create -p 9595:80/tcp web-server

**Correct answer:**
* [x] docker service create -p 9595:80/tcp web-server

**21. Which formula can be used to calculate the Quorum of N nodes?**


* [ ] N + 1
* [ ] N+1 / 2
* [ ] N / 2 + 1
* [ ] N / 2 - 1

**Correct answer:**
* [x] N / 2 + 1

**22. ... are one or more instances of a single application that runs across the Swarm Cluster.**


* [ ] docker stack
* [ ] services
* [ ] pods
* [ ] none

**Correct answer:**
* [x] services

**23. Create an overlay network that can also be connected by standalone containers that were not created as part of a swarm service.**


* [ ] docker network create --driver overlay --attachable aone-network
* [ ] docker network create --driver overlay --subnet 10.15.0.0/16 aone-network
* [ ] docker network create --driver overlay --opt encrypted aone-network
* [ ] docker network create --driver overlay aone-network

**Correct answer:**
* [x] docker network create --driver overlay --attachable aone-network

**24. What is the command to apply app=node label to worker3 in a swarm cluster?**


* [ ] docker node update --label-add app=node worker3
* [ ] docker node update --label-rm app=node worker3
* [ ] docker service update --labels app=node worker3
* [ ] docker service update --container-label app=node worker3

**Correct answer:**
* [x] docker node update --label-add app=node worker3

**25. Which command can be used to list the tasks that are running as part of a specified service?**


* [ ] docker service ps SERVICE-NAME
* [ ] docker service ls
* [ ] docker container ps SERVICE-NAME
* [ ] docker container ls

**Correct answer:**
* [x] docker service ps SERVICE-NAME

**26. What is the command to delete a `pyapp` service from your cluster?**


* [ ] docker swarm leave pyapp
* [ ] docker service rm pyapp
* [ ] docker service rollback pyapp
* [ ] docker service del pyapp

**Correct answer:**
* [x] docker service rm pyapp

**27. Which command can be used to enable auto lock on an existing swarm?**


* [ ] docker swarm update --autolock=true
* [ ] docker swarm lock --autolock=true
* [ ] docker swarm set --autolock=true
* [ ] docker swarm unlock-key

**Correct answer:**
* [x] docker swarm update --autolock=true

**28. What is the maximum number of managers possible in a swarm cluster?**


* [ ] 5
* [ ] 3
* [ ] 9
* [ ] No limit

**Correct answer:**
* [x] No limit

**29. Which option can be used to control the workload placements with the help of labels and constraints?**


* [ ] replicated services
* [ ] global services
* [ ] placement constraints
* [ ] resource restrictions

**Correct answer:**
* [x] placement constraints

**30. What feature of swarm closely relates to this use case - "If an instance of an application crashes, it is immediately replaced by a new one"?**


* [ ] Rolling updates
* [ ] Self healing
* [ ] Scaling
* [ ] Load Balancing

**Correct answer:**
* [x] Self healing

---


## Quiz - S3 Basics

**1. Objects are the fundamental entities stored in Amazon S3.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**2. What is the maximum supported file/object size in S3?**


* [ ] Unlimited
* [ ] 1 TB
* [ ] 5 TB
* [ ] 10 TB

**Correct answer:**
* [x] 5 TB

**3. What is the storage capacity of S3 Bucket?**


* [ ] Unlimited
* [ ] 20 TB
* [ ] 5 TB
* [ ] 10 TB

**Correct answer:**
* [x] Unlimited

**4. Do bucket names have to be unique globally?**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**5. What is AWS S3 service used for?**


* [ ] Storing and sharing file content
* [ ] DNS hosting
* [ ] Backend as a Service
* [ ] Load balancing

**Correct answer:**
* [x] Storing and sharing file content

**6. Which of the following does S3 Object comprise?**


* [ ] Key
* [ ] Value
* [ ] Version ID
* [ ] All of these

**Correct answer:**
* [x] All of these

**7. What will be the public S3 url for a bucket named `kodekloud` created in AWS  region `us-east-1` to get file `test.html` from that bucket?**


* [ ] `https://kodekloud.s3.us-east-1.amazonaws.com/test.html`
* [ ] `https://kodekloud.s3.amazonaws.com/test.html`
* [ ] `https://amazonaws.com/kodekloud/test.html`
* [ ] `https://s3.kodekloud..amazonaws.com/test.html`

**Correct answer:**
* [x] `https://kodekloud.s3.us-east-1.amazonaws.com/test.html`

**8. S3 replicates objects on multiple devices across a minimum of 3 available zones in a region.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**9. Which is the lowest-cost storage option in S3?**


* [ ] S3 Glacier
* [ ] S3 One Zone-IA
* [ ] S3 Intelligent-Tiering
* [ ] S3 Standard-IA

**Correct answer:**
* [x] S3 Glacier

**10. Which of these storage classes automatically reduces storage cost by intelligently moving data to the most cost-effective access tier?**


* [ ] S3 Glacier
* [ ] S3 Standard-IA
* [ ] S3 One Zone-IA
* [ ] S3 Intelligent-Tiering

**Correct answer:**
* [x] S3 Intelligent-Tiering

---


## Quiz - Hashicorp Packer

**1. What is the responsibility of the Builder stage?**


* [ ] Manage the image artifact after it has been created
* [ ] Create machines and generate images from those machines for various platforms
* [ ] Fetch data to use in a template, including information defined outside of Packer
* [ ] Store metadata about your images

**Correct answer:**
* [x] Create machines and generate images from those machines for various platforms

**2. What is HashiCorp Packer?**


* [ ] A tool that lets you create identical machine images for multiple platforms from a single source template
* [ ]  A tool that lets you build, change, and version cloud and on-prem resources safely and efficiently
* [ ] An identity-based secret and encryption management system
* [ ] A simple and flexible scheduler and orchestrator to deploy and manage containers and non-containerized applications across on-prem and clouds at scale

**Correct answer:**
* [x] A tool that lets you create identical machine images for multiple platforms from a single source template

**3. Which of the following action(s) can be accomplished in the Provisioner stage?**


* [ ] Installing packages
* [ ] Patching the kernel
* [ ] Downloading application code
* [ ] None of the above

**Correct answer:**
* [x] Installing packages
* [x] Patching the kernel
* [x] Downloading application code

**4. What are the challenges of the traditional mutable infrastructure?**


* [ ] Hard to apply configuration changes into a large number of servers
* [ ] Introducing config drift (change is made in one server but not in other servers)
* [ ] Hard to determine the differences among servers along the time
* [ ] Hard to ensure the security of infrastructure

**Correct answer:**
* [x] Hard to apply configuration changes into a large number of servers
* [x] Introducing config drift (change is made in one server but not in other servers)
* [x] Hard to determine the differences among servers along the time

**5. What is the correct order of Packer stages?**


* [ ] Post-Processors -> Builder -> Provisioner
* [ ] Builder -> Provisioner -> Post-Processors
* [ ] Provisioner -> Builder -> Post-Processors
* [ ] Provisioner -> Post-Processors -> Builder

**Correct answer:**
* [x] Builder -> Provisioner -> Post-Processors

**6. What are the benefits of using Packer to implement immutable infrastructure?**


* [ ] We don’t have to worry about config drift.
* [ ] We don’t have to worry about connecting to servers and manually making changes.
* [ ] We don’t have to worry about any mismatches.
* [ ] None of the above

**Correct answer:**
* [x] We don’t have to worry about config drift.
* [x] We don’t have to worry about connecting to servers and manually making changes.
* [x] We don’t have to worry about any mismatches.

**7. Which plugin do we need to use to create a custom Amazon AMI image?**


* [ ] Amazon AMI Builder
* [ ] Amazon Data Sources
* [ ] Amazon AMI Data Source
* [ ] Amazon Import Post-Processor

**Correct answer:**
* [x] Amazon AMI Builder

**8. Post-processors run after builders and provisioners; post-processors are optional.**


* [ ] TRUE
* [ ] FALSE

**Correct answer:**
* [x] TRUE

**9. In the Packer configuration file (.hcl), where can we define the `source_ami` information?**


* [ ] In the source block
* [ ] In the build block
* [ ] In the variable block
* [ ] In the provisioner block

**Correct answer:**
* [x] In the source block

**10. How can we grant permission to Packer to talk to the AWS API on our behalf?**


* [ ] Use static credentials
* [ ] Use environment variables
* [ ] Use shared credentials file
* [ ] None of the above

**Correct answer:**
* [x] Use static credentials
* [x] Use environment variables
* [x] Use shared credentials file

**11. If we want to run some custom commands in our VM image, where should we declare it?**


* [ ] Under packer block
* [ ] Under source block
* [ ] Under provisioner block
* [ ] Under locals block 

**Correct answer:**
* [x] Under provisioner block

**12. Which Packer command allows you to create a custom image from a template?**


* [ ] packer inspect 
* [ ] packer version
* [ ] packer console
* [ ] packer build 

**Correct answer:**
* [x] packer build 

**13. Which Packer command allows you to rewrite HCL2 config files to a canonical format?**


* [ ] packer fix 
* [ ] packer build 
* [ ] packer fmt
* [ ] packer validate

**Correct answer:**
* [x] packer fmt

**14. Which Packer command allows you to install missing plugins or upgrade plugins in our Packer configuration file?**


* [ ] packer hcl2_upgrade
* [ ] packer init
* [ ] packer plugins
* [ ] packer fmt

**Correct answer:**
* [x] packer init

**15. How can you validate Packer configuration?**


* [ ] Run packer validate command against the configuration file.
* [ ] Run packer inspect command against the configuration file.
* [ ] Run packer hcl2_upgrade command against the configuration file.
* [ ] Run packer fix command against the configuration file.

**Correct answer:**
* [x] Run packer validate command against the configuration file.

---


## Quiz - Just Enough Azure for AKS

**1. Which Azure database service provides a fully managed, globally distributed NoSQL database?**


* [ ] Azure SQL Database
* [ ] Azure Cosmos DB
* [ ] Azure Database for MySQL
* [ ] Azure Database for PostgreSQL

**Correct answer:**
* [x] Azure Cosmos DB

**2. Which Azure storage service provides a durable and highly available file system in the cloud?**


* [ ] Azure Queue Storage
* [ ] Azure File Storage
* [ ] Azure Disk Storage
* [ ] Azure Table Storage

**Correct answer:**
* [x] Azure File Storage

**3. Which of the following best describes Azure Landing Zone?**


* [ ] A physical location where Azure data centers are situated
* [ ] A cloud environment in Azure for hosting web applications
* [ ] A set of best practices and guidelines for building a well-architected Azure foundation
* [ ] An Azure service for managing user access and permissions

**Correct answer:**
* [x] A set of best practices and guidelines for building a well-architected Azure foundation

**4. Which Azure service allows you to run Windows and Linux virtual machines?**


* [ ] Azure Functions
* [ ] Azure App Service
* [ ] Azure Virtual Machines
* [ ] Azure Kubernetes Service

**Correct answer:**
* [x] Azure Virtual Machines

**5. What is the recommended approach for implementing governance and security controls in Azure Landing Zone?**


* [ ] Deploying resources directly in the Azure portal without any specific structure
* [ ] Following the Azure Well-Architected Framework and Azure Policy for governance
* [ ] Skipping governance and focusing solely on resource provisioning
* [ ] Using multiple Azure subscriptions for isolation and security

**Correct answer:**
* [x] Following the Azure Well-Architected Framework and Azure Policy for governance

**6. What are the benefits of using Azure Landing Zone? Select all that apply.**


* [ ] Improved security and compliance
* [ ] Reduced costs for Azure resources
* [ ] Increased development speed
* [ ] Consistent governance and management

**Correct answer:**
* [x] Improved security and compliance
* [x] Consistent governance and management

**7. Which Azure service provides a fully managed, relational database service for running SQL Server workloads in the cloud?**


* [ ] Azure Cosmos DB
* [ ] Azure Database for MySQL
* [ ] Azure SQL Database
* [ ] Azure Database for PostgreSQL

**Correct answer:**
* [x] Azure SQL Database

**8. What is the primary purpose of Azure Blob storage?**


* [ ] Storing structured data in tabular format
* [ ] Storing large amounts of unstructured data, such as files and images
* [ ] Hosting relational databases
* [ ] Backing up virtual machines

**Correct answer:**
* [x] Storing large amounts of unstructured data, such as files and images

**9. What is the purpose of Azure Virtual Machine Scale Sets?**


* [ ] To deploy and manage containerized applications in Azure
* [ ] To scale Azure virtual machines horizontally based on demand
* [ ] To monitor the performance of Azure virtual machines
* [ ] To automate the provisioning of Azure virtual machines

**Correct answer:**
* [x] To scale Azure virtual machines horizontally based on demand

**10. Which Azure service provides a platform for building, deploying, and managing containerized applications?**


* [ ] Azure Functions
* [ ] Azure App Service
* [ ] Azure Kubernetes Service (AKS)
* [ ] Azure Batch

**Correct answer:**
* [x] Azure Kubernetes Service (AKS)

---


## Quiz - Building and containerizing sample application

**1. Which editors are commonly used for developing C# web applications?**


* [ ] Visual Studio Code
* [ ] Sublime Text
* [ ] Notepad++
* [ ] All of the options are correct.

**Correct answer:**
* [x] All of the options are correct.

**Explaination**: A range of editors is commonly employed for developing C# web applications. Visual Studio Code, Sublime Text, and Notepad++ are widely utilized among developers for their features such as code editing, syntax highlighting, and extensibility, making them suitable choices for C# web application development.

**2. What is containerization?**


* [ ] A technique to package applications with their dependencies
* [ ] A method to deploy applications to a local machine
* [ ] A process to optimize network performance
* [ ] A tool for managing database migrations

**Correct answer:**
* [x] A technique to package applications with their dependencies

**Explaination**: Containerization is the process of encapsulating an application and its dependencies into a container, allowing it to run consistently across different computing environments.

**3. Which statement correctly describes Dockerfile?**


* [ ] It is a tool for managing Kubernetes clusters.
* [ ] It is a programming language for creating web applications.
* [ ] It is a configuration file used to define the steps to build a Docker image.
* [ ] It is a database management system for containerized environments.

**Correct answer:**
* [x] It is a configuration file used to define the steps to build a Docker image.

**Explaination**: Dockerfile is a text-based configuration file used to define the steps and instructions required to build a Docker image. It specifies the base image, any additional dependencies, configuration settings, and the commands needed to create the desired containerized environment.

**4. Which tool is commonly used to containerize applications?**


* [ ] Docker
* [ ] Ansible
* [ ] Jenkins
* [ ] Kubernetes

**Correct answer:**
* [x] Docker

**Explaination**: Docker is a popular platform that enables developers to build, package, and distribute applications as lightweight, portable containers.

**5. What is the purpose of the "RUN" instruction in a Dockerfile?**


* [ ] It defines the base image for the Docker image.
* [ ] It specifies the exposed ports for the Docker container.
* [ ] It installs dependencies and executes commands during the image build process.
* [ ] It sets environment variables for the Docker container.

**Correct answer:**
* [x] It installs dependencies and executes commands during the image build process.

**Explaination**: The "RUN" instruction in a Dockerfile is used to execute commands during the image build process. It is commonly used to install dependencies, run system updates, configure the environment, and perform any necessary setup tasks within the Docker image before the container is created.

**6. What is a local k8s cluster?**


* [ ] A group of managed Kubernetes clusters in the cloud
* [ ] A networking concept in Kubernetes
* [ ] A storage mechanism for Kubernetes pods
* [ ] A Kubernetes cluster running on a local development machine

**Correct answer:**
* [x] A Kubernetes cluster running on a local development machine

**Explaination**: A local k8s cluster refers to a Kubernetes cluster that is set up and running on a local development machine, allowing developers to test and experiment with Kubernetes locally.

**7. How can you run the sample ASP.NET Core Web App on a local k8s cluster?**


* [ ] Use the Azure portal
* [ ] Use the Azure CLI
* [ ] Use the Kubernetes dashboard
* [ ] Use the kubectl command-line tool

**Correct answer:**
* [x] Use the kubectl command-line tool

**Explaination**: To run the sample ASP.NET Core Web App on a local Kubernetes (k8s) cluster, you would typically use the kubectl command-line tool. It allows you to interact with the Kubernetes cluster, deploy applications, and manage various resources.

**8. What is the purpose of the "ENTRYPOINT" instruction in a Dockerfile?**


* [ ] It defines the base image for the Docker image.
* [ ] It specifies the exposed ports for the Docker container.
* [ ] It sets environment variables for the Docker container.
* [ ] It specifies the command and parameters to be executed when a container is started.

**Correct answer:**
* [x] It specifies the command and parameters to be executed when a container is started.

**Explaination**: The "ENTRYPOINT" instruction in a Dockerfile is used to specify the command and any associated parameters that will be executed when a container is started. This instruction sets the main process to be run within the container, allowing users to define the primary executable or script that forms the core functionality of the containerized application.

**9. Which command is used to build a Docker image from a Dockerfile?**


* [ ] docker create
* [ ] docker build
* [ ] docker run
* [ ] docker push

**Correct answer:**
* [x] docker build

**Explaination**: The "docker build" command is used to build a Docker image from a Dockerfile. It reads the instructions specified in the Dockerfile and executes them, creating a new image based on those instructions. This command is a fundamental step in the process of creating and managing Docker images.

**10. How can you specify the target port within a container when mapping ports using the "docker run" command?**


* [ ] Use the "--expose" flag.
* [ ] Use the "--link" flag.
* [ ] Use the "--target-port" flag.
* [ ] Use the "--publish" or "-p" flag with the "\<container-port\>:\<host-port\>" format.

**Correct answer:**
* [x] Use the "--publish" or "-p" flag with the "\<container-port\>:\<host-port\>" format.

**Explaination**: When using the "docker run" command, you can specify the target port within a container by using the "--publish" or "-p" flag. This flag allows you to map a port on the host system to a port on the container. It follows the format "<container-port>:<host-port>", where the container-port represents the target port within the container that you want to expose and the host-port represents the corresponding port on the host system that will be used for accessing the container's port.

---


## Quiz - Working with AKS

**1. What is the purpose of a Kubernetes namespace?**


* [ ] To separate application tiers within the same cluster
* [ ] To isolate different deployments or teams within the same cluster
* [ ] To allocate specific resources to each node in the cluster 
* [ ] To define access control policies for cluster resources

**Correct answer:**
* [x] To isolate different deployments or teams within the same cluster

**Explaination**: Kubernetes namespaces provide a way to partition or isolate different deployments or teams within the same cluster, allowing for better resource management, access control, and separation of concerns.

**2. How can you expose a service deployed on AKS to external traffic?**


* [ ] By creating an external load balancer service
* [ ] By using an internal load balancer service
* [ ] By directly assigning a public IP to the service
* [ ] By configuring a reverse proxy within the AKS cluster

**Correct answer:**
* [x] By creating an external load balancer service

**Explaination**: To expose a service deployed on AKS to external traffic, you can create an external load balancer service, which automatically provisions a public IP address and routes traffic to the service.

**3. What is Azure Kubernetes Fleet?**


* [ ] A monitoring tool for AKS clusters
* [ ] A load balancing mechanism for AKS
* [ ] A managed service for deploying and managing multiple AKS clusters
* [ ] An automation tool for scaling AKS deployments

**Correct answer:**
* [x] A managed service for deploying and managing multiple AKS clusters

**Explaination**: Azure Kubernetes Fleet is a managed service provided by Azure that allows you to deploy and manage multiple AKS clusters in a centralized manner, providing a unified control plane for managing large-scale Kubernetes deployments.

**4. What is Azure Container Registry (ACR)?**


* [ ] A service for managing Kubernetes clusters
* [ ] A tool for monitoring containerized applications
* [ ] A platform for managing container images
* [ ] A service for managing database migrations

**Correct answer:**
* [x] A platform for managing container images

**Explaination**: Azure Container Registry (ACR) is a managed service provided by Azure that allows you to store, manage, and secure container images for use in containerized applications.

**5. What does ACR stand for in the context of AKS?**


* [ ] Azure Container Repository
* [ ] Azure Container Registry
* [ ] Azure Cluster Resource
* [ ] Azure Container Runtime

**Correct answer:**
* [x] Azure Container Registry

**Explaination**: ACR stands for Azure Container Registry, which is a private registry for storing and managing container images used in AKS deployments.

**6. Which tool is commonly used to deploy applications to an AKS cluster?**


* [ ] Terraform
* [ ] Jenkins
* [ ] Kubernetes Dashboard
* [ ] kubectl

**Correct answer:**
* [x] kubectl

**Explaination**: kubectl is a command-line tool used to deploy and manage applications on Kubernetes clusters, including AKS.

**7. How can you scale the nodes in an AKS cluster using Azure CLI?**


* [ ] Using the "az aks create" command
* [ ] Using the "az aks scale" command
* [ ] Using the "az aks upgrade" command
* [ ] Using the "az aks nodepool" command

**Correct answer:**
* [x] Using the "az aks scale" command

**Explaination**: The "az aks scale" command in Azure CLI is used to scale the number of nodes in an AKS cluster.

**8. How do you push an image to ACR?**


* [ ] Using the "docker push" command
* [ ] Using the "kubectl push" command
* [ ] Using the "az acr push" command
* [ ] Using the "az aks push" command

**Correct answer:**
* [x] Using the "az acr push" command

**Explaination**: The "az acr push" command in Azure CLI is used to push a container image to an Azure Container Registry (ACR).

**9. How do you deploy an AKS cluster?**


* [ ] Using AWS CLI 
* [ ] Using Azure Portal
* [ ] Using Google Cloud Platform
* [ ] Using Azure CLI

**Correct answer:**
* [x] Using Azure CLI

**Explaination**: To deploy an AKS cluster, you can use the Azure CLI (Command-Line Interface) provided by Microsoft Azure.

**10. What is a pod in Kubernetes?**


* [ ] A group of containers that share the same IP address
* [ ] The smallest unit of deployment in Kubernetes
* [ ] An external load balancer for accessing applications
* [ ] A Kubernetes control plane component

**Correct answer:**
* [x] The smallest unit of deployment in Kubernetes

**Explaination**: A pod is the smallest unit of deployment in Kubernetes, representing one or more containers that share the same network namespace and can communicate with each other over the localhost.

**11. How can you enable automatic scaling of pods in AKS?**


* [ ] By modifying the pod definition file manually
* [ ] By using the "kubectl scale" command with the desired number of replicas
* [ ] By configuring the horizontal pod autoscaler (HPA)
* [ ] By adjusting the resource limits of each container

**Correct answer:**
* [x] By configuring the horizontal pod autoscaler (HPA)

**Explaination**: Automatic scaling of pods in AKS can be achieved by configuring the horizontal pod autoscaler (HPA). The HPA automatically adjusts the number of replicas based on CPU or custom metrics, ensuring optimal resource utilization.

**12. In Azure Kubernetes Service (AKS), what is the default namespace where resources are deployed if no namespace is specified?**


* [ ] default 
* [ ] kube-system
* [ ] azure-system
* [ ] kube-public

**Correct answer:**
* [x] default 

**Explaination**: In Azure Kubernetes Service (AKS), the default namespace where resources are deployed if no namespace is specified explicitly is "default". When you create resources like deployments, services, or pods without specifying a namespace, they are automatically deployed to the "default" namespace. Namespaces provide a way to logically isolate and manage resources within a cluster, and "default" is the initial namespace available in AKS clusters.

---


## Quiz - Networking in AKS

**1. What is the purpose of Azure Networking in the context of AKS?**


* [ ] To manage container orchestrations within AKS
* [ ] To create docker container for an application
* [ ] To establish network connectivity between AKS clusters and external resources
* [ ] To automate deployment and scaling of AKS clusters

**Correct answer:**
* [x] To establish network connectivity between AKS clusters and external resources

**Explaination**: Azure Networking in the context of AKS is used to establish network connectivity between AKS clusters and external resources, such as virtual networks, load balancers, or Azure services, enabling communication between containers and external systems.


**2. How can you enforce network policies in AKS?**


* [ ] By configuring ingress controllers for each pod
* [ ] By using Azure Firewall to filter network traffic
* [ ] By creating and applying network policy objects in Kubernetes
* [ ] By defining access control lists (ACLs) at the Azure subscription level

**Correct answer:**
* [x] By creating and applying network policy objects in Kubernetes

**Explaination**: Network policies in AKS are enforced by creating and applying network policy objects in Kubernetes. These objects define the allowed or denied traffic between pods based on labels and selectors.

**3. Which networking option in AKS enables you to limit traffic to specific IP addresses/ranges and ports?**


* [ ] Azure Application Gateway
* [ ] Azure Load Balancer
* [ ] Network Security Groups
* [ ] Virtual Network Service Endpoints

**Correct answer:**
* [x] Network Security Groups

**Explaination**: Network Security Groups (NSGs) in AKS allow you to limit traffic to specific IP addresses or ranges by applying inbound and outbound security rules. NSGs provide a way to control network traffic at the network interface level.

**4. Which networking option allows secure communication between AKS clusters and on-premises resources?**


* [ ] Azure Virtual Network Service Endpoints
* [ ] Azure ExpressRoute
* [ ] Azure VPN Gateway
* [ ] Azure Private Link

**Correct answer:**
* [x] Azure Private Link

**Explaination**: Azure Private Link allows secure and private communication between AKS clusters and on-premises resources by establishing a private connection through the Azure backbone network, without exposing services to the public internet.

**5. Which networking option in AKS allows you to expose services to the public internet with a static public IP address?**


* [ ] Azure Virtual Network Service Endpoints
* [ ] Azure Load Balancer
* [ ] Azure Private Link 
* [ ] Azure Application Gateway 

**Correct answer:**
* [x] Azure Application Gateway 

**Explaination**: Azure Application Gateway in AKS enables you to expose services to the public internet with a static public IP address. It acts as a load balancer and provides advanced application delivery capabilities.

**6. What are the two networking options available for Azure Kubernetes Service (AKS) to manage networking between pods?**


* [ ] Azure Container Networking Interface (CNI) and Azure Virtual Network Service Endpoints
* [ ] Azure Networking and Azure Load Balancer 
* [ ] Kubenet and Azure Container Networking Interface (CNI)
* [ ] Azure VPN Gateway and Azure ExpressRoute

**Correct answer:**
* [x] Kubenet and Azure Container Networking Interface (CNI)

**Explaination**: The two networking options available for AKS to manage networking between pods are Kubenet and Azure Container Networking Interface (CNI). Kubenet is a basic, simpler networking option provided by AKS, which uses a bridge network configuration. CNI, on the other hand, integrates AKS with Azure networking, allowing for more advanced networking capabilities and better integration with Azure services.
Note: When using Kubenet, the AKS cluster is associated with a virtual network, and each node within the cluster is assigned an IP address from the virtual network's subnet. On the other hand, when using CNI, each pod within the cluster gets its own IP address, and the networking is managed by Azure CNI plugin.


**7. What is a network policy in AKS?**


* [ ] A set of rules that control inbound and outbound traffic for AKS pods
* [ ] A policy that determines the maximum number of pods in an AKS cluster
* [ ] A security feature for controlling access to the AKS control plane
* [ ] A policy for automatically scaling AKS nodes based on resource utilization

**Correct answer:**
* [x] A set of rules that control inbound and outbound traffic for AKS pods

**Explaination**: A network policy in AKS is a set of rules that define how inbound and outbound traffic is allowed or denied for pods within the cluster. Network policies provide fine-grained control over communication between pods and can enhance security and isolation.


**8. Which YAML file format is commonly used to define and apply policies in Azure Kubernetes Service (AKS)?**


* [ ] Helm Chart
* [ ] Dockerfile
* [ ] Kubernetes Manifest
* [ ] Azure Resource Manager (ARM) Template

**Correct answer:**
* [x] Kubernetes Manifest

**Explaination**: The YAML file format commonly used to define and apply policies in AKS is the Kubernetes Manifest. Kubernetes Manifests are YAML files that describe the desired state of Kubernetes objects, such as deployments, services, or pods. Policies in AKS can be defined using Manifests to enforce rules and configurations for resource management, security, and networking within the cluster.
Note: Helm Charts are package managers for Kubernetes applications, Dockerfiles are used to build container images, and Azure Resource Manager (ARM) Templates are used for managing Azure resources at a higher level, but they are not specifically used for defining and applying policies in AKS.

**9. What is the difference between Azure Network Policies and Calico Network Policies in Azure Kubernetes Service (AKS) regarding their compatibility with the underlying networking options, Kubenet and Container Networking Interface (CNI)?**


* [ ] Azure Network Policies are specific to AKS and are compatible with both Kubenet and CNI, while Calico Network Policies are designed only for AKS clusters using Kubenet.
* [ ] Both Azure Network Policies and Calico Network Policies are compatible with CNI as the underlying networking option in AKS.
* [ ] Azure Network Policies are designed for AKS clusters using CNI, while Calico Network Policies are compatible with both Kubenet and CNI.
* [ ] Azure Network Policies are compatible with both Kubenet and CNI as the underlying networking options, while Calico Network Policies are designed exclusively for AKS clusters using CNI.

**Correct answer:**
* [x] Azure Network Policies are designed for AKS clusters using CNI, while Calico Network Policies are compatible with both Kubenet and CNI.

**Explaination**: Calico Policies are compatible with both Kubenet and CNI as the underlying networking options in AKS. They can be used to enforce network security policies within the cluster, regardless of whether Kubenet or CNI is being utilized.


**10. In Azure, which statement best describes the relationship between subnets and virtual networks (VNets)?**


* [ ] A subnet is a physical network, while a VNet represents a logical grouping of subnets.
* [ ] A VNet is a physical network, while a subnet represents a logical division within the VNet. 
* [ ] A subnet and a VNet are synonymous terms, representing the same concept in Azure networking.
* [ ] A VNet is a virtual network that can contain multiple subnets, each representing a segmented IP address range within the VNet.

**Correct answer:**
* [x] A VNet is a virtual network that can contain multiple subnets, each representing a segmented IP address range within the VNet.

**Explaination**: In Azure, a VNet (virtual network) is a logical representation of an isolated network infrastructure in the cloud. It serves as a foundational networking construct that can contain multiple subnets.
A subnet, on the other hand, represents a segmented IP address range within the VNet. It is a division of the VNet's IP address space that can be used to group resources or implement network security controls.


---


## Quiz - AKS Security

**1. Which authentication method does AKS support for user access control?**


* [ ] Azure Active Directory (AAD)
* [ ] JSON Web Tokens (JWT)
* [ ] X.509 client certificates
* [ ] All of the options are correct.

**Correct answer:**
* [x] All of the options are correct.

**Explaination**: AKS supports multiple authentication methods, including Azure Active Directory (AAD), JSON Web Tokens (JWT), and X.509 client certificates, allowing for flexible user access control options.

**2. What is Open Service Mesh (OSM) in the context of Azure Kubernetes Service (AKS)?**


* [ ] A tool for managing Azure Active Directory (AAD) integration with AKS
* [ ] An Azure service that provides advanced security features for AKS clusters
* [ ] An open-source, cloud-native service mesh implementation for AKS
* [ ] A built-in feature of AKS for enforcing Azure Policy compliance

**Correct answer:**
* [x] An open-source, cloud-native service mesh implementation for AKS

**Explaination**: Open Service Mesh (OSM) is an open-source, cloud-native service mesh implementation that can be used with AKS. It provides capabilities for managing and securing microservices-based applications running on AKS clusters.

**3. How does Azure Defender for AKS protect against security threats?**


* [ ] By analyzing and securing ingress and egress network traffic
* [ ] By automatically applying Azure Policy compliance rules to AKS clusters
* [ ] By providing encryption of data at rest and in transit within AKS clusters
* [ ] By detecting and responding to security events and vulnerabilities in AKS clusters

**Correct answer:**
* [x] By detecting and responding to security events and vulnerabilities in AKS clusters

**Explaination**: Azure Defender for AKS helps protect against security threats by detecting and responding to security events and vulnerabilities in AKS clusters. It provides real-time threat detection and offers recommendations for remediation.

**4. What does Azure Defender offer in conjunction with AKS?**


* [ ] Advanced networking capabilities for AKS clusters
* [ ] Automatic scaling of AKS clusters based on resource utilization
* [ ] Real-time threat detection and response for AKS clusters
* [ ] Integration with Azure Policy for enforcing compliance in AKS clusters

**Correct answer:**
* [x] Real-time threat detection and response for AKS clusters

**Explaination**: Azure Defender provides real-time threat detection and response capabilities for AKS clusters. It helps identify and protect against common threats and vulnerabilities, providing enhanced security for your AKS deployments.

**5. What is the purpose of Azure Policy for AKS?**


* [ ] To enforce compliance with industry regulations for AKS clusters
* [ ] To automatically scale AKS clusters based on resource utilization
* [ ] To provide advanced networking capabilities for AKS clusters
* [ ] To define and enforce organizational standards and best practices for AKS clusters

**Correct answer:**
* [x] To define and enforce organizational standards and best practices for AKS clusters

**Explaination**: Azure Policy for AKS allows you to define and enforce organizational standards and best practices for AKS clusters. It helps ensure that AKS resources and configurations align with specific policies, improving consistency and compliance across the cluster.

**6. Which of the following components are typically included in a service mesh architecture?**


* [ ] Ingress controller and egress gateway
* [ ] Kubernetes API server and etcd database
* [ ] Pods and containers running the application workloads
* [ ] Data plane and control plane

**Correct answer:**
* [x] Data plane and control plane

**Explaination**: A service mesh architecture typically includes a data plane responsible for handling network traffic between services (e.g., sidecar proxies) and a control plane that manages the configuration and behavior of the data plane. The other options mentioned are not specific to service mesh architecture.

**7. What is the role of Azure Active Directory (AAD) in AKS?**


* [ ] It is used for managing Azure Policy compliance for AKS clusters
* [ ] It enables authentication and authorization of users and applications accessing AKS clusters
* [ ] It provides advanced monitoring and observability features for AKS clusters 
* [ ] It allows integration of AKS clusters with Azure Defender for threat detection and response

**Correct answer:**
* [x] It enables authentication and authorization of users and applications accessing AKS clusters

**Explaination**: Azure Active Directory (AAD) enables authentication and authorization of users and applications accessing AKS clusters. It provides identity and access management capabilities, allowing fine-grained control (using RBAC) over who can access the cluster and what actions they can perform.

**8. How does Azure Policy complement Azure Defender in securing AKS clusters?**


* [ ] Azure Policy enforces compliance rules for AKS clusters, while Azure Defender provides threat detection and response capabilities.
* [ ] Azure Policy provides encryption for data at rest and in transit within AKS clusters, while Azure Defender focuses on network security.
* [ ] Azure Policy automatically scales AKS clusters based on resource utilization, while Azure Defender monitors security events.
* [ ] Azure Policy and Azure Defender serve the same purpose and have overlapping functionalities in securing AKS clusters.

**Correct answer:**
* [x] Azure Policy enforces compliance rules for AKS clusters, while Azure Defender provides threat detection and response capabilities.

**Explaination**: Azure Policy and Azure Defender have different roles in securing AKS clusters. Azure Policy focuses on enforcing compliance rules and organizational standards, ensuring that AKS clusters meet specific requirements. Azure Defender, on the other hand, provides threat detection and response capabilities, identifying security events and vulnerabilities. Both tools work together to enhance the security posture of AKS clusters.

---


## Quiz - Observability

**1. What is the purpose of Prometheus in AKS?**


* [ ] It is a container registry for storing Docker images
* [ ] It is a container orchestration platform for AKS
* [ ] It is a monitoring and alerting system for collecting metrics
* [ ] It is a service mesh for managing network traffic

**Correct answer:**
* [x] It is a monitoring and alerting system for collecting metrics

**Explaination**: Prometheus is a popular open-source monitoring and alerting system commonly used in AKS. It is designed for collecting metrics from applications and infrastructure components, enabling observability and performance monitoring.

**2. What is Grafana used for in AKS?**


* [ ] Managing and scaling AKS clusters
* [ ] Storing and querying log data in AKS
* [ ] Visualizing metrics and creating dashboards
* [ ] Automating CI/CD workflows in AKS

**Correct answer:**
* [x] Visualizing metrics and creating dashboards

**Explaination**: Grafana is a popular open-source platform used for visualizing metrics and creating dashboards. It is commonly integrated with Prometheus in AKS to provide a user-friendly interface for monitoring and visualization.

**3. Which Azure service is commonly used for logging and monitoring in AKS?**


* [ ] Azure Container Registry (ACR)
* [ ] Azure Monitor
* [ ] Azure Virtual Machine Scale Sets (VMSS)
* [ ] Azure Key Vault

**Correct answer:**
* [x] Azure Monitor

**Explaination**: Azure Monitor is commonly used for logging and monitoring in AKS. It provides a comprehensive solution for collecting, analyzing, and acting on telemetry data from AKS clusters.

**4. Which query language is commonly used with Prometheus for querying metrics data?**


* [ ] SQL
* [ ] JSON
* [ ] YAML
* [ ] PromQL

**Correct answer:**
* [x] PromQL

**Explaination**: Prometheus uses its own query language called PromQL (Prometheus Query Language) for querying metrics data. It provides a powerful and flexible syntax for analyzing and querying time-series data.

**5. How can you configure Azure Monitor for AKS?**


* [ ] Use the Azure portal and enable Azure Monitor for Containers integration
* [ ] Install and configure a third-party monitoring agent in AKS
* [ ] Run a custom script in AKS to collect and send metrics to Azure Monitor
* [ ] Azure Monitor for AKS is enabled by default and doesn't require any additional configuration

**Correct answer:**
* [x] Use the Azure portal and enable Azure Monitor for Containers integration

**Explaination**: To configure Azure Monitor for AKS, you can use the Azure portal and enable the Azure Monitor for Containers integration for your AKS cluster. This enables the collection and analysis of container metrics.

**6. What is the primary benefit of using Azure Monitor for AKS?**


* [ ] Centralized logging and monitoring for AKS clusters
* [ ] Improved scalability and performance of AKS clusters 
* [ ] Seamless integration with Azure DevOps for CI/CD pipelines 
* [ ] Simplified management and configuration of AKS clusters

**Correct answer:**
* [x] Centralized logging and monitoring for AKS clusters

**Explaination**: The primary benefit of using Azure Monitor for AKS is centralized logging and monitoring. It provides a unified solution for collecting, analyzing, and acting on telemetry data from AKS clusters, enabling efficient troubleshooting and performance monitoring.

**7. Which component of Azure Monitor is responsible for collecting metrics from AKS?**


* [ ] Azure Log Analytics
* [ ] Azure Application Insights 
* [ ] Azure Monitor Logs
* [ ] Azure Monitor for Containers

**Correct answer:**
* [x] Azure Monitor for Containers

**Explaination**: Azure Monitor for Containers is the component of Azure Monitor responsible for collecting metrics from AKS. It collects performance and diagnostic data from the containers running in AKS clusters.

**8. How can you view logs in AKS using Container Insights?**


* [ ] Use the Azure portal and navigate to the AKS cluster's Container Insights page
* [ ] Run kubectl logs command with the appropriate parameters
* [ ] Use the Azure CLI and run az aks browse command
* [ ] Install a separate log management tool like Elasticsearch and Kibana

**Correct answer:**
* [x] Use the Azure portal and navigate to the AKS cluster's Container Insights page

**Explaination**: To view logs in AKS using Container Insights, you can use the Azure portal and navigate to the Container Insights page of your AKS cluster. It provides a user-friendly interface to explore and analyze logs.

---


## Quiz - Basics of AWS CodePipeline

**1. Which of the following are Git-compatible repositories?**


* [ ] AWS CodeCommit​
* [ ] GitHub
* [ ] S3​
* [ ] AWS CodeCommit​ and GitHub

**Correct answer:**
* [x] AWS CodeCommit​ and GitHub

**2. Which service can be used to log API calls between CodePipeline and CodeBuild?**


* [ ] EventBridge​
* [ ] CodeCommit​
* [ ] CloudTrail​
* [ ] CodeDeploy​

**Correct answer:**
* [x] CloudTrail​

**3. Which AWS service can be used to add program invocations to your pipeline?​**


* [ ] S3​
* [ ] SNS​
* [ ] Lambda​
* [ ] EC2​

**Correct answer:**
* [x] Lambda​

**4. Which of these statements is Not True?​**


* [ ] You are limited to micro instance sizes when using EC2 free tier.​
* [ ] No fee is charged for using CodeDeploy to deploy to AWS EC2 instances. ​
* [ ] When using CodePipeline, any additional AWS service you use is free.​
* [ ] CodeBuild allows 100 free build minutes per month.​

**Correct answer:**
* [x] When using CodePipeline, any additional AWS service you use is free.​

**5. Which deployment methods are possible with CodePipeline?​**


* [ ] Containers​
* [ ] Virtual servers​
* [ ] Serverless​
* [ ] All options are correct

**Correct answer:**
* [x] All options are correct

**6. Which stage event can be monitored using EventBridge?​**


* [ ] Started​
* [ ] Stopped​
* [ ] Stopping​
* [ ] All options are correct

**Correct answer:**
* [x] All options are correct

**7. Which AWS service can be used to add automatic notifications for your pipeline?​**


* [ ] S3​
* [ ] SNS​
* [ ] Lambda​
* [ ] EC2​

**Correct answer:**
* [x] SNS​

**8. What is the cost of two pipelines that have been actively making changes for more than a month within your AWS account?**


* [ ] Free​
* [ ] $1 per month
* [ ] $2 per month​
* [ ] $200 per month​

**Correct answer:**
* [x] $1 per month

**9. What can be used to provide long-term credentials for CodePipeline?​**


* [ ] IAM roles​
* [ ] IAM users​
* [ ] IAM groups​
* [ ] IAM users and IAM groups

**Correct answer:**
* [x] IAM users and IAM groups

**10. Which of the following is a fully managed repository that is scalable and highly available?​**


* [ ] GitHub​
* [ ] CodeCommit​
* [ ] Bitbucket​
* [ ] CodeBuild​

**Correct answer:**
* [x] CodeCommit​

**11. Which products or services can be integrated with CodePipeline at the build stage?​**


* [ ] Jenkins​
* [ ] XebiaLabs​
* [ ] CodeBuild​
* [ ] CodeBuild​ and Jenkins​

**Correct answer:**
* [x] CodeBuild​ and Jenkins​

**12. Which of the following are the benefits of using CodePipeline?​**


* [ ] Rapid deployment​
* [ ] Ability to maintain high quality ​
* [ ] Approvals and invocations can be added to your pipeline​
* [ ] All options are correct

**Correct answer:**
* [x] All options are correct

**13. Where can you store credentials and passwords that you want to keep confidential?**


* [ ] AWS S3​
* [ ] AWS Identity and Access Management​
* [ ] AWS Secrets​
* [ ] Hardcode them into the program​

**Correct answer:**
* [x] AWS Secrets​

---


## Quiz – CI/CD Pipeline with CodeCommit, CodeBuild and CodeDeploy

**1. What is required on an EC2 instance when using it with CodeDeploy in a pipeline?​**


* [ ] Install the SSM management agent​
* [ ] Install the CodeDeploy agent​
* [ ] Encrypt the drive using AWS KMS​
* [ ] Enable programmatic access to the instance​

**Correct answer:**
* [x] Install the CodeDeploy agent​

**2. Which of the following is a stage naming restriction?​**


* [ ] Stage names must match those used in other pipelines in the same AWS account.​
* [ ] Stage names must be unique across all pipelines within a single AWS account.​
* [ ] Stage names must be unique within a single pipeline within an AWS account.​
* [ ] Stage names must be unique across all customer accounts within the AWS cloud.

**Correct answer:**
* [x] Stage names must be unique within a single pipeline within an AWS account.​

**3. What S3 feature must be enabled when using it as the source repository within CodePipeline?**


* [ ] Versioning​
* [ ] Object locking
* [ ] Public access
* [ ] Cross-region replication​

**Correct answer:**
* [x] Versioning​

**4. Which of the following are Git-based repositories?​**


* [ ] S3​
* [ ] CodeCommit​
* [ ] Bitbucket​
* [ ] CodeCommit​ and Bitbucket​

**Correct answer:**
* [x] CodeCommit​ and Bitbucket​

**5. What is the maximum number of stages you can configure in CodePipeline?​**


* [ ] 4
* [ ] 50
* [ ] 100
* [ ] 500

**Correct answer:**
* [x] 50

**6. How do you increase the number of pipelines after you’ve reached the default limit?​**


* [ ] Configure an increase in the AWS console
* [ ] Create a new AWS account to hold the additional pipelines
* [ ] Use the Support Center console to request an increase
* [ ] Provision multiple CodePipeline services in the same region and account

**Correct answer:**
* [x] Use the Support Center console to request an increase

**7. What is required to integrate Jenkins with CodeBuild?​**


* [ ] You must install the CodeBuild plugin for Jenkins​.
* [ ] You must install the CodeDeploy agent​.
* [ ] You must create an IAM role​.
* [ ] Jenkins can be replaced with CodeBuild, but they cannot be integrated.​

**Correct answer:**
* [x] You must install the CodeBuild plugin for Jenkins​.

**8. What is the minimum number of stages required in a CodePipeline CI/CD pipeline?**


* [ ] 1
* [ ] 2
* [ ] 3
* [ ] 4

**Correct answer:**
* [x] 2

**9. How long does it take pending approvals to time out if there is no action?​**


* [ ] 24 hours​
* [ ] 3 days​
* [ ] 1 week​
* [ ] 1 month​

**Correct answer:**
* [x] 1 week​

**10. Which of the following are benefits of using repositories?​**


* [ ] Track the changes made to the code
* [ ] Make changes to a file simultaneously without affecting other developers on the team
* [ ] Collaborate from any location
* [ ] All options are correct

**Correct answer:**
* [x] All options are correct

**11. Which of the following is an AWS repository service in the cloud?​**


* [ ] GitHub​
* [ ] Bitbucket​
* [ ] CodeCommit​
* [ ] CodeBuild​

**Correct answer:**
* [x] CodeCommit​

**12. Which Git commands does AWS CodeCommit support?​**


* [ ] Merge​
* [ ] Pull​
* [ ] Clone​
* [ ] All options are correct

**Correct answer:**
* [x] All options are correct

---


## Quiz-Kubernetes Fundamentals

**1. Which of the following is not a benefit of cloud computing?**


* [ ] Increased flexibility
* [ ] Scalability
* [ ] Cost-effectiveness
* [ ] Increased reliance on physical infrastructure

**Correct answer:**
* [x] Increased reliance on physical infrastructure

**2. What is containerization?**


* [ ] Having a separate workspace for each tool
* [ ] Running an application on the cloud
* [ ] Running microservices in their own container
* [ ] A set of practices that allows developers to continuously integrate, test, and deploy changes

**Correct answer:**
* [x] Running microservices in their own container

**3. Which of the following is an essential element of cloud native?**


* [ ] Physical infrastructure
* [ ] Monolithic applications
* [ ] Microservices
* [ ] Manual deployment

**Correct answer:**
* [x] Microservices

**4. What is DevOps?**


* [ ] A modern approach to building and deploying applications that leverages the benefits of cloud computing
* [ ] A set of practices that promotes collaboration between development and operations teams to manage and scale microservices effectively
* [ ] A project management methodology that focuses on sequential development and strict timelines
* [ ] A development methodology that exclusively focuses on infrastructure management without considering application development
* [ ] A methodology that views operations as a separate and isolated phase after development is complete

**Correct answer:**
* [x] A set of practices that promotes collaboration between development and operations teams to manage and scale microservices effectively

**5. What is Cloud Native?**


* [ ] A legacy approach to building and deploying applications that relies on on-premises infrastructure
* [ ] A modern approach to application development and deployment that embraces the advantages of cloud computing, harnessing its distributed computing capabilities to build and run applications effectively
* [ ] A model that requires users to have their own physical infrastructure or resources
* [ ] A model that involves hosting applications on public clouds exclusively, without considering private or hybrid cloud options

**Correct answer:**
* [x] A modern approach to application development and deployment that embraces the advantages of cloud computing, harnessing its distributed computing capabilities to build and run applications effectively

**6. What are containers?**


* [ ] Large storage units used for organizing data
* [ ] Virtual machines that emulate entire operating systems
* [ ] Completely isolated and portable environments for running applications
* [ ] Tools used for securely transporting software across networks

**Correct answer:**
* [x] Completely isolated and portable environments for running applications

**7. Which container technology does Docker primarily utilize for its containers?**


* [ ] LXC
* [ ] LXD
* [ ] LXCFS
* [ ] None of the options are correct

**Correct answer:**
* [x] LXC

**8. How do containers differ from virtual machines (VMs) with respect to their operating system (OS) kernel?**


* [ ] Containers use the same OS kernel as the host system, while VMs have their own independent OS kernel.
* [ ] Containers and VMs both have their own independent OS kernels.
* [ ] Containers share the same OS kernel as the host system, but VMs have their own separate OS kernel.
* [ ] Containers and VMs do not rely on an OS kernel.

**Correct answer:**
* [x] Containers share the same OS kernel as the host system, but VMs have their own separate OS kernel.

**9. When using Docker on an Ubuntu operating system, based on which of the following distributions can you run a container?**


* [ ] Debian
* [ ] Fedora
* [ ] CentOS
* [ ] All of the options are correct

**Correct answer:**
* [x] All of the options are correct

**10. Which of the following statement(s) are true?**


* [ ] Virtual machines consume higher disk space than in containers.
* [ ] Docker containers are lightweight than VMs.
* [ ] Docker containers boot up faster than virtual machines.
* [ ] All the answers are correct.

**Correct answer:**
* [x] All the answers are correct.

**11. What is the term used to describe the automated process of deploying and managing containers, including managing connectivity and scaling based on workload?**


* [ ] Container virtualization
* [ ] Containerization
* [ ] Container networking
* [ ] Container orchestration

**Correct answer:**
* [x] Container orchestration

**12. Which command can be used to run an instance of MongoDB using Docker?**


* [ ] docker pull
* [ ] docker exec
* [ ] docker build
* [ ] docker run

**Correct answer:**
* [x] docker run

**13. What is the primary purpose of Docker, distinguishing it from hypervisors?**


* [ ] Virtualizing and running different operating systems
* [ ] Containerizing applications for shipping and running
* [ ] Providing a hypervisor-like environment for running multiple VMs
* [ ] Emulating hardware to create virtualized environments

**Correct answer:**
* [x] Containerizing applications for shipping and running

**14. Where can you find containerized versions of applications readily available for use?**


* [ ] Kubernetes Marketplace
* [ ] GitHub Container Registry
* [ ] Docker Hub
* [ ] Google Registry

**Correct answer:**
* [x] Docker Hub

**15. What is the relationship between a container image and a container in containerization?**


* [ ] A container image is a running instance of a container.
* [ ] A container image is an isolated environment for running containers.
* [ ] A container image is a package or template used to create one or more containers.
* [ ] A container image is a set of processes that run within a container.

**Correct answer:**
* [x] A container image is a package or template used to create one or more containers.

**16. Which of the following is NOT mentioned as a container orchestration technology?**


* [ ] Kubernetes
* [ ] Docker Swarm
* [ ] Mesos
* [ ] Containerd

**Correct answer:**
* [x] Containerd

**17. Which container orchestration technology is known for being a bit difficult to set up and get started, but provides several features to customize the deployment?**


* [ ] Docker Swarm
* [ ] Mesos
* [ ] Kubernetes
* [ ] None of the options are correct

**Correct answer:**
* [x] Kubernetes

**18. What is the primary role of Kubernetes in containerized environments?**


* [ ] Managing networking configurations for containers
* [ ] Orchestrating the deployment and management of containers in a clustered environment
* [ ] Providing security measures for containerized applications
* [ ] Optimizing resource allocation for individual containers

**Correct answer:**
* [x] Orchestrating the deployment and management of containers in a clustered environment

**19. In the context of Kubernetes, what is a cluster?**


* [ ] A group of containers deployed together
* [ ] A set of nodes grouped together
* [ ] A collection of control plane components
* [ ] A network of interconnected nodes

**Correct answer:**
* [x] A set of nodes grouped together

**20. Which of the following components are installed when setting up Kubernetes on a system?**


* [ ] API server and etcd service
* [ ] Container runtime
* [ ] All of the options are correct
* [ ] Kubelet service

**Correct answer:**
* [x] All of the options are correct

**21. What is the primary function of a node(Minion) in a Kubernetes cluster?**


* [ ] Managing container networking within the cluster
* [ ] Running control plane components of Kubernetes
* [ ] Acting as a worker machine where containers are launched
* [ ] Providing load balancing services to containerized applications

**Correct answer:**
* [x] Acting as a worker machine where containers are launched

**22. Which component serves as the entry point for interacting with a Kubernetes cluster?**


* [ ] API server
* [ ] Kubelet service
* [ ] Container runtime
* [ ] Controllers and schedulers

**Correct answer:**
* [x] API server

**23. What is the role of the master node in a Kubernetes cluster?**


* [ ] Running the containers and executing the workload
* [ ] Managing container networking within the cluster
* [ ] Providing load balancing services to containerized applications
* [ ] Orchestrating containers on the worker nodes and overseeing the cluster

**Correct answer:**
* [x] Orchestrating containers on the worker nodes and overseeing the cluster

**24. What is the role of etcd in a Kubernetes cluster?**


* [ ] It stores all the data used to manage the cluster.
* [ ] It ensures distributed storage of information across all nodes in the cluster.
* [ ] It implements locks to prevent conflicts between the masters.
* [ ] All of the options are correct

**Correct answer:**
* [x] All of the options are correct

**25. Which component in Kubernetes is responsible for distributing containers across multiple nodes?**


* [ ] API server
* [ ] Scheduler
* [ ] Kubelet service
* [ ] Etcd service

**Correct answer:**
* [x] Scheduler

**26. Which component in Kubernetes is responsible for making decisions to bring up new containers in response to node, container, or endpoint failures?**


* [ ] API server
* [ ] Etcd service
* [ ] Kubelet service
* [ ] Controllers

**Correct answer:**
* [x] Controllers

**27. Which statement accurately describes the container runtime used with Kubernetes?**


* [ ] The container runtime is always Docker.
* [ ] The container runtime can only be Docker or Podman.
* [ ] The container runtime used with Kubernetes can vary, with Docker being a common option.
* [ ] The container runtime is determined by the operating system and cannot be changed.

**Correct answer:**
* [x] The container runtime used with Kubernetes can vary, with Docker being a common option.

**28. What is the role of the Kubelet in a Kubernetes cluster?**


* [ ] It acts as the entry point for interacting with the cluster.
* [ ] It stores all the data used to manage the cluster.
* [ ] It ensures distributed storage of information across all nodes in the cluster.
* [ ] It is the agent responsible for managing and monitoring containers on each node.

**Correct answer:**
* [x] It is the agent responsible for managing and monitoring containers on each node.

**29. Which command is commonly used with kubectl to deploy an application on a Kubernetes cluster?**


* [ ] kubectl get
* [ ] kubectl describe
* [ ] kubectl run
* [ ] kubectl delete

**Correct answer:**
* [x] kubectl run

**30. Which of the following components are typically found on a Kubernetes master node?**


* [ ] Kubelet and container runtime
* [ ] kube-apiserver, etcd store, controllers, and scheduler
* [ ] All of the options are correct
* [ ] Docker, CRI-O, and rkt

**Correct answer:**
* [x] kube-apiserver, etcd store, controllers, and scheduler

**31. What is the purpose of the kubectl tool in Kubernetes?**


* [ ] It is used to deploy and manage applications on a Kubernetes cluster.
* [ ] It is used to manage the container runtime on worker nodes.
* [ ] It is responsible for distributing work or containers across multiple nodes.
* [ ] It is used for storing and retrieving data in the etcd store.

**Correct answer:**
* [x] It is used to deploy and manage applications on a Kubernetes cluster.

**32. Which command is used to view information about the Kubernetes cluster?**


* [ ] kubectl get nodes
* [ ] kubectl describe nodes
* [ ] kubectl delete nodes
* [ ] kubectl cluster-info

**Correct answer:**
* [x] kubectl cluster-info

**33. What is the purpose of the CTR command line tool that comes with ContainerD?**


* [ ] It is used for deploying and managing containers on a Kubernetes cluster.
* [ ] It is a user-friendly tool for debugging containerD and supports a wide range of features.
* [ ] It is solely made for debugging containerD and supports a limited set of features.
* [ ] It is a specialized tool designed for container introspection and analysis, providing advanced performance profiling and resource utilization metrics.

**Correct answer:**
* [x] It is solely made for debugging containerD and supports a limited set of features.

**34. Which of the following statements is true about ContainerD?**


* [ ] ContainerD is a part of Docker and is not a separate project.
* [ ] ContainerD is a part of Kubernetes and is responsible for container orchestration.
* [ ] ContainerD is a deprecated technology and no longer used in modern container environments.
* [ ] ContainerD is a separate project and a member of Cloud Native Computing Foundation (CNCF).

**Correct answer:**
* [x] ContainerD is a separate project and a member of Cloud Native Computing Foundation (CNCF).

**35. How would you use the CTR command line tool to pull a Redis image?**


* [ ] ctr images push \<address of the redis image\>
* [ ] ctr images build \<address of the redis image\>
* [ ] ctr images pull \<address of the redis image\>
* [ ] ctr images remove \<address of the redis image\>

**Correct answer:**
* [x] ctr images pull \<address of the redis image\>

**36. Which command is used with the CTR command line tool to run a container by specifying the image address?**


* [ ] ctr run \<image address\>
* [ ] ctr create \<image address\>
* [ ] ctr start \<image address\>
* [ ] ctr execute \<image address\>

**Correct answer:**
* [x] ctr run \<image address\>

**37. What is the benefit of using the Nerd Control (NerdCTL) tool over the CTR command line tool?**


* [ ] NerdCTL provides a graphical user interface for container management.
* [ ] NerdCTL offers better performance and stability compared to CTR.
* [ ] NerdCTL provides access to the newest features implemented into containerD.
* [ ] NerdCTL is specifically designed for debugging containerD and supports advanced debugging features.

**Correct answer:**
* [x] NerdCTL provides access to the newest features implemented into containerD.

**38. Which of the following features are supported by NerdCTL but are not available in Docker?**


* [ ] Encrypted container images
* [ ] Lazy pulling of images
* [ ] P2P image distribution
* [ ] All of the options are correct

**Correct answer:**
* [x] All of the options are correct

**39. Which option is used with NerdCTL to specify port mappings and expose ports when running a container?**


* [ ] -d
* [ ] -p
* [ ] -n
* [ ] -s

**Correct answer:**
* [x] -p

**40. Which command is used with NerdCTL to run a container with the name "red" using the "alpine" image?**


* [ ] nerdctl run --name red alpine
* [ ] nerdctl start --name red alpine
* [ ] nerdctl create --name red alpine
* [ ] nerdctl execute --name red alpine

**Correct answer:**
* [x] nerdctl run --name red alpine

**41. Which of the following statements accurately describes the purpose of the CRI Control tool?**


* [ ] It is built specifically for ContainerD.
* [ ] It is used to create and manage containers across different container runtimes.
* [ ] It is a graphical user interface tool for container management.
* [ ] It is primarily used for inspecting and debugging container runtimes.

**Correct answer:**
* [x] It is primarily used for inspecting and debugging container runtimes.

**42. Who maintains and develops the CRI Control command line utility?**


* [ ] Docker Inc.
* [ ] Cloud Native Computing Foundation (CNCF)
* [ ] Kubernetes community
* [ ] Red Hat Inc.

**Correct answer:**
* [x] Kubernetes community

**43. How can you specify a specific runtime endpoint when using the CRI Control tool?**


* [ ] Using the --runtime-endpoint command line option
* [ ] Using the --endpoint-runtime command line option
* [ ] All of the options are correct
* [ ] Setting the CONTAINER_RUNTIME_ENDPOINT environment variable

**Correct answer:**
* [x] Setting the CONTAINER_RUNTIME_ENDPOINT environment variable
* [x] Using the --runtime-endpoint command line option

**44. What is the Container Runtime Interface (CRI) in Kubernetes?**


* [ ] A container runtime developed by Kubernetes for Docker containers only
* [ ] A plugin interface that enables any container vendor to work as a container runtime for Kubernetes
* [ ] A Kubernetes feature that allows for direct communication between containers
* [ ] A Kubernetes feature that enables automatic scaling of container instances

**Correct answer:**
* [x] A plugin interface that enables any container vendor to work as a container runtime for Kubernetes

**45. What is the purpose of dockershim in Kubernetes?**


* [ ] To allow Docker to communicate with Kubernetes without using the CRI
* [ ] To provide automatic scaling of Docker containers in Kubernetes
* [ ] To allow for direct communication between Docker containers
* [ ] To manage container images, containers, and networking in Docker

**Correct answer:**
* [x] To allow Docker to communicate with Kubernetes without using the CRI

**46. Why was dockershim removed from Kubernetes version 1.24?**


* [ ] To reduce compatibility and standardization across container runtimes
* [ ] To increase vendor lock-in for Docker users
* [ ] To encourage the use of container runtimes that support the CRI
* [ ] To provide better backwards compatibility with older versions of Kubernetes and Docker

**Correct answer:**
* [x] To encourage the use of container runtimes that support the CRI

**47. What is the benefit of using container runtimes that support the CRI?**


* [ ] It allows for direct communication between containers
* [ ] It enables automatic scaling of container instances
* [ ] It provides better compatibility and standardization across container runtimes
* [ ] It enables container runtimes to be developed independently of Kubernetes

**Correct answer:**
* [x] It provides better compatibility and standardization across container runtimes

**48. Why do Docker images continue to work seamlessly in Kubernetes even after the removal of Docker support?**


* [ ] Because Kubernetes still supports dockershim
* [ ] Because Docker images adhere to the Open Container Initiative (OCI) standard
* [ ] Because Kubernetes now uses containerd as its default container runtime
* [ ] Because Kubernetes has developed a new container runtime specifically for Docker images

**Correct answer:**
* [x] Because Docker images adhere to the Open Container Initiative (OCI) standard

---


## Quiz-Kubernetes Resources

**1. What is the recommended approach for expanding the physical capacity of a Kubernetes cluster when the current node lacks sufficient capacity?**


* [ ] Spin up additional pods on the same node to handle the increased load.
* [ ] Add more resources to the existing node to increase its capacity.
* [ ] Deploy additional pods on a new node in the cluster to expand the cluster's physical capacity.
* [ ] Adjust the resource limits of the existing pods to optimize their performance.

**Correct answer:**
* [x] Deploy additional pods on a new node in the cluster to expand the cluster's physical capacity.

**2. How does a replica set know which pods to monitor?**


* [ ] It monitors all the pods in the cluster.
* [ ] It monitors only the pods created by the same user.
* [ ] It uses labels to filter and select the pods to monitor.
* [ ] It monitors pods based on their creation order.

**Correct answer:**
* [x] It uses labels to filter and select the pods to monitor.

**3. What is the correct API version to use in the replication controller definition file?**


* [ ] apps/v1
* [ ] v1
* [ ] extensions/v1Beta
* [ ] core/v1

**Correct answer:**
* [x] v1

**4. Why is the template section required in the replica set specification, even if there are existing pods with matching labels?**


* [ ] The template section is not required in this case.
* [ ] The template section is required to create new pods when needed.
* [ ] The template section is required to delete the existing pods.
* [ ] The template section is only required for debugging purposes.

**Correct answer:**
* [x] The template section is required to create new pods when needed.

**5. When you create a deployment in Kubernetes, what additional objects are automatically created alongside the deployment?**


* [ ] ReplicationController only
* [ ] ReplicaSet and Pods
* [ ] StatefulSet and containers
* [ ] Service and Pods

**Correct answer:**
* [x] ReplicaSet and Pods

**6. When using a replication controller in Kubernetes, how can you specify the number of replicas (instances) of a pod that should be running?**


* [ ] Set the "replicas" property in the pod's metadata section.
* [ ] Use the "scale" command to specify the number of replicas.
* [ ] Specify the number of replicas in the "replicas" property under the pod's spec section.
* [ ] Define the number of replicas in the pod's annotations.

**Correct answer:**
* [x] Specify the number of replicas in the "replicas" property under the pod's spec section.

**7. In a Kubernetes definition file, the "kind" field is used to specify:**


* [ ] The version of the API being used
* [ ] The name of the object being created
* [ ] The type of object being created
* [ ] The namespace in which the object will be deployed

**Correct answer:**
* [x] The type of object being created

**8. How can you view the created Replicaset in Kubernetes?**


* [ ] kubectl get pods
* [ ] kubectl get deployments
* [ ] kubectl get services
* [ ]  kubectl get replicaset

**Correct answer:**
* [x]  kubectl get replicaset

**9. How can you check the status of a deployment rollout in Kubernetes?**


* [ ] Use the command kubectl rollout deploy [deployment-name].
* [ ] Use the command kubectl rollout status [deployment-name].
* [ ] Use the command kubectl get rollout [deployment-name].
* [ ] Use the command kubectl get status [deployment-name].

**Correct answer:**
* [x] Use the command kubectl rollout status [deployment-name].

**10. How do you apply changes to a modified Kubernetes deployment object using a definition file called “deployment-def.yml”?**


* [ ] kubectl create -f deployment-def.yml
* [ ] kubectl update -f deployment-def.yml
* [ ] kubectl deploy -f deployment-def.yml
* [ ] kubectl apply -f deployment-def.yml

**Correct answer:**
* [x] kubectl apply -f deployment-def.yml

**11. Which approach is used to specify what actions should be performed in Kubernetes without specifying how they should be executed?**


* [ ] Declarative approach
* [ ] Imperative approach
* [ ] Proactive approach
* [ ] Reactive approach

**Correct answer:**
* [x] Declarative approach

**12. In Kubernetes, which namespace is automatically created by Kubernetes when the cluster is first set up?**


* [ ] Custom Namespace
* [ ] Primary Namespace
* [ ] kube-system Namespace
* [ ] default Namespace

**Correct answer:**
* [x] default Namespace
* [x] kube-system Namespace

**13. Which of the following commands are considered as imperative approaches to managing objects in Kubernetes?**


* [ ] kubectl run
* [ ] kubectl apply
* [ ] kubectl create -f
* [ ] kubectl delete -f

**Correct answer:**
* [x] kubectl run
* [x] kubectl create -f
* [x] kubectl delete -f

**14. To list pods in the "kube-system" namespace, which command should you use?**


* [ ] kubectl get pods
* [ ] kubectl list pods --namespace=default
* [ ] kubectl get pods --namespace=kube-system
* [ ] kubectl view pods --namespace=production

**Correct answer:**
* [x] kubectl get pods --namespace=kube-system

**15. How can you view pods in all namespaces in Kubernetes?**


* [ ] Use the kubectl get pods --all command.
* [ ] Use the kubectl get pods --all-namespaces command.
* [ ] Use the kubectl get pods --namespace=all command.
* [ ] Use the kubectl get pods --global command.

**Correct answer:**
* [x] Use the kubectl get pods --all-namespaces command.

**16. What is the purpose of assigning quotas to Kubernetes namespaces?**


* [ ] To limit the number of pods that can be created within a namespace
* [ ] To restrict the amount of CPU and memory resources that can be utilized by objects within a namespace
* [ ] To control the network bandwidth available to pods within a namespace
* [ ] To define access controls and permissions for different user groups within a namespace

**Correct answer:**
* [x] To restrict the amount of CPU and memory resources that can be utilized by objects within a namespace

**17. When scaling an application in Kubernetes to handle increased user load, how are additional instances typically created?**


* [ ] By spinning up new container instances within the same pod
* [ ] By creating a new pod altogether with a new instance of the same application
* [ ] By adding more nodes to the cluster
* [ ] By adjusting the resource limits of the existing pod

**Correct answer:**
* [x] By creating a new pod altogether with a new instance of the same application

**18. What parameter is used with the "kubectl run" command to specify the application image when deploying a Docker container as a pod?**


* [ ] --container
* [ ] --pod-image
* [ ] --app-image
* [ ] --image

**Correct answer:**
* [x] --image

**19. What is the advantage of having multiple containers within the same pod in Kubernetes?**


* [ ] It allows for scaling the application more efficiently.
* [ ] It ensures that all containers within the pod have the same resource limits.
* [ ] It enables the containers to share the same network namespace and storage volumes.
* [ ] It provides a backup container in case the main application container fails.

**Correct answer:**
* [x] It enables the containers to share the same network namespace and storage volumes.

**20. Which command is used to view the list of pods in a Kubernetes cluster?**


* [ ] kubectl list pods
* [ ] kubectl show pods
* [ ] kubectl get pods
* [ ] kubectl view pods

**Correct answer:**
* [x] kubectl get pods

**21. By default, when a Docker container is deployed within a pod, how can users access the application?**


* [ ] Externally, from any node in the cluster
* [ ] Externally, using the pod's IP address
* [ ] Only through direct SSH access to the container
* [ ] Internally, from the node where the pod is running

**Correct answer:**
* [x] Only through direct SSH access to the container

**22. Which API version would you typically use when creating a Pod object in a Kubernetes definition file?**


* [ ] It depends on the specific requirements of the Pod.
* [ ] apps/v1
* [ ] extensions/v1Beta
* [ ] v1

**Correct answer:**
* [x] v1

**23. Which of the following fields are required in a Kubernetes definition file?**


* [ ] apiVersion, kind, metadata, spec
* [ ] name, containers, labels, replicas
* [ ] version, type, description, configuration
* [ ] image, ports, volumes, annotations

**Correct answer:**
* [x] apiVersion, kind, metadata, spec

**24. What is the smallest object that can be created in Kubernetes?**


* [ ] Container
* [ ] Node
* [ ] Pod
* [ ] Service

**Correct answer:**
* [x] Pod

**25. In a Kubernetes definition file, how is the "metadata" field typically defined?**


* [ ] As a string value
* [ ] As a numerical value
* [ ] As a boolean value
* [ ] As a dictionary-like structure

**Correct answer:**
* [x] As a dictionary-like structure

**26. Why is it beneficial to use labels in Kubernetes when deploying multiple pods?**


* [ ] Labels help to specify the version of the API being used.
* [ ] Labels allow for easy grouping and filtering of pods based on their characteristics.
* [ ] Labels determine the namespace in which the pods will be deployed.
* [ ] Labels provide additional metadata about the pods.

**Correct answer:**
* [x] Labels allow for easy grouping and filtering of pods based on their characteristics.

**27. How can you view detailed information about a specific pod named "my-app-pod" in Kubernetes?**


* [ ] Execute the command "kubectl get pod my-app-pod".
* [ ] Run the command "kubectl show pod my-app-pod".
* [ ] Run the command "kubectl describe pod my-app-pod".
* [ ] Use the command "kubectl info pod my-app-pod".

**Correct answer:**
* [x] Run the command "kubectl describe pod my-app-pod".

**28. In a Kubernetes definition file, why is the "containers" property within the "spec" section defined as a list or an array?**


* [ ] It allows for specifying multiple versions of the container image.
* [ ] It enables easy scaling of the containers within the pod.
* [ ] It supports the deployment of multiple containers within the pod.
* [ ] It ensures the containers have unique names within the pod.

**Correct answer:**
* [x] It supports the deployment of multiple containers within the pod.

**29. What is the primary role of controllers in Kubernetes?**


* [ ] They handle networking and communication between pods.
* [ ] They provide a graphical user interface for managing Kubernetes objects.
* [ ] They monitor Kubernetes objects and take appropriate actions based on their state.
* [ ] They manage the storage and persistent data within the Kubernetes cluster.

**Correct answer:**
* [x] They monitor Kubernetes objects and take appropriate actions based on their state.

**30. How can you create a pod in Kubernetes using a YAML file named "pod-definition.yml"?**


* [ ] Run the command "kubectl create -f pod-definition.yml".
* [ ] Execute the command "kubectl apply pod-definition.yml".
* [ ] Use the command "kubectl create pod-definition.yml".
* [ ] Enter the command "kubectl start pod-definition.yml".

**Correct answer:**
* [x] Run the command "kubectl create -f pod-definition.yml".

**31. Why is the use of a replication controller important in Kubernetes?**


* [ ] It ensures that multiple instances of a pod are running to provide high availability.
* [ ] It manages the replication of pods for load balancing purposes.
* [ ] All of the options are correct
* [ ]  It helps to automatically recover and replace failed pods.

**Correct answer:**
* [x] All of the options are correct

**32. What command would you use to view the list of created replication controllers in Kubernetes?**


* [ ] kubectl list replicationcontrollers
* [ ] kubectl get rc
* [ ] kubectl describe replicationcontrollers
* [ ] kubectl fetch replicationcontroller

**Correct answer:**
* [x] kubectl get rc

**33. Why does a ReplicaSet require a selector definition, even if the pod definition is provided in the template?**


* [ ] The selector helps the ReplicaSet identify pods that were created before the ReplicaSet.
* [ ] The selector is used to specify the number of replicas to create.
* [ ] The selector ensures that only pods created by the ReplicaSet are considered for scaling.
* [ ] The selector is a required field in the ReplicaSet definition.

**Correct answer:**
* [x] The selector helps the ReplicaSet identify pods that were created before the ReplicaSet.

**34. Is the “selector” field a required field in a ReplicationController?**


* [ ] Yes, the selector field is a required field in a ReplicationController.
* [ ] No, the selector field is not a required field in a ReplicationController.
* [ ] The requirement for the selector field depends on the Kubernetes version.
* [ ] The selector field is only required if the ReplicationController manages existing pods.

**Correct answer:**
* [x] No, the selector field is not a required field in a ReplicationController.

**35. What is the role of a Replicaset in Kubernetes?**


* [ ] To create and manage pods
* [ ] To manage storage volumes
* [ ] To expose services to external users
* [ ] To monitor and scale pods

**Correct answer:**
* [x] To monitor and scale pods

**36. Where should the user input be specified for the labels in a ReplicaSet's selector?**


* [ ] matchLabels
* [ ] selector
* [ ] template
* [ ] replicas

**Correct answer:**
* [x] matchLabels

**37. What is the purpose of a Deployment in Kubernetes?**


* [ ] Deploying multiple instances of an application using a ReplicaSet
* [ ] Monitoring and managing pods using labels and selectors
* [ ] Providinge high availability by running multiple replicas of a pod
* [ ] Managing and upgrading the underlying instances seamlessly using rolling updates

**Correct answer:**
* [x] Managing and upgrading the underlying instances seamlessly using rolling updates

**38. How can you update the number of replicas in a replica set using the kubectl scale command?**


* [ ] Modify the replicas field in the definition file and use the "kubectl scale" command.
* [ ] Use the "kubectl scale" command with the replicas parameter and provide the new number of replicas.
* [ ] Use the "kubectl scale" command with the file name as input, and the replicas will be updated automatically in the file.
* [ ] Use the "kubectl scale" command with the name of the replica set and provide the new number of replicas with the replicas parameter.

**Correct answer:**
* [x] Use the "kubectl scale" command with the name of the replica set and provide the new number of replicas with the replicas parameter.

**39. How can you update a replica set to scale from 3 replicas to 6 replicas?**


* [ ] Modify the number of replicas in the definition file and use the "kubectl update" command.
* [ ] Modify the number of replicas in the definition file and use the "kubectl apply" command.
* [ ] Modify the number of replicas in the definition file and use the "kubectl replace" command.
* [ ] Modify the number of replicas in the definition file and use the "kubectl edit" command.

**Correct answer:**
* [x] Modify the number of replicas in the definition file and use the "kubectl apply" command.

**40. How can you view the list of created deployments in Kubernetes?**


* [ ] kubectl list deployments
* [ ] kubectl show deployments
* [ ] kubectl get deployments
* [ ] kubectl view deployments

**Correct answer:**
* [x] kubectl get deployments

**41. To view all the objects created after a deployment, which command can you use?**


* [ ] kubectl get deployments
* [ ] kubectl get replicasets
* [ ] kubectl get all
* [ ] kubectl get pods

**Correct answer:**
* [x] kubectl get all

**42. Why are deployment rollouts and revisions important in Kubernetes?**


* [ ] They help in scaling up the pods.
* [ ] They ensure high availability of the application.
* [ ] They allow for tracking changes made to the deployment.
* [ ] They provide security for the application.

**Correct answer:**
* [x] They allow for tracking changes made to the deployment.

**43. When a new deployment is created, what is the result of the rollout process?**


* [ ] Creation of a new deployment revision
* [ ] Deletion of the previous deployment revision
* [ ] Scaling up of the pods
* [ ] Creation of a new replica set

**Correct answer:**
* [x] Creation of a new deployment revision

**44. What is the default deployment strategy in Kubernetes where the application never goes down during the update?**


* [ ] Recreate strategy
* [ ] Scale strategy
* [ ] Rolling update strategy
* [ ] Replace strategy

**Correct answer:**
* [x] Rolling update strategy

**45. How can you view the history and revisions of a deployment rollout in Kubernetes?**


* [ ] Use the command kubectl get history [deployment-name].
* [ ] Use the command kubectl get revisions [deployment-name].
* [ ] Use the command kubectl describe rollout [deployment-name].
* [ ] Use the command kubectl rollout history [deployment-name].

**Correct answer:**
* [x] Use the command kubectl rollout history [deployment-name].

**46. What command can you use to update the image of your application in a Kubernetes deployment without modifying the deployment definition file?**


* [ ] kubectl set image deployment/my-deployment my-container=new-image:tag
* [ ] kubectl apply -f deployment-def.yml
* [ ] kubectl update -f deployment-def.yml
* [ ] kubectl deploy -f deployment-def.yml

**Correct answer:**
* [x] kubectl set image deployment/my-deployment my-container=new-image:tag

**47. How can you rollback a Kubernetes deployment to a previous revision?**


* [ ] kubectl rollout update deployment/my-deployment
* [ ] kubectl rollback deployment my-deployment
* [ ] kubectl rollout undo deployment/my-deployment
* [ ] kubectl undo deployment my-deployment

**Correct answer:**
* [x] kubectl rollout undo deployment/my-deployment

**48. What are the two strategies for making deployments in Kubernetes?**


* [ ] Replicate and update strategies
* [ ] Recreate and rolling update strategies
* [ ] Scale and replace strategies
* [ ] Update and upgrade strategies

**Correct answer:**
* [x] Recreate and rolling update strategies

**49. Which approach is used to specify specific actions and how should they be performed in Kubernetes?**


* [ ] Declarative approach
* [ ] Imperative approach
* [ ] Proactive approach
* [ ] Reactive approach

**Correct answer:**
* [x] Imperative approach

**50. In Kubernetes, which approach involves running the 'kubectl apply' command to create, update, or delete an object?**


* [ ] Imperative approach
* [ ] Declarative approach

**Correct answer:**
* [x] Declarative approach

**51. What does the 'kubectl apply' command do in the declarative approach of managing objects in Kubernetes?**


* [ ] Creates a new object based on the provided configuration
* [ ] Updates an existing object based on the provided configuration
* [ ] Deletes an existing object based on the provided configuration
* [ ] All of the options are correct

**Correct answer:**
* [x] Creates a new object based on the provided configuration
* [x] Updates an existing object based on the provided configuration

**52. Regardless of the approach used to create an object in Kubernetes, what does Kubernetes use to store information about the object internally?**


* [ ] Local configuration file
* [ ] Live configuration on the Kubernetes cluster
* [ ] Last applied configuration
* [ ] Live object definition file

**Correct answer:**
* [x] Live configuration on the Kubernetes cluster

**53. When using the kubectl apply command in Kubernetes, which of the following sources does it consider before deciding what changes to make?**


* [ ] Only the local configuration file
* [ ] Only the live object definition file in Kubernetes memory
* [ ] Only the last applied configuration
* [ ] The local configuration file, live object definition file in Kubernetes memory, and the last applied configuration

**Correct answer:**
* [x] The local configuration file, live object definition file in Kubernetes memory, and the last applied configuration

**54. When using the kubectl apply command to create an object in Kubernetes, what happens to the YAML version of the local object config file?**


* [ ] It is stored as the live configuration on the Kubernetes cluster.
* [ ] It is converted to a JSON format and stored as the last applied configuration.
* [ ] It is discarded and not used for further operations.
* [ ] It is used as the primary reference for object creation.

**Correct answer:**
* [x] It is converted to a JSON format and stored as the last applied configuration.

**55. Why is the "last applied configuration" feature useful in Kubernetes?**


* [ ] It helps to track changes made to an object's configuration.
* [ ] It allows for easy rollback to the previous configuration of an object.
* [ ] It helps identify fields that have been removed from the local configuration file.
* [ ] It improves the efficiency of applying configuration changes.

**Correct answer:**
* [x] It helps identify fields that have been removed from the local configuration file.

**56. Which namespace in Kubernetes is created at cluster startup and contains internal services required for networking and DNS?**


* [ ] Custom Namespace
* [ ] kube-system Namespace
* [ ] kube-public Namespace
* [ ] default Namespace

**Correct answer:**
* [x] kube-system Namespace

**57. In which Kubernetes namespace are the resources created that should be made available to all users?**


* [ ] Custom Namespace
* [ ] Tuber System Namespace
* [ ] kube-public
* [ ] default Namespace

**Correct answer:**
* [x] kube-public

**58. How can you limit resources in a namespace in Kubernetes?**


* [ ] Use the kubectl create quota command and specify the namespace and resource limits.
* [ ] Use the kubectl create resourcequota command and specify the namespace and resource limits.
* [ ] Use the kubectl create -f quota-definition.yaml command and provide a definition file with the kind as ResourceQuota, namespace, and resource limits.
* [ ] Use the kubectl set quota command and specify the namespace and resource limits.

**Correct answer:**
* [x] Use the kubectl create -f quota-definition.yaml command and provide a definition file with the kind as ResourceQuota, namespace, and resource limits.

**59. Why would you create separate namespaces for dev and production environments in Kubernetes?**


* [ ] To improve resource utilization within the cluster
* [ ] To simplify the management of Kubernetes objects
* [ ] To facilitate communication and collaboration between dev and production teams
* [ ] To isolate resources and prevent accidental modifications in one environment affecting the other

**Correct answer:**
* [x] To isolate resources and prevent accidental modifications in one environment affecting the other

**60. You want to create a pod in the "dev" namespace instead of the default namespace using a YAML manifest file called "pod-def.yml". Which command should you use to achieve this?**


* [ ] kubectl create -f pod-def.yml --namespace=default
* [ ] kubectl create -f pod-def.yml 
* [ ] kubectl apply -f pod-def.yml --namespace=dev
* [ ] kubectl deploy -f pod-def.yml --namespace=dev

**Correct answer:**
* [x] kubectl apply -f pod-def.yml --namespace=dev

**61. How can you create a new namespace in Kubernetes?**


* [ ] Use the kubectl create namespace <namespace-name> command.
* [ ] Use the kubectl create -f namespace-def.yaml command.
* [ ] Use the kubectl new namespace <namespace-name> command.
* [ ] Use the kubectl generate namespace <namespace-name> command.

**Correct answer:**
* [x] Use the kubectl create namespace <namespace-name> command.
* [x] Use the kubectl create -f namespace-def.yaml command.

**62. How can you switch to a specific namespace permanently in Kubernetes, so that you don't have to specify the namespace option each time you run a command?**


* [ ] Use the kubectl switch namespace <namespace-name> command.
* [ ] Use the kubectl use namespace <namespace-name> command.
* [ ] Use the kubectl set-namespace <namespace-name> command.
* [ ] Use the kubectl config set-context $(kubectl config current-context) --namespace=<namespace-name> command.

**Correct answer:**
* [x] Use the kubectl config set-context $(kubectl config current-context) --namespace=<namespace-name> command.

**63. To connect a web-app pod in the default namespace to a service named "db-service" in the "dev" namespace, what is the correct hostname format?**


* [ ] web-app.default.svc.cluster.local
* [ ] db-service.dev.svc.cluster.local
* [ ] db-service.svc.cluster.local
* [ ] web-app..svc.cluster.local

**Correct answer:**
* [x] db-service.dev.svc.cluster.local

---


## Quiz-Scheduling

**1. How do you specify labels in a Kubernetes pod definition file?**


* [ ] Use the "labels" field under the "metadata" section and provide labels in a key-value format.
* [ ] Use the "annotations" field under the "metadata" section and provide labels in a key-value format.
* [ ] Use the "tags" field under the "metadata" section and provide labels in a key-value format.
* [ ] Use the "selectors" field under the "metadata" section and provide labels in a key-value format.

**Correct answer:**
* [x] Use the "labels" field under the "metadata" section and provide labels in a key-value format.

**2. What is the purpose of the NodeName field in a Kubernetes pod?**


* [ ] It specifies the name of the pod.
* [ ] It identifies the namespace of the pod.
* [ ] It indicates the desired state of the pod.
* [ ] It represents the name of the node where the pod is scheduled to run.

**Correct answer:**
* [x] It represents the name of the node where the pod is scheduled to run.

**3. What is one way to manually assign a node to an existing pod in Kubernetes by mimicking the behavior of the scheduler?**


* [ ] Modify the pod's nodeName field to the desired node name.
* [ ] Use the kubectl assign command to bind the pod to a specific node.
* [ ] Create a binding object with the target node's name and send a post request to the pod's binding API.
* [ ] Update the pod's namespace to match the desired node's namespace.

**Correct answer:**
* [x] Create a binding object with the target node's name and send a post request to the pod's binding API.

**4. In Kubernetes, what mechanism allows you to filter and view different objects based on categories or criteria, such as their type, application, or functionality?**


* [ ] Annotations and selectors
* [ ] Filters and tags
* [ ] Labels and selectors
* [ ] Categories and classifiers

**Correct answer:**
* [x] Labels and selectors

**5. Where are the labels defined for the pods in a ReplicaSet?**


* [ ] Under the template section of the Replicaset definition file.
* [ ]  At the top of the Replicaset definition file.
* [ ] In both the template section and at the top of the Replicaset definition file.
* [ ]  Labels are not used for pods in a Replicaset.

**Correct answer:**
* [x] Under the template section of the Replicaset definition file.

**6. What do the labels at the top of a Replicaset definition file represent?**


* [ ] Labels of the Replicaset itself.
* [ ] Labels of the pods in the Replicaset.
* [ ] Labels of the service associated with the Replicaset.
* [ ] Labels used for networking purposes.

**Correct answer:**
* [x] Labels of the Replicaset itself.

**7. How can you select pods based on labels using the kubectl command?**


* [ ] Use the kubectl pods command followed by the label name.
* [ ] Use the kubectl get pods --filter command followed by the label name.
* [ ] Use the kubectl get pods --selector command followed by the label key-value pair.
* [ ] Use the kubectl describe pods command followed by the label key-value pair.

**Correct answer:**
* [x] Use the kubectl get pods --selector command followed by the label key-value pair.

**8. What can be done to manually schedule a pod to a specific node in Kubernetes when there is no scheduler monitoring and scheduling the nodes?**


* [ ] Set the NodeName field in the pod manifest to the desired node name.
* [ ] Use the kubectl assign command to assign the pod to a specific node.
* [ ] Modify the pod's namespace to match the desired node's namespace.
* [ ] Create a custom binding object to associate the pod with the desired node.

**Correct answer:**
* [x] Set the NodeName field in the pod manifest to the desired node name.

**9. What is the purpose of using taints and tolerations in Kubernetes?**


* [ ] To group and select pods based on their characteristics
* [ ] To record additional details or metadata for informational purposes
* [ ] To set restrictions on which pods can be scheduled on specific nodes
* [ ] To define labels for pods and nodes

**Correct answer:**
* [x] To set restrictions on which pods can be scheduled on specific nodes

**10. To connect a ReplicaSet to the desired pods, which field in the ReplicaSet specification is used to match the labels defined on the pod?**


* [ ] template
* [ ] selector
* [ ] metadata
* [ ] replicas

**Correct answer:**
* [x] selector

**11. What does the taint effect determine for pods that do not tolerate the taint in Kubernetes?**


* [ ] The scheduling behavior of the pods
* [ ] The networking behavior of the pods
* [ ] The resource allocation behavior of the pods
* [ ] The logging behavior of the pods

**Correct answer:**
* [x] The scheduling behavior of the pods

**12. What is the primary purpose of using annotations in Kubernetes?**


* [ ] To group and select objects
* [ ] To define labels for objects
* [ ] To record additional details or metadata for informational purposes
* [ ] To configure networking settings for objects

**Correct answer:**
* [x] To record additional details or metadata for informational purposes

**13. What does the taint effect "PreferNoSchedule" indicate in Kubernetes?**


* [ ] Pods will not be scheduled on the node.
* [ ] Pods will be scheduled with a higher priority on the node.
* [ ] Pods will be evicted immediately from the node.
* [ ] System will try to avoid placing a pod on the node, but it is not guaranteed.

**Correct answer:**
* [x] System will try to avoid placing a pod on the node, but it is not guaranteed.

**14. Which of the following is a taint effect in Kubernetes?**


* [ ] LimitedSchedule
* [ ] PreferredSchedule
* [ ] NoSchedule
* [ ] RestrictedSchedule

**Correct answer:**
* [x] NoSchedule

**15. What does the taint effect "NoSchedule" indicate in Kubernetes?**


* [ ] Pods will not be scheduled on the node.
* [ ] Pods will be scheduled with a higher priority on the node.
* [ ] Pods will be evicted immediately from the node.
* [ ] Pods will be scheduled with a lower priority on the node.

**Correct answer:**
* [x] Pods will not be scheduled on the node.

**16. How can you taint a node in Kubernetes using the kubectl command?**


* [ ] kubectl taint nodes node-name key=value:taint-effect
* [ ] kubectl taint node-name key=value:taint-effect
* [ ] kubectl taint nodes key=value:taint-effect
* [ ] kubectl taint node node-name key=value:taint-effect

**Correct answer:**
* [x] kubectl taint nodes node-name key=value:taint-effect

**17. What does the taint effect "NoExecute" indicate in Kubernetes?**


* [ ] Pods will not be scheduled on the node.
* [ ] Pods will be scheduled with a higher priority on the node.
* [ ] System will try to avoid placing a pod on the node, but it is not guaranteed.
* [ ] Pods will be evicted immediately from the node, if they do not tolerate the taint

**Correct answer:**
* [x] Pods will be evicted immediately from the node, if they do not tolerate the taint

**18. How do you label a node in Kubernetes?**


* [ ] kubectl label nodes \<node_name\> \<label-key\>=\<label-value\>
* [ ] kubectl create label node \<node_name\> \<label-key\>=\<label-value\>
* [ ] kubectl annotate node \<node_name\> \<label-key\>=\<label-value\>
* [ ] kubectl apply label node \<node_name\> \<label-key\>=\<label-value\>

**Correct answer:**
* [x] kubectl label nodes \<node_name\> \<label-key\>=\<label-value\>

**19. What is the purpose of taints and tolerations in Kubernetes?**


* [ ] Taints are added to pods to specify their resource requirements, while tolerations are added to nodes to indicate their availability for pod scheduling.
* [ ]  Taints are added to nodes to define which pods they can tolerate, while tolerations are added to pods to allow them to be scheduled on specific nodes.
* [ ] Taints are added to pods to determine their behavior if they do not meet certain conditions, while tolerations are added to nodes to specify their preferred pod placement.
* [ ] Taints are added to nodes to restrict pod scheduling, while tolerations are added to pods to define their network connectivity requirements.

**Correct answer:**
* [x]  Taints are added to nodes to define which pods they can tolerate, while tolerations are added to pods to allow them to be scheduled on specific nodes.

**20. What limitation exists when using NodeSelectors in Kubernetes?**


* [ ] NodeSelectors cannot be used to select nodes based on labels.
* [ ] NodeSelectors can only be used to select a single node at a time.
* [ ] NodeSelectors can only handle simple queries and cannot handle complex queries like OR or NOT.
* [ ] NodeSelectors cannot be used to restrict pod placement.

**Correct answer:**
* [x] NodeSelectors can only handle simple queries and cannot handle complex queries like OR or NOT.

**21. How can you limit a pod to run on a specific node in Kubernetes?**


* [ ] Add a label to the node and use nodeSelector in the pod's spec section.
* [ ] Use taints and tolerations on the pod and node to restrict pod placement.
* [ ] Specify the node name in the pod's metadata section under the field "node".
* [ ] Use the nodeGroup field in the pod's spec section.

**Correct answer:**
* [x] Add a label to the node and use nodeSelector in the pod's spec section.

**22. What capability does the node affinity feature provide in Kubernetes?**


* [ ] It allows selecting multiple nodes based on a single label using the OR operator.
* [ ] It provides a way to restrict pod placement only on the master node.
* [ ] It allows selecting nodes based on their resource utilization.
* [ ] It enables complex queries like OR, IN, NOT, and EXISTS for limiting pod placement on specific nodes.

**Correct answer:**
* [x] It enables complex queries like OR, IN, NOT, and EXISTS for limiting pod placement on specific nodes.

**23. What are the two types of node affinity available in Kubernetes?**


* [ ] High affinity and low affinity
* [ ] Primary affinity and secondary affinity
* [ ] Absolute affinity and conditional affinity
* [ ] requiredDuringSchedulingIgnoredDuringExecution, preferredDuringSchedulingIgnoredDuringExecution

**Correct answer:**
* [x] requiredDuringSchedulingIgnoredDuringExecution, preferredDuringSchedulingIgnoredDuringExecution

**24. What does the type of node affinity in Kubernetes define?**


* [ ] The labels assigned to the nodes in the cluster.
* [ ] The priority of pods in relation to node placement.
* [ ] The types of workloads that can be deployed on a specific node.
* [ ] The behavior of the scheduler in relation to node affinity and the stages of the pod lifecycle.

**Correct answer:**
* [x] The behavior of the scheduler in relation to node affinity and the stages of the pod lifecycle.

**25. How can you ensure complete dedication of nodes for specific pods in Kubernetes?**


* [ ] By using only taints and tolerations
* [ ] By using only node affinity rules
* [ ] By using a combination of taints, tolerations, and node affinity rules
* [ ] By using pod anti-affinity rules

**Correct answer:**
* [x] By using a combination of taints, tolerations, and node affinity rules

**26. What is the default assumption made by Kubernetes regarding the resource request for a container?**


* [ ] 1 CPU and 512 MiB memory
* [ ] 0.5 CPU and 256 MiB memory
* [ ] 2 CPU and 1024 MiB memory
* [ ] 0.1 CPU and 128 MiB memory

**Correct answer:**
* [x] 0.5 CPU and 256 MiB memory

**27. What is the minimum value that can be set for CPU resource requests in Kubernetes?**


* [ ] 0.5 CPU
* [ ] 1 CPU
* [ ] 100m CPU
* [ ] 1m CPU

**Correct answer:**
* [x] 1m CPU

**28. How is 1 count of CPU (1 CPU) defined in Kubernetes?**


* [ ] It refers to 1 vCPU in AWS.
* [ ] It refers to 1 core in GCP or Azure.
* [ ] It refers to 1 Hyperthread.
* [ ] All of the options are correct

**Correct answer:**
* [x] All of the options are correct

**29. What is the difference between "Gigabyte (GB)" and "Gibibyte (GiB)" in terms of their size?**


* [ ] Gigabyte (GB) refers to 1000 Megabytes, while Gibibyte (GiB) refers to 1024 Mebibytes.
* [ ] Gigabyte (GB) refers to 1024 Megabytes, while Gibibyte (GiB) refers to 1000 Megabytes.
* [ ] Gigabyte (GB) and Gibibyte (GiB) are interchangeable units with the same size.
* [ ] Gigabyte (GB) refers to 1000 Megabytes, while Gibibyte (GiB) refers to 1000 Megabytes.

**Correct answer:**
* [x] Gigabyte (GB) refers to 1000 Megabytes, while Gibibyte (GiB) refers to 1024 Mebibytes.

**30. How can you modify the resource requests for a pod in Kubernetes?**


* [ ] Modify the default values in the cluster configuration file.
* [ ] Use the kubectl command to update the resource requests for the running pod.
* [ ] Specify the new values in the pod or deployment definition file under the "resources" section.
* [ ] Modify the resource requests directly in the container runtime.

**Correct answer:**
* [x] Specify the new values in the pod or deployment definition file under the "resources" section.

**31. What is the default resource limit set by Kubernetes for containers?**


* [ ] 1 vCPU and 512 Mebibytes
* [ ] 2 CPUs and 1 Gigabyte
* [ ] 512 Megabytes and 256 CPUs
* [ ] 1 vCPU and 1 Gigabyte

**Correct answer:**
* [x] 1 vCPU and 512 Mebibytes

**32. How can you change the default resource limit for containers in Kubernetes?**


* [ ] Modify the Kubernetes configuration file.
* [ ]  Use the kubectl command to update the default limit.
* [ ] Add a ‘limit’ section under the resources section in the pod definition file.
* [ ] Restart the Kubernetes cluster to apply the new default limit.

**Correct answer:**
* [x] Add a ‘limit’ section under the resources section in the pod definition file.

**33. Which Kubernetes resource ensures that a copy of a pod is automatically added to each node in a cluster, including newly added nodes?**


* [ ] Deployment
* [ ] ReplicaSet
* [ ] DaemonSet
* [ ] StatefulSet

**Correct answer:**
* [x] DaemonSet

**34. What is the key distinction between a DaemonSet and a ReplicaSet?**


* [ ] A DaemonSet ensures that a copy of a pod runs on every node in a Kubernetes cluster, while a ReplicaSet maintains a specified number of pod replicas across the cluster.
* [ ] A DaemonSet is used for stateless applications, while a ReplicaSet is used for stateful applications.
* [ ] A DaemonSet manages long-running services, while a ReplicaSet manages short-lived batch jobs.
* [ ] A DaemonSet scales pods horizontally based on CPU utilization, while a ReplicaSet scales pods vertically based on memory consumption.

**Correct answer:**
* [x] A DaemonSet ensures that a copy of a pod runs on every node in a Kubernetes cluster, while a ReplicaSet maintains a specified number of pod replicas across the cluster.

**35. Which command can be used to display all the created DaemonSets in a Kubernetes cluster?**


* [ ] kubectl get pods
* [ ] kubectl get services
* [ ] kubectl get deployments
* [ ] kubectl get daemonsets

**Correct answer:**
* [x] kubectl get daemonsets

**36. Which of the following is a suitable use case for deploying a DaemonSet in Kubernetes?**


* [ ] Deploying a stateless web application on each node in the cluster
* [ ] Deploying a database service on a specific subset of nodes in the cluster
* [ ] Deploying a monitoring agent or log collector on each node in the cluster
* [ ] Deploying a job that runs periodically on a single node in the cluster

**Correct answer:**
* [x] Deploying a monitoring agent or log collector on each node in the cluster

**37. Which command should be used to obtain detailed information about a specific DaemonSet in Kubernetes?**


* [ ] kubectl describe pods \<daemon-name\>
* [ ] kubectl describe services \<daemon-name\>
* [ ] kubectl describe deployments \<daemon-name\>
* [ ] kubectl describe daemonsets \<daemon-name\>

**Correct answer:**
* [x] kubectl describe daemonsets \<daemon-name\>

**38. When using the kubelet alone, without involving other Kubernetes control plane components, it is possible to create replicas, deployments, or services.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] False

**39. What is the term used for the pods that are created by the kubelet independently, without any involvement from the API server or other Kubernetes cluster components?**


* [ ] Dynamic pods
* [ ] Managed pods
* [ ] Autonomous pods
* [ ] Static pods

**Correct answer:**
* [x] Static pods

**40. Where is the location specified for the kubelet to look for static pod definition files?**


* [ ] The kubeconfig file
* [ ] The container runtime directory
* [ ] The kubelet configuration file
* [ ] The --pod-manifest-path parameter in the kubelet.service file

**Correct answer:**
* [x] The --pod-manifest-path parameter in the kubelet.service file

**41. What approach can be used to specify the path for static pod definition files when not directly specifying it in the kubelet.service file?**


* [ ] Defining the path in the kubeconfig file
* [ ] Modifying the container runtime directory
* [ ] Creating a separate config file and defining the directory path as ‘staticPodPath’
* [ ] Using the --pod-manifest-path parameter in the kubelet.service file

**Correct answer:**
* [x] Creating a separate config file and defining the directory path as ‘staticPodPath’

**42. The kubectl utility relies on the presence of the Kubernetes API server, and if the API server is not available, kubectl commands will not work.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**43. The Kubernetes API server is aware of the static pods created by the kubelet.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**44. What is a potential use case for static pods in a Kubernetes cluster?**


* [ ] Deploying control plane components as pods on the master nodes
* [ ] Running user applications that require high availability
* [ ] Scaling worker nodes dynamically based on resource utilization
* [ ] Managing networking configurations for the cluster

**Correct answer:**
* [x] Deploying control plane components as pods on the master nodes

**45. Which statement accurately describes the behavior of a static pod in a Kubernetes cluster?**


* [ ] Static pods are not visible in the Kube API server and cannot be managed.
* [ ] When a kubelet creates a static pod, it also creates a read-only mirror object in the Kube API server.
* [ ] Static pods are fully manageable from the Kube API server, allowing editing and deletion like regular pods.
* [ ] Static pods do not require a mirror object in the Kube API server as they are independent entities.

**Correct answer:**
* [x] When a kubelet creates a static pod, it also creates a read-only mirror object in the Kube API server.

**46. What are the key distinctions between static pods and DaemonSets in Kubernetes?**


* [ ] Static pods are created using kubelet, while DaemonSets utilize kubeapi-server.
* [ ] Static pods can be used to deploy control plane nodes, while DaemonSets are suitable for deploying monitoring or logging agents on each node.
* [ ] Both static pods and DaemonSets are ignored by the kube-scheduler.
* [ ] All of the above statements are true.

**Correct answer:**
* [x] All of the above statements are true.

---


## Quiz-Networking

**1. What is a fundamental requirement for each Kubernetes master or worker node in terms of network connectivity?**


* [ ] Each node should have multiple interfaces connected to different networks.
* [ ] Each node must have at least one interface connected to a network.
* [ ] It is optional for nodes to have network connectivity.
* [ ] Nodes should be disconnected from any network.

**Correct answer:**
* [x] Each node must have at least one interface connected to a network.

**2. On which port should the Kubernetes master node be configured to accept connections for the API server?**


* [ ] Port 80
* [ ] Port 443
* [ ] Port 6443
* [ ] Port 8080

**Correct answer:**
* [x] Port 6443

**3. What are the two unique identifiers required for  hosts in a network?**


* [ ] Unique hostname and IP address
* [ ] Unique hostname and MAC address
* [ ] Unique IP address and subnet mask
* [ ] Unique MAC address and subnet mask

**Correct answer:**
* [x] Unique hostname and MAC address

**4. On which port do the kubelets on both master and worker nodes listen?**


* [ ] Port 80
* [ ] Port 443
* [ ] Port 10250
* [ ] Port 8080

**Correct answer:**
* [x] Port 10250

**5. Which port does the kube-controller-manager component of Kubernetes require to be open?**


* [ ] Port 80
* [ ] Port 443
* [ ] Port 10250
* [ ] Port 10257

**Correct answer:**
* [x] Port 10257

**6. What is a mandatory requirement for every interface on a node in terms of network configuration?**


* [ ] Interfaces should remain unconfigured.
* [ ] Each interface must have multiple addresses configured.
* [ ] Interfaces must be connected to different networks.
* [ ] Each node interface must have an address configured.

**Correct answer:**
* [x] Each node interface must have an address configured.

**7. Kubelets can be present on both master nodes and worker nodes in a Kubernetes cluster.**


* [ ] True
* [ ] False

**Correct answer:**
* [x] True

**8. Which port does the kube-scheduler component of Kubernetes require to be open?**


* [ ] Port 80
* [ ] Port 443
* [ ] Port 10250
* [ ] Port 10259

**Correct answer:**
* [x] Port 10259

**9. In Kubernetes, which range of ports is typically used by worker nodes to expose services for external access?**


* [ ] Ports 80 to 443
* [ ] Ports 1024 to 65535
* [ ] Ports 30000 to 32767
* [ ] Ports 5000 to 6000

**Correct answer:**
* [x] Ports 30000 to 32767

**10. On which port does the ETCD server typically listen for communication?**


* [ ] Port 80
* [ ] Port 443
* [ ] Port 2375
* [ ] Port 2379

**Correct answer:**
* [x] Port 2379

**11. Which layer of networking is crucial for the functioning of a cluster at the pod level?**


* [ ] Network layer
* [ ] Transport layer
* [ ] Application layer
* [ ] Pod layer

**Correct answer:**
* [x] Pod layer

**12. Does Kubernetes currently provide a built-in solution for pod networking?**


* [ ] Yes, Kubernetes has a built-in solution for pod networking.
* [ ] No, Kubernetes does not have a built-in solution for pod networking.
* [ ] It depends on the specific version of Kubernetes.
* [ ] The availability of a built-in solution for pod networking varies based on the cloud provider.

**Correct answer:**
* [x] No, Kubernetes does not have a built-in solution for pod networking.

**13. In addition to port 2379, which additional port needs to be open for ETCD clients to communicate with each other?**


* [ ] Port 80
* [ ] Port 443
* [ ] Port 2375
* [ ] Port 2380

**Correct answer:**
* [x] Port 2380

**14. Which of the following are popular networking solutions used to implement pod networking requirements in Kubernetes?**


* [ ] WeaveWorks
* [ ] Cilium
* [ ] Flannel
* [ ] All of the options are correct.

**Correct answer:**
* [x] All of the options are correct.

**15. What are the requirements for pod networking in Kubernetes?**


* [ ] Pods should be able to communicate across different nodes using their IP addresses.
* [ ] Each pod should have a unique IP address.
* [ ] Pods within the same node should be able to communicate using their IP addresses.
* [ ] All of the options are correct.

**Correct answer:**
* [x] All of the options are correct.

**16. What is the responsibility of a container runtime in terms of networking?**


* [ ] Creating container network namespaces
* [ ] Identifying and attaching namespaces to the appropriate network
* [ ] Calling the appropriate network plugin
* [ ] All of the options are correct.

**Correct answer:**
* [x] All of the options are correct.

**17. Which command can be used to view the running kubelet service?**


* [ ] ps -ef | grep kubelet
* [ ] ps -ax | grep kubelet
* [ ] ps -aux | grep kubelet
* [ ] top | grep kubelet

**Correct answer:**
* [x] ps -aux | grep kubelet

**18. In a typical Container Network Interface (CNI) setup, where are the supported CNI plugins stored?**


* [ ] In the Kubernetes API server directory
* [ ] In the container runtime's configuration directory
* [ ] In the CNI bin directory as executables
* [ ] In the kubelet service directory

**Correct answer:**
* [x] In the CNI bin directory as executables

**19. Where is the Container Network Interface (CNI) plugin typically configured in a Kubernetes cluster?**


* [ ] In the container runtime configuration file
* [ ] In the Kubernetes API server configuration
* [ ] In the kube-proxy configuration
* [ ] In the kubelet service on each node in the cluster

**Correct answer:**
* [x] In the kubelet service on each node in the cluster

**20. Who defines the format for the plugin configuration file in a Container Network Interface (CNI) setup?**


* [ ] Kubernetes API server
* [ ] Container runtime
* [ ] CNI standard
* [ ] kubelet service

**Correct answer:**
* [x] CNI standard

**21. In a Container Network Interface (CNI) setup, where does the kubelet look to determine which plugin should be used?**


* [ ] In the kubelet configuration file
* [ ] In the container runtime's configuration directory
* [ ] In the CNI bin directory as executables
* [ ] In the CNI config directory containing configuration files

**Correct answer:**
* [x] In the CNI config directory containing configuration files

**22. In a Container Network Interface (CNI) plugin configuration file, what value would you typically set for the "type" field if you want to use a bridge network?**


* [ ] host
* [ ] overlay
* [ ] macvlan
* [ ] bridge

**Correct answer:**
* [x] bridge

**23. What does the "isGateway" field in a Container Network Interface (CNI) plugin configuration file determine?**


* [ ] Whether the bridge network should have an assigned IP address
* [ ] Whether the plugin supports gateway functionality
* [ ] Whether the bridge network should act as a gateway
* [ ] Whether the plugin configuration file is valid

**Correct answer:**
* [x] Whether the bridge network should act as a gateway

**24. What does the "ipMasquerade" field in a Container Network Interface (CNI) plugin configuration file determine?**


* [ ] Whether the plugin supports IP masquerading
* [ ] Whether the bridge network should have an assigned IP address
* [ ] Whether the plugin configuration file is valid
* [ ] Whether a NAT rule should be added for IP masquerading

**Correct answer:**
* [x] Whether a NAT rule should be added for IP masquerading

**25. What does the IPAM section in a Container Network Interface (CNI) plugin configuration file define?**


* [ ] The plugin's IP address management capabilities
* [ ] The subnet or range of IP addresses assigned to pods
* [ ] The DNS server configuration for the network
* [ ] The plugin's authentication and authorization settings

**Correct answer:**
* [x] The subnet or range of IP addresses assigned to pods

**26. When setting up a Kubernetes cluster, does Kubernetes deploy a built-in DNS server by default?**


* [ ] Yes, a built-in DNS server is automatically deployed by default.
* [ ] No, a built-in DNS server needs to be manually configured.
* [ ] It depends on the type of cluster being set up.
* [ ] None of the options are correct.

**Correct answer:**
* [x] Yes, a built-in DNS server is automatically deployed by default.

**27. In a Container Network Interface (CNI) plugin configuration file, what does the "type" field indicate when set to "host-local" under the IPAM section?**


* [ ] The type of network interface used for communication
* [ ] The type of IP address management used for pods
* [ ] The type of network protocol used for pod communication
* [ ] The type of DNS resolution method used for pods

**Correct answer:**
* [x] The type of IP address management used for pods

**28. What does the DNS record created for a Kubernetes service typically map?**


* [ ] The service name to the IP address
* [ ] The service name to the port number
* [ ] The IP address to the service name
* [ ] The IP address to the pod name

**Correct answer:**
* [x] The service name to the IP address

**29. How can a pod within a Kubernetes cluster reach a service?**


* [ ] By using the service name
* [ ]  By using the pod's IP address
* [ ] By using the service's DNS record
* [ ] By using the cluster's DNS server

**Correct answer:**
* [x] By using the service name

**30. What happens when a Kubernetes service is created?**


* [ ] DNS service automatically creates a record for the service.
* [ ] The service needs to manually register with the DNS service.
* [ ] The service is assigned an IP address by the DNS service.
* [ ] DNS service is not involved in the creation of a Kubernetes service.

**Correct answer:**
* [x] DNS service automatically creates a record for the service.

**31. In a Kubernetes cluster, how can a pod reach a service using its service name?**


* [ ] Only if the pod and service are in the same namespace
* [ ] By using the service name regardless of the namespace
* [ ] By using the service's IP address instead of the service name
* [ ] By explicitly specifying the namespace in the service name

**Correct answer:**
* [x] Only if the pod and service are in the same namespace

**32. How should you refer to a service located in a separate namespace called "apps" from the default namespace in Kubernetes?**


* [ ] By using the service name alone
* [ ] By using the service name with the "default" namespace prefix
* [ ] By using the service name with the “apps” namespace prefix
* [ ] By using the pattern: <service-name>.apps

**Correct answer:**
* [x] By using the pattern: <service-name>.apps

**33. In a Kubernetes cluster, how are all the services grouped together within the DNS subdomain?**


* [ ] They are grouped under the "services" subdomain.
* [ ] They are grouped under the "clustersvc" subdomain.
* [ ] They are grouped under the "svc" subdomain.
* [ ] They are grouped under the "k8sservices" subdomain.

**Correct answer:**
* [x] They are grouped under the "svc" subdomain.

**34. How are all the pods and services for a namespace grouped together within the DNS subdomain in Kubernetes?**


* [ ] They are grouped under the "cluster" subdomain.
* [ ] They are grouped under the "services" subdomain.
* [ ] They are grouped under the "pods" subdomain.
* [ ] They are grouped under the subdomain with the name of the namespace.

**Correct answer:**
* [x] They are grouped under the subdomain with the name of the namespace.

**35. In Kubernetes, how are all the pods and services for a namespace grouped together within the DNS root domain for the cluster?**


* [ ] They are grouped under the "services" subdomain.
* [ ] They are grouped under the "clustersvc" subdomain.
* [ ] They are grouped under the "pods" subdomain.
* [ ] They are grouped under a root domain set to "cluster.local" by default.

**Correct answer:**
* [x] They are grouped under a root domain set to "cluster.local" by default.

**36. When pod records are enabled in Kubernetes, how are the records created for pods, and what naming convention is used?**


* [ ] Records are created using the pod name as the record name.
* [ ] Records are created using the IP address as the record name.
* [ ] Records are created by replacing dots in the IP address with dashes as the record name.
* [ ] Records are created based on the namespace and pod name combination.

**Correct answer:**
* [x] Records are created by replacing dots in the IP address with dashes as the record name.

**37. What is the fully qualified domain name (FQDN) for a web service called 'web-service' in the 'apps' namespace within a Kubernetes cluster?**


* [ ] web-service.apps.svc.cluster.local
* [ ] web-service.svc.apps.cluster.local
* [ ] web-service.cluster.local.apps.svc
* [ ] web-service.svc.cluster.local.apps

**Correct answer:**
* [x] web-service.apps.svc.cluster.local

---


## Quiz-ServiceMesh

**1. In a service mesh architecture, how do the proxies communicate with each other?**


* [ ] Through a control plane
* [ ] Through a data plane
* [ ] Through API gateways
* [ ] Through load balancers

**Correct answer:**
* [x] Through a data plane

**2. Which feature of a service mesh enables automatic detection and registration of services within a network?**


* [ ] Dynamic configuration
* [ ] Mutual TLS
* [ ] Observability
* [ ] Service discovery

**Correct answer:**
* [x] Service discovery

**3. What is the role of a service mesh in a microservice architecture?**


* [ ] It is a dedicated and configurable infrastructure layer that handles the communication between services without code changes.
* [ ] It is a database management system specifically designed for microservices.
* [ ] It is a security mechanism that encrypts data between microservices.
* [ ] It is a development framework for building microservices.

**Correct answer:**
* [x] It is a dedicated and configurable infrastructure layer that handles the communication between services without code changes.

**4. Which feature of a service mesh provides secure communication between services?**


* [ ] Dynamic configuration
* [ ] Mutual TLS
* [ ] Observability
* [ ] Service discovery

**Correct answer:**
* [x] Mutual TLS

**5. Which of the following is NOT a main topic covered by service discovery in a service mesh?**


* [ ] Discovery
* [ ] Health check
* [ ] Load balancing
* [ ] Traffic encryption

**Correct answer:**
* [x] Traffic encryption

**6. What is the primary purpose of load balancing in a service mesh?**


* [ ] Routing traffic to healthy instances
* [ ] Enforcing traffic encryption
* [ ] Facilitating service discovery
* [ ] Monitoring service performance

**Correct answer:**
* [x] Routing traffic to healthy instances

**7. Which of the following statements is true about Istio?**


* [ ] It is a paid service mesh solution.
* [ ] It is a closed-source service mesh.
* [ ] It provides a free and open-source solution for securing, connecting, and monitoring services.
* [ ] It is primarily used for load balancing purposes.

**Correct answer:**
* [x] It provides a free and open-source solution for securing, connecting, and monitoring services.

**8. In a service mesh architecture, what is the role of the control plane?**


* [ ] It manages all the traffic into and out of services via proxies.
* [ ] It handles the communication between services without code changes.
* [ ] It encrypts data between microservices for security.
* [ ] It is responsible for load balancing and scaling services.

**Correct answer:**
* [x] It manages all the traffic into and out of services via proxies.

**9. What is Istio known for in relation to complex deployments?**


* [ ] Enabling telemetry and monitoring
* [ ] Enhancing security
* [ ] All of the options are correct
* [ ] Providing universal traffic management

**Correct answer:**
* [x] All of the options are correct

**10. What technology does Istio utilize to implement proxies for service communication?**


* [ ] OpenAPI
* [ ] Envoy
* [ ] Nginx
* [ ] HAProxy

**Correct answer:**
* [x] Envoy

**11. Which three components were originally part of the control plane in Istio?**


* [ ] Citadel, Pilot, and Galley
* [ ] Prometheus, Grafana, and Jaeger
* [ ] Envoy, Mixer, and Kiali
* [ ] Sidecar, Ingress, and Egress

**Correct answer:**
* [x] Citadel, Pilot, and Galley

**12. Which component in Istio combines the three control plane components (Citadel, Pilot, and Galley) into a single daemon?**


* [ ] Istiod
* [ ] Envoy
* [ ] Prometheus
* [ ] Kiali

**Correct answer:**
* [x] Istiod

**13. How can different applications in a Kubernetes cluster communicate with each other?**


* [ ] Through pods
* [ ] Through containers
* [ ] Through services
* [ ] Through nodes

**Correct answer:**
* [x] Through services

**14. What is the term used to describe the ability of pods in Kubernetes to be created and killed?**


* [ ] Mutable
* [ ] Immutable
* [ ] Ephemeral
* [ ] Persistent

**Correct answer:**
* [x] Ephemeral

**15. Which component in Istio is responsible for delivering configuration secrets to the Envoy proxies?**


* [ ] Citadel
* [ ] Istio agent
* [ ] Pilot
* [ ] Galley

**Correct answer:**
* [x] Istio agent

**16. How does Kubernetes services identify and select specific pods from a large pool of other pods?**


* [ ] By using pod names
* [ ] By using pod IP addresses
* [ ] By using pod annotations
* [ ] By using pod labels

**Correct answer:**
* [x] By using pod labels

**17. Which of the following is NOT one of the three types of services in Kubernetes?**


* [ ] NodePort
* [ ] ClusterIP
* [ ] LoadBalancer
* [ ] All of the options are types of services in Kubernetes

**Correct answer:**
* [x] All of the options are types of services in Kubernetes

**18. What is the default type of service in Kubernetes that facilitates communication between applications within the cluster?**


* [ ] NodePort
* [ ] ClusterIP
* [ ] LoadBalancer
* [ ] ExternalIP

**Correct answer:**
* [x] ClusterIP

**19. Which type of service must be created in Kubernetes if you want to make an application accessible outside of the cluster?**


* [ ] ClusterIP
* [ ] NodePort
* [ ] ExternalIP
* [ ] NetworkPolicy

**Correct answer:**
* [x] NodePort

**20. What is the key difference between a LoadBalancer service and a NodePort service in Kubernetes?**


* [ ] LoadBalancer services are only accessible within the cluster, while NodePort services can be accessed from outside the cluster.
* [ ] LoadBalancer services require the use of an external load balancer, while NodePort services do not.
* [ ] LoadBalancer services provide automatic load balancing, while NodePort services require manual configuration.
* [ ] LoadBalancer services are only used for internal communication, while NodePort services are used for external communication.

**Correct answer:**
* [x] LoadBalancer services require the use of an external load balancer, while NodePort services do not.

**21. What is the term used to describe additional containers that support the main container in Kubernetes?**


* [ ] Helpers
* [ ] Attachments
* [ ] Sidecars
* [ ] Extensions

**Correct answer:**
* [x] Sidecars

**22. Which of the following tasks are typically performed by sidecar containers in Kubernetes?**


* [ ] Database management and configuration
* [ ] Load balancing and traffic routing
* [ ] User interface rendering and display
* [ ] Log shipping, monitoring, file loading, and proxying

**Correct answer:**
* [x] Log shipping, monitoring, file loading, and proxying

**23. Which significant achievement did the Envoy project accomplish in its timeline with the Cloud Native Computing Foundation (CNCF)?**


* [ ] It was accepted into the CNCF in 2017.
* [ ] It reached the graduation level in 2018.
* [ ] It surpassed other projects in terms of popularity.
* [ ] It received an award for its contribution to cloud native technologies.

**Correct answer:**
* [x] It reached the graduation level in 2018.

**24. Which of the following is true about a proxy service in an application architecture?**


* [ ] It is responsible for executing the core business logic of the application.
* [ ] It ensures secure connections through TLS encryption.
* [ ] It manages user authentication and retries failed requests.
* [ ] It is an additional functionality that is not necessary for an application.

**Correct answer:**
* [x] It ensures secure connections through TLS encryption.
* [x] It manages user authentication and retries failed requests.

**25. What is the primary function of Envoy?**


* [ ] Acting as a proxy and communication bus.
* [ ] Performing advanced data analysis.
* [ ] Providing cloud storage solutions.
* [ ] Facilitating user authentication processes.

**Correct answer:**
* [x] Acting as a proxy and communication bus.

---


## Quiz-Storage

**1. What are the two types of drivers in Docker for storage?**


* [ ] Network drivers and image drivers
* [ ] Storage drivers and volume drivers
* [ ] Container drivers and pod drivers
* [ ] Host drivers and volume drivers

**Correct answer:**
* [x] Storage drivers and volume drivers

**2. In which folder does Docker store data files related to images and containers?**


* [ ] /opt/docker/data
* [ ] /usr/local/docker/storage
* [ ] /var/docker/storage
* [ ] /var/lib/docker

**Correct answer:**
* [x] /var/lib/docker

**3. What architecture does Docker use to build images?**


* [ ] Monolithic architecture
* [ ] Microservices architecture
* [ ] Layered architecture
* [ ] Modular architecture

**Correct answer:**
* [x] Layered architecture

**4. Which of the following statements is true about Docker's caching mechanism for building images?**


* [ ] Docker doesn't use caching mechanism for building images.
* [ ] Docker always uses the cache, even if it's outdated.
* [ ] Docker checks if the previous build layers are up-to-date and uses the cache whenever possible.
* [ ] Docker doesn't allow caching for security reasons.

**Correct answer:**
* [x] Docker checks if the previous build layers are up-to-date and uses the cache whenever possible.

**5. Which of the following statements is true regarding Docker image layers?**


* [ ] Docker image layers are stacked on top of each other, with each layer representing a change or addition to the previous layer.
* [ ]  They are read only and can not be modified after creation.
* [ ] They are created when we run the docker build command.
* [ ] All of the options are correct.

**Correct answer:**
* [x] All of the options are correct.

**6. What happens when you run a container based on a Docker image using the docker run command?**


* [ ] A new image layer is created on top of the existing image layer.
* [ ] A new writeable layer is created on top of the existing image layer.
* [ ] The image layer is overwritten with a new layer.
* [ ] The image layer and writeable layer merge together into a single layer.

**Correct answer:**
* [x] A new writeable layer is created on top of the existing image layer.

**7. What is the purpose of Docker's writable layer?**


* [ ] To store the container image
* [ ] To store the host's file system
* [ ] To store data created by the container
* [ ] To store the container configuration

**Correct answer:**
* [x] To store data created by the container

**8. What is the lifespan of the container layer in Docker?**


* [ ] The lifespan of the container layer is infinite.
* [ ] The lifespan of the container layer is tied to the lifespan of the container.
* [ ] The lifespan of the container layer is tied to the lifespan of the host machine.
* [ ] The lifespan of the container layer is tied to the lifespan of the Docker engine.

**Correct answer:**
* [x] The lifespan of the container layer is tied to the lifespan of the container.

**9. When multiple containers are created using the same Docker image, which layer is shared by all containers?**


* [ ] The application layer
* [ ] The host layer
* [ ] The image layer
* [ ] The container layer

**Correct answer:**
* [x] The image layer

**10. What is the name of the mechanism used by Docker to create a copy of a modified file in the read-write layer of a container?**


* [ ] Copy and Modify
* [ ] Copy on Write
* [ ] Write and Copy
* [ ] Write on Modify

**Correct answer:**
* [x] Copy on Write

**11. Which command is used to create a volume in Docker?**


* [ ] docker volume list
* [ ] docker volume create
* [ ] docker create volume
* [ ] docker volume add

**Correct answer:**
* [x] docker volume create

**12. What happens to the data stored in the container layer when a container is deleted?**


* [ ] The data is retained and can be accessed by other containers.
* [ ] The data is moved to a backup location.
* [ ] The data is deleted along with the container.
* [ ] The data is moved to the host machine.

**Correct answer:**
* [x] The data is deleted along with the container.

**13. Which option is used to mount a volume in read-write mode inside a Docker container?**


* [ ] -r
* [ ] -m
* [ ] -rw
* [ ] -v

**Correct answer:**
* [x] -v

**14. What happens when you modify a source file in a Docker image layer?**


* [ ] The modification is saved directly to the image layer.
* [ ]  Docker automatically creates a copy of the file in the read-only layer.
* [ ] Docker automatically creates a copy of the file in the read-write layer.
* [ ] The modification is discarded when the container is stopped.

**Correct answer:**
* [x] Docker automatically creates a copy of the file in the read-write layer.

**15. Where is the folder for a Docker volume created by the "docker volume create" command located?**


* [ ] /usr/local/bin
* [ ] /etc/docker/volumes
* [ ] /var/lib/docker/volumes
* [ ] /opt/docker/data

**Correct answer:**
* [x] /var/lib/docker/volumes

**16. What is the purpose of adding a persistent volume to a container in Kubernetes?**


* [ ] To increase the container's processing power
* [ ] To provide additional security to the container
* [ ] To store data created by the container persistently
* [ ] To allow the container to communicate with other containers

**Correct answer:**
* [x] To store data created by the container persistently

**17. What is a bind mount in Docker and where can it be mounted from?**


* [ ] A bind mount is a type of volume that is mounted from a directory within a Docker container.
* [ ] A bind mount is a type of volume that is mounted from a specific location on the Docker host.
* [ ] A bind mount is a type of network share that is mounted from a remote server.
* [ ] A bind mount is a type of block storage that is mounted from a storage array.

**Correct answer:**
* [x] A bind mount is a type of volume that is mounted from a specific location on the Docker host.

**18. Where does Docker mount a volume from when using volume mount?**


* [ ] /usr/local/bin
* [ ] /etc/docker/volumes
* [ ] /var/lib/docker/volumes
* [ ] /opt/docker/data

**Correct answer:**
* [x] /var/lib/docker/volumes

**19. What is the preferred option to mount a volume in the Docker run command?**


* [ ] -v
* [ ] --bind
* [ ] --mount
* [ ] -mount

**Correct answer:**
* [x] --mount

**Explaination**: Link for reference: https://docs.docker.com/storage/bind-mounts/#choose-the--v-or---mount-flag

**20. What is the default volume driver plugin for Docker?**


* [ ] NFS
* [ ] Amazon EBS
* [ ] local
* [ ] Google Cloud Storage

**Correct answer:**
* [x] local

**21. Which of the following are examples of third-party volume driver plugins for Docker?**


* [ ] Local, NFS, Amazon EBS
* [ ] Azure file storage, Convoy, DigitalOcean block storage
* [ ] Overlay2, ZFS, BTRFS
* [ ] AUFS, Device Mapper, VFS

**Correct answer:**
* [x] Azure file storage, Convoy, DigitalOcean block storage

**22. Which of the following are common storage drivers used by Docker?**


* [ ] Ceph, GlusterFS, NFS, iSCSI
* [ ] AUFS, ZFS, BTRFS, Device Mapper
* [ ] Apache Cassandra, Elasticsearch, MongoDB, PostgreSQL
* [ ] Apache Hadoop, Apache Spark, Apache Kafka, Apache Flink

**Correct answer:**
* [x] AUFS, ZFS, BTRFS, Device Mapper

**23. Which of the following is responsible for handling Docker volumes?**


* [ ] Storage drivers
* [ ] Docker engine
* [ ] Volume driver plugins
* [ ] Container runtime

**Correct answer:**
* [x] Volume driver plugins

**24. What is the purpose of Docker storage drivers?**


* [ ] To manage the network interfaces of Docker containers
* [ ] To enable the creation of Docker volumes for persistent data storage
* [ ] To manage the security of Docker containers
* [ ] To enable the layered architecture of Docker images

**Correct answer:**
* [x] To enable the layered architecture of Docker images

**25. How can you specify a preferred volume driver plugin solution for Docker volumes?**


* [ ] Use the --driver option with the docker run command.
* [ ] Use the --volume option with the docker run command.
* [ ] Use the --mount option with the docker run command.
* [ ] Use the --volume-driver option with the docker run command.

**Correct answer:**
* [x] Use the --driver option with the docker run command.

**26. What is the purpose of the Container Storage Interface (CSI)?**


* [ ] To provide an interface for containerized applications to access the host file system
* [ ] To enable communication between containers running on different hosts
* [ ] To manage container image registries
* [ ] To support multiple storage solutions to work with kubernetes

**Correct answer:**
* [x] To support multiple storage solutions to work with kubernetes

**27. Which section of the pod definition file is used to mount the created volume to a specific storage solution in the host?**


* [ ] container
* [ ] volume
* [ ] hostPath
* [ ] storage

**Correct answer:**
* [x] hostPath

**28. Which of the following are common Container Storage Interface (CSI) drivers used for container orchestrators?**


* [ ] Docker, Kubernetes, Mesos
* [ ] Amazon S3, Google Cloud Storage, Azure Blob Storage
* [ ] Portworx, Amazon EBS, GlusterFs, Dell EMS
* [ ] PostgreSQL, MySQL, Oracle

**Correct answer:**
* [x] Portworx, Amazon EBS, GlusterFs, Dell EMS

**29. What is the purpose of attaching a volume to a pod in Kubernetes?**


* [ ] To increase the storage capacity of the container
* [ ] To allow the container to communicate with other containers
* [ ] To persist data processed by the pod
* [ ] To improve the performance of the container

**Correct answer:**
* [x] To persist data processed by the pod

**30. Which of the following statements is true regarding the Container Storage Interface (CSI)?**


* [ ] CSI is a Kubernetes-specific standard for storage management.
* [ ] CSI is a proprietary storage management tool developed by Docker.
* [ ] CSI is a universal standard for container orchestrators to communicate with storage providers.
* [ ] CSI is a built-in feature of all major cloud providers.

**Correct answer:**
* [x] CSI is a universal standard for container orchestrators to communicate with storage providers.

**31. Which of the following fields can be used in the pod definition file to configure an AWS Elastic Block Store volume as the storage or the volume?**


* [ ] hostPath
* [ ] nfs
* [ ] awsElasticBlockStore
* [ ] emptyDir

**Correct answer:**
* [x] awsElasticBlockStore

**32. What is the role of storage drivers in the Container Storage Interface (CSI) standard?**


* [ ] To define a set of remote procedure calls that will be called by the container orchestrator
* [ ] To implement the set of remote procedure calls defined by CSI
* [ ] To ensure compatibility between the container orchestrator and the storage solutions
* [ ] To manage the container volumes and storage resources

**Correct answer:**
* [x] To implement the set of remote procedure calls defined by CSI

**33. What is a persistent volume in Kubernetes?**


* [ ] A volume that is automatically created and attached to a container when it is launched
* [ ] A volume that is created by the application developer and shared across all containers in a pod
* [ ] A cluster-wide pool of storage volumes configured by an administrator to be used by users deploying apps on the cluster
* [ ] A temporary volume that is destroyed when the pod is deleted

**Correct answer:**
* [x] A cluster-wide pool of storage volumes configured by an administrator to be used by users deploying apps on the cluster

**34. How can users select storage from the persistent volume pool in Kubernetes?**


* [ ] By specifying the storage directly in the pod definition file
* [ ] By creating a persistent volume directly
* [ ] By using a persistent volume claim
* [ ] By creating a storage class

**Correct answer:**
* [x] By using a persistent volume claim

**35. What are the different access modes available for a volume in Kubernetes?**


* [ ] readonlyonce, readwriteonce, readwritemany
* [ ] readonlymany, writeonlyonce, readwritemany
* [ ] readonlymany, readwriteonce, readwritemany
* [ ] readonlyonce, writeonlyonce, readwritemany

**Correct answer:**
* [x] readonlymany, readwriteonce, readwritemany

**36. What happens if there are no volumes available for a persistent volume claim in a Kubernetes cluster?**


* [ ] The cluster creates a new volume for the claim.
* [ ] The claim is deleted automatically by the cluster.
* [ ] The claim remains in a pending state until new volumes are made available to the cluster.
* [ ] The cluster creates a new node to allocate a volume to the claim.

**Correct answer:**
* [x] The claim remains in a pending state until new volumes are made available to the cluster.

**37. What is the kind of the Kubernetes object used to request storage from a persistent volume?**


* [ ] Claim
* [ ] PersistentVolumeClaim
* [ ] VolumeClaim
* [ ] VolumeClaimTemplate

**Correct answer:**
* [x] PersistentVolumeClaim

**38. Which of the following statements is true about the relationship between claims and volumes in Kubernetes?**


* [ ] Multiple claims can share a single volume.
* [ ] Every claim is bound to a single volume.
* [ ] Claims have no relationship with volumes.
* [ ] Volumes are automatically created when a claim is made.

**Correct answer:**
* [x] Every claim is bound to a single volume.

**39. What happens when persistent volume claims are created in Kubernetes?**


* [ ] Kubernetes automatically creates persistent volumes based on the claims.
* [ ] Kubernetes binds the persistent volumes to claims based on the request and properties set on the volume.
* [ ] Kubernetes does not support persistent volume claims.
* [ ] None of the options are correct.

**Correct answer:**
* [x] Kubernetes binds the persistent volumes to claims based on the request and properties set on the volume.

**40. What command is used to list the created volumes in Kubernetes?**


* [ ] kubectl get volumes
* [ ] kubectl list persistentvolumes
* [ ] kubectl get persistentvolume
* [ ] kubectl list volumes

**Correct answer:**
* [x] kubectl get persistentvolume

**41. What command should you run to delete a persistent volume claim?**


* [ ] kubectl delete persistentvolume \<claim-name\>
* [ ] kubectl remove persistentvolume \<claim-name\>
* [ ] kubectl delete persistentvolumeclaim \<claim-name\>
* [ ] kubectl remove persistentvolumeclaim \<claim-name\>

**Correct answer:**
* [x] kubectl delete persistentvolumeclaim \<claim-name\>

**42. What command can you run in Kubernetes to view the created persistent volume claims?**


* [ ] kubectl get claims
* [ ] kubectl list persistentvolume
* [ ] kubectl get pvc
* [ ] kubectl describe pvc

**Correct answer:**
* [x] kubectl get pvc

**43. What happens to the bound volume when a persistent volume claim gets deleted in Kubernetes?**


* [ ] The bound volume gets deleted along with the claim.
* [ ] The bound volume becomes unbound.
* [ ] The bound volume becomes read-only.
* [ ] The bound volume becomes inaccessible.

**Correct answer:**
* [x] The bound volume becomes unbound.

**44. What happens to the data in a persistent volume when a persistent volume claim is deleted and the 'persistentVolumeReclaimPolicy' is set to "recycle"?**


* [ ] The data is destroyed permanently.
* [ ] The data is kept intact, and no action is taken.
* [ ] The data in the volume is scrubbed before making it available to other claims.
* [ ] The volume remains in the 'Released' state and cannot be used by any other claims.

**Correct answer:**
* [x] The data in the volume is scrubbed before making it available to other claims.

**45. What is the correct way to use a storage class with a persistent volume claim (PVC)?**


* [ ] Specify the storage class name in the metadata field of the PVC definition.
* [ ] Specify the storage class name in the storageClassName field under the spec section of the PVC definition.
* [ ] Specify the storage class name in the data field of the PVC definition.
* [ ] Specify the storage class name in the status field of the PVC definition.

**Correct answer:**
* [x] Specify the storage class name in the storageClassName field under the spec section of the PVC definition.

**46. Which of the following statements about storage classes in Kubernetes is correct?**


* [ ] Storage classes can only be created using standard disk.
* [ ] There is only one storage class available in Kubernetes.
* [ ] Different storage classes can be created using different types of disk, such as standard disk, SSD drives, and SSD drives with replication.
* [ ] Storage classes are not used in Kubernetes to manage disk resources.

**Correct answer:**
* [x] Different storage classes can be created using different types of disk, such as standard disk, SSD drives, and SSD drives with replication.

**47. Which of the following provisioners can be used with Kubernetes for dynamic provisioning of persistent volumes?**


* [ ] GCE provisioner
* [ ] AWSEBS
* [ ] ScaleIO
* [ ] All of the options are correct

**Correct answer:**
* [x] All of the options are correct

**48. What is dynamic provisioning of volumes in Kubernetes?**


* [ ] The process of manually creating persistent volumes and binding them to persistent volume claims.
* [ ] The process of automatically provisioning storage on a cloud provider and attaching it to pods when a claim is made.
* [ ] The process of creating new Kubernetes nodes to handle increased storage demand.
* [ ] The process of migrating data from one persistent volume to another.

**Correct answer:**
* [x] The process of automatically provisioning storage on a cloud provider and attaching it to pods when a claim is made.

**49. Which of the following container orchestrators have adopted the Container Storage Interface (CSI)?**


* [ ] Kubernetes
* [ ] All of the options are correct
* [ ] Nomad
* [ ] Mesos

**Correct answer:**
* [x] All of the options are correct

**50. What are two types of mounts available in Docker?**


* [ ] Static and dynamic
* [ ] Persistent and temporary
* [ ] Volume and bind mounting
* [ ] Local and remote

**Correct answer:**
* [x] Volume and bind mounting

---


## Quiz-Cloud Native Architecture

**1. What is autoscaling in the context of computing?**


* [ ] Automatically adjusting the number of resources based on demand for an application or service
* [ ] Manually adjusting the number of resources based on demand for an application or service
* [ ] Increasing the number of resources for an application or service without considering the demand
* [ ] Decreasing the number of resources for an application or service without considering the demand

**Correct answer:**
* [x] Automatically adjusting the number of resources based on demand for an application or service

**2. Which auto scaling feature of Kubernetes allows businesses to adjust the number of pods running in a deployment based on demand?**


* [ ] Horizontal Pod Autoscaler
* [ ] Vertical Pod Autoscaler
* [ ] Cluster Autoscaler
* [ ] Both Horizontal Pod Autoscaler and Cluster Autoscaler

**Correct answer:**
* [x] Horizontal Pod Autoscaler

**3. What are the three key factors essential for achieving true cloud-native autoscaling?**


* [ ] Effective application and infrastructure design, automatic scaling, bidirectional scaling
* [ ] Effective application design, manual scaling, unidirectional scaling
* [ ] Only effective infrastructure design , automatic scaling, unidirectional scaling
* [ ] Effective application and infrastructure design, manual scaling, bidirectional scaling

**Correct answer:**
* [x] Effective application and infrastructure design, automatic scaling, bidirectional scaling

**4. Which auto scaling feature of Kubernetes allows businesses to automatically resize the entire cluster to meet the demands of the application?**


* [ ] Horizontal Pod Autoscaler
* [ ] Vertical Pod Autoscaler
* [ ] Cluster Autoscaler
* [ ] Both, Vertical Pod Autoscaler and Cluster Autoscaler

**Correct answer:**
* [x] Cluster Autoscaler

**5. Which auto scaling feature of Kubernetes allows businesses to adjust the resource allocation of individual pods based on demand?**


* [ ] Horizontal Pod Autoscaler
* [ ] Vertical Pod Autoscaler
* [ ] Cluster Autoscaler
* [ ] Both Horizontal Pod Autoscaler and Vertical Pod Autoscaler

**Correct answer:**
* [x] Vertical Pod Autoscaler

**6. What is the benefit of bidirectional scaling in autoscaling?**


* [ ] It allows the system to scale up and down as demand fluctuates, increasing efficiency and saving costs.
* [ ] It only allows the system to scale up, which can result in resource overloading during periods of low demand.
* [ ] It only allows the system to scale down, which can result in resource underutilization during periods of high demand.
* [ ] It does not provide any benefit for autoscaling.

**Correct answer:**
* [x] It allows the system to scale up and down as demand fluctuates, increasing efficiency and saving costs.

**7. What is the difference between horizontal and vertical scaling?**


* [ ] Vertical scaling is adding more resources to each unit, while horizontal scaling is adding more units.
* [ ] Vertical scaling is adding more units, while horizontal scaling is adding more resources to each unit.
* [ ] Vertical scaling and horizontal scaling are the same concepts.
* [ ] Vertical scaling and horizontal scaling are not relevant to autoscaling.

**Correct answer:**
* [x] Vertical scaling is adding more resources to each unit, while horizontal scaling is adding more units.

**8. Which two-layered approach does Kubernetes utilize for its autoscaling mechanism?**


* [ ] Pod-based scaling and node-based scaling
* [ ] Pod-based scaling and cluster-based scaling
* [ ] Node-based scaling and cluster-based scaling
* [ ] Pod-based scaling and service-based scaling

**Correct answer:**
* [x] Pod-based scaling and cluster-based scaling

**9. What is pod-based scaling in Kubernetes autoscaling?**


* [ ] Adjusting the number of nodes in a cluster based on demand
* [ ] Adjusting the number of pods running in a deployment based on demand
* [ ] Adjusting the amount of resources allocated to each pod based on demand
* [ ] Adjusting the amount of resources allocated to each node based on demand

**Correct answer:**
* [x] Adjusting the number of pods running in a deployment based on demand

**10. Why is Kubernetes considered a powerful tool for achieving true cloud-native autoscaling?**


* [ ] It has the ability to monitor the workload and automatically scale resources based on demand.
* [ ] It can only perform vertical scaling, which is more flexible than horizontal scaling.
* [ ] It can only perform horizontal scaling, which is more fault-tolerant than vertical scaling.
* [ ] It does not have any features related to autoscaling.

**Correct answer:**
* [x] It has the ability to monitor the workload and automatically scale resources based on demand.

**11. Which of the following is NOT one of the autoscaling features offered by Kubernetes?**


* [ ] Horizontal Pod Autoscaler
* [ ] Vertical Pod Autoscaler
* [ ] Cluster Autoscaler
* [ ] Node Autoscaler

**Correct answer:**
* [x] Node Autoscaler

**12. What is the purpose of autoscaling?**


* [ ] To effectively adapt to changing demand without manual intervention
* [ ] To manually adjust the number of resources based on demand
* [ ] To only scale up the number of resources, regardless of demand
* [ ] To only scale down the number of resources, regardless of demand

**Correct answer:**
* [x] To effectively adapt to changing demand without manual intervention

**13. What is the purpose of Cluster Autoscaler in Kubernetes autoscaling?**


* [ ] To adjust the number of pods running in a deployment based on demand
* [ ] To adjust the amount of resources allocated to each pod based on demand
* [ ] To automatically resize the entire cluster to meet the demands of the application
* [ ] To adjust the number of nodes in a cluster based on demand

**Correct answer:**
* [x] To adjust the number of nodes in a cluster based on demand

**14. What is the purpose of Horizontal Pod Autoscaler in Kubernetes autoscaling?**


* [ ] To adjust the number of pods running in a deployment based on demand
* [ ] To adjust the amount of resources allocated to each pod based on demand
* [ ] To automatically resize the entire cluster to meet the demands of the application
* [ ] To adjust the number of nodes in a cluster based on demand

**Correct answer:**
* [x] To adjust the number of pods running in a deployment based on demand

**15. Which of the following is a benefit of horizontal scaling over vertical scaling?**


* [ ] Vertical scaling is often limited by the maximum capacity of the server.
* [ ] Horizontal scaling provides better fault tolerance.
* [ ] Vertical scaling is more flexible than horizontal scaling.
* [ ] Horizontal scaling can efficiently handle varying workloads.

**Correct answer:**
* [x] Horizontal scaling can efficiently handle varying workloads.

**16. What is the purpose of Vertical Pod Autoscaler in Kubernetes autoscaling?**


* [ ] To adjust the number of pods running in a deployment based on demand
* [ ] To adjust the amount of resources allocated to each pod based on demand
* [ ] To automatically resize the entire cluster to meet the demands of the application
* [ ] To adjust the number of nodes in a cluster based on demand

**Correct answer:**
* [x] To adjust the amount of resources allocated to each pod based on demand

**17. What is serverless computing?**


* [ ] A computing model where the server is removed completely
* [ ] A computing model where the server is managed by a cloud provider
* [ ] A computing model where the server is managed by the infrastructure team
* [ ] A computing model where the server is rented or bought by the company

**Correct answer:**
* [x] A computing model where the server is managed by a cloud provider

**18. What are the drawbacks of traditional server management?**


* [ ] It is cost-effective and time-efficient.
* [ ] It requires no vigilance and monitoring.
* [ ] It can result in needless expenses.
* [ ] It gives total control over servers.

**Correct answer:**
* [x] It can result in needless expenses.

**19. What is Function-as-a-Service (FaaS)?**


* [ ] A model where the cloud provider manages the server infrastructure
* [ ] A model where developers write custom server-side code
* [ ] A model where the infrastructure team manages the server infrastructure
* [ ] A model where the company rents or buys the server

**Correct answer:**
* [x] A model where the cloud provider manages the server infrastructure

**20. Which of the following is a benefit of Kubernetes autoscaling?**


* [ ] It can only perform horizontal scaling, which is more fault-tolerant than vertical scaling.
* [ ] It does not require any manual intervention to adjust the number of resources.
* [ ] It cannot handle spikes in traffic without overloading resources.
* [ ] It does not provide a cost-effective and scalable solution for businesses.

**Correct answer:**
* [x] It does not require any manual intervention to adjust the number of resources.

**21. What is the benefit of using FaaS?**


* [ ] It is always cheaper than traditional server management.
* [ ] It requires no custom code to be written.
* [ ] It is flexible and affordable, and companies only pay for the computing resources they use.
* [ ] It is time-efficient as developers don't have to coordinate with the infrastructure team.

**Correct answer:**
* [x] It is flexible and affordable, and companies only pay for the computing resources they use.

**22. Which cloud providers offer FaaS?**


* [ ] AWS, Azure, and Google Cloud
* [ ] AWS, Microsoft Office, and Google Cloud
* [ ] AWS, IBM Cloud, and Microsoft Azure
* [ ] AWS, Microsoft Azure, Google Cloud, and IBM Cloud

**Correct answer:**
* [x] AWS, Microsoft Azure, Google Cloud, and IBM Cloud

**23. How does FaaS handle scaling?**


* [ ] It requires the infrastructure team to scale the server infrastructure.
* [ ] It is automatically scaled based on the number of requests.
* [ ] It requires the company to scale the server infrastructure.
* [ ] It does not handle scaling at all.

**Correct answer:**
* [x] It is automatically scaled based on the number of requests.

**24. What is the most common form of serverless computing?**


* [ ] A serverless model where there are no servers involved at all
* [ ] A serverless model where the server is managed by the infrastructure team
* [ ] A serverless model where the server is rented or bought by the company
* [ ] A serverless model using Function-as-a-Service (FaaS)

**Correct answer:**
* [x] A serverless model using Function-as-a-Service (FaaS)

**25. What are open standards designed to promote in the cloud native ecosystem?**


* [ ] Interoperability, portability, and vendor neutrality
* [ ] Proprietary container formats and runtime specifications
* [ ] Product lock-in and vendor lock-in
* [ ] Competition and innovation among vendors

**Correct answer:**
* [x] Interoperability, portability, and vendor neutrality

**26. Which cloud service provider offers AWS Lambda?**


* [ ] Amazon Web Services
* [ ] Google Cloud
* [ ] IBM Cloud
* [ ] Microsoft Azure

**Correct answer:**
* [x] Amazon Web Services

**27. What is the problem with using different container technologies that lack open standards in a cloud native architecture?**


* [ ] There is no problem as long as you only use one container technology.
* [ ] Different technologies may not be able to communicate or work together seamlessly, leading to interoperability challenges and vendor lock-in.
* [ ] There is no need for open standards if you use the most popular container technologies.
* [ ] Lack of open standards only affects the data storage solutions in cloud native applications.

**Correct answer:**
* [x] Different technologies may not be able to communicate or work together seamlessly, leading to interoperability challenges and vendor lock-in.

**28. Which standard created by the OCI outlines how a file system bundle should be packaged into an image?**


* [ ] Image spec
* [ ] Runtime specification
* [ ] Distribution spec
* [ ] Container Runtime Interface (CRI)

**Correct answer:**
* [x] Image spec

**29. What is the pay-as-you-go model in FaaS?**


* [ ] The company only pays for the computing resources they use, rather than having to pay for a whole server all the time.
* [ ] The company pays for the whole server all the time, even if they don't use it.
* [ ] The company only pays for the cloud provider's service fees.
* [ ] The company pays for the cloud provider's service fees and a fixed amount for the computing resources.

**Correct answer:**
* [x] The company only pays for the computing resources they use, rather than having to pay for a whole server all the time.

**30. Why is it important to have a common or open standard for container technology in cloud native architecture?**


* [ ] To promote proprietary container formats and runtime specifications
* [ ] To enable different cloud native technologies to work together seamlessly
* [ ] To restrict developers from building cloud native applications using a variety of different technologies and services
* [ ] To lock users into a particular vendor or product

**Correct answer:**
* [x] To enable different cloud native technologies to work together seamlessly

**31. What does the container runtime specification, governed by the OCI, include?**


* [ ] Details on downloading, unpacking, and running the file system bundle using an OCI-compliant runtime
* [ ] Guidelines on how to create an OCI-compliant runtime
* [ ] Guidelines on how to create an OCI-compliant image
* [ ] Guidelines on how to distribute an OCI-compliant image

**Correct answer:**
* [x] Details on downloading, unpacking, and running the file system bundle using an OCI-compliant runtime

**32. Which platform was the initial distribution platform for container images, but now other platforms such as Amazon ECR and Azure also comply with the OCI distribution standard?**


* [ ] OCI Hub
* [ ] Docker Hub
* [ ] Kubernetes Hub
* [ ] Container Hub

**Correct answer:**
* [x] Docker Hub

**33. What does Kubernetes enable by decoupling layers for Runtime, Networking, Service Mesh, and Storage?**


* [ ] A highly flexible and modular architecture that avoids vendor and product lock-in
* [ ] A rigid and inflexible architecture that promotes vendor and product lock-in
* [ ] A way to restrict users from mixing and matching components from different vendors and products
* [ ] A way to force users to only use components from one vendor or product

**Correct answer:**
* [x] A highly flexible and modular architecture that avoids vendor and product lock-in

**34. What is the Container Runtime Interface (CRI)?**


* [ ] An open standard adopted by Kubernetes, which allows for a pluggable container runtime layer
* [ ] A proprietary container format and runtime specification created by Docker
* [ ] An open standard that only applies to data storage solutions in cloud native applications
* [ ] A way to restrict users from selecting the optimal container runtime that suits their specific needs

**Correct answer:**
* [x] An open standard adopted by Kubernetes, which allows for a pluggable container runtime layer

**35. What is the purpose of a Kubernetes Enhancement Proposal (KEP)?**


* [ ] To propose changes to the Kubernetes platform
* [ ] To evaluate the impact of new features on Kubernetes
* [ ] To provide a structured process for proposing, evaluating, and implementing changes in Kubernetes
* [ ] To document bugs and issues in Kubernetes

**Correct answer:**
* [x] To provide a structured process for proposing, evaluating, and implementing changes in Kubernetes

**36. What flexibility does the Container Runtime Interface (CRI) provide in Kubernetes?**


* [ ] It empowers users to select the optimal container runtime that suits their specific needs.
* [ ] It forces users to only use one specific container runtime provided by a particular vendor or product.
* [ ] It restricts users from selecting a container runtime that suits their specific needs.
* [ ] It allows users to use any

**Correct answer:**
* [x] It empowers users to select the optimal container runtime that suits their specific needs.

**37. How are proposed changes or new features documented in Kubernetes?**


* [ ] Through GitHub issues
* [ ] Through Kubernetes Enhancement Proposals (KEPs)
* [ ] Through design documents
* [ ] Through user stories and test plans

**Correct answer:**
* [x] Through Kubernetes Enhancement Proposals (KEPs)

**38. What is the recommended way to propose a change in Kubernetes?**


* [ ] Open a GitHub issue
* [ ] Write a KEP
* [ ] Contact a Special Interest Group (SIG)
* [ ] Submit a design document

**Correct answer:**
* [x] Write a KEP

**39. Which section of a KEP outlines the motivation and goals of a proposed change or feature?**


* [ ] Feature description
* [ ] Effort-tracking document
* [ ] Product requirements document
* [ ] Goals and non-goals

**Correct answer:**
* [x] Goals and non-goals

**40. What is the purpose of a test plan in a KEP?**


* [ ] To ensure that the proposed change is thoroughly tested before implementation
* [ ] To track the progress of the proposal
* [ ] To outline the requirements for a successful enhancement
* [ ] To provide a high-level overview of the proposed change

**Correct answer:**
* [x] To ensure that the proposed change is thoroughly tested before implementation

**41. What is the significance of the provisional status in the KEP workflow?**


* [ ] The KEP has been approved and is ready for implementation.
* [ ] The KEP has been suggested and is being discussed.
* [ ] The KEP has been rejected and will not be implemented.
* [ ] The KEP has been withdrawn by the authors.

**Correct answer:**
* [x] The KEP has been suggested and is being discussed.

**42. What are the limitations of using GitHub issues for proposing changes in Kubernetes?**


* [ ] Lack of transparency and collaboration
* [ ] Difficulty in navigating and categorizing issues
* [ ] Challenges in managing proposed changes across multiple releases
* [ ] All of the options are correct

**Correct answer:**
* [x] All of the options are correct

**43. Which of the following is not a Kubernetes Special Interest Group (SIG)?**


* [ ] SIG Architecture
* [ ] SIG Cluster Lifecycle
* [ ] SIG Bug Reporting
* [ ] SIG Storage

**Correct answer:**
* [x] SIG Bug Reporting

**44. Who developed the Kubernetes project?**


* [ ] Amazon Web Services
* [ ] Microsoft
* [ ] Google
* [ ] Red Hat

**Correct answer:**
* [x] Google

**45. What is the role of the Kubernetes Steering Committee?**


* [ ] Code development for Kubernetes
* [ ] Managing specific areas of the project
* [ ] Overseeing the overall direction of the Kubernetes project
* [ ] Testing and validating Kubernetes releases

**Correct answer:**
* [x] Overseeing the overall direction of the Kubernetes project

**46. When did Kubernetes join the Cloud Native Computing Foundation (CNCF)?**


* [ ] 2014
* [ ] 2016
* [ ] 2018
* [ ] 2020

**Correct answer:**
* [x] 2016

**47. What are SIGs in the Kubernetes community?**


* [ ] Special architectural features in Kubernetes
* [ ] Communication channels used by contributors
* [ ] Project management tools for Kubernetes
* [ ] Groups responsible for managing specific areas of the project

**Correct answer:**
* [x] Groups responsible for managing specific areas of the project

**48. How do KEPs address the limitations of using GitHub issues for proposing changes?**


* [ ] By providing a structured and well-defined process
* [ ] By promoting transparency and collaboration
* [ ] By enabling efficient management of proposed changes
* [ ] All of the options are correct

**Correct answer:**
* [x] All of the options are correct

**49. Which SIG in Kubernetes is in charge of the overall design and structure of the platform?**


* [ ] SIG Cluster Lifecycle
* [ ] SIG Storage
* [ ] SIG Network
* [ ] SIG Architecture

**Correct answer:**
* [x] SIG Architecture

**50. How are technical specifications and design proposals discussed in the Kubernetes community?**


* [ ] Through in-person meetings only
* [ ] Via Slack and GitHub issues
* [ ] Through closed-door discussions
* [ ] By email communication only

**Correct answer:**
* [x] Via Slack and GitHub issues

**51. How are SIG leaders elected in Kubernetes?**


* [ ] They are appointed by the Kubernetes Steering Committee.
* [ ] They are appointed by a nomination and voting process within the SIG.
* [ ] They are selected based on their contributions to the project.
* [ ] The community votes for the leaders in a general election.

**Correct answer:**
* [x] They are appointed by a nomination and voting process within the SIG.

**52. What is the purpose of the Kubernetes SIG Network?**


* [ ] Managing the release process for Kubernetes
* [ ] Developing and maintaining Kubernetes documentation
* [ ] Defining communication and connectivity between its components and services
* [ ] Ensuring architectural consistency in Kubernetes

**Correct answer:**
* [x] Defining communication and connectivity between its components and services

**53. What is the role of Working Groups in the Kubernetes community?**


* [ ] Developing new features and fixing bugs in Kubernetes
* [ ] Come together to work on specific issues of the project
* [ ] Coordinating feature development in Kubernetes releases
* [ ] Organizing community events and education initiatives

**Correct answer:**
* [x] Come together to work on specific issues of the project

**54. How can individuals get involved and contribute to the Kubernetes community?**


* [ ] By joining the community through the Kubernetes Slack channel and mailing lists
* [ ] By attending community events such as meetups and conferences
* [ ] By starting small with small tasks and picking up issues on GitHub
* [ ] All of the options are correct

**Correct answer:**
* [x] All of the options are correct

**55. What is the Open Container Initiative (OCI) and what standards has it created for container images and runtimes?**


* [ ] It is a group that focuses on creating open standards for container images, runtimes, and distributions. It has created the image spec, the runtime specification, and the distribution spec.
* [ ] It is a group that focuses on creating proprietary container formats and runtime specifications.
* [ ] It is a group that focuses on promoting vendor lock-in and product lock-in in the container ecosystem.
* [ ] It is a group that focuses on creating open standards for the Docker container technology only.

**Correct answer:**
* [x] It is a group that focuses on creating open standards for container images, runtimes, and distributions. It has created the image spec, the runtime specification, and the distribution spec.

---


## Quiz-Cloud Native Observability

**1. Which of the following statements accurately describes the purpose of logs in a system or application?**


* [ ] Logs are used for real-time event monitoring.
* [ ] Logs provide insights into system performance and resource utilization.
* [ ] Logs are historical records that capture information about specific events.
* [ ] Logs are primarily used for error tracking and debugging.

**Correct answer:**
* [x] Logs are historical records that capture information about specific events.

**2. Why is observability more crucial in distributed systems and microservices-based applications compared to traditional monolithic applications?**


* [ ] Distributed systems and microservices have higher performance requirements.
* [ ] Traditional monolithic applications have built-in observability features.
* [ ] Distributed systems and microservices are composed of multiple interconnected components and are more complex than monoliths.
* [ ] Traditional monolithic applications are easier to deploy and manage.

**Correct answer:**
* [x] Distributed systems and microservices are composed of multiple interconnected components and are more complex than monoliths.

**3. What term is used to describe the ability to understand and measure the state of a system based upon data generated by the system?**


* [ ] Scalability
* [ ] Availability
* [ ] Observability
* [ ] Reliability

**Correct answer:**
* [x] Observability

**4. What is one of the key benefits of observability in dynamic environments?**


* [ ] Improved system performance
* [ ] Enhanced scalability
* [ ] Actionable outputs from unexpected scenarios
* [ ] Increased availability

**Correct answer:**
* [x] Actionable outputs from unexpected scenarios

**5. What is the purpose of traces in a system or application?**


* [ ] Traces allow you to follow operations as they traverse through various systems and services.
* [ ] Traces help in identifying performance bottlenecks.
* [ ] Traces provide real-time monitoring of system operations.
* [ ] Traces capture error messages and exceptions.

**Correct answer:**
* [x] Traces allow you to follow operations as they traverse through various systems and services.

**6. What are the essential components of a log entry in a system or application? (Select all that apply)**


* [ ] The name of the process generating the log
* [ ] The severity level of the log
* [ ] The timestamp of when the log occurred
* [ ] A message containing information

**Correct answer:**
* [x] The timestamp of when the log occurred
* [x] A message containing information

**7. Which of the following are considered the three pillars of observability?**


* [ ] Logging, monitoring, and troubleshooting
* [ ] Metrics, monitoring, and error handling
* [ ] Logging, metrics, and traces
* [ ] Monitoring, debugging, and performance optimization

**Correct answer:**
* [x] Logging, metrics, and traces

**8. Which of the following are benefits of observability in a system or application?**


* [ ] Provide better insight into the internal working of a system/app.
* [ ] Speed up troubleshooting.
* [ ] Monitor the performance of an app.
* [ ] All of the options are correct

**Correct answer:**
* [x] All of the options are correct

**9. In distributed tracing, what information is typically tracked by each span?**


* [ ] End time, elapsed time, and span-id.
* [ ] Start time, duration, and parent-id.
* [ ] Error count, log messages, and span-id.
* [ ] Request size, response time, and correlation-id.

**Correct answer:**
* [x] Start time, duration, and parent-id.

**10. In the context of distributed systems, what is the purpose of a trace-id?**


* [ ] To track the performance of individual system components.
* [ ] To identify and correlate log entries related to a specific request.
* [ ] To provide real-time monitoring of system operations.
* [ ] To capture error messages and exceptions.

**Correct answer:**
* [x] To identify and correlate log entries related to a specific request.

**11. How many spans are in the following request?**


* [ ] 4
* [ ] 2
* [ ] 0
* [ ] 1

**Correct answer:**
* [x] 4

**Code**: 
->Gateway -> auth -> user -> redis db

**12. What type of information do metrics provide about a system?**


* [ ] Operational logs for troubleshooting.
* [ ] Real-time monitoring data.
* [ ] Information about the state of the system using numerical values.
* [ ] Error messages and exceptions.

**Correct answer:**
* [x] Information about the state of the system using numerical values.

**13. Which of the following types of information can be included in metrics?**


* [ ] CPU load
* [ ] HTTP response time of an API
* [ ] Number of open files
* [ ] All of the options are correct

**Correct answer:**
* [x] All of the options are correct

**14. What does SLI stand for and represent in the context of service level management?**


* [ ] Service Level Identifier, indicating the unique identification of a service
* [ ] Service Level Indicator, representing a quantitative measure of some aspect of the provided service level
* [ ] Service Level Inspection, referring to the process of auditing service levels
* [ ] Service Level Integration, describing the incorporation of multiple service levels into a unified system

**Correct answer:**
* [x] Service Level Indicator, representing a quantitative measure of some aspect of the provided service level

**15. Which monitoring solution is responsible for collecting and aggregating metrics?**


* [ ] Grafana
* [ ] Nagios
* [ ] Prometheus
* [ ] Splunk

**Correct answer:**
* [x] Prometheus

**16. What are the four pieces of information typically included in metrics?**


* [ ] Metric type, frequency, threshold, and source
* [ ] Metric Name, value, timestamp, and dimensions
* [ ] Metric category, severity, frequency, and description
* [ ] Metric ID, unit, interval, and scope

**Correct answer:**
* [x] Metric Name, value, timestamp, and dimensions

**17. Which of the following are common Service Level Indicators (SLIs) used to measure the performance of a system?**


* [ ] Request latency
* [ ] Error rate
* [ ] Saturation
* [ ] All of the options are correct

**Correct answer:**
* [x] All of the options are correct

**18. What does SLO stand for and represent in the context of service level management?**


* [ ] Service Level Organizer, responsible for coordinating SLIs
* [ ] Service Level Operator, managing the operational aspects of SLIs
* [ ] Service Level Optimization, continuously improving SLIs
* [ ]  Service Level Objective, a target value or range for an SLI

**Correct answer:**
* [x]  Service Level Objective, a target value or range for an SLI

**19. Why are metrics like high CPU or high memory usage considered poor Service Level Indicators (SLIs)?**


* [ ] These metrics are not easily measurable.
* [ ] They do not provide meaningful insights into the user experience.
* [ ] High CPU and memory usage have minimal impact on system performance.
* [ ] SLIs should only focus on user-visible impacts.

**Correct answer:**
* [x] They do not provide meaningful insights into the user experience.

**20. Given the following SLI and SLO examples, what does the SLO value represent?**


* [ ] The desired latency threshold to be achieved
* [ ] The maximum allowable latency
* [ ] The average latency observed
* [ ] The minimum acceptable latency

**Correct answer:**
* [x] The maximum allowable latency

**Code**: 
SLI: Latency
SLO: Latency < 100ms


**21. What is the benefit of Prometheus' built-in dashboarding utility?**


* [ ] It allows data to be collected more efficiently
* [ ] It enables data to be analyzed in real time
* [ ] It provides a way to present data from multiple data centers in a single page
* [ ] It can alert engineers when metrics exceed predetermined thresholds

**Correct answer:**
* [x] It provides a way to present data from multiple data centers in a single page

**22. What is Prometheus capable of doing in terms of data collection?**


* [ ] Collecting data from a single data center
* [ ] Collecting data from multiple data centers
* [ ] Collecting data from multiple servers within a single data center
* [ ] Collecting data only from servers running Linux

**Correct answer:**
* [x] Collecting data from multiple data centers

**23. What is the relationship between SLA (Service Level Agreement) and SLO (Service Level Objective)?**


* [ ] SLA is a subset of SLO.
* [ ] SLO is a subset of SLA.
* [ ] SLA and SLO are mutually exclusive.
* [ ] SLA is a contract between a vendor and a user that guarantees a certain SLO.

**Correct answer:**
* [x] SLA is a contract between a vendor and a user that guarantees a certain SLO.

**24. What is the primary purpose of an SLO (Service Level Objective) in the context of service reliability?**


* [ ] To monitor system performance and health.
* [ ] To establish benchmarks for service availability.
* [ ] To quantify the reliability of a product to a customer.
* [ ] To define the target response time for customer support.

**Correct answer:**
* [x] To quantify the reliability of a product to a customer.

**25. What triggers an alert in Prometheus?**


* [ ] A scheduled time interval
* [ ] An administrator's manual intervention
* [ ] Metrics crossing a specific threshold
* [ ] A sudden increase in network traffic

**Correct answer:**
* [x] Metrics crossing a specific threshold

**26. What is one of the key features of Prometheus when it comes to monitoring metrics?**


* [ ] continuous deployment
* [ ] Automated system updates
* [ ] Continuous integration
* [ ] Built-in alerting

**Correct answer:**
* [x] Built-in alerting

**27. What is the primary purpose of Prometheus, an open-source monitoring tool?**


* [ ] To collect log data from various sources
* [ ] To automate software deployments
* [ ] To analyze network traffic patterns
* [ ] To collect metric data and provide tools for data visualization

**Correct answer:**
* [x] To collect metric data and provide tools for data visualization

**28. How does Prometheus collect metrics from targets?**


* [ ] By sending metrics directly to the Prometheus server
* [ ] By querying a centralized database for metric data
* [ ] By scraping targets that expose metrics through an HTTP endpoint
* [ ] By using agents installed on target systems to collect metrics

**Correct answer:**
* [x] By scraping targets that expose metrics through an HTTP endpoint

**29. In Prometheus, what type of database is used to store the scraped metrics?**


* [ ] Relational database
* [ ] Document database
* [ ] Time series database
* [ ] Graph database

**Correct answer:**
* [x] Time series database

**30. What is the name of the built-in query language used by Prometheus to query its stored metrics?**


* [ ] GraphiteQL
* [ ] InfluxQL
* [ ] PrometheusSQL
* [ ] PromQL

**Correct answer:**
* [x] PromQL

**31. What type of data is Prometheus designed to monitor?**


* [ ] Textual data
* [ ] Image data
* [ ] Audio data
* [ ] Numeric time-series data

**Correct answer:**
* [x] Numeric time-series data

**32. Which of the following metrics can Prometheus monitor?**


* [ ] Network bandwidth
* [ ] Disk I/O operations
* [ ] Database connections
* [ ] All of the options are correct

**Correct answer:**
* [x] All of the options are correct

**33. Which of the following data types are not monitored by Prometheus?**


* [ ] Event data
* [ ] System logs
* [ ] Traces data
* [ ] All of the options are correct

**Correct answer:**
* [x] All of the options are correct

**34. Which of the following components is not part of the main Prometheus architecture?**


* [ ] Data retrieval worker
* [ ] Time series database
* [ ] HTTP server
* [ ] Alerting system

**Correct answer:**
* [x] Alerting system

**35. What is the primary responsibility of the data retrieval worker in Prometheus?**


* [ ] Storing collected metrics in a time series database
* [ ] Parsing and analyzing collected metric data
* [ ] Sending HTTP requests to targets and collecting metrics
* [ ] Generating alerts based on metric thresholds

**Correct answer:**
* [x] Sending HTTP requests to targets and collecting metrics

**36. Which component of Prometheus is responsible for storing the collected metrics in a time series database?**


* [ ] Data retrieval worker
* [ ] Time series database(TSDB)
* [ ] HTTP server
* [ ] Alert manager

**Correct answer:**
* [x] Time series database(TSDB)

**37. What is the purpose of the HTTP server component in Prometheus?**


* [ ] Collecting metric data from targets
* [ ] Storing metric data in a time series database
* [ ] Sending alerts and notifications
* [ ] Allowing retrieval of stored metric data

**Correct answer:**
* [x] Allowing retrieval of stored metric data

**38. What is the role of Prometheus exporters in the monitoring setup?**


* [ ] Collecting and storing metric data
* [ ] Visualizing metric data
* [ ] Serving metric data from targets and Converting internal data into Prometheus-understandable metrics
* [ ] Generating alerts based on metric thresholds

**Correct answer:**
* [x] Serving metric data from targets and Converting internal data into Prometheus-understandable metrics

**39. Which method does Prometheus use to collect data from targets?**


* [ ] Push method
* [ ] Streaming method
* [ ] Pull method
* [ ] Event-driven method

**Correct answer:**
* [x] Pull method

**40. In Prometheus, how does it handle short-lived jobs that need to send data?**


* [ ] Short-lived jobs directly send data to Prometheus server
* [ ] Short-lived jobs store data in a separate time series database
* [ ] Short-lived jobs send data to a push gateway, which Prometheus queries
* [ ] Short-lived jobs send data to a separate metric storage system

**Correct answer:**
* [x] Short-lived jobs send data to a push gateway, which Prometheus queries

**41. What is the purpose of service discovery in Prometheus?**


* [ ] Generating alerts based on predefined thresholds
* [ ] Storing and querying metric data in a time series database
* [ ] Dynamically providing a list of targets for Prometheus scraping
* [ ] Visualizing and analyzing collected metric data

**Correct answer:**
* [x] Dynamically providing a list of targets for Prometheus scraping

**42. In the Prometheus monitoring system, what role does Alertmanager play when an alert is triggered?**


* [ ] Collecting and storing metric data
* [ ] Parsing and analyzing metric data
* [ ] Visualizing and graphing metric data
* [ ] Notifying and handling alerts through various integrations

**Correct answer:**
* [x] Notifying and handling alerts through various integrations

**43. Which tool can be used to visualize and interact with data collected by Prometheus using the PromQL language?**


* [ ] Kibana
* [ ] Terraform
* [ ] Grafana
* [ ] Nagios

**Correct answer:**
* [x] Grafana

**44. How does Prometheus collect metrics from the targets?**


* [ ] By pulling data from a centralized database
* [ ] By subscribing to event streams from the targets
* [ ] By listening to metrics pushed by the targets
* [ ] By sending HTTP requests to the "/metrics" endpoint of each target

**Correct answer:**
* [x] By sending HTTP requests to the "/metrics" endpoint of each target

**45. Which of the following is a native exporter in Prometheus that collects metrics specifically for Linux systems?**


* [ ] Node exporter
* [ ] Windows exporter
* [ ] MySQL exporter
* [ ] Apache exporter

**Correct answer:**
* [x] Node exporter

**46. Which monitoring tools utilize a pull-based model for data collection, similar to Prometheus?**


* [ ] Zabbix and Nagios
* [ ] Splunk and ELK Stack
* [ ] Grafana and InfluxDB
* [ ] New Relic and Datadog

**Correct answer:**
* [x] Zabbix and Nagios

**47. How can you expose custom application metrics to Prometheus for tracking?**


* [ ] Use built-in exporters for popular applications
* [ ] Utilize Prometheus client libraries
* [ ] Enable automatic metric scraping from the application
* [ ] Configure custom metric endpoints in Prometheus configuration

**Correct answer:**
* [x] Utilize Prometheus client libraries

**48. Which of the following monitoring systems rely on a push-based model for data collection, as mentioned in the given content?**


* [ ] Logstash and Graphite
* [ ] Prometheus and OpenTSDB
* [ ] Zabbix and Nagios
* [ ] Splunk and ELK Stack

**Correct answer:**
* [x] Logstash and Graphite

**49. Which of the following is a benefit of a pull-based monitoring system, as mentioned in the given content?**


* [ ] Real-time notifications for target availability changes
* [ ] Automatic discovery of new targets to monitor
* [ ] Centralized management of target configurations
* [ ] Reduced network bandwidth usage

**Correct answer:**
* [x] Centralized management of target configurations

---


## Quiz-Cloud Native Application Delivery

**1. How does implementing a GitOps approach simplify the process of recovering from a disaster?**


* [ ] By automating the entire process of building, testing, and deploying code changes
* [ ] By tracking changes made to the system and allowing rollbacks if necessary
* [ ] By facilitating manual changes to infrastructure and application resources using the CLI
* [ ] By reducing the risk of configuration drift and manual errors

**Correct answer:**
* [x] By tracking changes made to the system and allowing rollbacks if necessary

**2. What is the main challenge faced by the software company in delivering new features to users before implementing a CI/CD process?**


* [ ] Slow and error-prone deployment
* [ ] Lack of skilled developers
* [ ] Inadequate testing infrastructure
* [ ] Insufficient user feedback

**Correct answer:**
* [x] Slow and error-prone deployment

**3. What is the main benefit of Continuous Deployment (CD) in the context of delivering new features to users?**


* [ ] It reduces the risk of configuration drift and manual errors.
* [ ] It automates the deployment process to production environments.
* [ ] It enables tracking and version control of changes to infrastructure.
* [ ] It speeds up the recovery process after a cloud computing disaster.

**Correct answer:**
* [x] It automates the deployment process to production environments.

**4. How does Continuous Integration (CI) help in catching bugs early in the development process?**


* [ ] It automates the entire process of building, and testing code changes.
* [ ] It tracks changes to infrastructure and application resources as code.
* [ ] It ensures the system can be quickly and efficiently restored after a disaster.
* [ ] It facilitates manual changes to infrastructure and application resources using the CLI.

**Correct answer:**
* [x] It automates the entire process of building, and testing code changes.

**5. How does GitOps ensure that changes to Kubernetes objects are deployed to the live environment?**


* [ ] By directly updating the live Kubernetes environment
* [ ] By committing changes to a Git repository
* [ ] By notifying team members via email
* [ ] By using a GitOps operator/tool like Flux or Argo CD

**Correct answer:**
* [x] By using a GitOps operator/tool like Flux or Argo CD

**6. What is the purpose of GitOps?**


* [ ] To track and version infrastructure changes
* [ ] To automate Kubernetes deployments
* [ ] To manage Git repositories
* [ ] To integrate with continuous delivery tools

**Correct answer:**
* [x] To track and version infrastructure changes

**7. What potential issues can arise when team members make manual changes to infrastructure and application resources using the CLI?**


* [ ] Slow and error-prone deployment
* [ ] Lack of skilled developers
* [ ] Configuration drift and manual errors
* [ ] Insufficient user feedback

**Correct answer:**
* [x] Configuration drift and manual errors

**8. What is the main advantage of using Git as the single source of truth for both infrastructure and application resources?**


* [ ] It automates the entire process of building, testing, and deploying code changes.
* [ ] It tracks changes to infrastructure and application resources as code.
* [ ] It speeds up the recovery process after a cloud computing disaster.
* [ ] It reduces the risk of configuration drift and manual errors.

**Correct answer:**
* [x] It tracks changes to infrastructure and application resources as code.

**9. Which GitOps tool was developed by Weaveworks and is now a graduated CNCF project?**


* [ ] Flux
* [ ] Argo
* [ ] Jenkins X
* [ ] GitOps operator/tool

**Correct answer:**
* [x] Flux

**10. What is FluxCD primarily focused on?**


* [ ] Managing Git repositories
* [ ] Versioning infrastructure changes
* [ ] Continuous delivery to a Kubernetes cluster
* [ ] Tracking discrepancies between Git and Kubernetes

**Correct answer:**
* [x] Continuous delivery to a Kubernetes cluster

**11. Which tool allows multiple Git repositories and deployments to different Kubernetes namespaces?**


* [ ] Flux
* [ ] Argo
* [ ] Jenkins X
* [ ] GitOps operator/tool

**Correct answer:**
* [x] Argo

**12. What is the purpose of the OpenGitOps Project?**


* [ ] To create a community-driven GitOps tool
* [ ] To standardize best practices for GitOps
* [ ] To establish a proprietary GitOps framework
* [ ] To promote a specific GitOps vendor

**Correct answer:**
* [x] To standardize best practices for GitOps

**13. What is the key advantage of using GitOps for managing infrastructure changes?**


* [ ] Greater efficiency and accuracy
* [ ] Centralized Git repositories
* [ ] Real-time monitoring of Kubernetes clusters
* [ ] Seamless integration with Kubernetes objects

**Correct answer:**
* [x] Greater efficiency and accuracy

**14. Which organization serves as the governing body for the Kubernetes project?**


* [ ] Docker
* [ ] Cloud Native Computing Foundation (CNCF)
* [ ] Apache Foundation
* [ ] Red Hat

**Correct answer:**
* [x] Cloud Native Computing Foundation (CNCF)

**15. Which GitOps tool covers the entire CI/CD process and is relatively simple to deploy?**


* [ ] Flux
* [ ] Argo
* [ ] Jenkins X
* [ ] GitOps operator/tool

**Correct answer:**
* [x] Jenkins X

**16. What is the goal of the GitOps Working Group under the CNCF?**


* [ ] To promote vendor-specific implementations of GitOps
* [ ] To define a vendor-neutral, principal-led meaning of GitOps
* [ ] To establish a proprietary GitOps standard
* [ ] To provide financial support to GitOps projects

**Correct answer:**
* [x] To define a vendor-neutral, principal-led meaning of GitOps

**17. Which principle of GitOps emphasizes describing the entire system declaratively?**


* [ ] Versioned and immutable
* [ ] Pulled automatically
* [ ] Continuously reconciled
* [ ] Declarative

**Correct answer:**
* [x] Declarative

**18. What happens when approved changes are pushed to Git in GitOps?**


* [ ] The GitOps tool triggers a manual deployment process
* [ ] The changes are automatically applied to the system
* [ ] The changes are rejected and require further review
* [ ] The changes are placed in a queue for deployment

**Correct answer:**
* [x] The changes are automatically applied to the system

**19. How is the desired state of an application versioned in GitOps?**


* [ ] By manually updating the state in the YAML file
* [ ] By creating a new version of the YAML file for each update
* [ ] By directly modifying the deployed application
* [ ] By using a centralized database to store the state

**Correct answer:**
* [x] By creating a new version of the YAML file for each update

**20. Which of the following GitOps tools is commonly used for automating the deployment process?**


* [ ] Jenkins
* [ ] Ansible
* [ ] Flux
* [ ] Prometheus

**Correct answer:**
* [x] Flux

**21. What is the purpose of using a GitOps tool to continuously reconcile the system state?**


* [ ] To manually monitor the system and make changes as needed
* [ ] To automate the process of creating new YAML files
* [ ] To ensure the system remains in the desired state defined in Git
* [ ] To prevent unauthorized changes to the system

**Correct answer:**
* [x] To ensure the system remains in the desired state defined in Git

**22. What is the main difference between push-based and pull-based deployment approaches?**


* [ ] Push-based approach requires an external system with direct access to the cluster, while pull-based approach applies changes from within the cluster.
* [ ] Push-based approach is more secure than pull-based approach.
* [ ] Pull-based approach involves a continuous delivery (CD) pipeline triggering deployments.
* [ ] Pull-based approach requires exposing cluster credentials.

**Correct answer:**
* [x] Push-based approach requires an external system with direct access to the cluster, while pull-based approach applies changes from within the cluster.

**23. In a pull-based deployment approach, how are updates to the cluster initiated?**


* [ ] Updates are pushed by an external system with direct access to the cluster.
* [ ] Updates are initiated internally from within the cluster.
* [ ] Updates are automatically synchronized with Helm charts.
* [ ] Updates are triggered by monitoring Docker registries.

**Correct answer:**
* [x] Updates are initiated internally from within the cluster.

**24. Which approach exposes cluster credentials outside the cluster, potentially posing security risks?**


* [ ] Push-based deployment approach
* [ ] Pull-based deployment approach
* [ ] Both push-based and pull-based approaches
* [ ] Neither push-based nor pull-based approaches

**Correct answer:**
* [x] Push-based deployment approach

**25. Which of the following is a benefit of a pull-based deployment approach?**


* [ ] Reduced risk of unauthorized access or malicious attacks
* [ ] Easy synchronization of updates to Helm charts
* [ ] Streamlined and automated management of secrets
* [ ] Ability to inject container version updates through the build pipeline

**Correct answer:**
* [x] Reduced risk of unauthorized access or malicious attacks

**26. What is a potential drawback of the pull-based deployment approach?**


* [ ] Exposing cluster credentials outside the cluster
* [ ] Having the CI system with read/write access to the cluster
* [ ] Being bound to specific tools and requiring agent installation/configuration for each cluster
* [ ] Being coupled with the CD system, making migration to other platforms complex

**Correct answer:**
* [x] Being bound to specific tools and requiring agent installation/configuration for each cluster

**27. How can a pull-based approach support a multi-tenant model in Kubernetes?**


* [ ] By granting external clients permissions to publish updates to the cluster
* [ ] By utilizing lightweight tools that do not burden the system
* [ ] By distributing tools across namespaces with distinct access rights and corresponding git repositories
* [ ] By decoupling the deployment process from CD pipelines

**Correct answer:**
* [x] By distributing tools across namespaces with distinct access rights and corresponding git repositories

**28. What is the main benefit of using GitOps in managing Kubernetes infrastructure and applications?**


* [ ] Rapid and predictable deployments
* [ ] Reduced reliance on Git repositories
* [ ] Increased complexity in deployment processes
* [ ] Manual synchronization of desired and actual states

**Correct answer:**
* [x] Rapid and predictable deployments

**29. How can a developer update the Kubernetes manifest repository after pushing a newly built image to the container registry?**


* [ ] Raise a pull request (PR) to the application code repository
* [ ] Modify the YAML files referencing the image and commit the changes
* [ ] Execute commands like argocd app history and argocd app rollback
* [ ] Review and approve the PR by the project manager or architect

**Correct answer:**
* [x] Modify the YAML files referencing the image and commit the changes

**30. Which component of GitOps is responsible for monitoring the Kubernetes manifest repository and deploying resources to achieve the desired state?**


* [ ] CI pipeline
* [ ] Docker image
* [ ] ArgoCD operator
* [ ] Container registry

**Correct answer:**
* [x] ArgoCD operator

**31. What does ArgoCD do when it detects changes in the Kubernetes manifest repository?**


* [ ] Syncs the cluster with the desired state
* [ ] Creates a new branch in the repository
* [ ] Triggers a CI pipeline for testing and building artifacts
* [ ] Manually deploys the updated version of the application

**Correct answer:**
* [x] Syncs the cluster with the desired state

**32. In GitOps with ArgoCD, what command can be used to rollback to the previously stable version of the application?**


* [ ] git pull
* [ ] argocd app history
* [ ] argocd app rollback
* [ ] kubectl apply

**Correct answer:**
* [x] argocd app rollback

---


## Mock Exam - KCNA

**1. Which tool is commonly used for advanced data visualization and analysis in conjunction with Prometheus for monitoring and observability purposes?**


* [ ] Grafana
* [ ] Loki
* [ ] InfluxDB
* [ ] Dynatrace

**Correct answer:**
* [x] Grafana

**2. Which approach in DevOps involves managing infrastructure changes by storing them as code in a version control repository, followed by automated processes that deploy and update the infrastructure accordingly?**


* [ ] Infrastructure as Code (IaC)
* [ ] Continuous Integration (CI)
* [ ] Continuous Deployment (CD)
* [ ] GitOps

**Correct answer:**
* [x] GitOps

**3. Which CNCF project provides a platform for distributed tracing, monitoring, and performance optimization in microservices architectures?**


* [ ] Envoy
* [ ] Linkerd
* [ ] Jaeger
* [ ] Fluentd

**Correct answer:**
* [x] Jaeger

**4. In Kubernetes, which Higher Level construct is primarily responsible for managing the desired state of identical, stateless pods and overseeing their lifecycle?**


* [ ] ReplicaSet
* [ ] Deployment
* [ ] StatefulSet
* [ ] PodDisruptionBudget

**Correct answer:**
* [x] Deployment

**5. Among the different deployment strategies, which approach ensures both zero-downtime deployment and the ability to immediately rollback in case of issues?**


* [ ] Progressive deployment
* [ ] Blue-Green deployment
* [ ] Recreate deployment
* [ ] A/B deployment

**Correct answer:**
* [x] Blue-Green deployment

**6. Which container runtime, known for its emphasis on simplicity, robustness, and portability, has become an industry standard?**


* [ ] Containerd
* [ ] Kubernetes
* [ ] CRI-O
* [ ] Docker

**Correct answer:**
* [x] Containerd

**7. Which mechanism in Kubernetes enables you to specify criteria for selecting information based on fields such as name, namespace, or status of a Kubernetes object?**


* [ ] Tag Matchers
* [ ] Field Selectors
* [ ] Label Selectors
* [ ] Attribute Filters

**Correct answer:**
* [x] Field Selectors

**8. Which CNCF project focuses on providing a unified framework for generating, managing, and analyzing logs, metrics, and traces through a comprehensive set of tools, APIs, and SDKs?**


* [ ] Envoy
* [ ] Fluentd
* [ ] Prometheus
* [ ] OpenTelemetry

**Correct answer:**
* [x] OpenTelemetry

**9. In the evolving landscape of Kubernetes, what is the recommended successor to PodSecurityPolicies (PSP) for enforcing fine-grained security policies?**


* [ ] Kubernetes Role-Based Access Control (RBAC)
* [ ] Kubernetes Admission Controllers
* [ ] Kubernetes Network Policies
* [ ] Kubernetes Secrets Management

**Correct answer:**
* [x] Kubernetes Admission Controllers

**10. Which Kubernetes components play a pivotal role in mediating communication between the control plane nodes and worker nodes in a cluster?**


* [ ] kube-apiserver
* [ ] kube-controller-manager
* [ ] kube-scheduler
* [ ] kube-proxy

**Correct answer:**
* [x] kube-proxy

**11. In the context of distributed tracing, which statement accurately describes the relationship between a trace and a span?**


* [ ] A trace represents an individual operation within a transaction, while a span encompasses multiple transactions.
* [ ] A trace consists of multiple spans that collectively represent a transaction, while a span refers to an individual operation within a trace.
* [ ] A trace and a span are interchangeable terms used to represent different aspects of a transaction.
* [ ] A trace and a span are unrelated concepts in the context of distributed tracing.

**Correct answer:**
* [x] A trace consists of multiple spans that collectively represent a transaction, while a span refers to an individual operation within a trace.

**12. In a Kubernetes cluster, when a developer requires the assurance that specific pods are running on every node, which component should be utilized?**


* [ ] CronJob
* [ ] Deployment
* [ ] DaemonSet 
* [ ] PodDisruptionBudget

**Correct answer:**
* [x] DaemonSet 

**13. Which DevOps methodology involves managing infrastructure configurations declaratively through code and applying changes by committing them to a version control repository?**


* [ ] Infrastructure as Code (IaC)
* [ ] Continuous Delivery (CD)
* [ ] Continuous Integration (CI)
* [ ] Site Reliability Engineering (SRE)

**Correct answer:**
* [x] Infrastructure as Code (IaC)

**14. In the context of application deployment, which term refers to a self-contained software package that includes all the necessary components and dependencies to run the application?**


* [ ] Container
* [ ] Container Image
* [ ] Microservice
* [ ] Serverless Function

**Correct answer:**
* [x] Container Image

**15. Kubernetes follows an API pattern where a specific component acts as the central hub for communication. Which Kubernetes component serves as the hub for API interactions and manages the overall control of the cluster?**


* [ ] Container runtime
* [ ] Scheduler
* [ ] Controller Manager
* [ ] API server

**Correct answer:**
* [x] API server

**16. Which statement accurately describes the role and purpose of the Container Networking Interface (CNI) in Kubernetes?**


* [ ] A standardized set of tools for managing container storage and volumes in Kubernetes.
* [ ] A framework that enables seamless integration between Kubernetes and cloud provider APIs.
* [ ] A container orchestration engine designed specifically for multi-cloud deployments.
* [ ] A specification and libraries for writing plugins to configure network interfaces in Linux containers.

**Correct answer:**
* [x] A specification and libraries for writing plugins to configure network interfaces in Linux containers.

**17. To access the interactive shell of a container within a pod, which command or action is commonly used?**


* [ ] kubectl shell -it test-pod --  /bin/sh
* [ ] kubectl exec -it test-pod -- /shell
* [ ] kubectl exec -it test-pod -- /bin/bash
* [ ] kubectl run -it test-pod --  /bin/sh

**Correct answer:**
* [x] kubectl exec -it test-pod -- /bin/bash

**18. Which Kubernetes component is responsible for maintaining the desired state of the cluster and triggering actions to reconcile any deviations?**


* [ ] kube-apiserver
* [ ] kube-controller-manager
* [ ] etcd
* [ ] kube-proxy

**Correct answer:**
* [x] kube-controller-manager

**19. Which component in a Kubernetes cluster is responsible for managing the state of worker nodes and ensuring their proper functioning?**


* [ ] kubelet
* [ ] kube controller Manager
* [ ] kube-apiserver
* [ ] kube-proxy

**Correct answer:**
* [x] kubelet

**20. Which endpoint is typically used to access metrics on an HTTP server?**


* [ ] /metrics
* [ ] /logs
* [ ] /traces
* [ ] /endpoints

**Correct answer:**
* [x] /metrics

**21. Which component in Kubernetes continuously monitors the state of the cluster, detects any deviations from the desired state, and initiates or requests changes to ensure the cluster remains in the desired state?**


* [ ] Scheduler
* [ ] Orchestrator
* [ ] Controller
* [ ] Resource manager

**Correct answer:**
* [x] Controller

**22. In Kubernetes, when considering network policies within a namespace, what is the default behavior regarding ingress and egress traffic to and from pods if no policies are defined?**


* [ ] All ingress and egress traffic is allowed to and from pods in the namespace
* [ ] All ingress and egress traffic is denied to and from pods in the namespace
* [ ] Ingress traffic is allowed, but egress traffic is denied to and from pods in the namespace
* [ ] Ingress traffic is denied, but egress traffic is allowed to and from pods in the namespace

**Correct answer:**
* [x] All ingress and egress traffic is allowed to and from pods in the namespace

**23. In a Kubernetes cluster, which component is responsible for integrating the cluster with the API of the cloud provider, enabling seamless interaction with cloud-specific resources and services?**


* [ ] kube-proxy
* [ ] kube-controller-manager
* [ ] kubelet
* [ ] cloud-controller-manager

**Correct answer:**
* [x] cloud-controller-manager

**24. Which feature in Kubernetes enables the segregation of resources into distinct groups within a single cluster?**


* [ ] Resource groups
* [ ] Labels
* [ ] Namespaces
* [ ] Isolation sets

**Correct answer:**
* [x] Namespaces

**25. Which of the following container runtimes is a lightweight, low-level runtime designed specifically for running containers in Linux?**


* [ ] Containerd
* [ ] Docker
* [ ] LXC
* [ ] runc

**Correct answer:**
* [x] LXC

**26. In Kubernetes, what feature is utilized to retrieve a collection of objects based on a key/value pair?**


* [ ] Annotations
* [ ] Pods
* [ ] Selectors
* [ ] Namespaces

**Correct answer:**
* [x] Selectors

**27. What is the expanded form of the acronym CNCF, which represents a prominent organization driving the adoption of cloud-native technologies?**


* [ ] Containerized Networking and Computing Forum
* [ ] Cloud Native Computing Foundation
* [ ] Continuous Networking and Computing Framework
* [ ] Container Native Computing Foundation

**Correct answer:**
* [x] Cloud Native Computing Foundation

**28. Among the following features in Kubernetes, which one enables kubelet to seamlessly integrate with multiple container runtimes without requiring recompilation of cluster components?**


* [ ] Kubernetes Resource Manager
* [ ] Container Orchestration Service (COS)
* [ ] Container Runtime Interface (CRI)
* [ ] Pod Networking Plugin

**Correct answer:**
* [x] Container Runtime Interface (CRI)

**29. In Kubernetes, what are the fundamental units of deployment that encapsulate one or more containers and their shared resources?**


* [ ] Nodes
* [ ] Microservices
* [ ] Pods
* [ ] Deployments

**Correct answer:**
* [x] Pods

**30. In a Kubernetes YAML manifest file, which attribute controls the behavior of the kubelet when pulling a specified container image?**


* [ ] ImagePullPolicy
* [ ] ImageRetrievalPolicy
* [ ] ImageFetchStrategy
* [ ] ImageDownloadMode

**Correct answer:**
* [x] ImagePullPolicy

**31. In Kubernetes, when a developer needs to securely store and transmit sensitive cloud credentials to applications running within a cluster, which component should be utilized to maintain the confidentiality of the credentials?**


* [ ] ConfigMaps
* [ ] Environment Variables
* [ ] Persistent Volumes
* [ ] Secrets

**Correct answer:**
* [x] Secrets

**32. Which Kubernetes distribution is specifically optimized for running a lightweight, single-node cluster for local development on Windows, Linux, and macOS?**


* [ ] K3s
* [ ] Minikube
* [ ] MicroK8s
* [ ] Rancher Kubernetes Engine (RKE)

**Correct answer:**
* [x] Minikube

**33. What is the term for the process in Kubernetes that involves assigning pods to nodes based on their resource requirements and constraints, ensuring that the kubelet can successfully run them?**


* [ ] Pod Preemption
* [ ] Node allocation
* [ ] Pod assignment
* [ ] Pod scheduling

**Correct answer:**
* [x] Pod scheduling

**34. In the domain of ensuring optimal system performance and reliability, what roles and tasks are typically associated with a Site Reliability Engineer (SRE)?**


* [ ] Enabling developers with secure and efficient access to logs, events, and performance metrics for effective troubleshooting and issue resolution.
* [ ] Designing and implementing automated incident response workflows to minimize response times and reduce human error.
* [ ] Mitigating service disruptions and restoring functionality during outages in underlying infrastructure services.
* [ ] All of the options are correct

**Correct answer:**
* [x] All of the options are correct

**35. Which of the following methods are used to attach additional descriptive information to Kubernetes objects?**


* [ ] Tags, selectors
* [ ] Manifests, Configurations
* [ ] Annotations, Labels
* [ ] Properties, selectors

**Correct answer:**
* [x] Annotations, Labels

**36. Which Kubernetes component enables developers to schedule jobs based on user-defined time intervals?**


* [ ] Jobs
* [ ] CronJobs
* [ ] TaskManager
* [ ] Scheduler

**Correct answer:**
* [x] CronJobs

**37. Which kubectl command is used to create a Kubernetes deployment named "test-dep" with the nginx image and configure it to have 3 replicas?**


* [ ] kubectl run test-dep --image=nginx --replicas=3
* [ ] kubectl new deployment my-dep --image=nginx --replicas=3
* [ ] kubectl deploy test-dep --image=nginx --replicas=3
* [ ] kubectl create deployment test-dep --image=nginx --replicas=3

**Correct answer:**
* [x] kubectl create deployment test-dep --image=nginx --replicas=3

**38. Which kubectl command allows you to retrieve a list of all available API groups in a Kubernetes cluster?**


* [ ] kubectl api-groups
* [ ] kubectl get api-groups
* [ ] kubectl api-versions
* [ ] kubectl api-resources

**Correct answer:**
* [x] kubectl api-versions

**39. In the context of containerized applications, which networking approach employs a Sidecar pattern, co-locating an auxiliary container with the primary container, to handle port mapping and traffic management tasks?**


* [ ] Service Proxy
* [ ] Reverse Proxy
* [ ] Ingress Controller
* [ ] Service Mesh

**Correct answer:**
* [x] Service Mesh

**40. In Kubernetes, which component was specifically introduced to supersede the outdated ReplicationControllers?**


* [ ] Pods
* [ ] DaemonSets
* [ ] StatefulSets
* [ ] Deployments

**Correct answer:**
* [x] Deployments

**41. In Kubernetes, which component serves as the default Domain Name System (DNS) solution for service discovery and name resolution within the cluster?**


* [ ] DNSMasq
* [ ] kube-dns
* [ ] CoreDNS
* [ ] Route53

**Correct answer:**
* [x] CoreDNS

**42. In the realm of Kubernetes, which serverless framework stands out for its exceptional speed, developer-centric approach, and optimal performance, aiming to enhance developer productivity?**


* [ ] OpenShift
* [ ] Knative
* [ ] Istio
* [ ] Fission

**Correct answer:**
* [x] Fission

**43. In Kubernetes, which storage API resource represents a distinct unit of storage in the cluster, provisioned either by an administrator or dynamically provisioned using storage classes, and can be mounted as volumes in pods?**


* [ ] PersistentVolumeClaim (PVC)
* [ ] StatefulSet (STS)
* [ ] StorageClass (SC)
* [ ] PersistentVolume (PV)

**Correct answer:**
* [x] PersistentVolume (PV)

**44. When it comes to securing Kubernetes, which two areas require careful attention and implementation of security measures?**


* [ ] Securing the network connections between pods and nodes
* [ ] application security and cluster component security
* [ ] Securing the physical infrastructure of the data center
* [ ] Securing the Kubernetes documentation and knowledge base

**Correct answer:**
* [x] application security and cluster component security

**45. When considering the 4C's of cloud native security, what is the correct order, starting from the outermost layer and progressing towards the innermost layer?**


* [ ] Container, Cluster, Cloud, Code
* [ ] Code, Cloud, Cluster, Container
* [ ] Cluster, Cloud, Container, Code
* [ ] Cloud, Cluster, Container, Code

**Correct answer:**
* [x] Cloud, Cluster, Container, Code

**46. The Role-Based Access Control (RBAC) API in Kubernetes encompasses the declaration of which four specific types of objects that govern authorization within the cluster?**


* [ ] ClusterRoleBinding, ClusterRole, Role, RoleBinding
* [ ] ServiceAccount, Namespace, Pod, Deployment
* [ ] Ingress, ConfigMap, PersistentVolumeClaim, Service
* [ ] Job, CronJob, DaemonSet, StatefulSet

**Correct answer:**
* [x] ClusterRoleBinding, ClusterRole, Role, RoleBinding

**47. Which industry-standard specification enables seamless integration of diverse block and file storage systems with container orchestration systems like Kubernetes?**


* [ ] Container Storage Interface (CSI)
* [ ] Storage Container Interface (SCI)
* [ ] Cloud Storage Integration (CSI)
* [ ] Container Storage Connector (CSC)

**Correct answer:**
* [x] Container Storage Interface (CSI)

**48. What are the supported authorization mechanisms or modules in Kubernetes?**


* [ ] Role-based access controls(RBAC)
* [ ] Webhooks
* [ ] Attribute-based access control(ABAC)
* [ ] All of the options are correct

**Correct answer:**
* [x] All of the options are correct

**49. In order to achieve optimal resource utilization and meet variable load demands in a 24/7 production service, which combination of features in Kubernetes can dynamically adjust the number of nodes in the cluster and the number of pods in a deployment?**


* [ ] Vertical Pod Autoscaler and pod autoscaling
* [ ] Cluster autoscaling and pod anti-affinity
* [ ] Horizontal Pod Autoscaler and cluster autoscaling
* [ ] Pod disruption budget and node affinity

**Correct answer:**
* [x] Horizontal Pod Autoscaler and cluster autoscaling

**50. Within the collaborative landscape of Kubernetes, what defines Special Interest Groups (SIGs)?**


* [ ] Exclusive teams focusing on specific areas or domains of the project.
* [ ] Dedicated working groups responsible for maintaining the stability and performance of the Kubernetes core components.
* [ ] Independent entities with authority over Kubernetes releases, overseeing the development and testing processes.
* [ ] Dynamic task forces formed to address immediate issues and challenges faced by Kubernetes users.

**Correct answer:**
* [x] Exclusive teams focusing on specific areas or domains of the project.

**51. Within the realm of ensuring reliable and high-performance services, which technical role assumes the responsibility of meticulously monitoring and upholding Service Level Agreements (SLAs), Service Level Indicators (SLIs), and Service Level Objectives (SLOs)?**


* [ ] System Administrators
* [ ] DevOps Engineers
* [ ] Cloud Architects
* [ ] Site Reliability Engineers(SRE)

**Correct answer:**
* [x] Site Reliability Engineers(SRE)

**52. Which of the following are common Service Level Indicators (SLIs) used to measure the performance of a system?**


* [ ] Request latency
* [ ] Error rate
* [ ] Saturation
* [ ] All of the options are correct

**Correct answer:**
* [x] All of the options are correct

**53. Which approach exposes cluster credentials outside the cluster, potentially posing security risks?**


* [ ] Push-based deployment approach
* [ ] Pull-based deployment approach
* [ ] Both push-based and pull-based approaches
* [ ] Neither push-based nor pull-based approaches

**Correct answer:**
* [x] Push-based deployment approach

**54. Which monitoring solution is responsible for collecting and aggregating metrics?**


* [ ] Grafana
* [ ] Nagios
* [ ] Prometheus
* [ ] Splunk

**Correct answer:**
* [x] Prometheus

**55. Given the following SLI and SLO examples, what does the SLO value represent?**


* [ ] The desired latency threshold to be achieved
* [ ] The maximum allowable latency
* [ ] The average latency observed
* [ ] The minimum acceptable latency

**Correct answer:**
* [x] The maximum allowable latency

**Code**: 
SLI: Latency
SLO: Latency < 100ms


**56. What is the name of the built-in query language used by Prometheus to query its stored metrics?**


* [ ] GraphiteQL
* [ ] InfluxQL
* [ ] PrometheusSQL
* [ ] PromQL

**Correct answer:**
* [x] PromQL

**57. In Prometheus, what type of database is used to store the scraped metrics?**


* [ ] Relational database
* [ ] Document database
* [ ] Time series database
* [ ] Graph database

**Correct answer:**
* [x] Time series database

**58. Which tool can be used to visualize and interact with data collected by Prometheus using the PromQL language?**


* [ ] Kibana
* [ ] Terraform
* [ ] Grafana
* [ ] Nagios

**Correct answer:**
* [x] Grafana

**59. Which two-layered approach does Kubernetes utilize for its autoscaling mechanism?**


* [ ] Pod-based scaling and node-based scaling
* [ ] Pod-based scaling and cluster-based scaling
* [ ] Node-based scaling and cluster-based scaling
* [ ] Pod-based scaling and service-based scaling

**Correct answer:**
* [x] Pod-based scaling and cluster-based scaling

**60. What sets apart horizontal scaling from vertical scaling in terms of resource allocation and capacity management?**


* [ ] Vertical scaling involves increasing the resources allocated to each individual unit, such as CPU or memory, while horizontal scaling focuses on adding more units or instances to the system.
* [ ] Vertical scaling revolves around adding more units or instances to the system, while horizontal scaling emphasizes increasing the resources allocated to each individual unit, such as CPU or memory.
* [ ] Vertical scaling and horizontal scaling are synonymous, representing the same concepts of expanding the system's capacity and resources.
* [ ] Vertical scaling and horizontal scaling are not relevant to autoscaling, as they pertain to manual adjustment of resources without considering automatic scaling mechanisms.

**Correct answer:**
* [x] Vertical scaling involves increasing the resources allocated to each individual unit, such as CPU or memory, while horizontal scaling focuses on adding more units or instances to the system.

---


## Quiz – Conventions

**1. Which of the following are acceptable function names from a conventions standpoint according to the recommendations from this course:**


* [ ] my_function() {}
* [ ] calculate_area() {}
* [ ] SUM_NUMBERS() {}
* [ ] clone_repo() {}

**Correct answer:**
* [x] calculate_area() {}
* [x] clone_repo() {}

**2. Why is it a recommended practice to expand variables using double quotes?**


* [ ] It isn’t necessary, neither recommended
* [ ] It looks cleaner and makes our code easier to read
* [ ] To protect the variable when concatenating other characters
* [ ] To prevent values to be split into different characters, among other things

**Correct answer:**
* [x] To prevent values to be split into different characters, among other things

**3. We may generally want to separate words in our variables or functions using which character:**


* [ ] The underscore character (_)
* [ ] There shouldn’t be separations between words when writing scripts in bash
* [ ] The space character
* [ ] The dash character (-)

**Correct answer:**
* [x] The underscore character (_)

**4. Why is it a recommended practice to surround variables using curly braces?**


* [ ] It isn’t necessary, neither recommended
* [ ] It looks cleaner and makes our code easier to read
* [ ] To protect the variable when concatenating other characters
* [ ] To prevent values to be split into different characters

**Correct answer:**
* [x] To protect the variable when concatenating other characters

**5. What is the definition of “Following a coding convention” in the context of a coding language?**


* [ ] Adopting a style when doing regular coding tasks like declaring variables, functions, etc
* [ ] Agreements made between groups of people on how to write code
* [ ] Ways to organize and structure our code when it spans across multiple files and folders
* [ ] All of the options are correct

**Correct answer:**
* [x] All of the options are correct

**6. Non-interactive shell refers generally to executing commands using which method:**


* [ ] Typing commands directly in the terminal
* [ ] Invoking commands through running scripts from files
* [ ] None of these options are correct
* [ ] There is no such concept in the Unix like Shell languages context.

**Correct answer:**
* [x] Invoking commands through running scripts from files

---


## Quiz 1 - Good Practices applied

**1. What is the purpose of the command || exit 1 construct?**


* [ ] It exits the script regardless of the command’s result.
* [ ] It continues execution, even if the command fails.
* [ ] It exits the script if the preceding command fails.
* [ ] None of the options are correct

**Correct answer:**
* [x] It exits the script if the preceding command fails.

**2. What does the set -e command do in a shell script?**


* [ ] Disables error checking
* [ ] Enables automatic exit on command errors
* [ ] Ignores command errors 
* [ ] None of the options are correct

**Correct answer:**
* [x] Enables automatic exit on command errors

**3. What is a no-op command in shell scripting?**


* [ ] A command that performs a complex operation
* [ ] A command that doesn’t do anything
* [ ] A command that redirects output
* [ ] A command that only works on certain operating systems

**Correct answer:**
* [x] A command that doesn’t do anything

**4. In shell scripting, where does standard error output go by default?**


* [ ] It is saved in a log file
* [ ] It is redirected to /dev/null
* [ ]  It is directed to the terminal
* [ ] It is redirected to standard input

**Correct answer:**
* [x]  It is directed to the terminal

**5. What does the set -o pipefail command do?**


* [ ] Disables error checking for piped commands
* [ ] Executes commands in a pipe in parallel
* [ ] Causes a pipeline to fail if any command fails
* [ ] None of the options are correct

**Correct answer:**
* [x] Causes a pipeline to fail if any command fails

**6. What does a robust script mean in the context of shell scripting?**


* [ ] A script that never fails
* [ ] A script that handles and anticipates errors gracefully
* [ ] A script that contains many commands
* [ ] A script that executes commands quickly

**Correct answer:**
* [x] A script that handles and anticipates errors gracefully

---


## Quiz - Expansions

**1. Which of the following is a command substitution expansion?**


* [ ] "${name/Bob/John}"
* [ ] "${name:-John}"
* [ ] echo "Today is $(date)"
* [ ] "${#name}"

**Correct answer:**
* [x] echo "Today is $(date)"

**2. Which of the following is a parameter length expansion?**


* [ ] "${name/Bob/John}"
* [ ] "${name:-John}"
* [ ] echo "Today is $(date)"
* [ ] "${#name}"

**Correct answer:**
* [x] "${#name}"

**3. Which of the following explains what the “command substitution expansion” does?**


* [ ] Replaces the content of a variable with a specified string.
* [ ] Executes the command inside the parentheses and replaces the entire expression with the output of that command.
* [ ] Counts the number of characters in a variable’s value.
* [ ] Expands command line arguments in a shell script.

**Correct answer:**
* [x] Executes the command inside the parentheses and replaces the entire expression with the output of that command.

**4. Which of the following explains what the “parameter expansion with a replace value” does?**


* [ ] It replaces all instances of a value in a shell script.
* [ ] It substitutes a default value when a variable is unset or null.
* [ ] It replaces a specified string or pattern within a variable’s value with another string.
* [ ] It expands arguments in a command line.

**Correct answer:**
* [x] It replaces a specified string or pattern within a variable’s value with another string.

**5. Which of the following explains what the “parameter length expansion” does?**


* [ ] Replaces the content of a variable with a specified string.
* [ ] Executes the command inside the parentheses and replaces the entire expression with the output of that command.
* [ ] Counts the number of characters in a variable’s value.
* [ ] Expands command line arguments in a shell script.

**Correct answer:**
* [x] Counts the number of characters in a variable’s value.

**6. Which of the following is a parameter expansion with a replace value?**


* [ ] echo "${name/Bob/John}"
* [ ] echo "${name}"
* [ ] echo "${name:-John}"
* [ ] All of the options are correct

**Correct answer:**
* [x] echo "${name/Bob/John}"

**7. Which of the following explains what the “parameter expansion to default value” does?**


* [ ] Replaces a value found in a variable with a specified replacement string.
* [ ] It assigns a variable with a default value so it can be used safely across the script.
* [ ] Attempts to expand a variable. If the variable is empty, it expands to a default value specified inside the curly braces expansion block.
* [ ] There is no such concept in the Unix like Shell languages context.

**Correct answer:**
* [x] Attempts to expand a variable. If the variable is empty, it expands to a default value specified inside the curly braces expansion block.

---


## Quiz - Spacelift

**1. Which of these is/are policies of Spacelift?**


* [ ] Login
* [ ] Access
* [ ] Approval
* [ ] All options are correct

**Correct answer:**
* [x] All options are correct

**2. What is/are the key features of Spacelift?**


* [ ] Drift detection
* [ ] Manage Infrastructure-as-Code (IaC) state
* [ ] User access control policies management
* [ ] Generate and preview plans before before applying pull request

**Correct answer:**
* [x] Drift detection
* [x] Manage Infrastructure-as-Code (IaC) state
* [x] User access control policies management
* [x] Generate and preview plans before before applying pull request

**3. Which policy engine does Spacelift use to write policy as code declaratively?**


* [ ] Open Policy Agent (OPA)
* [ ] Open Container Initiative (OCI)
* [ ] Container Network Interface (CNI)
* [ ] Open Policy Standard (OPS)

**Correct answer:**
* [x] Open Policy Agent (OPA)

**4. Which of the following Infrastructure-as-Code technologies can be used with Spacelift?**


* [ ] Terraform, Ansible, AWS CloudFormation, Pulumi
* [ ] Only Terraform
* [ ] Only AWS CloudFormation
* [ ] Only Ansible

**Correct answer:**
* [x] Terraform, Ansible, AWS CloudFormation, Pulumi

**5. Which Terraform command allows you to preview the changes that Terraform plans to make to your infrastructure?**


* [ ] The terraform preview command
* [ ] The terraform apply command
* [ ] The terraform plan command 
* [ ] The terraform run –dry-run command

**Correct answer:**
* [x] The terraform plan command 

**6. What is/are the challenges with traditional CI/CD tools with Infrastructure as Code?**


* [ ] Problem with concurrence runs due to state-lock 
* [ ] Hard to define and manage the access control policies in different application environments
* [ ] Does not work well with stateful applications 
* [ ] All options are correct

**Correct answer:**
* [x] All options are correct

**7. What are the correct steps to create a Spacelift Stack?**


* [ ] Configure Backend > Integrate VCS > Define Behavior > Name Stack
* [ ] Define Behavior > Configure Backend > Integrate VCS > Name Stack
* [ ] Name Stack > Define Behavior > Configure Backend > Integrate VCS
* [ ] Name Stack > Integrate VCS > Configure Backend > Define Behavior

**Correct answer:**
* [x] Name Stack > Integrate VCS > Configure Backend > Define Behavior

**8. What is Spacelift?**


* [ ] An identity provider 
* [ ] A CI/CD plugin with built-in access control
* [ ] An Infrastructure-as-Code (IaC) management platform
* [ ] A configuration management platform for CI/CD tools

**Correct answer:**
* [x] An Infrastructure-as-Code (IaC) management platform

**9. Which step in the Spacelift creation process allows us to define the Git repository and branch?**


* [ ] Configure Backend step
* [ ] Integrate VCS step
* [ ] Define Behavior step
* [ ] Name Stack step

**Correct answer:**
* [x] Integrate VCS step

**10. Which of these is a secure way to store environment variables in Spacelift?**


* [ ] PLAIN
* [ ] SECRET
* [ ] HASH
* [ ] VAULT

**Correct answer:**
* [x] SECRET

**11. How can we view the stack-level resources in the Spacelift dashboard?**


* [ ] Under Stack/Resources
* [ ] Under Stack/Output
* [ ] Under Stack/Dependencies 
* [ ] Under Stack/Runs 

**Correct answer:**
* [x] Under Stack/Resources

**12. Which Spacelift policy allows you to block suspicious runs before they start?**


* [ ] Initialization
* [ ] Push
* [ ] Task
* [ ] Trigger

**Correct answer:**
* [x] Initialization

**13. What is the main purpose of Task in Spacelift?**


* [ ] A key-value mapping available to all processes running in a given environment.
* [ ] Perform arbitrary changes to your infrastructure in a coordinated, safe, and audited way
* [ ] A bundle of configuration elements (environment variables and mounted files) independent of any stack that can be managed separately and attached to as many or as few stacks as necessary
* [ ] Keep policies functionally pure and relatively snappy

**Correct answer:**
* [x] Perform arbitrary changes to your infrastructure in a coordinated, safe, and audited way

**14. Which language does Spacelift use to execute policies?**


* [ ] Rego 
* [ ] Golang
* [ ] Python
* [ ] C++

**Correct answer:**
* [x] Rego 

**15. Which Spacelift policy allows you to grant or deny the appropriate level of stack access?**


* [ ] Approval policy
* [ ] Access policy
* [ ] Login policy
* [ ] Permission policy

**Correct answer:**
* [x] Access policy

**16. Which state indicates the successful run of the Spacelift Stack?**


* [ ] Completed
* [ ] Canceled 
* [ ] Finished
* [ ] Succeed 

**Correct answer:**
* [x] Finished

**17. Which of these is NOT correct about Spacelift policy?**


* [ ] User can’t define custom policy in Spacelift
* [ ] Spacelift uses an open-source project called Open Policy Agent (OPA)
* [ ] Spacelift uses Rego as its rule/policy language
* [ ] Spacelift policies can be tested locally using opa test command

**Correct answer:**
* [x] User can’t define custom policy in Spacelift

**18. If we need to use secret credentials (eg, AWS access key), where should we define them in Spacelift?**


* [ ] As Spacelift environment variable(s)
* [ ] In the Spacelift policy with administrator access-only (restricted mode)
* [ ] Spacelift can’t manage secret credentials; need to use third-party solutions
* [ ] In the application source codes

**Correct answer:**
* [x] As Spacelift environment variable(s)

**19. How would a kubeconfig file be managed in Spacelift?**


* [ ] Define environment with name KUBE_CONFIG and value as kubeconfig.json file content
* [ ] Upload kubeconfig.json file as Mounted file 
* [ ] Upload kubeconfig.json file in the folder .spacelift/; file must be located at the root of your repository
* [ ] Spacelift doesn’t support Kubernetes at all

**Correct answer:**
* [x] Upload kubeconfig.json file as Mounted file 

---


## Quiz-Security

**1. In a Kubernetes cluster, what determines the actions and operations a user can perform once they have been authenticated?**


* [ ] Authentication mechanisms
* [ ] Network segmentation and firewall rules
* [ ] Authorization mechanisms
* [ ] Encryption protocols for secure communication

**Correct answer:**
* [x] Authorization mechanisms

**2. Which authentication methods can be used to control access to the API server in a Kubernetes environment, as mentioned in the given content?**


* [ ] User IDs and passwords stored in static files
* [ ] Two-factor authentication (2FA) using tokens
* [ ] Integration with external authentication providers like LDAP
* [ ] All of the options are correct

**Correct answer:**
* [x] All of the options are correct

**3. Which of the following modules can be used as part of the authorization process in a Kubernetes environment?**


* [ ] Node Authorizater modules and Attribute-based access control (ABAC) modules
* [ ] Certificate Authority (CA) modules and Role-based access control (RBAC) modules
* [ ] Network segmentation and firewall modules
* [ ] Encryption protocols for secure communication modules

**Correct answer:**
* [x] Node Authorizater modules and Attribute-based access control (ABAC) modules

**4. Which feature can be used to restrict access between the pods in a cluster?**


* [ ] Ingress Controller
* [ ] Load Balancer
* [ ] Network Policies
* [ ] Service Mesh

**Correct answer:**
* [x] Network Policies

**5. Which of the following measures should be taken to secure a Kubernetes infrastructure and ensure secure access to the cluster's hosts?**


* [ ] Enable root access and password-based authentication on the hosts.
* [ ] Disable root access and password-based authentication on the hosts.
* [ ] Enable root access and password-based authentication, while also providing SSH key-based authentication.
* [ ] Disable root access but allow password-based authentication on the hosts.

**Correct answer:**
* [x] Disable root access and password-based authentication on the hosts.

**6. How does Kubernetes manage user accounts by default?**


* [ ] Kubernetes creates and manages user accounts natively.
* [ ] Kubernetes relies on a centralized database service to manage user accounts by default.
* [ ] Kubernetes utilizes a built-in LDAP service to manage user accounts.
* [ ] Kubernetes uses a third-party identity service like LDAP to manage user accounts.

**Correct answer:**
* [x] Kubernetes uses a third-party identity service like LDAP to manage user accounts.

**7. Which security mechanism is used to ensure secure communication between the various components of a cluster, including the ETCD cluster, Kube controller, API server, and worker node components?**


* [ ] TLS encryption
* [ ] Service Accounts
* [ ] Node authorization
* [ ] Role-based access control

**Correct answer:**
* [x] TLS encryption

**8. What approach is commonly used to implement authorization in a Kubernetes environment, where users are assigned specific permissions based on their group associations?**


* [ ] Access control lists (ACLs) for fine-grained permission management
* [ ] Attribute-based access control (ABAC) for dynamic authorization decisions
* [ ] Role-based access control (RBAC) for defining user permissions
* [ ] Security Assertion Markup Language (SAML) for federated authentication

**Correct answer:**
* [x] Role-based access control (RBAC) for defining user permissions

**9. Can you create or view users directly within a Kubernetes cluster?**


* [ ] Yes, Kubernetes provides native user management capabilities.
* [ ] No, Kubernetes does not support creating or viewing users within the cluster.

**Correct answer:**
* [x] No, Kubernetes does not support creating or viewing users within the cluster.

**10. Which component in a Kubernetes cluster handles all user requests from the kubectl utility or direct API access?**


* [ ] Kubelet
* [ ] ETCD Cluster
* [ ] Kube-controller-manager
* [ ] Kube-api server

**Correct answer:**
* [x] Kube-api server

**11. Can Kubernetes manage service accounts?**


* [ ] Yes, Kubernetes provides built-in capabilities to manage service accounts.
* [ ] No, Kubernetes does not support managing service accounts.
* [ ] Yes, but only through third-party plugins or extensions.
* [ ] No, service accounts are managed externally outside of Kubernetes.

**Correct answer:**
* [x] Yes, Kubernetes provides built-in capabilities to manage service accounts.

**12. Which of the following statements is true about the default behavior of kubectl tool?**


* [ ] kubectl looks for a file named kubeconfig under a directory .kube in the user's home directory.
* [ ] kubectl looks for a file named config under a directory .kubernetes in the user's home directory.
* [ ] kubectl looks for a file named config under a directory .kube in the user's home directory.
* [ ] kubectl looks for a file named kubectl-config under a directory .kube in the user's home directory

**Correct answer:**
* [x] kubectl looks for a file named config under a directory .kube in the user's home directory.

**13. What does the clusters section contain in the kubeconfig file?**


* [ ] Information about Kubernetes nodes in the cluster
* [ ] Configuration details for Kubernetes services
* [ ] The names of different Kubernetes namespaces
* [ ] Details of different Kubernetes clusters that you need access to

**Correct answer:**
* [x] Details of different Kubernetes clusters that you need access to

**14. Which of the following statements is true about using KubeConfig in kubectl?**


* [ ] KubeConfig is a file that contains Kubernetes configuration information.
* [ ] KubeConfig is a built-in plugin that extends the functionality of kubectl with additional features and commands.
* [ ] KubeConfig is a replacement for the kubectl utility.
* [ ] KubeConfig cannot be used to store configuration information for Kubernetes clusters.

**Correct answer:**
* [x] KubeConfig is a file that contains Kubernetes configuration information.

**15. Which of the following sections are included in the kubeconfig file?**


* [ ] Namespaces, Pods, and Deployments
* [ ] Clusters, Services, and Nodes
* [ ] Clusters, Users, and Contexts
* [ ] Jobs, ConfigMaps, and Secrets

**Correct answer:**
* [x] Clusters, Users, and Contexts

**16. What does the users section contain in the kubeconfig file?**


* [ ] Information about Kubernetes API resources
* [ ] Configuration details for Kubernetes deployments
* [ ] The names of different Kubernetes namespaces
* [ ] Details of the user accounts that you have access to in the Kubernetes cluster

**Correct answer:**
* [x] Details of the user accounts that you have access to in the Kubernetes cluster

**17. What does the context section define in the kubeconfig file?**


* [ ] The Kubernetes namespace in which resources will be created
* [ ] The Kubernetes API version to be used
* [ ] The user account to be used to access the Kubernetes cluster
* [ ] The mapping between user accounts and Kubernetes clusters

**Correct answer:**
* [x] The mapping between user accounts and Kubernetes clusters

**18. Which kubectl command can be used to view the current kubeconfig file being used?**


* [ ] kubectl config create
* [ ] kubectl config edit
* [ ] kubectl config view
* [ ] kubectl config delete

**Correct answer:**
* [x] kubectl config view

**19. What field can be added to the kubeconfig file to specify a default context to use when multiple contexts are defined?**


* [ ] default-context
* [ ] active-context
* [ ] current-context
* [ ] main-context

**Correct answer:**
* [x] current-context

**20. What is the kubectl command to use a kubeconfig file other than the default one?**


* [ ] kubectl config edit --kubeconfig=my-config
* [ ] kubectl config use my-config
* [ ] kubectl config view --kubeconfig=my-config
* [ ] kubectl config switch my-config

**Correct answer:**
* [x] kubectl config view --kubeconfig=my-config

**21. What is the kubectl command to change the current context?**


* [ ] kubectl config use-context \<context\>
* [ ] kubectl config set-context \<context\>
* [ ] kubectl config change-context \<context\>
* [ ] kubectl config edit-context \<context\>

**Correct answer:**
* [x] kubectl config use-context \<context\>

**22. Which field can be used to provide the contents of a certificate itself instead of using certificate-authority and the path to the file?**


* [ ] certificate-authority-path
* [ ] certificate-data
* [ ] certificate-authority-data
* [ ] certificate-content

**Correct answer:**
* [x] certificate-authority-data

**23. What additional field can be added to the context section in the kubeconfig file to specify a particular namespace?**


* [ ] pod
* [ ] container
* [ ] namespace
* [ ] service

**Correct answer:**
* [x] namespace

**24. Which of the following is true about the format of the sections in the kubeconfig file?**


* [ ] All sections are in the JSON format.
* [ ] The sections are separated by commas.
* [ ] Each section is in an array format.
* [ ]  The sections are in the YAML format.

**Correct answer:**
* [x] Each section is in an array format.

**25. What do you need to do before providing the contents of the certificate file in the certificate-authority-data field of the kubeconfig file?**


* [ ] Compress the file to reduce its size.
* [ ] Encrypt the file to protect its contents.
* [ ] Base-64 encode the content.
* [ ] Convert the file to a different format.

**Correct answer:**
* [x] Base-64 encode the content.

**26. What are the different subgroups of the Kubernetes API?**


* [ ] Pods, Services, Deployments
* [ ] Resources, Permissions, Requests
* [ ] Ingress, ConfigMaps, Secrets
* [ ] APIs, Healthz, Metrics, Logs

**Correct answer:**
* [x] APIs, Healthz, Metrics, Logs

**27. What are the two groups into which the Kubernetes APIs responsible for cluster functionality are categorized?**


* [ ] Primary group and secondary group
* [ ] System group and user group
* [ ] Core group and named group
* [ ] Health group and metrics group

**Correct answer:**
* [x] Core group and named group

**28. Which Kubernetes API is used to monitor the health of the cluster?**


* [ ] version
* [ ] logs
* [ ] metrics and healthz
* [ ] config

**Correct answer:**
* [x] metrics and healthz

**29. What is the purpose of the version API in Kubernetes?**


* [ ] To create new Kubernetes resources
* [ ] To delete Kubernetes resources
* [ ] To view the version of the cluster
* [ ] To update Kubernetes resources

**Correct answer:**
* [x] To view the version of the cluster

**30. What is the purpose of the named group APIs in Kubernetes?**


* [ ] To provide core functionality such as namespaces, pods, and services
* [ ] To monitor the health of the cluster
* [ ] To organize and provide access to newer features
* [ ] To view the version of the cluster

**Correct answer:**
* [x] To organize and provide access to newer features

**31. Which of the following API groups contain core functionality in Kubernetes?**


* [ ] metrics
* [ ] healthz
* [ ] core group
* [ ] named group

**Correct answer:**
* [x] core group

**32. Which of the following is a subgroup of the named group in Kubernetes API?**


* [ ] networking.k8s.io
* [ ] apps
* [ ] extensions
* [ ] All of the options are correct

**Correct answer:**
* [x] All of the options are correct

**33. Which of the following resources belong to the "apps" subgroup in Kubernetes?**


* [ ] ConfigMaps
* [ ] deployments, replicasets
* [ ] Services, PersistentVolumes
* [ ] endpoints, namespaces

**Correct answer:**
* [x] deployments, replicasets

**34. Which of the following statements are true about the resources in the apps subgroup of the named group in Kubernetes API?**


* [ ] Each resource in the apps subgroup has a set of actions associated with it.
* [ ] The apps subgroup includes services and endpoints.
* [ ] The resources in the apps subgroup are deprecated and will be removed in future versions of Kubernetes.
* [ ] The apps subgroup is primarily used for managing networking and communication within the Kubernetes cluster.

**Correct answer:**
* [x] Each resource in the apps subgroup has a set of actions associated with it.

**35. Which of the following statements is true regarding Kube Proxy and Kubectl Proxy?**


* [ ] Kube Proxy and Kubectl Proxy are the same thing.
* [ ] Kube Proxy and Kubectl Proxy are different things.
* [ ] Kube Proxy is a subcomponent of Kubectl Proxy.
* [ ] Kubectl Proxy is a subcomponent of Kube Proxy.

**Correct answer:**
* [x] Kube Proxy and Kubectl Proxy are different things.

**36. What is the purpose of Kube Proxy in a Kubernetes cluster?**


* [ ] It is used to create and manage Kubernetes objects like Pods and Services.
* [ ] It is used to enable connectivity between pods and services across different nodes in the cluster.
* [ ] It is used to monitor the health of the Kubernetes cluster.
* [ ] It is used to manage the storage resources in a Kubernetes cluster.

**Correct answer:**
* [x] It is used to enable connectivity between pods and services across different nodes in the cluster.

**37. What is Kubectl Proxy used for?**


* [ ] To enable connectivity between pods and service across different nodes in the cluster.
* [ ] To access the kube-api server via an HTTP proxy service.
* [ ] To monitor the health of the Kubernetes cluster.
* [ ] To manage the networking of Kubernetes clusters.

**Correct answer:**
* [x] To access the kube-api server via an HTTP proxy service.

**38. What is the role of the node authoriser in Kubernetes cluster?**


* [ ] Handles requests from human users to the kube-apiserver
* [ ] Manages access control policies for the Kubernetes cluster
* [ ] Deploys and manages nodes within the Kubernetes cluster
* [ ] Handles requests from the kubelets on nodes to the kube-apiserver

**Correct answer:**
* [x] Handles requests from the kubelets on nodes to the kube-apiserver

**39. Who is authorized by the node authorisers to make requests to kube-apiserver?**


* [ ] Any user with system administrator privileges
* [ ] Any user part of the Kubernetes cluster
* [ ] Any requests coming from a user with the name system node and part of the system nodes group
* [ ] Only the kubelet running on the master node

**Correct answer:**
* [x] Any requests coming from a user with the name system node and part of the system nodes group

**40. What is attribute-based authorization?**


* [ ] A type of authorization where users are associated with a set of permissions.
* [ ] A type of authorization that only allows access based on the user's username and password.
* [ ] A type of authorization that only allows access based on the user's IP address.
* [ ] A type of authorization that only allows access based on the user's device.

**Correct answer:**
* [x] A type of authorization where users are associated with a set of permissions.

**41. What is the role of Open Policy Agent in a Kubernetes cluster with regard to authorisation?**


* [ ] It is a tool for managing Kubernetes resources such as pods, services, and deployments.
* [ ]  It provides a graphical user interface for managing users and permissions in Kubernetes.
* [ ]  It helps with admission control and authorisation by making API calls to decide whether a user should be permitted access based on their access requirements.
* [ ] It is a built-in authorization mechanism in Kubernetes.

**Correct answer:**
* [x]  It helps with admission control and authorisation by making API calls to decide whether a user should be permitted access based on their access requirements.

**42. What is the authorisation mode that allows all requests without performing any authorisation checks?**


* [ ] AlwaysDeny
* [ ] Attribute-based
* [ ] AlwaysAllow
* [ ] Open Policy Agent

**Correct answer:**
* [x] AlwaysAllow

**43. What format is used to define a set of policies in Attribute-based authorisation?**


* [ ] YAML format
* [ ] XML format
* [ ] JSON format
* [ ] HTML format

**Correct answer:**
* [x] JSON format

**44. What is the behavior of the AlwaysDeny authorisation mode in Kubernetes?**


* [ ] Allows all requests without performing any authorisation checks
* [ ] Denies all requests
* [ ] Allows requests based on a policy file with a set of policies defined in a JSON format
* [ ] Authorises requests coming from a user with the name system node and part of the system nodes group

**Correct answer:**
* [x] Denies all requests

**45. What is Role-Based Access Control (RBAC)?**


* [ ] It is a way of associating users with a set of permissions.
* [ ] It is a way of creating a role with a set of permissions required for a specific group and then associating users to that role.
* [ ] It is a way of outsourcing all the authorization mechanisms to a third party tool like Open Policy Agent.
* [ ] It is a way of denying all requests without performing any authorization checks.

**Correct answer:**
* [x] It is a way of creating a role with a set of permissions required for a specific group and then associating users to that role.

**46. What API version and kind do you need to set to create a role object in Kubernetes using a role definition file?**


* [ ] API version: rbac.authorization.k8s.io/v2, kind: Role
* [ ] API version: rbac.authorization.k8s.io/v1, kind: RoleBinding
* [ ] API version: rbac.authorization.k8s.io/v1, kind: Role
* [ ] API version: rbac.authorization.k8s.io/v2, kind: ClusterRole

**Correct answer:**
* [x] API version: rbac.authorization.k8s.io/v1, kind: Role

**47. What is the default authorisation mode for the Kube-api server if the 'authorisation mode’ option is not specified?**


* [ ] AlwaysDeny
* [ ] AlwaysAllow
* [ ] Node authorisation
* [ ] Attribute-based authorisation

**Correct answer:**
* [x] AlwaysAllow

**48. How can you create a role in Kubernetes using the command line interface?**


* [ ] kubectl create role
* [ ] kubectl create rolebinding
* [ ] kubectl create clusterrole
* [ ] kubectl create clusterrolebinding

**Correct answer:**
* [x] kubectl create role

**49. What is the purpose of a RoleBinding object in Kubernetes?**


* [ ] It defines a role with a set of permissions for a specific group of users.
* [ ] It defines a role with a set of permissions for a specific user.
* [ ] It links a user object to a role.
* [ ] It manages the lifecycle of pods in a cluster.

**Correct answer:**
* [x] It links a user object to a role.

**50. Which of the following statements is true about the scope of roles and role bindings in Kubernetes?**


* [ ] Roles and role bindings are global and apply to all namespaces.
* [ ] Roles are global, but role bindings apply only to the namespace they are defined in.
* [ ] Both roles and role bindings fall under the scope of namespaces.
* [ ] Neither roles nor role bindings are specific to namespaces.

**Correct answer:**
* [x] Both roles and role bindings fall under the scope of namespaces.

**51. What are the two sections of the RoleBinding object?**


* [ ] Role and User
* [ ] Subjects and RoleRef
* [ ] Users and Permissions
* [ ] APIGroups and Resources

**Correct answer:**
* [x] Subjects and RoleRef

**52. In a Kubernetes cluster, if you want to view the roles defined within a specific namespace, which command should you use?**


* [ ] kubectl get roles
* [ ] kubectl view roles
* [ ] kubectl show roles
* [ ] kubectl list roles

**Correct answer:**
* [x] kubectl get roles

**53. What command should you use to list all the role bindings in a Kubernetes cluster?**


* [ ] kubectl get rolebindings
* [ ] kubectl view rolebindings
* [ ] kubectl show rolebindings
* [ ] kubectl list rolebindings

**Correct answer:**
* [x] kubectl get rolebindings

**54. What command should you use to view more details about a specific role in a Kubernetes cluster?**


* [ ] kubectl describe role \<role-name\>
* [ ] kubectl get role \<role-name\>
* [ ] kubectl view role \<role-name\>
* [ ] kubectl show role \<role-name\>

**Correct answer:**
* [x] kubectl describe role \<role-name\>

**55. What command should you use to check if you have access to create deployments in a Kubernetes cluster?**


* [ ] kubectl check access create deployments
* [ ] kubectl auth can-i create deployments
* [ ] kubectl test permissions create deployments
* [ ] kubectl validate access create deployments

**Correct answer:**
* [x] kubectl auth can-i create deployments

**56. What command should you use to impersonate another user and check their permission to create deployments in a Kubernetes cluster?**


* [ ] kubectl auth can-i create deployments --as \<user\>
* [ ] kubectl check access create deployments --as \<user\>
* [ ] kubectl test permissions create deployments --as \<user\>
* [ ] kubectl validate access create deployments --as \<user\>

**Correct answer:**
* [x] kubectl auth can-i create deployments --as \<user\>

**57. What field can you use to allow access to specific resources alone in Kubernetes?**


* [ ] resourceNames field
* [ ] resourceTypes field
* [ ] resourceKind field
* [ ] resourceVersion field

**Correct answer:**
* [x] resourceNames field

**58. In Kubernetes, how are resources categorized based on their scope?**


* [ ] As public or private
* [ ] As restricted or unrestricted
* [ ] As namespaced or clusters scoped
* [ ] As read-only or read-write

**Correct answer:**
* [x] As namespaced or clusters scoped

**59. Which of the following Kubernetes resources are namespaced?**


* [ ] Pods
* [ ] Roles
* [ ] Deployments
* [ ] All of the options are correct

**Correct answer:**
* [x] All of the options are correct

**60. Which of the following Kubernetes resources are cluster-scoped?**


* [ ] Pods
* [ ] Services
* [ ] Containers
* [ ] Persistent volumes

**Correct answer:**
* [x] Persistent volumes

**61. Which command would you use to see a full list of non-namespaced resources in Kubernetes?**


* [ ] kubectl api-resources --namespaced=true
* [ ] kubectl api-resources --namespaced=false
* [ ] kubectl api-resources --all
* [ ] kubectl api-resources

**Correct answer:**
* [x] kubectl api-resources --namespaced=false

**62. Which of the following commands is used to see a full list of Kubernetes namespaced resources?**


* [ ] kubectl api-resources –namespaced=true
* [ ] kubectl get nodes –namespaced=false
* [ ] kubectl get persistentvolumes –namespaced=true
* [ ] kubectl get deployments

**Correct answer:**
* [x] kubectl api-resources –namespaced=true

**63. What is used to authorize users to cluster-wide resources like nodes and persistent volumes in Kubernetes?**


* [ ] Resource quotas
* [ ] Namespaces
* [ ] Role bindings and Roles
* [ ] Cluster roles and cluster role bindings

**Correct answer:**
* [x] Cluster roles and cluster role bindings

**64. Which keyword is used in a cluster role definition file to create a cluster role?**


* [ ] kind: Role
* [ ] kind: ClusterRole
* [ ] type: Role
* [ ] type: ClusterRole

**Correct answer:**
* [x] kind: ClusterRole

**65. What is a service account in Kubernetes?**


* [ ] An account used to log in to the Kubernetes dashboard
* [ ] An account used by an application to interact with a Kubernetes cluster
* [ ] An account used by a cluster administrator to manage Kubernetes resources
* [ ] An account used to manage access control for Kubernetes namespaces

**Correct answer:**
* [x] An account used by an application to interact with a Kubernetes cluster

**66. What is the purpose of a cluster role binding object in Kubernetes?**


* [ ] To create a new cluster role
* [ ] To link a role to a namespace
* [ ] To link a cluster role to a user/group
* [ ] To provide access to a specific resource

**Correct answer:**
* [x] To link a cluster role to a user/group

**67. Which command is used to create a service account in Kubernetes?**


* [ ] kubectl create sa \<account-name\>
* [ ] kubectl create account \<account-name\>
* [ ] kubectl create serviceaccount \<account-name\>
* [ ] kubectl add sa \<account-name\>

**Correct answer:**
* [x] kubectl create serviceaccount \<account-name\>
* [x] kubectl create sa \<account-name\>

**68. What command do you use to list all the service accounts in a Kubernetes cluster?**


* [ ] kubectl list serviceaccounts
* [ ] kubectl get serviceaccount
* [ ] kubectl list sa
* [ ] kubectl describe serviceaccount

**Correct answer:**
* [x] kubectl get serviceaccount

**69. What is the relationship between the secret object and the service account in Kubernetes?**


* [ ] The secret object is used to authenticate the service account.
* [ ] The service account is used to authenticate the secret object.
* [ ] The secret object is linked to the service account and stores the service account token.
* [ ] The service account is linked to the secret object and stores the API server endpoint.

**Correct answer:**
* [x] The secret object is linked to the service account and stores the service account token.

**70. If you create a cluster role for namespaced resources, what access do the users get?**


* [ ] Access to all resources within the namespace only
* [ ] Access to all resources across all namespaces in the cluster
* [ ] No access to any resources
* [ ] Access to only a few specified resources

**Correct answer:**
* [x] Access to all resources across all namespaces in the cluster

**71. What command can be used to view the contents of a secret object in Kubernetes?**


* [ ] kubectl get secret \<token-name\>
* [ ] kubectl show secret \<token-name\>
* [ ] kubectl list secret \<token-name\>
* [ ] kubectl describe secret \<token-name\>

**Correct answer:**
* [x] kubectl describe secret \<token-name\>

**72. What is the default service account created for every namespace in Kubernetes?**


* [ ] kube-system
* [ ] default
* [ ] system
* [ ] admin

**Correct answer:**
* [x] default

**73. What field should be added to a pod definition file to use a service account other than the default one in Kubernetes?**


* [ ] apiVersion
* [ ] kind
* [ ] metadata
* [ ] serviceAccountName

**Correct answer:**
* [x] serviceAccountName

**74. Can you edit the service account of an existing deployment in Kubernetes?**


* [ ] Yes, by modifying the deployment manifest file and updating the service account field.
* [ ] No, the service account is set when the deployment is created and cannot be changed afterwards.

**Correct answer:**
* [x] Yes, by modifying the deployment manifest file and updating the service account field.

**75. What statement/s is/are true about the service account of an existing pod in Kubernetes?**


* [ ] The service account is set when the pod is created and cannot be changed afterwards.
* [ ] The service account of a pod can be changed by directly modifying the service account field in the running pod using the kubectl apply command without modifying the pod definition file.
* [ ] You can change the service account by modifying the pod definition file and updating the service account name field.
* [ ] You can change the service account by running the command "kubectl edit pod <pod-name>" and modifying the service account field.

**Correct answer:**
* [x] The service account is set when the pod is created and cannot be changed afterwards.

**76. How can you choose not to automatically mount a service account token to a Kubernetes pod?**


* [ ] By setting the serviceAccountName field to an empty value in the pod's spec
* [ ] By removing the service account from the namespace associated with the pod
* [ ] By setting the automountServiceAccountToken field to false in the pod's spec
* [ ] By editing the pod's configuration file after it has been deployed

**Correct answer:**
* [x] By setting the automountServiceAccountToken field to false in the pod's spec

**77. What are the characteristics of tokens generated by the Kubernetes Token Request API?**


* [ ] Audience bound, object bound, and cluster bound
* [ ] Time bound, cluster bound, and namespace bound
* [ ] Audience bound, time bound, and object bound
* [ ] Time bound, object bound, and namespace bound

**Correct answer:**
* [x] Audience bound, time bound, and object bound

**78. Which of the following statements is true regarding service accounts in Kubernetes version 1.24?**


* [ ] When a service account is created, it automatically creates a secret and a token access secret.
* [ ] When a service account is created, it no longer automatically creates a secret or a token access secret.
* [ ] When a service account is created, it only creates a secret but not a token access secret.
* [ ] When a service account is created, it only creates a token access secret but not a regular secret.

**Correct answer:**
* [x] When a service account is created, it no longer automatically creates a secret or a token access secret.

**79. Which of the following commands is used to generate a token for a service account in Kubernetes version 1.24?**


* [ ] kubectl create secret generic \<secret-name\> --from-file=\<file-path\>
* [ ] kubectl create token \<service-account-name\>
* [ ] kubectl create deployment \<deployment-name\> --image=\<image-name\>
* [ ] kubectl create serviceaccount \<service-account-name\>

**Correct answer:**
* [x] kubectl create token \<service-account-name\>

**80. In the context of a Docker image name in a Kubernetes pod definition file, when the image name is provided without a prefix, what does the prefix usually represent, and what is the default value assumed if none is provided?**


* [ ] It specifies the image registry, and the default value is docker.io.
* [ ] It specifies the version of the image, and the default value is latest.
* [ ] It specifies the user or account name, and the default value is library.
* [ ] It specifies the operating system, and the default value is linux.

**Correct answer:**
* [x] It specifies the user or account name, and the default value is library.

**81. Which DNS name is used for Google's Docker registry?**


* [ ] docker.io
* [ ] hub.docker.com
* [ ] gcr.io
* [ ] quay.io

**Correct answer:**
* [x] gcr.io

**82. What is Docker's default registry?**


* [ ] Docker Cloud
* [ ] Docker Engine
* [ ] Docker Hub
* [ ] Docker Repository

**Correct answer:**
* [x] Docker Hub

**83. What is the purpose of the "library" in the image name (library/nginx)?**


* [ ] It is a reserved account for system-level images in Docker.
* [ ] It is the default account where Docker's official images are stored.
* [ ] It is a user account created by Docker for managing images.
* [ ] It is a namespace where custom images are stored.

**Correct answer:**
* [x] It is the default account where Docker's official images are stored.

**84. What command can you use to log in to your private Docker registry?**


* [ ] docker build
* [ ] docker push
* [ ] docker login
* [ ] docker pull

**Correct answer:**
* [x] docker login

**85. How can we specify the secret to our private registry inside our pod definition file?**


* [ ] By using the command line and specifying the secret name and registry URL
* [ ] By setting the environment variable in the container definition
* [ ] By using the annotations in the pod definition file
* [ ] By specifying the secret under the imagePullSecrets section in the pod definition file

**Correct answer:**
* [x] By specifying the secret under the imagePullSecrets section in the pod definition file

**86. What is the purpose of the Docker registry secret type in Kubernetes?**


* [ ] To store secrets related to Docker containers
* [ ] To store Docker image metadata
* [ ] To store Docker credentials for accessing private registries
* [ ] To store Docker container logs and metrics

**Correct answer:**
* [x] To store Docker credentials for accessing private registries

**87. When configuring security settings at the pod level, what happens to the settings?**


* [ ] They only apply to the first container in the pod.
* [ ] They only apply to the last container in the pod.
* [ ] They apply to all containers within the pod.
* [ ] They apply only to the pod and not the containers within it.

**Correct answer:**
* [x] They apply to all containers within the pod.

**88. What happens when you configure security settings at both the pod and container levels in Kubernetes?**


* [ ] The settings at the container level override the settings at the pod level.
* [ ] The settings at the pod level override the settings at the container level.
* [ ] The settings at both levels are combined and applied to the pod and container.
* [ ] Kubernetes will generate an error and prevent deployment of the pod and container.

**Correct answer:**
* [x] The settings at the container level override the settings at the pod level.

**89. What is the difference between ingress and egress traffic in a web server?**


* [ ] There is no difference between ingress and egress traffic in a web server.
* [ ] Ingress traffic is outgoing traffic and egress traffic is incoming traffic.
* [ ] Ingress traffic is incoming traffic and egress traffic is outgoing traffic.
* [ ] Ingress and egress traffic both refer to incoming traffic only.

**Correct answer:**
* [x] Ingress traffic is incoming traffic and egress traffic is outgoing traffic.

**90. Which of the following statements is true about Kubernetes network policies?**


* [ ] Kubernetes is configured by default with an “All Allow” rule that allows traffic from any pod to any other pods or services.
* [ ] By default, Kubernetes denies all ingress and egress traffic between pods.
* [ ] Kubernetes has no built-in network policies, and they must be implemented by a third-party tool.
* [ ] Network policies only apply to egress traffic and not to ingress traffic.

**Correct answer:**
* [x] Kubernetes is configured by default with an “All Allow” rule that allows traffic from any pod to any other pods or services.

**91. Which field should be added to the specs section of a pod in order to configure security context on a pod in Kubernetes?**


* [ ] containerSecurity
* [ ] contextSecurity
* [ ] securityContext
* [ ] secureContainer

**Correct answer:**
* [x] securityContext

**92. What is the purpose of Kubernetes network policy?**


* [ ] To allow unrestricted traffic to all pods in the cluster
* [ ] To restrict traffic to a specific node in the cluster
* [ ] To restrict traffic to a specific pod or set of pods in the cluster
* [ ] To restrict traffic to a specific service in the cluster

**Correct answer:**
* [x] To restrict traffic to a specific pod or set of pods in the cluster

**93. Which technique can be used to link a network policy to a pod in Kubernetes?**


* [ ] Annotations
* [ ] Labels and selectors
* [ ] Environment variables
* [ ] Resource limits and requests

**Correct answer:**
* [x] Labels and selectors

**94. Where do we specify the pod labels in a Kubernetes network policy definition file?**


* [ ] spec.podSelector
* [ ] spec.matchLabels
* [ ] spec.egress
* [ ] spec.ingress

**Correct answer:**
* [x] spec.podSelector

**95. Where do we build our network rules in a Kubernetes network policy definition file?**


* [ ] matchLabels section
* [ ] podSelector section
* [ ] policyTypes section
* [ ] Ingress or Egress section

**Correct answer:**
* [x] Ingress or Egress section

**96. If you have only the ingress in the policyTypes, what does it mean?**


* [ ] Only the ingress traffic is isolated, and all egress traffic is unaffected.
* [ ] Only the egress traffic is isolated, and all ingress traffic is unaffected.
* [ ] Both ingress and egress traffic are isolated.
* [ ] There is no network policy applied.

**Correct answer:**
* [x] Only the ingress traffic is isolated, and all egress traffic is unaffected.

**97. Which of the following solutions support Kubernetes network policies?**


* [ ] Kube router, calico, and flannel
* [ ] Calico, weave-net, and flannel
* [ ] Kube router, calico, and weave-net
* [ ] Flannel, weave-net, and kube-proxy

**Correct answer:**
* [x] Kube router, calico, and weave-net

---


## test-visibility

**1. test question 4**


* [ ] q
* [ ] b
* [ ] g
* [ ] t

**Correct answer:**
* [x] t

**2. test question 1**


* [ ] a
* [ ] b
* [ ] c
* [ ] g

**Correct answer:**
* [x] b

---


## Quiz - Cloud Computing

**1. Which of the following is not a standard design principle or pattern used in AWS?**


* [ ] Design for Failure 
* [ ] Decouple Components 
* [ ] Implement Elasticity
* [ ] Think Serial

**Correct answer:**
* [x] Think Serial

**Explaination**: Common design principles in AWS include:
• Design for Failure - Expect failures and design systems to handle them.
• Decouple Components - Build components that are loosely coupled. 
• Implement Elasticity - Scale up and down based on demand. 
• Think Parallel - Perform operations in parallel to be efficient. 
"Think Serial" is not a standard AWS design principle. 

Reference: https://d1.awsstatic.com/whitepapers/AWS_Cloud_Best_Practices.pdf

**Documentation Link**: https://docs.aws.amazon.com/wellarchitected/latest/framework/define-implement.html

**2. In which year did AWS launch its first service, S3, the simple storage service?**


* [ ] 2004 
* [ ] 2007 
* [ ] 2006 
* [ ] 2005 

**Correct answer:**
* [x] 2006 

**Explaination**: Amazon S3 was launched in the United States in March 2006. It was the first publicly available service from AWS. EC2 was launched later in 2006. AWS was founded in 2004.

Reference: https://aws.amazon.com/about-aws/whats-new/2006/#s3

**Documentation Link**: https://en.wikipedia.org/wiki/Amazon_S3

**3. When you insert a load balancer in front of your web servers so that users are directed towards the load balancer instead of your ever-changing web servers, what design principles are you following?**


* [ ] Decouple Components 
* [ ] Detach Components
* [ ] Hide Components
* [ ] Microservice Components

**Correct answer:**
* [x] Decouple Components 

**Explaination**: Using a load balancer to direct traffic to web servers is an example of loosely coupling components. The load balancer abstracts the web servers from the end users, decoupling the two components. 

Reference: https://d1.awsstatic.com/whitepapers/AWS_Cloud_Best_Practices.pdf

**Documentation Link**: https://docs.aws.amazon.com/wellarchitected/latest/framework/define-operate.html

**4. AWS has a framework for thinking about Architecture Principles and how they apply to AWS. What is the name of this framework?**


* [ ] AWS Cloud Adoption Framework
* [ ] AWS Well-Architected Framework
* [ ] AWS White Paper Framework 
* [ ] AWS Serverless Framework

**Correct answer:**
* [x] AWS Well-Architected Framework

**Explaination**: The AWS Well-Architected Framework provides guidelines and best practices for building secure, high-performing, resilient, and efficient infrastructure for applications in the cloud. It covers Five Pillars: Security, Reliability, Performance Efficiency, Cost Optimization, and Operational Excellence.

**Documentation Link**: https://aws.amazon.com/architecture/well-architected/

**5. In Cloud Economics terms, what is another name for a lower price per unit when you use more units?**


* [ ] Free Tier
* [ ] Reservations 
* [ ] On-Demand
* [ ] Volume Discounts 

**Correct answer:**
* [x] Volume Discounts 

**Explaination**: Volume discounts refer to lower unit pricing as volume increases. This is another term for "Benefit from Massive Economies of Scale" in cloud computing. 

Reference: https://aws.amazon.com/economics/

**Documentation Link**: https://aws.amazon.com/what-is-cloud-computing/

**6. AWS has a billing construct where you are able to try some services for free in your account, other services are only free for 12 months, and others require you to activate a trial period. What is this called?**


* [ ] AWS Artifact
* [ ] AWS Organizations
* [ ] AWS Student Tier
* [ ] AWS Free Tier 

**Correct answer:**
* [x] AWS Free Tier 

**Explaination**: The AWS Free Tier provides free access to select AWS services for 12 months as well as always-free services. It allows trying AWS with no upfront cost.

Reference: https://aws.amazon.com/free/ 

**Documentation Link**: https://aws.amazon.com/free/faqs/

**7. When you are removing single points of failure from a system, you are following what design principle?**


* [ ] Design for Performance 
* [ ] Design for Cost Optimization
* [ ] Design for Scalability
* [ ] Design for Failure

**Correct answer:**
* [x] Design for Failure

**Explaination**: The "Design for Failure" principle means planning for failures and building resiliency into systems. Removing single points of failure and adding redundancy is a key part of designing for failure.

Reference: https://d1.awsstatic.com/whitepapers/AWS_Cloud_Best_Practices.pdf 

**Documentation Link**: https://docs.aws.amazon.com/wellarchitected/latest/framework/design-for-failure.html

**8. Reservations are an economic construct where a business agrees to commit to using AWS for what duration or time frame?**


* [ ] 1 year or 3 years 
* [ ] Anywhere between 1 and 3 years
* [ ] 13 months or 35 months
* [ ] No time frame

**Correct answer:**
* [x] 1 year or 3 years 

**Explaination**: AWS Reserved Instances provide discounts on Amazon EC2 pricing in exchange for upfront payment and commitment to use EC2 instances for 1 year or 3 years. 

Reference: https://aws.amazon.com/ec2/pricing/reserved-instances/

**Documentation Link**: https://aws.amazon.com/economics/

**9. Name the AWS benefit that gives you a lower price per unit of use for using more units? Meaning you pay less per unit if you use more units.**


* [ ] Go Global in Minutes
* [ ] Benefit from Massive Economies of Scale
* [ ] Increase Provisioning Speed and Business Agility
* [ ] Stop focusing on Data Centers 

**Correct answer:**
* [x] Benefit from Massive Economies of Scale

**Explaination**: The "Benefit from Massive Economies of Scale" means that as AWS usage scales, the cost per service goes down. AWS is able to offer lower prices due to their large scale. 

Reference: https://d1.awsstatic.com/whitepapers/AWS_Cloud_Best_Practices.pdf

**Documentation Link**: https://aws.amazon.com/what-is-cloud-computing/

**10. Name the AWS benefit that allows you to easily fix either over-sizing of an application server or an under-sizing of an application server?**


* [ ] Trade Upfront Expense for Variable Expense
* [ ] Increase Provisioning Speed and Business Agility
* [ ] Go Global in Minutes
* [ ] Stop Guessing Capacity

**Correct answer:**
* [x] Stop Guessing Capacity

**Explaination**: The "Stop Guessing Capacity" benefit of AWS means the ability to scale computing resources up and down based on demand. This allows quickly fixing issues with over-provisioning or under-provisioning resources.

Reference: https://d1.awsstatic.com/whitepapers/Introduction_to_AWS.pdf

**Documentation Link**: https://aws.amazon.com/what-is-cloud-computing/

**11. In AWS’ Pay-As-You-Go model, when you start and stop a service, you usually pay:**


* [ ] Instantly for a month’s usage of that service 
* [ ] The amount the contract says that you pay every month
* [ ] Unknown until you talk to your sales representative 
* [ ] Only for what you use for the length of time that you were using it

**Correct answer:**
* [x] Only for what you use for the length of time that you were using it

**Explaination**: AWS uses a pay-as-you-go model where you only pay for the services you use, for as long as you use them. When you stop a service, billing stops automatically. You do not pay for a full month upfront or have set contracts and amounts. The pricing is public and you can calculate costs based on your usage. 

Reference: https://aws.amazon.com/what-is-cloud-computing/

**Documentation Link**: https://aws.amazon.com/pricing/

**12. In the AWS benefit called Stop Focusing on Data Centers, what is a business asked to focus on instead?**


* [ ] Cloud Computing Concepts and Services
* [ ] Internal Employees and the Business
* [ ] Revenue, Profit, and Shareholders 
* [ ] The Customer

**Correct answer:**
* [x] The Customer

**Explaination**: The "Stop Focusing on Data Centers" benefit of AWS means businesses can shift their focus away from managing infrastructure to focusing on their customers and business priorities.

Reference: https://d1.awsstatic.com/whitepapers/Introduction_to_AWS.pdf 

**Documentation Link**: https://aws.amazon.com/what-is-cloud-computing/ 

**13. Which job role is primarily best suited for using a Software Development Kit (SDK) to import AWS-purpose built components into your application?**


* [ ] Software Engineers of all types
* [ ] Project Managers and Scrum Masters
* [ ] Business Leaders and C-level executives
* [ ] Financial and Business Analysts

**Correct answer:**
* [x] Software Engineers of all types

**Explaination**: Software Development Kits (SDKs) provide programmatic access to AWS from various programming languages. They are used by software engineers and developers to build applications and systems that utilize AWS services. Project managers, business executives, and analysts typically do not develop software directly and would not use an SDK.

**Documentation Link**: https://aws.amazon.com/tools/

**14. Which of the following is one of the three main ways to interact with AWS?**


* [ ] AWS Console
* [ ] AWS Desktop Application
* [ ] AWS Chrome Plugin 
* [ ] FireFox 

**Correct answer:**
* [x] AWS Console

**Explaination**: The three main ways to interact with AWS are:
1. The AWS Console: A web-based graphical user interface
2. The AWS Command Line Interface: A unified tool to manage AWS services
3. Software Development Kits: Allows programmatic access to AWS from various languages
An AWS desktop application and browser plugins are not tools provided by AWS to interact with their services. Firefox is a web browser but not an AWS interface.

Reference:  
https://docs.aws.amazon.com/awsconsolehelpdocs/latest/gsg/getting-started.html
https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-welcome.html


**Documentation Link**: https://aws.amazon.com/tools/

**15. In the AWS benefit called Trade Upfront Expense for Variable Expense, you often:**


* [ ] Make large upfront investments in hardware and let them depreciate over time
* [ ] Spend a lot of time researching the best facility to host your application
* [ ] Identify which services you need and then call your sales representative for a contract 
* [ ] Move from using high-dollar data centers every few years to a pay-as-you-go service model every month

**Correct answer:**
* [x] Move from using high-dollar data centers every few years to a pay-as-you-go service model every month

**Explaination**: The "Trade CapEx for OpEx" benefit of AWS means switching from large upfront capital expenditures (CapEx) required to build and maintain on-premises infrastructure to the operating expense (OpEx) of paying only for what you use on AWS each month. 
Reference: https://d1.awsstatic.com/whitepapers/Proof%20Points_Migrating_Enterprise_Appilcations_to_AWS.pdf

**Documentation Link**: https://aws.amazon.com/what-is-cloud-computing/

**16. Which of the following is NOT a general service category in the AWS console?**


* [ ] Compute
* [ ] Networking and Content Delivery
* [ ] Database
* [ ] Data and Memory Storage

**Correct answer:**
* [x] Data and Memory Storage

**Explaination**: The major service categories in AWS are:
1. Compute: Services such as EC2, Lambda, Elastic Beanstalk, etc.
2. Storage: Services such as S3, EBS, Glacier, etc.
3. Databases: Services such as RDS, DynamoDB, Redshift, etc.
4. Networking & Content Delivery: Services such as VPC, Route 53, CloudFront, etc.
"Data and Memory Storage" is not a category used by AWS to classify their services. 

**Documentation Link**: https://d1.awsstatic.com/training-and-certification/docs-cloud-practitioner/AWS_Certified_Cloud_Practitioner_Exam_Guide.pdf

**17. Clients who want to access AWS will use this kind of architectural model to access AWS.**


* [ ] The Peer-to-Peer model 
* [ ] The Client-Server model
* [ ] The Asynchronous model
* [ ] The Remote Procedure Call model

**Correct answer:**
* [x] The Client-Server model

**Explaination**: To access AWS services, clients interact with AWS servers using APIs and web interfaces. This follows a client-server architecture where clients request information or resources from centralized servers. Peer-to-peer architectures involve direct communication between network nodes. Asynchronous and Remote Procedure Call are communication models but not full architectural models themselves.
Reference: https://d1.awsstatic.com/whitepapers/Introduction_to_AWS.pdf

**Documentation Link**: https://www.educba.com/client-server-model-vs-peer-to-peer-model/

**18. Fill in the missing word from this official definition of Cloud Computing: “Cloud Computing is the ________delivery of IT resources.”**


* [ ] On-demand
* [ ] Instant 
* [ ] Utility
* [ ] Rapid

**Correct answer:**
* [x] On-demand

**Explaination**: The full definition of cloud computing from the National Institute of Standards and Cloud Computing Concepts is: "Cloud computing is a model for enabling ubiquitous, convenient, on-demand network access to a shared pool of configurable computing resources." On-demand delivery of IT resources is a key characteristic of cloud computing. The options "instant, utility" and "rapid" do not match the official definition.

**Documentation Link**: https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-145.pdf

**19. In a traditional data center, requests for services can take weeks. In a cloud computing environment, requests for services usually take:**


* [ ] Weeks to Months 
* [ ] Days to Weeks
* [ ] Months to Years
* [ ] Seconds to Minutes 

**Correct answer:**
* [x] Seconds to Minutes 

**Explaination**: A key benefit of cloud computing is the on-demand provisioning of computing resources. In AWS, servers, storage, and other resources can be provisioned automatically in seconds to minutes. This is much faster than a traditional data center where it can take weeks or months to procure and configure new hardware.
Reference: https://d1.awsstatic.com/whitepapers/AWS_Cloud_Best_Practices.pdf

**Documentation Link**: https://aws.amazon.com/what-is-cloud-computing/

**20. Which of the three Cloud Deployment models is a blend of two other models?**


* [ ] Cloud 
* [ ] On-Premise
* [ ] Hybrid
* [ ] Blended

**Correct answer:**
* [x] Hybrid

**Explaination**: The hybrid cloud deployment model combines public cloud and private cloud or on-premises infrastructure. It allows workloads and data to move between the two environments. The cloud model refers to public cloud. On-premise is typically a private data center. There is no "blended" deployment model.

**Documentation Link**: https://d1.awsstatic.com/whitepapers/Introduction_to_AWS.pdf

**21. Most services in AWS have the following economic model when it comes to paying:**


* [ ] Pay as you go for what you request or use
* [ ] Pay ahead of time for what you want and you get a refund for excess 
* [ ] Sign a contract ahead of time for a specific amount of dollars over time 
* [ ] Commit to a 1-year or 3-year contract with AWS for a particular workload

**Correct answer:**
* [x] Pay as you go for what you request or use

**Explaination**: The default billing model for AWS services is pay-as-you-go pricing where you pay for what you use. You have the option to purchase Reserved Instances for services that provide a discount from on-demand pricing over 1-3 year terms. Most AWS services do not require upfront payments or long-term contracts. 
Reference: https://aws.amazon.com/pricing/

**Documentation Link**: https://aws.amazon.com/ec2/pricing/reserved-instances/

**22. What Cloud Deployment model is sometimes referred to as “Private Cloud”?**


* [ ] Cloud 
* [ ] On-Premise 
* [ ] Hybrid 
* [ ] Community Cloud 

**Correct answer:**
* [x] On-Premise 

**Explaination**: An on-premises or "on-prem" deployment model refers to a private data center. It is sometimes called a "private cloud." This is infrastructure dedicated to a single organization. The cloud deployment model refers to public cloud. A community cloud is public cloud for a dedicated community. A hybrid cloud combines public cloud and on-premises infrastructure.

**Documentation Link**: https://d1.awsstatic.com/whitepapers/Introduction_to_AWS.pdf

**23. Which of the following is a benefit of using Traditional IT over Cloud Computing?**


* [ ] Offload Responsibility to AWS
* [ ] Increased Focus on what is Core to your business
* [ ] Go Global with global data centers in minutes
* [ ] Increase Control over Infrastructure

**Correct answer:**
* [x] Increase Control over Infrastructure

**Explaination**: A benefit of traditional IT compared to cloud computing is having more direct control over infrastructure. With on-premises data centers, organizations have full control and responsibility over hardware, software, networking, security, and operations. Cloud computing provides less direct control as much of the infrastructure is managed by the service provider. 
Reference: https://d1.awsstatic.com/whitepapers/Introduction_to_AWS.pdf


**Documentation Link**: https://aws.amazon.com/what-is-cloud-computing/ 

**24. Implement Elasticity means which of the following in relation to Cloud Design principles?**


* [ ] Scale your servers up when you don’t need them and down when you do
* [ ] Scale your servers down when you don’t need them and up when you do
* [ ] Add a load balancer or some other application to application flow control
* [ ] Make sure your systems can survive any outage with a high SLA

**Correct answer:**
* [x] Scale your servers down when you don’t need them and up when you do

**Explaination**: The "Implement Elasticity" design principle means being able to scale computing resources up and down as needed. This includes scaling servers up during high load and down when demand is low.

**Documentation Link**: https://d1.awsstatic.com/whitepapers/AWS_Cloud_Best_Practices.pdf 

**25. The use of 20 computers to get a job done in 1 hour versus 1 computer taking 20 hours is an example of what cloud design principle?**


* [ ] Decouple your components
* [ ] Think Parallel
* [ ] Implement Elasticity
* [ ] Design for Failure

**Correct answer:**
* [x] Think Parallel

**Explaination**: Using multiple computers to complete a job in less time is an example of parallel processing. This is captured in the "Think Parallel" design principle for the AWS Cloud. Decoupling components refers to loosely coupled systems. Elasticity is the ability to scale resources up and down. Designing for failure ensures systems are resilient to individual component failures.

**Documentation Link**: https://d1.awsstatic.com/whitepapers/AWS_Cloud_Best_Practices.pdf

---


## Quiz - AWS Security and Compliance

**1. What is the primary purpose of Amazon Macie?**


* [ ] Detect and analyze security events and logs
* [ ] Monitor and protect web applications from common security vulnerabilities
* [ ] Automatically encrypt and decrypt data stored in Amazon S3
* [ ] Discover, classify, and protect sensitive data stored in AWS

**Correct answer:**
* [x] Discover, classify, and protect sensitive data stored in AWS

**Explaination**: The correct answer is "Discover, classify, and protect sensitive data stored in AWS". Amazon Macie is a security service that uses machine learning to automatically discover, classify, and protect sensitive data stored in AWS. It helps identify sensitive data such as personally identifiable information (PII) and intellectual property, and it provides recommendations for improving data protection and access controls.

**2. Which AWS service is used to manage and control the encryption keys used to encrypt your data stored in AWS services?**


* [ ] AWS Key Management Service (KMS)
* [ ] AWS CloudTrail
* [ ] AWS Config
* [ ] Amazon GuardDuty

**Correct answer:**
* [x] AWS Key Management Service (KMS)

**Explaination**: The correct answer is AWS Key Management Service (KMS). AWS KMS is a managed service that allows you to create and control the encryption keys used to encrypt your data stored in AWS services. It provides a highly secure and scalable key management solution.

**3. Which AWS service is used to centrally manage and store secrets such as database credentials, API keys, and encryption keys?**


* [ ] Secrets Manager
* [ ] Certificate Manager
* [ ] IAM Access Analyzer
* [ ] Amazon Macie

**Correct answer:**
* [x] Secrets Manager

**Explaination**: The correct answer is Secrets Manager. AWS Secrets Manager is a service that helps you protect secrets needed to access your applications, services, and IT resources. It allows you to securely store and manage secrets, such as database credentials and API keys, and retrieve them programmatically when needed.

**4. Which AWS service provides a managed solution for generating and managing SSL/TLS certificates for use with AWS resources?**


* [ ] Secrets Manager
* [ ] Certificate Manager
* [ ] AWS Directory Service
* [ ] AWS Key Management Service (KMS)

**Correct answer:**
* [x] Certificate Manager

**Explaination**: The correct answer is Certificate Manager. AWS Certificate Manager (ACM) is a service that provides a managed solution for generating, deploying, and managing SSL/TLS certificates for use with AWS resources. It simplifies the process of obtaining and renewing SSL/TLS certificates for your applications and resources.

**5. Which of the following actions can be logged by AWS CloudTrail? (Choose two.)**


* [ ] User authentication events
* [ ] Changes to AWS service limits
* [ ] S3 bucket object-level operations
* [ ] EC2 instance performance metrics

**Correct answer:**
* [x] User authentication events
* [x] S3 bucket object-level operations

**Explaination**: The correct answers are "User authentication events" and "S3 bucket object-level operations". AWS CloudTrail can log user authentication events, such as successful or failed login attempts, and it can also log S3 bucket object-level operations, such as object uploads, downloads, and deletions.

**6. What is the main purpose of Amazon Cognito?**


* [ ] Perform vulnerability assessments on AWS resources
* [ ] Centrally manage firewall rules and security group configurations
* [ ] Authenticate and authorize users for your applications
* [ ] Monitor and respond to security incidents in real time

**Correct answer:**
* [x] Authenticate and authorize users for your applications

**Explaination**: The correct answer is "Authenticate and authorize users for your applications". Amazon Cognito is a fully managed service that provides authentication, authorization, and user management for web and mobile applications. It allows you to securely authenticate users, manage user profiles, and control access to your applications.

**7. What is the main purpose of AWS Firewall Manager?**


* [ ] Securely manage and store access keys and secrets
* [ ] Monitor and respond to security incidents in real time
* [ ]  Centrally manage firewall rules and security group configurations
* [ ] Perform vulnerability assessments on EC2 instances

**Correct answer:**
* [x]  Centrally manage firewall rules and security group configurations

**Explaination**: The correct answer is "Centrally manage firewall rules and security group configurations". AWS Firewall Manager is a security management service that allows you to centrally configure and manage firewall rules and security group settings across multiple AWS accounts and resources. It helps ensure consistent security controls and compliance across your organization.

**8. What is the purpose of AWS Security Hub?**


* [ ] A fully managed DDoS protection service
* [ ] A service that provides security monitoring and governance across multiple AWS accounts
* [ ] A security analytics service that uses machine learning to detect anomalous behavior
* [ ] A managed service for encryption key management

**Correct answer:**
* [x] A service that provides security monitoring and governance across multiple AWS accounts

**Explaination**: The correct answer is "A service that provides security monitoring and governance across multiple AWS accounts". AWS Security Hub is a comprehensive security service that helps you centrally manage security and compliance across multiple AWS accounts. It aggregates and prioritizes security findings from various AWS services, third-party products, and custom integrations, providing a unified view of security posture across your AWS environment.

**9. Which of the following statements accurately describes AWS Organizations?**


* [ ] AWS Organizations is used to manage user authentication and authorization in an AWS account.
* [ ] AWS Organizations is a service that helps monitor and secure your AWS resources.
* [ ] AWS Organizations is a service that enables centralized management of multiple AWS accounts.
* [ ] AWS Organizations is a service that provides built-in security controls for AWS services.

**Correct answer:**
* [x] AWS Organizations is a service that enables centralized management of multiple AWS accounts.

**Explaination**: The correct answer is "AWS Organizations is a service that enables centralized management of multiple AWS accounts". With AWS Organizations, you can create and manage a hierarchy of AWS accounts and apply policies across them. It helps you simplify billing, manage access control, and implement security and compliance policies across your organization's AWS accounts.

**10. What is the primary purpose of Amazon GuardDuty?**


* [ ] Automatic data encryption for your AWS resources
* [ ] Real-time threat detection and continuous monitoring
* [ ] Identity and access management for AWS services
* [ ] Network traffic analysis and monitoring

**Correct answer:**
* [x] Real-time threat detection and continuous monitoring

**Explaination**: The correct answer is "Real-time threat detection and continuous monitoring". Amazon GuardDuty is a managed threat detection service that continuously monitors and analyzes events and logs from various AWS data sources, such as VPC Flow Logs, CloudTrail logs, and DNS logs. It uses machine learning algorithms and threat intelligence to identify potential security threats and malicious activity in your AWS environment.

**11. Which of the following can be used to apply fine-grained access controls and permissions across your organization's AWS accounts in AWS Organizations?**


* [ ] AWS Organizations policies
* [ ] Amazon VPC security groups
* [ ] AWS Single Sign-On (SSO)
* [ ] Amazon Route 53 routing policies

**Correct answer:**
* [x] AWS Organizations policies

**Explaination**: The correct answer is AWS Organizations policies. AWS Organizations allows you to create and apply policies at the organizational level to manage access controls and permissions across multiple AWS accounts. These policies can be used to define fine-grained permissions for individual accounts or groups of accounts within your organization.

**12. Which of the following types of security assessments does AWS Inspector provide?**


* [ ] Vulnerability assessments
* [ ] DDoS attack mitigation
* [ ] Network traffic monitoring
* [ ] Intrusion detection

**Correct answer:**
* [x] Vulnerability assessments

**Explaination**: The correct answer is Vulnerability assessments. AWS Inspector performs automated vulnerability assessments on your EC2 instances and applications. It identifies security vulnerabilities, common misconfigurations, and deviations from security best practices. It provides detailed findings and recommendations to help you remediate potential issues.

**13. Which of the following best describes AWS Inspector?**


* [ ] A service that provides continuous monitoring and threat detection for your AWS resources
* [ ] A service that automatically encrypts your data at rest and in transit
* [ ] A service that analyzes network traffic to identify potential security vulnerabilities
* [ ] A service that performs automated penetration testing on your applications

**Correct answer:**
* [x] A service that provides continuous monitoring and threat detection for your AWS resources

**Explaination**: The correct answer is "A service that provides continuous monitoring and threat detection for your AWS resources". AWS Inspector helps you identify security vulnerabilities and deviations from security best practices in your AWS EC2 instances and applications. It provides a comprehensive security assessment of your resources by analyzing their configurations, network traffic, and application behavior.

**14. Which AWS IAM feature allows you to grant temporary access to AWS resources to users, without sharing long-term credentials?**


* [ ] IAM Roles
* [ ] IAM Policies
* [ ] IAM Users
* [ ] IAM Groups

**Correct answer:**
* [x] IAM Roles

**Explaination**: The correct answer is IAM Roles. IAM Roles in AWS IAM allow you to grant temporary access to AWS resources to users or entities without sharing long-term credentials. IAM roles are typically used for applications running on AWS resources, like EC2 instances or Lambda functions, to access other AWS services securely. By assigning roles to these resources, you can define the specific permissions and duration of access for each role, providing temporary access without exposing long-term credentials.

**15. Which of the following best describes AWS WAF?**


* [ ] A service that provides DDoS protection for your AWS resources
* [ ] A service that monitors and analyzes network traffic in real time
* [ ] A service that protects your web applications from common web exploits
* [ ] A service that encrypts data at rest and in transit

**Correct answer:**
* [x] A service that protects your web applications from common web exploits

**Explaination**: The correct answer is "A service that protects your web applications from common web exploits". AWS WAF is a web application firewall that helps protect your web applications from common web attacks, such as SQL injection, cross-site scripting (XSS), and cross-site request forgery (CSRF). It allows you to create rules to block or allow traffic based on configurable conditions and rule sets.

**16. Which of the following features allows you to apply policies and settings across multiple AWS accounts in AWS Organizations?**


* [ ] Organizational Units (OUs)
* [ ] Identity and Access Management (IAM) roles
* [ ] Security Groups
* [ ] AWS CloudFormation templates

**Correct answer:**
* [x] Organizational Units (OUs)

**Explaination**: The correct answer is Organizational Units (OUs). In AWS Organizations, you can create Organizational Units to group accounts and apply policies and settings at different levels of your organization's hierarchy. OUs provide a way to organize and manage multiple accounts within an organization and simplify the application of policies and controls across those accounts.

**17. Which of the following best describes the purpose of IAM in AWS?**


* [ ] IAM is responsible for monitoring and auditing your AWS resources.
* [ ] IAM enables you to manage and control access to AWS services and resources.
* [ ] IAM provides real-time threat detection and protection for your AWS environment.
* [ ] IAM automates the encryption of data at rest for your AWS resources.

**Correct answer:**
* [x] IAM enables you to manage and control access to AWS services and resources.

**Explaination**: The correct answer is "IAM enables you to manage and control access to AWS services and resources". AWS Identity and Access Management (IAM) is a service that allows you to securely control access to AWS resources. It enables you to create and manage users, groups, and permissions, allowing you to grant or deny access to AWS services and resources based on the principle of least privilege. IAM helps you enforce security and compliance by providing centralized control over user identities and their permissions within your AWS account.

**18. Which AWS service automates the process of assessing and managing the compliance of your AWS resources with industry standards and regulations?**


* [ ] AWS Security Hub
* [ ] AWS Config
* [ ] AWS Control Tower
* [ ] AWS Audit Manager

**Correct answer:**
* [x] AWS Audit Manager

**Explaination**: The correct answer is AWS Audit Manager. AWS Audit Manager is a service that automates the process of assessing and managing the compliance of your AWS resources with industry standards and regulations. It provides pre-built frameworks that map to popular compliance standards, such as PCI DSS, HIPAA, GDPR, and more. With AWS Audit Manager, you can easily create and customize assessments, collect evidence, and generate comprehensive reports to demonstrate compliance to auditors and stakeholders.

**19. Which statement best describes the concept of the AWS shared responsibility model?**


* [ ] AWS takes full responsibility for securing and protecting customer data and applications.
* [ ] Customers are responsible for securing the underlying infrastructure and physical data centers.
* [ ] Both AWS and customers share equal responsibility for securing and protecting customer data and applications.
* [ ] Customers are solely responsible for securing the AWS services they utilize.

**Correct answer:**
* [x] Both AWS and customers share equal responsibility for securing and protecting customer data and applications.

**Explaination**: The correct answer is "Both AWS and customers share equal responsibility for securing and protecting customer data and applications". The AWS shared responsibility model defines the division of security responsibilities between AWS and its customers. AWS is responsible for the security of the cloud infrastructure, such as the physical data centers, networking, and compute resources. Customers, on the other hand, are responsible for securing their applications, data, operating systems, and configurations within the AWS environment. It is essential for customers to understand and fulfill their part of the shared responsibility to ensure a secure and compliant AWS environment.

**20. Which AWS service provides a fully managed Distributed Denial of Service (DDoS) protection to help protect your applications and data from DDoS attacks?**


* [ ] AWS Web Application Firewall (WAF)
* [ ] AWS Shield
* [ ] AWS Inspector
* [ ] AWS GuardDuty

**Correct answer:**
* [x] AWS Shield

**Explaination**: The correct answer is AWS Shield. AWS Shield is an AWS service that provides fully managed DDoS protection to help protect your applications and data from DDoS attacks. It safeguards your applications against both volumetric and state-exhaustion DDoS attacks, allowing your applications to remain available and accessible to legitimate users. AWS Shield provides two tiers of protection: AWS Shield Standard and AWS Shield Advanced. AWS Shield Standard provides automatic protection for all AWS customers at no additional cost, while AWS Shield Advanced offers enhanced DDoS protection and additional features for a subscription fee.

**21. Which AWS service provides pre-built, standardized controls to help you establish and maintain compliance with various regulatory requirements?**


* [ ] AWS IAM
* [ ] AWS Organizations
* [ ] AWS Control Tower
* [ ] AWS Artifact

**Correct answer:**
* [x] AWS Artifact

**Explaination**: The correct answer is AWS Artifact. AWS Artifact is a service that provides access to AWS compliance reports, enabling you to establish and maintain compliance with various regulatory requirements. It offers a centralized repository of compliance reports and documentation, including the AWS Service Organization Controls (SOC) reports, Payment Card Industry Data Security Standard (PCI DSS) reports, and more. AWS Artifact helps you meet your compliance needs by providing readily available, pre-built controls and documentation that demonstrate the security and compliance of AWS services.

**22. Which of the following statements best describes the purpose of AWS CloudTrail?**


* [ ] AWS CloudTrail provides a fully managed DDoS protection service to safeguard applications running on AWS.
* [ ] AWS CloudTrail enables you to track user activity and API usage in your AWS account.
* [ ] AWS CloudTrail is a web application firewall that protects web applications from common web exploits.
* [ ] AWS CloudTrail helps you discover, classify, and protect sensitive data stored in Amazon S3.

**Correct answer:**
* [x] AWS CloudTrail enables you to track user activity and API usage in your AWS account.

**Explaination**: The correct answer is "AWS CloudTrail enables you to track user activity and API usage in your AWS account". AWS CloudTrail is a service that provides governance, compliance, operational auditing, and risk auditing of your AWS account. It records API calls and events for your AWS account and delivers log files containing this information to an Amazon S3 bucket or sends them to Amazon CloudWatch Logs. CloudTrail allows you to monitor and log actions taken by users, services, and resources in your account, providing visibility into who did what, when, and from where.



**23. Which of the following statements best describes the purpose of AWS Config?**


* [ ] AWS Config provides a fully managed DDoS protection service to safeguard applications running on AWS.
* [ ] AWS Config enables you to track user activity and API usage in your AWS account.
* [ ] AWS Config automatically provisions and manages SSL/TLS certificates for your AWS resources.
* [ ] AWS Config helps you assess, audit, and evaluate the configurations of your AWS resources.

**Correct answer:**
* [x] AWS Config helps you assess, audit, and evaluate the configurations of your AWS resources.

**Explaination**: The correct answer is "AWS Config helps you assess, audit, and evaluate the configurations of your AWS resources". AWS Config is a service that provides a detailed view of the configuration of AWS resources in your account. It continuously monitors and records configurations of supported resources and allows you to assess the overall compliance of your resources against desired configurations. With AWS Config, you can track changes, monitor compliance, troubleshoot resource configuration changes, and simplify security and compliance auditing.

**24. What is the primary difference between managed and unmanaged services in AWS?**


* [ ] Managed services are controlled and maintained by AWS, while unmanaged services require customers to handle administration tasks.
* [ ] Managed services provide more scalability options than unmanaged services.
* [ ] Managed services are only available for enterprise customers, while unmanaged services are available for all types of customers.
* [ ] Managed services offer more advanced features and functionalities compared to unmanaged services.

**Correct answer:**
* [x] Managed services are controlled and maintained by AWS, while unmanaged services require customers to handle administration tasks.

**Explaination**: The correct answer is "Managed services are controlled and maintained by AWS, while unmanaged services require customers to handle administration tasks". Managed services in AWS, such as Amazon RDS and Amazon DynamoDB, are fully managed by AWS, which means that AWS takes care of the underlying infrastructure, patching, and backups. Customers can focus on using the service without worrying about administrative tasks. On the other hand, unmanaged services, like EC2 instances or self-managed databases, require customers to handle administrative tasks such as provisioning, patching, and backups themselves.

---


## Quiz - AWS Technology - Part One

**1. Which container orchestration framework is used by AWS Elastic Kubernetes Service (EKS)?**


* [ ] Docker Compose
* [ ] Apache Mesos
* [ ] Kubernetes
* [ ] AWS Fargate

**Correct answer:**
* [x] Kubernetes

**Explaination**: AWS Elastic Kubernetes Service (EKS) uses Kubernetes as the container orchestration framework. Kubernetes is an open-source platform for automating the deployment, scaling, and management of containerized applications. EKS manages the Kubernetes control plane for you, allowing you to focus on deploying and managing your applications. For more details, see the official Kubernetes documentation: Kubernetes Documentation

**2. What is an Amazon Machine Image (AMI) in the context of EC2?**


* [ ] A network security group
* [ ] A virtual private network (VPN) configuration
* [ ] A template for launching EC2 instances
* [ ] A load balancer configuration

**Correct answer:**
* [x] A template for launching EC2 instances

**Explaination**: An Amazon Machine Image (AMI) is a template that contains the necessary information to launch an instance in Amazon EC2. It includes the operating system, software packages, and other configurations. For more details, see the official AWS documentation on Amazon EC2 AMIs: Amazon EC2 AMIs

**3. What is AWS Elastic Container Service (ECS)?**


* [ ] A managed service for running containers on Kubernetes
* [ ] A serverless compute service for running Docker containers
* [ ] A fully managed container orchestration service
* [ ] A service for managing virtual machines on AWS

**Correct answer:**
* [x] A fully managed container orchestration service

**Explaination**: AWS Elastic Container Service (ECS) is a fully managed container orchestration service provided by AWS. It allows you to run Docker containers in a scalable and highly available manner, without the need to manage the underlying infrastructure. ECS supports both Fargate (serverless) and EC2 (managed instances) launch types. For more information, refer to the official AWS documentation on ECS: AWS Elastic Container Service (ECS)

**4. What is Amazon S3?**


* [ ] A database service
* [ ] A serverless compute service
* [ ] An object storage service
* [ ] A virtual private network service

**Correct answer:**
* [x] An object storage service

**Explaination**: Amazon S3 is an object storage service offered by AWS. It is designed to store and retrieve any amount of data from anywhere on the web. For more information, see the official AWS documentation on Amazon S3: Amazon Simple Storage Service (S3)

**5. What is AWS Lambda?**


* [ ] A fully managed database service
* [ ] A container orchestration service
* [ ] A serverless compute service
* [ ] A message queue service

**Correct answer:**
* [x] A serverless compute service

**Explaination**: AWS Lambda is a serverless compute service provided by AWS. It allows you to run your code without provisioning or managing servers. You can write your functions in various programming languages and trigger them in response to events from other AWS services or custom sources. For more information, refer to the official AWS documentation on AWS Lambda: AWS Lambda

**6. What is Amazon EC2?**


* [ ] A fully managed database service
* [ ] A serverless computing platform
* [ ] A scalable virtual server in the cloud
* [ ] A content delivery network (CDN) service

**Correct answer:**
* [x] A scalable virtual server in the cloud

**Explaination**: Amazon EC2 provides resizable compute capacity in the cloud and allows users to launch virtual servers called instances. It offers flexibility in terms of choosing the instance types, operating systems, and other configurations. For more information, refer to the official AWS documentation on Amazon EC2: Amazon EC2

**7. Which of the following is a characteristic of Amazon EC2 instances?**


* [ ] Pre-configured network infrastructure
* [ ] Dedicated physical servers
* [ ] Shared tenancy for efficient resource utilization
* [ ] Managed operating system updates

**Correct answer:**
* [x] Shared tenancy for efficient resource utilization

**Explaination**: Amazon EC2 instances are provisioned on shared tenancy hardware, allowing for efficient resource utilization. However, EC2 also offers dedicated instances and dedicated hosts for customers who require isolated hardware resources. For more details, see the official AWS documentation on Amazon EC2 instances: Amazon EC2 Instances

**8. What is an EC2 instance type?**


* [ ] A unique identifier for an EC2 instance
* [ ] A specific configuration of CPU, memory, storage, and networking
* [ ] An IP address assigned to an EC2 instance
* [ ] A security group associated with an EC2 instance

**Correct answer:**
* [x] A specific configuration of CPU, memory, storage, and networking

**Explaination**: An EC2 instance type defines the hardware configuration of the virtual server, including the number of CPUs, amount of memory, storage capacity, and networking capabilities. Different instance types are optimized for various use cases. For more information, refer to the official AWS documentation on Amazon EC2 instance types: Amazon EC2 Instance Types

**9. Which of the following storage classes in Amazon S3 provides the lowest cost per GB?**


* [ ] Standard-IA
* [ ] Intelligent-Tiering
* [ ] Glacier
* [ ] One Zone-IA

**Correct answer:**
* [x] Glacier

**Explaination**: The Glacier storage class in Amazon S3 provides the lowest cost per GB. It is designed for long-term archival storage where retrieval times are less critical. For more information, refer to the official AWS documentation on Amazon S3 storage classes: Amazon S3 Storage Classes

**10. Which of the following best describes Elastic Block Store (EBS) in AWS?**


* [ ] A fully managed file storage service.
* [ ] A service for creating virtual private networks.
* [ ] A scalable object storage service.
* [ ] A block-level storage service for EC2 instances.

**Correct answer:**
* [x] A block-level storage service for EC2 instances.

**Explaination**: Elastic Block Store (EBS) in AWS is a block-level storage service that provides persistent block storage volumes for EC2 instances. It allows you to create, attach, and detach volumes to EC2 instances for storing data. For more information, refer to the official AWS documentation on EBS: Elastic Block Store (EBS)

**11. Which of the following best describes Elastic File System (EFS) in AWS?**


* [ ] A relational database service.
* [ ] A managed file storage service.
* [ ] A content delivery network.
* [ ] A distributed caching service.

**Correct answer:**
* [x] A managed file storage service.

**Explaination**: Elastic File System (EFS) in AWS is a fully managed file storage service that provides scalable and shared file storage for use with EC2 instances. It allows multiple EC2 instances to access the same file system concurrently. For more information, refer to the official AWS documentation on EFS: Elastic File System (EFS)

**12. What is an AWS security group?**


* [ ] A managed service that performs automated security audits on AWS resources
* [ ] A virtual firewall that controls inbound and outbound traffic for EC2 instances
* [ ] A service that provides data encryption for data at rest in Amazon S3
* [ ] A service that scans code for security vulnerabilities in AWS Lambda functions.

**Correct answer:**
* [x] A virtual firewall that controls inbound and outbound traffic for EC2 instances

**Explaination**: The correct answer is "A virtual firewall that controls inbound and outbound traffic for EC2 instances". An AWS security group is a fundamental component of the network security in AWS. It acts as a virtual firewall that controls inbound and outbound traffic at the instance level.

**13. What is the Default VPC in AWS?**


* [ ] A pre-configured VPC that is automatically created in each AWS region
* [ ] A VPC that provides advanced networking features for high-performance applications.
* [ ] A virtual network that spans across multiple AWS accounts
* [ ] A VPC that is specifically designed for cross-region communication

**Correct answer:**
* [x] A pre-configured VPC that is automatically created in each AWS region

**Explaination**: The correct answer is "A pre-configured VPC that is automatically created in each AWS region". The Virtual Default VPC is a default VPC that is automatically created for every AWS account in each region. It provides a default networking environment for EC2 instances and other AWS resources.

**14. What is the primary purpose of AWS Direct Connect?**


* [ ] To connect on-premises data centers to the AWS Cloud
* [ ] To enable direct communication between Amazon EC2 instances
* [ ] To provide secure access to S3 buckets from within a VPC
* [ ] To establish VPN connections between AWS regions

**Correct answer:**
* [x] To connect on-premises data centers to the AWS Cloud

**Explaination**: The correct answer is "To connect on-premises data centers to the AWS Cloud". AWS Direct Connect provides a dedicated network connection between an organization's on-premises data centers and AWS, allowing for secure and reliable data transfer.

**15. Which of the following statements is true about Network Access Control Lists (NACLs) in AWS?**


* [ ] NACLs are used to control access to Amazon RDS instances.
* [ ] NACLs are stateful, meaning they automatically allow return traffic for allowed inbound traffic.
* [ ] NACLs can be associated with multiple subnets within a VPC.
* [ ] NACLs operate at the subnet level

**Correct answer:**
* [x] NACLs operate at the subnet level

**Explaination**: Network Access Control Lists (NACLs) in AWS operate at the subnet level, controlling both inbound and outbound traffic. They provide an additional layer of security for VPCs by allowing or denying traffic based on rules. For more information, refer to the official AWS documentation on NACLs: Network Access Control Lists (NACLs)

**16. Which of the following resources are automatically created when a Default VPC is provisioned?**


* [ ] Internet Gateway, subnet, and security group.
* [ ] Elastic Load Balancer, network ACL, and NAT Gateway.
* [ ] Virtual Private Gateway, VPC endpoint, and VPC peering connection.
* [ ] Route table, network interface, and VPC endpoint service.

**Correct answer:**
* [x] Internet Gateway, subnet, and security group.

**Explaination**: The correct answer is "Internet Gateway, subnet, and security group". When a Virtual Default VPC is provisioned, an Internet Gateway, subnet, and a default security group are automatically created.

**17. What is the primary purpose of an AWS NAT Gateway?**


* [ ] To provide secure access to S3 buckets from within a VPC
* [ ] To enable connectivity between VPCs in different regions
* [ ] To control inbound and outbound traffic for Amazon EC2 instances
* [ ] To allow instances in private subnets to access the internet

**Correct answer:**
* [x] To allow instances in private subnets to access the internet

**Explaination**: The correct answer is "To allow instances in private subnets to access the internet". An AWS NAT Gateway allows instances in private subnets to establish outbound connections to the internet while keeping the instances themselves private and protected from direct internet access.

**18. Which of the following statements accurately describes a public subnet in Amazon VPC?**


* [ ] A public subnet is accessible from the internet.
* [ ] A public subnet is a subnet with a public IP assigned to every resource.
* [ ] A public subnet is a subnet with an internet gateway attached.
* [ ]  A public subnet is a subnet with a NAT gateway for outbound traffic.

**Correct answer:**
* [x] A public subnet is a subnet with an internet gateway attached.

**Explaination**: The correct answer is "A public subnet is a subnet with an internet gateway attached". An internet gateway allows resources within a public subnet to communicate with the internet, making the subnet accessible from the internet.

**19. Which of the following statements accurately describes AWS Global Infrastructure's approach to data center redundancy?**


* [ ] It relies on a single data center for each region, ensuring maximum availability.
* [ ] It uses multiple Availability Zones within each region for high availability and fault tolerance.
* [ ] It maintains a backup data center in a different geographic region for disaster recovery purposes.
* [ ] It replicates data across multiple regions in real time for data redundancy.

**Correct answer:**
* [x] It uses multiple Availability Zones within each region for high availability and fault tolerance.

**Explaination**: The correct answer is "It uses multiple Availability Zones within each region for high availability and fault tolerance". Each AWS region is designed with multiple physically separated Availability Zones (AZs) that are interconnected through low-latency links. This design provides fault tolerance and allows customers to architect highly available applications.

**20. What is the primary purpose of AWS edge locations?**


* [ ] To host EC2 instances in different geographic locations
* [ ] To provide low-latency access to content and services for end-users
* [ ] To replicate data across multiple AWS regions for redundancy
* [ ] To provide secure access to AWS accounts using VPN connections

**Correct answer:**
* [x] To provide low-latency access to content and services for end-users

**Explaination**: The correct answer is "To provide low-latency access to content and services for end-users". AWS edge locations are a part of Amazon CloudFront's global content delivery network (CDN). They are strategically located around the world to bring data closer to end-users, reducing latency and improving the performance of content delivery.

**21. Which of the following is a valid use case for using multiple Amazon VPCs within an AWS account?**


* [ ] To isolate development, testing, and production environments
* [ ] To establish secure connectivity between on-premises data centers and the AWS Cloud
* [ ] To distribute workloads across multiple AWS regions for redundancy
* [ ] To provide low-latency access to AWS services for specific geographic areas

**Correct answer:**
* [x] To isolate development, testing, and production environments

**Explaination**: The correct answer is "To isolate development, testing, and production environments". Using multiple Amazon VPCs allows you to segregate and isolate different environments, such as development, testing, and production, within the same AWS account. This separation provides enhanced security and control over the network traffic and resources associated with each environment.

**22. What is the purpose of AWS Local Zones in terms of workload placement?**


* [ ] To distribute workloads across multiple AWS regions for redundancy
* [ ] To ensure high availability and fault tolerance for applications
* [ ] To provide low-latency access to AWS services in specific geographic areas
* [ ] To establish secure connectivity between on-premises data centers and the AWS Cloud

**Correct answer:**
* [x] To provide low-latency access to AWS services in specific geographic areas

**Explaination**: The correct answer is "To provide low-latency access to AWS services in specific geographic areas". AWS Local Zones are designed to bring AWS services closer to end-users in specific geographic locations. By placing workloads in Local Zones, you can achieve low-latency access to AWS services, improving application performance and user experience.

**23. Which of the following statements accurately describes an Amazon VPC subnet?**


* [ ] A subnet is a virtual network within an AWS region.
* [ ] A subnet is a security group that controls inbound and outbound traffic.
* [ ] A subnet is an EC2 instance running in a private network.
* [ ] A subnet is a collection of IP addresses for routing traffic within a VPC.

**Correct answer:**
* [x] A subnet is a collection of IP addresses for routing traffic within a VPC.

**Explaination**: The correct answer is "A subnet is a collection of IP addresses for routing traffic within a VPC". Subnets are logical divisions of an Amazon VPC that allow you to segment and manage IP addresses for routing network traffic between resources within the VPC.

**24. Which of the following statements accurately describes AWS regions?**


* [ ] AWS regions are specific geographic locations where AWS data centers are located.
* [ ] AWS regions are virtual environments that segregate customer resources within the same geographic location.
* [ ] AWS regions are highly available zones within a single data center.
* [ ] AWS regions are virtual private networks used to connect on-premises data centers to the AWS Cloud.

**Correct answer:**
* [x] AWS regions are specific geographic locations where AWS data centers are located.

**Explaination**: The correct answer is "AWS regions are specific geographic locations where AWS data centers are located". An AWS region is a physical location where AWS data centers are situated. Each region is designed to be isolated from other regions, providing customers with options for geographic redundancy and compliance requirements.

**25. Which of the following statements is true?**


* [ ] Both security groups and Network ACLs are stateful firewalls
* [ ] Both security groups and Network ACLs are stateless firewalls
* [ ] Network ACLs are stateful firewalls and security groups are stateless
* [ ] Security groups are stateful firewalls and Network ACLs are stateless

**Correct answer:**
* [x] Security groups are stateful firewalls and Network ACLs are stateless

**Explaination**: Network ACLs (Network Access Control Lists) and security groups are both used for controlling inbound and outbound traffic in AWS, but they function differently. Network ACLs are stateless firewalls at the subnet level, meaning they keep track of the state of each packet and allow the return traffic automatically. They operate at the network layer (Layer 3) of the OSI model and evaluate rules in sequential order. Security groups, on the other hand, are stateful firewalls that control traffic at the instance level. They operate at the instance level (Layer 4 of the OSI model) and evaluate rules independently for inbound and outbound traffic.

---


## Quiz - AWS Technology - Part Two

**1. What is the purpose of AWS Application Discovery Service?**


* [ ] To monitor the performance and availability of applications
* [ ] To discover and collect data about on-premises applications and dependencies
* [ ] To analyze logs and generate insights for application troubleshooting
* [ ] To automate the deployment of applications on AWS infrastructure

**Correct answer:**
* [x] To discover and collect data about on-premises applications and dependencies

**Explaination**: AWS Application Discovery Service is designed to help you plan application migrations to the cloud by discovering and collecting data about your on-premises applications and their dependencies. It provides insights into the applications' configuration, usage patterns, and interdependencies.

**2. Which AWS service provides a fully managed solution for migrating applications from on-premises environments to AWS?**


* [ ] AWS Database Migration Service
* [ ] AWS Snowball
* [ ] AWS Application Discovery Service
* [ ] AWS Application Migration Service

**Correct answer:**
* [x] AWS Application Migration Service

**Explaination**: AWS MGN is a fully managed service that simplifies and accelerates the migration of applications from on-premises environments to AWS. It provides automated replication, scheduling, and orchestration capabilities to streamline the migration process.

**3. Which AWS Snow device is used for large-scale data migrations?**


* [ ] AWS Snowcone
* [ ] AWS Snowball
* [ ] AWS Snowmobile
* [ ] AWS Glacier

**Correct answer:**
* [x] AWS Snowmobile

**Explaination**: AWS Snowmobile is a secure data transfer device that can handle exabyte-scale data migrations to AWS. It is a ruggedized shipping container pulled by a semi-trailer truck, designed to transfer extremely large datasets in a secure and efficient manner.

**4. What is the primary purpose of AWS DataSync?**


* [ ] To migrate on-premises databases to Amazon RDS
* [ ] To replicate data between multiple Amazon S3 buckets
* [ ] To perform real-time analytics on streaming data
* [ ] To synchronize data between on-premises storage and AWS

**Correct answer:**
* [x] To synchronize data between on-premises storage and AWS

**Explaination**: AWS DataSync is a data transfer service that is used to automate and accelerate the movement of data between on-premises storage systems and AWS storage services. It simplifies and speeds up data migration, data replication, and data processing workflows.


**5. What is the main benefit of the re-platform migration strategy in AWS?**


* [ ] It provides better scalability and performance for the application.
* [ ] It allows for a complete redesign of the application architecture.
* [ ] It reduces the complexity of migrating legacy applications.
* [ ] It minimizes downtime during the migration process.

**Correct answer:**
* [x] It reduces the complexity of migrating legacy applications.

**Explaination**: It reduces the complexity of migrating legacy applications. Re-platform migration strategy involves making minor adjustments to the application stack to leverage cloud-native services. It allows you to modernize the application while reducing the complexity and risks associated with a complete application redesign.

**6. Which migration strategy allows for the highest level of scalability and performance optimization in AWS?**


* [ ] Rehost
* [ ] Replatform
* [ ] Refactor
* [ ] Repurchase

**Correct answer:**
* [x] Refactor

**Explaination**: Refactor migration strategy involves making significant changes to the application code and its underlying architecture to optimize it for cloud-native services. By refactoring the application, you can leverage the full scalability and performance benefits provided by AWS.

**7. What is the purpose of AWS Snow devices in the AWS Snow Family?**


* [ ] To provide edge computing capabilities
* [ ] To transfer large amounts of data to and from AWS
* [ ] To enable real-time data analytics on AWS
* [ ] To optimize cost and performance of AWS resources

**Correct answer:**
* [x] To transfer large amounts of data to and from AWS

**Explaination**: AWS Snow devices are rugged, portable devices designed to help you transfer large amounts of data offline securely. They are used when you need to move data sets, backups, or archives to or from AWS in situations where a direct network connection is not feasible or efficient.

**8. What is the purpose of the repurchase migration strategy in AWS?**


* [ ] To migrate applications from on-premises to AWS
* [ ] To replace existing software with a different vendor's solution
* [ ] To optimize the application for cost savings on AWS
* [ ] To redesign the application architecture for improved performance

**Correct answer:**
* [x] To replace existing software with a different vendor's solution

**Explaination**: Repurchase migration strategy involves replacing the existing application or software with a different vendor's solution. This strategy is commonly used when the current application or software no longer meets the business requirements or when a better alternative is available.

**9. What is an example of a rehost migration strategy in AWS?**


* [ ] Refactoring the application code to optimize performance
* [ ] Migrating the application to a different database engine
* [ ] Replicating the application environment on AWS
* [ ] Redesigning the application architecture for scalability

**Correct answer:**
* [x] Replicating the application environment on AWS

**Explaination**: Rehost migration strategy, also known as lift-and-shift, involves replicating the application environment as-is on AWS without making any changes to the application code. It is a common approach to quickly migrate applications to AWS with minimal modifications.

**10. What is the purpose of AWS Migration Hub?**


* [ ] To manage and monitor AWS resources
* [ ] To provide cost optimization recommendations for AWS deployments
* [ ] To track the progress of application migrations to AWS
* [ ] To automate the process of migrating databases to AWS

**Correct answer:**
* [x] To track the progress of application migrations to AWS

**Explaination**: AWS Migration Hub is designed to help you track the progress and status of your application migrations to AWS. It provides a centralized view of your migration tasks, allowing you to monitor the migration process and identify any issues or delays.

**11. What is AWS CloudFormation primarily used for?**


* [ ] Managing virtual networks
* [ ] Creating and managing virtual machines
* [ ] Provisioning and managing AWS resources
* [ ] Monitoring and analyzing application performance

**Correct answer:**
* [x] Provisioning and managing AWS resources

**Explaination**: AWS CloudFormation is a service that helps you provision and manage AWS resources in a predictable and automated way. It allows you to define your infrastructure as code using templates and then deploy and manage the resources defined in those templates. For more information about AWS CloudFormation, you can refer to the official AWS documentation: AWS CloudFormation

**12. What is AWS Service Catalog?**


* [ ] A service for managing virtual networks in AWS
* [ ] A service for monitoring and analyzing application performance
* [ ] A service for creating and managing catalogs of approved IT services
* [ ] A service for automating backups of AWS databases

**Correct answer:**
* [x] A service for creating and managing catalogs of approved IT services

**Explaination**: AWS Service Catalog enables organizations to create and manage catalogs of IT services that are approved for use within their environment. It provides a controlled and consistent way to distribute and manage services, allowing users to find and deploy the approved resources they need.

**13. What is the purpose of AWS Control Tower?**


* [ ] To automate the deployment of serverless applications.
* [ ] To manage and configure Amazon RDS database instances.
* [ ] To establish a secure and compliant multi-account AWS environment.
* [ ] To monitor and troubleshoot network connectivity in AWS.

**Correct answer:**
* [x] To establish a secure and compliant multi-account AWS environment.

**Explaination**: AWS Control Tower helps organizations set up and govern a secure and compliant multi-account AWS environment. It provides guardrails, best practices, and automation to ensure consistent security, compliance, and governance across multiple AWS accounts.

**Documentation Link**: https://aws.amazon.com/controltower/

**14. What is the primary purpose of AWS Systems Manager Automation?**


* [ ] To manage security groups for EC2 instances
* [ ] To automate common operational tasks and workflows
* [ ] To configure and manage AWS networking components
* [ ] To monitor and analyze application performance

**Correct answer:**
* [x] To automate common operational tasks and workflows

**Explaination**: AWS Systems Manager Automation enables you to automate manual and repetitive tasks across your AWS environment. It provides a way to define and execute workflows, known as automation documents, that perform actions on AWS resources. This helps improve operational efficiency and reduces the need for manual intervention. For more details about AWS Systems Manager Automation, you can refer to the official AWS documentation: AWS Systems Manager Automation

**15. Which of the following statements best describes AWS OpsWorks?**


* [ ] A service for automated backups of AWS resources
* [ ] A managed service for running containerized applications
* [ ] A configuration management service for deploying and managing applications
* [ ] A fully managed big data processing service

**Correct answer:**
* [x] A configuration management service for deploying and managing applications

**Explaination**:  AWS OpsWorks is a configuration management service that helps you automate the deployment and management of applications. It provides features for defining application architecture, managing instances, and automating tasks. OpsWorks supports multiple configuration management tools, including Chef and Puppet. For more information about AWS OpsWorks, you can refer to the official AWS documentation: AWS OpsWorks

**16. Which AWS service can be integrated with Auto Scaling to distribute incoming traffic across multiple instances?**


* [ ] Amazon EC2
* [ ] Amazon S3
* [ ] Elastic Load Balancer
* [ ] AWS Lambda

**Correct answer:**
* [x] Elastic Load Balancer

**Explaination**: Elastic Load Balancer (ELB) can be integrated with Auto Scaling to distribute incoming traffic across multiple instances within an Auto Scaling group. ELB helps ensure that the load is balanced across instances and improves the availability and fault tolerance of the application. For more details about integrating Auto Scaling with Elastic Load Balancer, you can refer to the official AWS documentation: Integration with Elastic Load Balancer

**17. What is the purpose of AWS Auto Scaling?**


* [ ] To automate the provisioning of AWS infrastructure resources
* [ ] To automatically scale EC2 instances based on demand
* [ ] To manage and deploy serverless applications
* [ ] To monitor and analyze application performance

**Correct answer:**
* [x] To automatically scale EC2 instances based on demand

**Explaination**: AWS Auto Scaling enables you to automatically adjust the number of EC2 instances in a fleet based on predefined conditions or metrics. It helps maintain application availability, optimize performance, and reduce costs by scaling the capacity up or down as needed. For more information about AWS Auto Scaling, you can refer to the official AWS documentation: AWS Auto Scaling

**18. Which AWS service is used to distribute incoming traffic across multiple instances within a single or multiple Availability Zones?**


* [ ] Amazon EC2
* [ ] Amazon S3
* [ ] Elastic Load Balancer
* [ ] AWS Lambda

**Correct answer:**
* [x] Elastic Load Balancer

**Explaination**: Elastic Load Balancer (ELB) is an AWS service that automatically distributes incoming traffic across multiple instances within a single or multiple Availability Zones. It helps improve the availability and fault tolerance of your applications. For more information about Elastic Load Balancer, you can refer to the official AWS documentation: Elastic Load Balancer

**19. What is the primary purpose of Amazon Simple Notification Service (SNS)?**


* [ ] Real-time data analytics
* [ ] File storage and sharing
* [ ] Scalable and flexible messaging
* [ ] Database management and querying

**Correct answer:**
* [x] Scalable and flexible messaging

**Explaination**: Amazon SNS is a fully managed pub/sub messaging service that enables the creation and delivery of messages to distributed systems or standalone endpoints. It provides the ability to publish and subscribe to topics, allowing for scalable and flexible messaging patterns. SNS is commonly used for applications that require event-driven architectures, push notifications, or fan-out scenarios. For more information about Amazon SNS and its use cases, you can refer to the official AWS documentation: Amazon Simple Notification Service (SNS)

**20. What is the primary purpose of Amazon Simple Queue Service (SQS)?**


* [ ] Storing and retrieving structured data
* [ ] Hosting static websites
* [ ] Message queuing and decoupling of distributed systems
* [ ] Analyzing large datasets

**Correct answer:**
* [x] Message queuing and decoupling of distributed systems

**Explaination**: Amazon SQS is a fully managed message queuing service that enables decoupling of components in distributed systems. It allows you to send, store, and receive messages between software components, helping to decouple and scale your systems independently. SQS provides reliable and scalable queuing functionality, making it ideal for building distributed applications and microservices architectures. For more information about Amazon SQS and its use cases, you can refer to the official AWS documentation: Amazon Simple Queue Service (SQS)

**21. Which AWS service provides a fully managed NoSQL document database service compatible with MongoDB?**


* [ ] Amazon RDS
* [ ] Amazon Neptune
* [ ] Amazon DocumentDB
* [ ] Amazon ElastiCache

**Correct answer:**
* [x] Amazon DocumentDB

**Explaination**: Amazon DocumentDB is a fully managed NoSQL document database service provided by AWS. It is compatible with existing MongoDB applications, drivers, and tools, making it easy to migrate MongoDB workloads to DocumentDB. For more information, refer to the official AWS documentation on Amazon DocumentDB: Amazon DocumentDB Documentation


**22. Which AWS service provides a fully managed NoSQL database service?**


* [ ] Amazon RDS
* [ ] Amazon Redshift
* [ ] Amazon DynamoDB
* [ ] Amazon Aurora

**Correct answer:**
* [x] Amazon DynamoDB

**Explaination**: Amazon DynamoDB is a fully managed NoSQL database service provided by AWS. It offers seamless scalability, high availability, and low latency for applications that require fast and predictable performance. DynamoDB supports both key-value and document data models. For more information, refer to the official AWS documentation on Amazon DynamoDB: Amazon DynamoDB Documentation


**23. Which AWS service is a fully managed, petabyte-scale data warehousing service?**


* [ ] Amazon DynamoDB
* [ ] Amazon Redshift
* [ ] Amazon RDS
* [ ] Amazon Athena

**Correct answer:**
* [x] Amazon Redshift

**Explaination**: Amazon Redshift is a fully managed, petabyte-scale data warehousing service provided by AWS. It allows you to analyze large datasets using SQL queries efficiently. Redshift is optimized for online analytical processing (OLAP) workloads and provides high-performance, columnar storage, and parallel query execution. For more information, refer to the official AWS documentation on Amazon Redshift: Amazon Redshift Documentation

**24. Which of the following statements accurately describes a key difference between Amazon RDS and Amazon Aurora?**


* [ ] Amazon RDS is a fully managed relational database service, while Amazon Aurora is a self-managed database service.
* [ ] Amazon RDS is designed for high availability and durability, while Amazon Aurora offers better performance and scalability.
* [ ] Amazon RDS supports a wide range of database engines, while Amazon Aurora only supports MySQL and PostgreSQL.
* [ ] Amazon RDS is a serverless database service, while Amazon Aurora requires manual capacity management

**Correct answer:**
* [x] Amazon RDS is designed for high availability and durability, while Amazon Aurora offers better performance and scalability.

**Explaination**:  Amazon RDS is a fully managed relational database service that provides high availability and durability for databases. It offers automated backups, automated software patching, and automatic failover capabilities. On the other hand, Amazon Aurora is a MySQL and PostgreSQL-compatible database engine that is built for better performance and scalability compared to traditional database engines. Aurora achieves this through an architecture that utilizes distributed storage and replication. It also offers features like read replicas and automatic scaling.

**25. Which AWS service provides a fully managed, relational database service?**


* [ ] Amazon DynamoDB
* [ ] Amazon Redshift
* [ ] Amazon RDS
* [ ] Amazon Neptune

**Correct answer:**
* [x] Amazon RDS

**Explaination**: Amazon Relational Database Service (RDS) is a fully managed service that provides a relational database in the cloud. It supports popular database engines such as MySQL, PostgreSQL, Oracle, SQL Server, and MariaDB. With RDS, you can focus on your application, while AWS handles the management of database instances, backups, patching, and replication. For more information, refer to the official AWS documentation on Amazon RDS: Amazon RDS Documentation

---


## Quiz – AWS Pricing and Billing

**1. What is the name of the AWS catalog of third-party vendors that are ready to install and bill in your AWS account?**


* [ ] AWS Reserved Instances Marketplace
* [ ] AWS Infiniband 
* [ ] AWS Marketplace
* [ ]  AWS Builder Lab

**Correct answer:**
* [x] AWS Marketplace

**Explaination**: AWS Marketplace is the catalog of third-party vendors that provide software and services on the AWS platform. These offerings are ready to install in AWS accounts and are billed using the account's payment method. Reserved Instances Marketplace is specifically for purchasing reserved capacity. AWS Infiniband and AWS Builder Lab do not exist.

**Documentation Link**: https://aws.amazon.com/marketplace/

**2. Which AWS Support plan gives you access to a dedicated Technical Account Manager for incident management and single-point escalation?**


* [ ] Developer
* [ ] Business
* [ ] Enterprise On-Ramp
* [ ] Enterprise

**Correct answer:**
* [x] Enterprise

**Explaination**: The AWS Enterprise Support plan provides access to a Technical Account Manager (TAM) who acts as a single point of contact for incident management and escalation. The TAM has a deep understanding of your workloads and environment and helps coordinate AWS resources to resolve critical issues. The Developer and Business Support plans do not include a TAM. The Enterprise On-Ramp plan includes limited access to a TAM for advisory purposes but not for incident response.

**Documentation Link**: https://aws.amazon.com/premiumsupport/features/#Technical_Account_Manager

**3. Which AWS billing tool would you recommend if you wanted the most detailed report for a third-party analysis?**


* [ ] Billing Dashboard 
* [ ] Cost Explorer
* [ ] The Cost and Usage Report 
* [ ] AWS Budgets

**Correct answer:**
* [x] The Cost and Usage Report 

**Explaination**: The Cost and Usage Report provides the most granular data for billing analysis. It includes hourly usage data for every AWS resource and service. This report can be generated on-demand or scheduled for delivery to an S3 bucket. The data can then be analyzed by third-party tools for cost allocation, chargeback, budgeting, and optimization.
The Billing Dashboard and Cost Explorer provide aggregated views of charges which may lack some detail. AWS Budgets is a tool for setting custom budgets but does not provide billing data directly.


**Documentation Link**: https://docs.aws.amazon.com/cur/latest/userguide/what-is-cur.html

**4. What factors contribute to the cost calculation of AWS Lambda?**


* [ ] Size of function, number of requests, length of execution
* [ ] Size of requests, number of functions, length of execution
* [ ] Size of function, number of service calls, number of backups
* [ ] Number of Aurora capacity units, Read Capacity Units, and Write Capacity Units

**Correct answer:**
* [x] Size of function, number of requests, length of execution

**Explaination**: AWS Lambda charges are primarily based on the number of requests and the duration of code execution. The size of the function also plays a role as it can affect the execution time. 

**5. Which type of Elastic Block Store (EBS) hard drive generally carries the highest cost per GB?**


* [ ] Cold mechanical hard drive
* [ ] Throughput Optimized mechanical hard drive
* [ ] General Purpose Solid State Drive
* [ ] Provisioned IOPS Solid State Drive

**Correct answer:**
* [x] Provisioned IOPS Solid State Drive

**Explaination**: AWS Elastic Block Store (EBS) provides different types of volumes that cater to different needs. Among these, Provisioned IOPS Solid State Drive (SSD) is typically the most expensive on a per GB basis. This is because it delivers high performance for I/O intensive workloads and allows users to specify both volume size and the number of I/O operations per second (IOPS) the volume can support.

**Documentation Link**: https://aws.amazon.com/ebs/pricing/

**6. What are the typical charges associated with transferring data from an Availability Zone to another Availability Zone within the same region?**


* [ ] No data transfer charges
* [ ] Normal price as data transfer charges
* [ ] Maximum price as data transfer charges
* [ ] Small fee as data transfer charges 

**Correct answer:**
* [x] Normal price as data transfer charges

**Explaination**: When transferring data between Availability Zones within the same region, there are typically  data transfer charges. AWS does charge for inter-AZ data transfer. No Charge, Maximum Price, and Small fee are incorrect as they suggest no or various types of fees, whereas AWS does not typically charge for this type of data transfer.

**Documentation Link**: https://aws.amazon.com/ec2/pricing/on-demand/

**7. Which AWS support plan is the lowest you can choose and still get access to all of the Trusted Advisor Health Checks?**


* [ ] Developer
* [ ] Business 
* [ ] Enterprise On-Ramp
* [ ] Enterprise 

**Correct answer:**
* [x] Business 

**Explaination**: The AWS Business support plan is the lowest tier that provides access to all Trusted Advisor checks. The Developer support plan only includes a subset of Trusted Advisor checks. The Enterprise and Enterprise On-Ramp plans provide additional features beyond Trusted Advisor, such as access to solutions architects, service quotas increases, and a service health dashboard.

**Documentation Link**: https://aws.amazon.com/premiumsupport/trustedadvisor/

**8. What are the typical charges associated with transferring data from an Availability Zone in one region to any service in another region?**


* [ ] No data transfer charges
* [ ] Normal price as data transfer charges
* [ ] Maximum price as data transfer charges
* [ ]  Small fee as data transfer charges 

**Correct answer:**
* [x] Normal price as data transfer charges

**Explaination**: When transferring data from an Availability Zone in one region to any service in another region, you typically pay normal data transfer charges. AWS charges for inter-region data transfers.

**Documentation Link**: https://aws.amazon.com/ec2/pricing/on-demand/

**9. What are the typical charges associated with transferring data from an Availability Zone to Amazon S3 within the same region?**


* [ ] No data transfer charges
* [ ] Maximum price as data transfer charges
* [ ] Normal price as data transfer charges
* [ ] Small fee as data transfer charges

**Correct answer:**
* [x] No data transfer charges

**Explaination**: Typically, transferring data from an Availability Zone to Amazon S3 within the same region does not incur any data transfer charges. AWS does not charge for intra-region data transfer. However, inter-region or outbound data transfer may incur charges.

**Documentation Link**: https://aws.amazon.com/s3/pricing/

**10. Which of the following Amazon RDS modes or sub-services utilizes virtual machines in a manner similar to Amazon EC2?**


* [ ] RDS "main"
* [ ] RDS Snapshots
* [ ] RDS Aurora Serverless
* [ ] RDS Quantum

**Correct answer:**
* [x] RDS "main"

**Explaination**: Amazon RDS "main" operates on virtual machines similar to Amazon EC2. It provisions and operates the relational database for you. However, RDS Snapshots are backups, not a running instance. RDS Aurora Serverless, on the other hand, automatically adjusts computing capacity based on the application's requirements and does not operate on a specific virtual machine. At the time of my last update in September 2021, there was no service called "RDS Quantum."

**Documentation Link**: https://aws.amazon.com/rds/

**11. What are the typical charges associated with transferring data into a VPC or AWS service?**


* [ ] No data transfer charges
* [ ] Maximum price as data transfer charges
* [ ] Normal price as data transfer charges
* [ ] Payment to a third-party for data transfer 

**Correct answer:**
* [x] No data transfer charges

**Explaination**: When you transfer data into an Amazon VPC or an AWS service, you typically do not incur any data transfer charges. AWS does not charge for inbound data transfer. However, outbound data transfer can incur charges.

**Documentation Link**: https://aws.amazon.com/ec2/pricing/on-demand/

**12. Which Amazon EC2 option should be chosen for an application that requires isolation on its own physical hardware due to strict security requirements?**


* [ ] On-Demand
* [ ] Spot
* [ ] Reservation/Savings Plan
* [ ] Dedicated

**Correct answer:**
* [x] Dedicated

**Explaination**: For applications with strict security requirements that need to be isolated to their own physical hardware, Dedicated Instances or Dedicated Hosts are the best choice. These options provide instances that run on hardware that's dedicated to a single customer, which helps meet compliance requirements and reduce costs by reusing existing server-bound software licenses.

**Documentation Link**: https://aws.amazon.com/ec2/dedicated-hosts/

**13. Which Amazon EC2 pricing model is most suitable for an application that can withstand interruptions and recover gracefully?**


* [ ] On-Demand
* [ ] Spot
* [ ] Reservation/Savings Plan
* [ ] Dedicated

**Correct answer:**
* [x] Spot

**Explaination**: Amazon EC2 Spot Instances are best suited for applications that can withstand interruptions and recover gracefully. These instances let you take advantage of unused EC2 capacity in the AWS cloud at steep discounts compared to On-Demand prices. Spot Instances are ideal for various fault-tolerant and flexible applications such as big data, containerized workloads, CI/CD, web servers, high-performance computing (HPC), and test & development workloads.

**Documentation Link**: https://aws.amazon.com/ec2/spot/

**14. Which of the following Amazon EC2 options can provide up to a 90% discount on on-demand costs?**


* [ ] Reservation
* [ ] Savings Plan
* [ ] Dedicated
* [ ] Spot

**Correct answer:**
* [x] Spot

**Explaination**: Amazon EC2 Spot Instances allow you to take advantage of unused EC2 capacity in the AWS cloud. Spot Instances are available at up to a 90% discount compared to On-Demand prices. Spot Instances can significantly lower your AWS costs but are subject to availability and can be interrupted by AWS with two minutes of notification when AWS needs the capacity back.

**Documentation Link**: https://aws.amazon.com/ec2/spot/

**15. Which Amazon EC2 option should be chosen for cost-effectiveness while testing an application for 6 months?**


* [ ] On-Demand
* [ ] Spot
* [ ] Reservation/Savings Plan
* [ ] Dedicated

**Correct answer:**
* [x] On-Demand

**Explaination**: If you're testing an application for a limited period of time (like 6 months), On-Demand Instances would likely be the most cost-effective choice. On-Demand Instances let you pay for compute capacity by the hour or second (minimum of 60 seconds) with no long-term commitments. This frees you from the costs and complexities of planning, purchasing, and maintaining hardware and transforms what are commonly large fixed costs into much smaller variable costs.

**Documentation Link**: https://aws.amazon.com/ec2/pricing/on-demand/

**16. Which Amazon EC2 pricing model is most cost-effective for an application that will run continuously for over 10 months?**


* [ ] On-Demand
* [ ] Spot
* [ ] Reservation/Savings Plan
* [ ] Dedicated

**Correct answer:**
* [x] Reservation/Savings Plan

**Explaination**: For applications that will run continuously for over 10 months, Reserved Instances or Savings Plans are usually the most cost-effective choice. These options provide significant discounts (up to 75%) compared to On-Demand instance pricing. Customers commit to using EC2 over a 1-year or 3-year term and can reduce their total computing costs significantly. 

**Documentation Link**: https://aws.amazon.com/ec2/pricing/reserved-instances/

**17. On what frequency or time frame are most Amazon EC2 instances typically billed?**


* [ ] Per Month/Per year
* [ ] Per Week/Per Month
* [ ]  Per Hour/Per Week
* [ ] Per Second/Per Minute

**Correct answer:**
* [x] Per Second/Per Minute

**Explaination**: Most Amazon EC2 instances are charged on a per-second basis with a minimum of 60 seconds. This flexible pricing allows businesses to pay only for the compute time they use, making it cost-effective for workloads with variable compute requirements. The per-second billing applies to instances launched in On-Demand, Reserved, and Spot form. Dedicated Hosts are billed per hour. 

**Documentation Link**: https://aws.amazon.com/ec2/pricing/

**18. How many size options are available for Amazon EC2 virtual machines?**


* [ ] Only one
* [ ] Less than ten
* [ ] Approximately two dozen
* [ ] Hundreds

**Correct answer:**
* [x] Hundreds

**Explaination**: Amazon EC2 provides a variety of instance types optimized to fit different use cases. Each instance type belongs to a larger instance family and comes in various sizes to address specific workload requirements. Amazon EC2 offers hundreds of instance types. These are grouped into categories based on their optimized capacities, such as compute optimized, memory optimized, storage optimized, GPU instances, and more.

**Documentation Link**: https://aws.amazon.com/ec2/instance-types/

**19. Which AWS billing model permits upfront payment (for instance, $100) and yields increased value (for example, $150) over a one or three-year period?**


* [ ] On-Demand
* [ ] Spot
* [ ] Reservation
* [ ] Savings Plan

**Correct answer:**
* [x] Savings Plan

**Explaination**: AWS Savings Plans is a billing model that offers significant savings on AWS usage, similar to Reserved Instances. You commit to the usage of individual resources, such as compute power or storage, for a 1-year or 3-year term and in return, you receive a considerable discount compared to On-Demand pricing. The On-Demand, Spot, and Reservation models do not typically involve this kind of upfront payment with a guaranteed increase in value over a defined term.

**Documentation Link**: https://aws.amazon.com/savingsplans/

**20. When does the "Free Tier" come into effect for your account with regards to billing?**


* [ ] It applies continuously until the AWS account is 12 months old, after which it ceases
* [ ] It applies indefinitely, but trial periods are brief after activation and 12-month services last only for 12 months
* [ ] Until it is activated in your account
* [ ] Until you purchase a reservation or a savings plan

**Correct answer:**
* [x] It applies indefinitely, but trial periods are brief after activation and 12-month services last only for 12 months

**Explaination**: The AWS Free Tier is designed to give you hands-on experience with a range of AWS services at no charge. It includes two types of offerings - 'Always Free' and '12 Months Free'. The 'Always Free' offer is available to both existing and new AWS customers indefinitely, and the '12 Months Free' tier is available to new customers for the first 12 months after the sign-up date. Once these periods end, or if your application use exceeds the free tier limits, you'll pay the standard, pay-as-you-go service rates. The Free Tier is not connected to the activation of specific features or the purchase of a reservation or a savings plan.

**Documentation Link**: https://aws.amazon.com/free/

**21. Which AWS billing model is analogous to a buffet dining experience, where you can consume as much as you desire and simply stop when you're done?**


* [ ] On-Demand
* [ ] Spot
* [ ] Reservation
* [ ] Savings Plan

**Correct answer:**
* [x] On-Demand

**Explaination**: In AWS, the On-Demand pricing model allows users to pay for compute capacity by the hour or second, depending on which instances are being run. This is similar to a buffet dining experience, where you consume as much as you want and simply stop when you're satisfied. No upfront payment is required, and you can increase or decrease compute capacity at any time. On the other hand, Spot instances, Reservation instances, and Savings Plans involve certain commitments or fluctuating prices, unlike a buffet where you can consume as you please without any commitment.

**Documentation Link**: https://aws.amazon.com/ec2/pricing/

**22. What does the term "Maximize the Power of Elasticity" imply in the context of AWS billing?**


* [ ] Increase the number of servers when customer demand is low; decrease when customer demand is high
* [ ] Increase the number of servers when customer demand is high; decrease when customer demand is low
* [ ]  Increase the number of servers when server demand is low; decrease when server demand is high
* [ ] Increase the number of servers when server demand is high; decrease when server demand is low

**Correct answer:**
* [x] Increase the number of servers when customer demand is high; decrease when customer demand is low

**Explaination**: The power of elasticity in AWS billing refers to the capability of quickly adjusting resources based on demand. As such, when customer demand is high, the number of servers should be increased to meet this demand. Conversely, when customer demand is low, the number of servers should be decreased to avoid incurring unnecessary costs.

**23. What is the recommended approach when using elasticity in your AWS architectures?**


* [ ] Scale up whenever you want to
* [ ] Don’t worry about scaling down
* [ ] Try to do as much manual setup as possible
* [ ] Use Automatic Scaling (like AutoScaling) whenever you can

**Correct answer:**
* [x] Use Automatic Scaling (like AutoScaling) whenever you can

**Explaination**: When utilizing elasticity in your AWS architectures, it's recommended to leverage automatic scaling features, such as AWS AutoScaling. This service automatically adjusts resources as needed based on demand, which can help to optimize costs and maintain performance.

**Documentation Link**: https://aws.amazon.com/autoscaling/

**24. In the context of cost optimization with AWS, what should be prioritized?**


* [ ] Unmanaged services over managed services when possible
* [ ] Managed services over unmanaged services when possible
* [ ] Traditional data centers over cloud data centers when possible
* [ ] Software with restrictive licensing agreements over open-source software

**Correct answer:**
* [x] Managed services over unmanaged services when possible

**Explaination**: Cost optimization in AWS often involves choosing managed services over unmanaged services whenever feasible. Managed services reduce the operational burden by handling many of the underlying administrative tasks, thus potentially reducing overall costs. Unmanaged services may require additional resources and time, increasing costs.

**Documentation Link**: https://aws.amazon.com/pricing/cost-optimization/

**25. What are the three primary cost determinants for most services in AWS?**


* [ ] Compute, Storage, and Network
* [ ] Compute, Memory, and Processor
* [ ] Data, Storage, and Backups
* [ ] Containers, Storage, and Database

**Correct answer:**
* [x] Compute, Storage, and Network

**Explaination**: Most AWS services base their pricing on three main factors - Compute, Storage, and Network. Compute refers to the processing power required, Storage refers to the amount of data to be stored, and Network refers to data transfer rates or bandwidth needs. Other factors, such as memory or processing power, may influence pricing for specific services, but they are not the primary determinants for most AWS services.

**Documentation Link**: https://aws.amazon.com/pricing/

---


## What you know at the end - Exam

**1. What is the benefit of utilizing Pay-as-you-go pricing on AWS?**


* [ ] Unlimited usage of all AWS services
* [ ] Fixed monthly costs regardless of usage
* [ ] Pay only for the compute, storage, and other resources you use
* [ ] Ability to receive discounts on unused resources

**Correct answer:**
* [x] Pay only for the compute, storage, and other resources you use

**Explaination**: The benefit of utilizing Pay-as-you-go pricing on AWS is that you only pay for the resources you use, whether it be compute, storage, or other services. This allows for cost optimization and eliminates the need for upfront cost commitments.

**Documentation Link**: https://aws.amazon.com/pricing/

**2. What is the highest support level provided by AWS for its services?**


* [ ] Basic Support
* [ ] Developer Support
* [ ] Business Support
* [ ] Enterprise Support

**Correct answer:**
* [x] Enterprise Support

**Explaination**: AWS provides five different levels of support - Basic Support, Developer Support, Business Support, Enterprise On-Ramp, and Enterprise Support. Enterprise Support is the highest and most comprehensive level of support offered by AWS. It includes 24/7 global support, a dedicated Technical Account Manager (TAM), and a range of proactive and personalized support features. It is designed to meet the needs of large enterprises with mission-critical workloads that require high levels of support and personalized solutions. 

Basic Support is the free level of support that is automatically enabled for all AWS customers. Developer Support is designed for developers or individual users running production workloads. Business Support is designed for small and medium-sized businesses that require business-critical support features.  Enterprise On-Ramp is a lower tier of Enterprise support for smaller Enterprises looking for a lower cost yet still strong Production focused option.  


**Documentation Link**: https://aws.amazon.com/premiumsupport/compare-plans/

**3. Which of the following Amazon EC2 options can provide up to a 90% discount on on-demand costs?**


* [ ] Reservation
* [ ] Savings Plan
* [ ] Dedicated
* [ ] Spot

**Correct answer:**
* [x] Spot

**Explaination**: Amazon EC2 Spot Instances allow you to take advantage of unused EC2 capacity in the AWS cloud. Spot Instances are available at up to a 90% discount compared to On-Demand prices. Spot Instances can significantly lower your AWS costs but are subject to availability and can be interrupted by AWS with two minutes of notification when AWS needs the capacity back.

**Documentation Link**: https://aws.amazon.com/ec2/spot/

**4. What is the name of the AWS catalog of third-party vendors that are ready to install and bill in your AWS account?**


* [ ] AWS Reserved Instances Marketplace
* [ ] AWS Infiniband 
* [ ] AWS Marketplace
* [ ] AWS Builder Lab

**Correct answer:**
* [x] AWS Marketplace

**Explaination**: AWS Marketplace is the catalog of third-party vendors that provide software and services on the AWS platform. These offerings are ready to install in AWS accounts and are billed using the account's payment method. Reserved Instances Marketplace is specifically for purchasing reserved capacity. AWS Infiniband and AWS Builder Lab do not exist.

**Documentation Link**: https://aws.amazon.com/marketplace/

**5. What is consolidated billing in AWS?**


* [ ] A feature that allows multiple AWS accounts to be billed together as a single entity
* [ ] A feature that bills different services separately for each AWS account
* [ ] A feature that allows billing in different regions and currencies
* [ ] A feature that applies a discount to the billing of a single AWS account

**Correct answer:**
* [x] A feature that allows multiple AWS accounts to be billed together as a single entity

**Explaination**: Consolidated billing is a feature of AWS Organizations that allows multiple AWS accounts to be billed together as a single entity, simplifying payment and cost allocation for organizations with multiple accounts. This feature can help organizations gain a better overall view of their AWS usage and reduce administrative overhead. 

A feature that bills different services separately for each AWS account is incorrect as consolidated billing bills the same services together for all accounts. 

A feature that allows billing in different regions and currencies is incorrect as consolidated billing does not allow billing in different regions and currencies. 

A feature that applies a discount to the billing of a single AWS account is incorrect as consolidated billing does not apply a discount to a single AWS account's billing.


**Documentation Link**: https://aws.amazon.com/organizations/features/consolidated-billing/

**6. Which AWS billing tool would you recommend if you wanted the most detailed report for a third-party analysis?**


* [ ] Billing Dashboard
* [ ] Cost Explorer
* [ ] The Cost and Usage Report
* [ ] AWS Budgets 

**Correct answer:**
* [x] The Cost and Usage Report

**Explaination**: The Cost and Usage Report provides the most granular data for billing analysis. It includes hourly usage data for every AWS resource and service. This report can be generated on-demand or scheduled for delivery to an S3 bucket. The data can then be analyzed by third-party tools for cost allocation, chargeback, budgeting, and optimization.
The Billing Dashboard and Cost Explorer provide aggregated views of charges which may lack some detail. AWS Budgets is a tool for setting custom budgets but does not provide billing data directly.
Reference: https://aws.amazon.com/aws-cost-management/aws-cost-and-usage-report/ 


**Documentation Link**: https://docs.aws.amazon.com/cur/latest/userguide/what-is-cur.html

**7. Which of the following is the correct way to use tags for cost allocation on each execution on AWS?**


* [ ] Tags can only be used for cost allocation on EC2 instances.
* [ ] Tags are used to allocate costs to different AWS accounts.
* [ ] Tags should be used for all resources and services that you want to allocate costs.
* [ ] Tags are not used for cost allocation on AWS.

**Correct answer:**
* [x] Tags should be used for all resources and services that you want to allocate costs.

**Explaination**: Tags should be used for all resources and services that you want to allocate costs. By using tags, you can easily categorize and group resources to analyze your spending on AWS. You can tag each resource with a key-value pair, where the key is a category and the value is the specific label for that category. This allows you to track and analyze your costs by different categories and labels.

**Documentation Link**: https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/allocation-using-tags.html

**8. What is the behavior of Reserved Instances in AWS Organizations?**


* [ ] The Reserved Instances are automatically merged across all accounts in the organization.
* [ ] The Reserved Instances are shared between all accounts in the organization.
* [ ] The Reserved Instances are only applied to the account in which they were purchased.
* [ ] The Reserved Instances are deleted after each execution.

**Correct answer:**
* [x] The Reserved Instances are shared between all accounts in the organization.

**Explaination**: Reserved Instances in AWS Organizations provide capacity reservation and significant discounts on your Amazon EC2 usage. These benefits are applicable across all the accounts in your AWS Organization. The billing benefit applies to usage in all of your accounts. So, if you have Reserved Instances in your master account, the reservations automatically apply to usage across all member accounts

**Documentation Link**: https://repost.aws/knowledge-center/ec2-ri-consolidated-billing

**9. In which scenario is On-Demand Instance pricing the best fit for AWS customers?**


* [ ]  For predictable workloads with a set schedule
* [ ] For workloads with fluctuating demand and unpredictable spikes
* [ ] For workloads that require a dedicated instance for extended periods of time
* [ ] For workloads with a fixed budget and limited resources

**Correct answer:**
* [x] For workloads with fluctuating demand and unpredictable spikes

**Explaination**: On-Demand Instance pricing is ideal for workloads with fluctuating demand and unpredictable spikes, as customers have the flexibility to pay for the time they use without any upfront costs or long-term commitments. This allows customers to scale capacity up or down as needed, without incurring excessive costs. In contrast, reserved instances or savings plan options might be more appropriate in case of predictable workloads, and Spot instances might be used when customers have a flexible budget and are willing to accept a higher level of instance failure rate due to interruptions or terminations with two-minute notification.

**Documentation Link**: https://aws.amazon.com/ec2/pricing/on-demand/ 

**10. Which AWS Support plan gives you access to a dedicated Technical Account Manager for incident management and single-point escalation?**


* [ ] Developer
* [ ] Business
* [ ] Enterprise On-Ramp
* [ ] Enterprise

**Correct answer:**
* [x] Enterprise

**Explaination**: The AWS Enterprise Support plan provides access to a Technical Account Manager (TAM) who acts as a single point of contact for incident management and escalation. The TAM has a deep understanding of your workloads and environment and helps coordinate AWS resources to resolve critical issues. The Developer and Business Support plans do not include a TAM. The Enterprise On-Ramp plan includes limited access to a TAM for advisory purposes but not for incident response.

**Documentation Link**: https://aws.amazon.com/premiumsupport/features/#Technical_Account_Manager

**11. Which container orchestration framework is used by AWS Elastic Kubernetes Service (EKS)?**


* [ ] Docker Compose
* [ ] Apache Mesos
* [ ] Kubernetes
* [ ] AWS Fargate

**Correct answer:**
* [x] Kubernetes

**Explaination**: AWS Elastic Kubernetes Service (EKS) uses Kubernetes as the container orchestration framework. Kubernetes is an open-source platform for automating the deployment, scaling, and management of containerized applications. EKS manages the Kubernetes control plane for you, allowing you to focus on deploying and managing your applications. 

**Documentation Link**: https://aws.amazon.com/eks/

**12. Which of the following options is a benefit of using Dedicated Instances under Reserved Instances pricing?**


* [ ] Reduced hourly charges compared to On-Demand Instances
* [ ] The ability to use unused reserved capacity for Spot Instances
* [ ] Higher discounts for longer commitments
* [ ] No upfront payments required

**Correct answer:**
* [x] Higher discounts for longer commitments

**Explaination**: Reserved Instances under Dedicated Instances pricing offer a higher discount for longer commitments than for shorter ones. The longer the commitment, the higher the discount offered for the hourly rate. This results in significant savings in costs in the long run for those who can commit to usage over an extended period. On the other hand, Spot Instances offer significant cost savings for workloads that are flexible in terms of time and can tolerate interruptions. Spot Instances pricing is market-driven and fluctuates based on supply and demand. However, they do not come with any commitment or guarantee of capacity. 

**Documentation Link**: https://aws.amazon.com/ec2/pricing/reserved-instances/

**13. What is the Default VPC in AWS?**


* [ ] A pre-configured VPC that is automatically created in each AWS region
* [ ] A VPC that provides advanced networking features for high-performance applications
* [ ] A VPC that is specifically designed for cross-region communication
* [ ] A virtual network that spans across multiple AWS accounts

**Correct answer:**
* [x] A pre-configured VPC that is automatically created in each AWS region

**Explaination**: The correct answer is "A pre-configured VPC that is automatically created in each AWS region". The Virtual Default VPC is a default VPC that is automatically created for every AWS account in each region. It provides a default networking environment for EC2 instances and other AWS resources.

**Documentation Link**: https://docs.aws.amazon.com/vpc/latest/userguide/default-vpc.html

**14. Which of the following statements is true?**


* [ ] Both security groups and Network ACLs are stateful firewalls
* [ ] Both security groups and Network ACLs are stateless firewalls
* [ ] Network ACLs are stateful firewalls and security groups are stateless
* [ ] Security groups are stateful firewalls and Network ACLs are stateless

**Correct answer:**
* [x] Security groups are stateful firewalls and Network ACLs are stateless

**Explaination**: Network ACLs (Network Access Control Lists) and security groups are both used for controlling inbound and outbound traffic in AWS, but they function differently. Network ACLs are stateless firewalls at the subnet level, meaning they keep track of the state of each packet and allow the return traffic automatically. They operate at the network layer (Layer 3) of the OSI model and evaluate rules in sequential order. Security groups, on the other hand, are stateful firewalls that control traffic at the instance level. They operate at the instance level (Layer 4 of the OSI model) and evaluate rules independently for inbound and outbound traffic.

**Documentation Link**: https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Security.html

**15. What is the purpose of Auto Scaling in AWS?**


* [ ] To automatically deploy a new DynamoDB instance when the existing instance is terminated
* [ ] To automatically add or remove compute resources based on traffic demand
* [ ] To ensure that the data stored in S3 is always accessible
* [ ] To ensure that all network traffic within the VPC is encrypted

**Correct answer:**
* [x] To automatically add or remove compute resources based on traffic demand

**Explaination**: Auto Scaling in AWS allows for automatic adjustment of compute resources based on demand. This ensures that resources are available to handle traffic spikes and reduces resource waste during periods of low traffic. 

“...a new DynamoDB instance..”  is incorrect because DynamoDB doesn’t use instances…

“To ensure that the data stored in S3… is incorrect because Auto Scaling is not related to S3; it is specific to compute resources such as EC2 instances, ECS tasks, and Spot Fleets. 

“To ensure that all network traffic…” is incorrect because network traffic encryption is achieved through other services such as AWS VPN or AWS Direct Connect.


**Documentation Link**: https://aws.amazon.com/autoscaling/

**16. Which of the following statements accurately describes a public subnet in Amazon VPC?**


* [ ] A public subnet is accessible from the internet.
* [ ] A public subnet is a subnet with a public IP assigned to every resource.
* [ ] A public subnet is a subnet with an internet gateway attached.
* [ ] A public subnet is a subnet with a NAT gateway for outbound traffic.

**Correct answer:**
* [x] A public subnet is a subnet with an internet gateway attached.

**Explaination**: The correct answer is "A public subnet is a subnet with an internet gateway attached". An internet gateway allows resources within a public subnet to communicate with the internet, making the subnet accessible from the internet.

A public subnet is accessible from the internet.----) While public subnets are typically accessible from the internet, this statement is not comprehensive enough to describe a public subnet accurately.

A public subnet is a subnet with a public IP assigned to every resource—-) Not every resource in a public subnet is assigned a public IP address. Public IP addresses are assigned on a per-resource basis.

A public subnet is a subnet with a NAT gateway for outbound traffic–) A NAT gateway is used for outbound internet access from private subnets, not public subnets.


**Documentation Link**: https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Scenario2.html

**17. Which AWS Snow device is used for large-scale data migrations (petabytes)?**


* [ ] AWS Snowcone
* [ ] AWS Snowball
* [ ] AWS Snowmobile
* [ ] AWS Glacier

**Correct answer:**
* [x] AWS Snowmobile

**Explaination**: AWS Snowmobile is a secure data transfer device that can handle (petabytes) exabyte-scale data migrations to AWS. It is a ruggedized shipping container pulled by a semi-trailer truck, designed to transfer extremely large datasets in a secure and efficient manner.

**Documentation Link**: https://aws.amazon.com/snowmobile/

**18. Which of the following is true about AWS Trusted Advisor?**


* [ ] It is a tool that helps customers optimize cost, security, performance, and fault tolerance through best practices.
* [ ] It is a tool that provides detailed reports on the security of your AWS environment, including recommendations for remediation.
* [ ] It is a tool that allows you to simulate traffic to your Amazon EC2 instances to assess how well they can handle the expected load.
* [ ] It is a tool that provides guidance on how to migrate to AWS, including strategies for analyzing your existing environment and designing your new AWS environment.

**Correct answer:**
* [x] It is a tool that helps customers optimize cost, security, performance, and fault tolerance through best practices.

**Explaination**: AWS Trusted Advisor is a tool that provides real-time guidance to help you optimize performance, improve security, and reduce costs on your AWS infrastructure. It analyzes your AWS environment against best practices and recommends optimizations across five categories: cost optimization, performance improvement, security hardening, fault tolerance, and service limits. 

The other options describe other AWS tools, such as AWS Security Hub, Amazon EC2 Instance Connect, and AWS Migration Hub, which are not focused on providing best practice guidance and optimization recommendations. 


**Documentation Link**: https://aws.amazon.com/trustedadvisor/ 

**19. What is the primary purpose of a VPN in AWS?**


* [ ] To provide users with access to the AWS Management Console
* [ ] To encrypt traffic between an on-premises network and an Amazon VPC
* [ ] To enable cross-region communication within an Amazon VPC
* [ ] To resize EC2 instances based on user demand

**Correct answer:**
* [x] To encrypt traffic between an on-premises network and an Amazon VPC

**Explaination**: A VPN allows users to securely connect an on-premises network with an Amazon VPC, encrypting all traffic that passes through. This helps protect sensitive data and communication from being intercepted or compromised. 

“...access to the AWS Management Console” is incorrect as the AWS Management Console does not require a VPN connection, but can be accessed through IAM user accounts. 

“To enable cross-region communication…..” is incorrect as communication within an Amazon VPC does not require a VPN connection either. 

“To resize EC2 instances based on user demand” is incorrect as resizing EC2 instances is a separate function and does not involve VPN connections.


**Documentation Link**: https://docs.aws.amazon.com/vpc/latest/userguide/what-is-amazon-vpc.html#vpn-connections

**20. What is an example of a rehost migration strategy in AWS?**


* [ ] Refactoring the application code to optimize performance
* [ ] Migrating the application to a different database engine
* [ ] Replicating the application environment on AWS
* [ ] Redesigning the application architecture for scalability

**Correct answer:**
* [x] Replicating the application environment on AWS

**Explaination**: The correct answer is "Replicating the application environment on AWS". Rehost migration strategy, also known as lift-and-shift, involves replicating the application environment as-is on AWS without making any changes to the application code. It is a common approach to quickly migrate applications to AWS with minimal modifications.

**Documentation Link**: https://docs.aws.amazon.com/prescriptive-guidance/latest/large-migration-guide/migration-strategies.html

**21. Which of the following scenarios/best fits for using Spot Instance pricing for EC2 applications and workloads?**


* [ ] Workloads that can tolerate interruption, have flexible start or completion times, and require large amounts of compute resources at a lower cost.
* [ ] Workloads that require a consistently high level of compute resources and need to run for an extended period without the risk of being interrupted.
* [ ] Workloads that are predictable and consistent, require a fixed set of compute resources, and need to run for a predetermined period.
* [ ] Workloads that require a high throughput, low-latency network connection, and need to run for a short duration.

**Correct answer:**
* [x] Workloads that can tolerate interruption, have flexible start or completion times, and require large amounts of compute resources at a lower cost.

**Explaination**: Spot instances are spare EC2 instances that are available at a discounted rate compared to On-Demand instances. They allow you to optimize your costs on workloads that can tolerate some level of interruption. Spot instances are ideal for workloads with flexible starting and stopping times, and high compute requirements, such as big data analytics, containerized applications, CI/CD, and web servers. In contrast, for workloads that require consistent and long-running compute capacity, On-Demand or Reserved Instances might be more economical. Therefore, the correct answer is “Workloads that can tolerate interruption…”.

**Documentation Link**: https://aws.amazon.com/ec2/spot/

**22. Which of the following is true about the different compute families on AWS?**


* [ ] Instances within a compute family have similar compute, memory, and network capabilities but with different capacities.
* [ ] Instances across different compute families have identical compute, memory, and network capabilities, but with the same capacities.
* [ ] There is only one compute family available on AWS.
* [ ] All instances within a compute family are optimized for the same size of workload.

**Correct answer:**
* [x] Instances within a compute family have similar compute, memory, and network capabilities but with different capacities.

**Explaination**: The different compute families on AWS are designed to optimally support various types of workloads. Instances within a compute family share similar compute, memory, and network capabilities, while instances across different compute families have different specifications. For example, the M family is optimized for general-purpose workloads, while the C family is optimized for compute-intensive workloads. Similarly, the R family is optimized for memory-intensive workloads, and the X family is optimized for high-performance computing. Therefore, similar capabilities is the correct answer.

“...identical compute, memory, and network capabilities…”  is incorrect because there are different compute families on AWS with different specifications.

“...only one compute family available on AWS” is incorrect because there are multiple compute families available on AWS.

“...optimized for the same size of workload” is incorrect because instances within a compute family are optimized for different-sized workloads, not the same workload.


**Documentation Link**: https://aws.amazon.com/ec2/instance-types/

**23. What is the purpose of AWS Local Zones in terms of workload placement?**


* [ ] To distribute workloads across multiple AWS regions for redundancy
* [ ] To ensure high availability and fault tolerance for applications
* [ ] To provide low-latency access to AWS services in specific geographic areas
* [ ] To establish secure connectivity between on-premises data centers and the AWS Cloud

**Correct answer:**
* [x] To provide low-latency access to AWS services in specific geographic areas

**Explaination**: The correct answer is "To provide low-latency access to AWS services in specific geographic areas. AWS Local Zones are designed to bring AWS services closer to end-users in specific geographic locations". By placing workloads in Local Zones, you can achieve low-latency access to AWS services, improving application performance and user experience.
 
While workload distribution across multiple regions is important for redundancy, it is not the primary purpose of Local Zones.
 
High availability and fault tolerance can be achieved using other AWS services, such as Availability Zones and multi-region architectures, but it is not the primary purpose of Local Zones.
 
Secure connectivity between on-premises data centers and the AWS Cloud is facilitated by services like AWS Direct Connect or VPN, not specifically by Local Zones.


**Documentation Link**: https://docs.aws.amazon.com/local-zones/latest/ug/what-is-aws-local-zones.html

**24. What is the maximum object or file size for an S3 object?**


* [ ] 2 GB
* [ ] 5 TB
* [ ] 10 GB
* [ ] 1 TB

**Correct answer:**
* [x] 5 TB

**Explaination**: Amazon S3 allows objects of up to 5 TB to be uploaded using a single PUT or POST request. However, multipart upload is recommended for objects larger than 100 MB.

**Documentation Link**: https://docs.aws.amazon.com/AmazonS3/latest/dev/UploadingObjects.html 

**25. What is the purpose of load balancers in AWS?**


* [ ] To manage resources and balance traffic to ensure high availability and fault tolerance
* [ ] To encrypt data in transit between the client and the server
* [ ] To store and manage data in multiple availability zones for high durability
* [ ] To monitor and collect metrics for resource utilization

**Correct answer:**
* [x] To manage resources and balance traffic to ensure high availability and fault tolerance

**Explaination**: Load balancers are a critical component of any highly available and fault-tolerant architecture on AWS. They help manage resources and balance traffic across multiple instances to ensure that the application can handle a significant amount of traffic and can dynamically add or remove servers to adjust to varying traffic patterns. Load balancers act as entry points for all incoming traffic to your application, providing a clean slate to examine network traffic before forwarding it to the server instances. They also automatically distribute incoming traffic to multiple targets, such as EC2 instances, and monitor the health of these targets to ensure consistent and reliable application performance.
“To encrypt data..” is incorrect because load balancers do not encrypt data in transit between the client and the server. This is the responsibility of other services such as SSL/TLS certificates.

“To store and manage data…” is incorrect because load balancers do not store or manage data across multiple availability zones. It is the responsibility of storage services such as Amazon S3 to provide high durability for data stored.

“To monitor and collect metrics…” is incorrect because monitoring and collecting metrics for resource utilization is the responsibility of other services such as Amazon CloudWatch. 


**Documentation Link**: https://aws.amazon.com/elasticloadbalancing/what-is-elastic-load-balancing/

**26. What is the purpose of Amazon CloudFront's Edge locations?**


* [ ] To cache frequently accessed content closer to end-users
* [ ] To serve as data centers for AWS services
* [ ] To provide virtual private networking (VPN) between AWS services
* [ ]  To manage DNS records for AWS resources

**Correct answer:**
* [x] To cache frequently accessed content closer to end-users

**Explaination**: Amazon CloudFront's Edge locations are used to cache frequently accessed content closer to end-users, reducing latency and delivering content quickly. This helps improve the performance of applications and ensures that users have a seamless experience. While AWS services may use Edge locations for additional functionality, such as AWS Lambda@Edge, their primary purpose is to serve as cache locations for CloudFront. 

“...as data centers for AWS services” is incorrect because edge locations are not full data centers and only work for Edge services in AWS.

“...virtual private networking..” is incorrect because it refers to Amazon VPC, which is a separate networking service. 

“To manage DNS records…” is incorrect because DNS management is handled by the Amazon Route 53 service.


**Documentation Link**: https://aws.amazon.com/cloudfront/edge-locations/

**27. Which AWS service is responsible for automatically monitoring the resources and applications that run on AWS in real time?**


* [ ] Amazon CloudFront
* [ ] Amazon CloudWatch
* [ ] AWS Config
* [ ] AWS CloudTrail

**Correct answer:**
* [x] Amazon CloudWatch

**Explaination**: Amazon CloudWatch is an AWS service that provides monitoring for AWS resources and the applications running on them in real time. It collects and tracks metrics, collects and stores log files, and provides alarms that can trigger actions and send notifications. It helps you gain system-wide visibility into the health and performance of your resources, applications, and services that run on AWS.

**Documentation Link**: https://aws.amazon.com/cloudwatch/

**28. Which AWS service provides automated event-driven computing, serverless architecture, and automatically scales applications?**


* [ ] Amazon S3
* [ ] Amazon EC2
* [ ] AWS Lambda
* [ ] Amazon RDS

**Correct answer:**
* [x] AWS Lambda

**Explaination**: AWS Lambda is a serverless computing service that enables users to run code without provisioning or managing servers. It automatically scales your application in response to incoming requests or events and performs all the administration of the underlying compute resources for you. Amazon S3 is a cloud storage service, EC2 is an elastic compute cloud for virtual machine provisioning, and Amazon RDS is a managed relational database service.

**Documentation Link**: https://aws.amazon.com/lambda/

**29. What is the main difference between installing databases on Amazon EC2 and using AWS managed databases?**


* [ ] Managed databases provide automatic software patching and upgrades, while databases installed on EC2 require manual updates.
* [ ] Databases installed on EC2 offer better performance and scalability, while managed databases have limited capacity.
* [ ] Managed databases allow for greater customization and control over database configuration, while installing databases on EC2 is a one-size-fits-all solution.
* [ ] Installing databases on EC2 is less expensive than using managed databases, which require additional fees for maintenance and support

**Correct answer:**
* [x] Managed databases provide automatic software patching and upgrades, while databases installed on EC2 require manual updates.

**Explaination**: When running databases on Amazon EC2, users are responsible for managing the software, including patching and maintenance. In contrast, AWS managed databases, like Amazon RDS and Amazon Aurora, automatically apply patches and upgrades to the underlying infrastructure and software. This saves time and effort for database administrators and ensures that the databases are always running on the latest and most secure software. 

“Databases installed on EC2…” is incorrect, as managed databases have considerable performance and scalability benefits such as autoscaling, read and write replicas, and built-in failover capability. 

“Managed databases allow for greater customization…” is incorrect because managed databases offer a high degree of flexibility and customization, allowing users to choose specific instance types, storage options, and configurations. 

“Installing Databases on EC2…” is incorrect because while AWS managed databases do have an additional cost compared to using EC2, the cost is often justified by the reduction in management and maintenance tasks. 


**Documentation Link**: https://aws.amazon.com/relational-database/managed-options/

**30. How is elasticity achieved through Auto Scaling in AWS?**


* [ ] By automatically increasing the number of instances to meet demand and reducing the number of instances when demand decreases
* [ ] By manually adjusting the size and capacity of instances to meet demand
* [ ] By ensuring that each instance has a fixed amount of resources allocated to it at all times
* [ ] By limiting the number of instances that can be launched to meet demand

**Correct answer:**
* [x] By automatically increasing the number of instances to meet demand and reducing the number of instances when demand decreases

**Explaination**: Elasticity refers to the ability of an application or infrastructure to automatically scale up or down in response to changes in demand. Auto Scaling enables users to automatically adjust the number of instances in an application, based on the demand for the application. This allows for the efficient use of resources and cost savings. Auto Scaling works by monitoring a predefined metric such as CPU utilization or network traffic and then launching or terminating instances according to predefined rules. As demand increases, Auto Scaling automatically launches additional instances, and as demand decreases, it terminates instances, ensuring that the infrastructure scales seamlessly based on demand.

**Documentation Link**: https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html

**31. Which of the following statements is true regarding Availability Zones in AWS?**


* [ ] Availability Zones share a single point of failure.
* [ ] Availability Zones are simply multiple instances running on the same server.
* [ ] Availability Zones are physically isolated from each other within a single region.
* [ ] Availability Zones are only available for use in specific regions.

**Correct answer:**
* [x] Availability Zones are physically isolated from each other within a single region.

**Explaination**: Availability Zones are separate data centers that are interconnected through high-speed communication links within a single region. They are designed to provide virtually independent cloud computing resources that are distinct from each other in terms of power, network, and physical location. This allows AWS to minimize the impact of failures or disasters, ensuring high availability and reliability for customer applications and workloads. 

“...share a single point of failure.” is incorrect because, by design, Availability Zones don't share a single point of failure. “...running on the same server.” is incorrect because Availability Zones are separate data centers, not instances running on the same server. “...for use in specific regions.” is incorrect because AWS provides Availability Zones in several regions across the globe.


**Documentation Link**: https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/RegionsAndAZs.html 

**32. Which AWS managed service enables you to run containers on a fully managed cluster of EC2 instances and provides a scalable and secure way to deploy applications?**


* [ ] Amazon RDS
* [ ] Amazon ECS
* [ ] Amazon EKS
* [ ] Amazon DynamoDB

**Correct answer:**
* [x] Amazon ECS

**Explaination**: Amazon Elastic Container Service (Amazon ECS) is an AWS managed service that enables you to run containers on a fully managed cluster of EC2 instances. It provides a scalable and secure way to deploy applications, allowing you to easily run and manage Docker containers on a cluster of Amazon Elastic Compute Cloud (Amazon EC2) instances. Amazon RDS is a managed database service; Amazon EKS is a managed Kubernetes service that makes it easy to deploy, manage, and scale containerized applications; and Amazon DynamoDB is a fully managed NoSQL database service. Therefore, option B is the correct answer.

**Documentation Link**: https://aws.amazon.com/ecs/ 

**33. What is the primary purpose of Amazon Macie?**


* [ ] Detect and analyze security events and logs
* [ ] Monitor and protect web applications from common security vulnerabilities
* [ ] Automatically encrypt and decrypt data stored in Amazon S3
* [ ] Discover, classify, and protect sensitive data stored in AWS

**Correct answer:**
* [x] Discover, classify, and protect sensitive data stored in AWS

**Explaination**: The correct answer is "Discover, classify, and protect sensitive data stored in AWS". Amazon Macie is a security service that uses machine learning to automatically discover, classify, and protect sensitive data stored in AWS. It helps identify sensitive data such as personally identifiable information (PII) and intellectual property, and it provides recommendations for improving data protection and access controls.

**Documentation Link**: https://docs.aws.amazon.com/macie/latest/user/what-is-macie.html

**34. Which of the following statements accurately describes AWS Organizations?**


* [ ] AWS Organizations is used to manage user authentication and authorization in an AWS account.
* [ ] AWS Organizations is a service that helps monitor and secure your AWS resources.
* [ ] AWS Organizations is a service that enables centralized management of multiple AWS accounts.
* [ ] AWS Organizations is a service that provides built-in security controls for AWS services.

**Correct answer:**
* [x] AWS Organizations is a service that enables centralized management of multiple AWS accounts.

**Explaination**: The correct answer is AWS Organizations is a service that enables centralized management of multiple AWS accounts. With AWS Organizations, you can create and manage a hierarchy of AWS accounts and apply policies across them. It helps you simplify billing, manage access control, and implement security and compliance policies across your organization's AWS accounts.

**Documentation Link**: https://docs.aws.amazon.com/organizations/latest/userguide/orgs_introduction.html

**35. Which AWS IAM feature allows you to grant temporary access to AWS resources to users, without sharing long-term credentials?**


* [ ] IAM Roles
* [ ] IAM Policies
* [ ] IAM Users
* [ ] IAM Groups

**Correct answer:**
* [x] IAM Roles

**Explaination**: The correct answer is IAM Roles. IAM Roles in AWS IAM allow you to grant users or entities temporary access to AWS resources without sharing long-term credentials. IAM roles are typically used for applications running on AWS resources, like EC2 instances or Lambda functions, to access other AWS services securely. By assigning roles to these resources, you can define the specific permissions and duration of access for each role, providing temporary access without exposing long-term credentials.

**36. What is the best practice for protecting the root account in AWS?**


* [ ] Use the root account for daily tasks to ensure full access to all resources.
* [ ] Share the root account credentials with other users to simplify management.
* [ ] Enable multi-factor authentication (MFA) for the root account and create individual IAM users with appropriate permissions.
* [ ] Do not worry about protecting the root account as it cannot cause harm without user actions.

**Correct answer:**
* [x] Enable multi-factor authentication (MFA) for the root account and create individual IAM users with appropriate permissions.

**Explaination**: The root account in AWS has full access to all resources and is the most privileged account. For security reasons, it is recommended to protect the root account by enabling MFA and creating individual IAM users with appropriate permissions. This will reduce the risk of unauthorized access and accidental or intentional misuse of the account. Using the root account for daily tasks is not recommended as it can potentially lead to security breaches. Sharing the root account credentials with other users is also not recommended as it can lead to a lack of accountability and make it difficult to track actions taken by each user. Neglecting to protect the root account can lead to security breaches and loss of data.

**Documentation Link**: https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys

**37. Which of the following statements best describes the purpose of AWS Config?**


* [ ] AWS Config provides a fully managed DDoS protection service to safeguard applications running on AWS.
* [ ] AWS Config enables you to track user activity and API usage in your AWS account.
* [ ] AWS Config automatically provisions and manages SSL/TLS certificates for your AWS resources.
* [ ] AWS Config helps you assess, audit, and evaluate the configurations of your AWS resources.

**Correct answer:**
* [x] AWS Config helps you assess, audit, and evaluate the configurations of your AWS resources.

**Explaination**: The correct answer is "AWS Config helps you assess, audit, and evaluate the configurations of your AWS resources". AWS Config is a service that provides a detailed view of the configuration of AWS resources in your account. It continuously monitors and records configurations of supported resources and allows you to assess the overall compliance of your resources against desired configurations. With AWS Config, you can track changes, monitor compliance, troubleshoot resource configuration changes, and simplify security and compliance auditing.

**Documentation Link**: https://docs.aws.amazon.com/config/index.html

**38. Which AWS service is used to manage and control the encryption keys used to encrypt your data stored in AWS services?**


* [ ] AWS Key Management Service (KMS)
* [ ] AWS CloudTrail
* [ ] AWS Config
* [ ] Amazon GuardDuty

**Correct answer:**
* [x] AWS Key Management Service (KMS)

**Explaination**: The correct answer is AWS Key Management Service (KMS). AWS KMS is a managed service that allows you to create and control the encryption keys used to encrypt your data stored in AWS services. It provides a highly secure and scalable key management solution.

**Documentation Link**: https://docs.aws.amazon.com/kms/latest/developerguide/overview.html

**39. What is the purpose of AWS Security Hub?**


* [ ] A fully managed DDoS protection service
* [ ] A service that provides security monitoring and governance across multiple AWS accounts
* [ ] A security analytics service that uses machine learning to detect anomalous behavior
* [ ] A managed service for encryption key management

**Correct answer:**
* [x] A service that provides security monitoring and governance across multiple AWS accounts

**Explaination**: The correct answer is "A service that provides security monitoring and governance across multiple AWS accounts". AWS Security Hub is a comprehensive security service that helps you centrally manage security and compliance across multiple AWS accounts. It aggregates and prioritizes security findings from various AWS services, third-party products, and custom integrations, providing a unified view of security posture across your AWS environment.

**Documentation Link**: https://docs.aws.amazon.com/securityhub/index.html

**40. Which AWS resource provides a list of recognized available compliance controls for HIPAA and SOCs on each execution?**


* [ ] Amazon Glacier
* [ ] Amazon EC2
* [ ] AWS Artifact
* [ ] AWS Lambda

**Correct answer:**
* [x] AWS Artifact

**Explaination**: AWS Artifact is a portal that provides access to AWS compliance reports, so customers can track the security and compliance controls in place that are relevant to their data. One of the controls that AWS Artifact provides is the list of recognized available compliance controls for HIPAA, SOCs, and other regulations. Amazon Glacier, Amazon EC2, and AWS Lambda are all AWS services, but they do not provide access to the AWS Artifact portal or compliance reports.

**Documentation Link**: https://aws.amazon.com/artifact/ 

**41. Which of the following accurately describes AWS responsibilities on each execution within the Security and Compliance domain?**


* [ ] AWS is responsible for securing the customer's physical environment and ensuring regulatory compliance.
* [ ] AWS is responsible for securing the underlying infrastructure and physical data centers within the AWS cloud.
* [ ] AWS is responsible for ensuring that the customer's security and compliance policies are followed within the AWS cloud.
* [ ] AWS is responsible for all aspects of security and compliance within the AWS cloud.

**Correct answer:**
* [x] AWS is responsible for securing the underlying infrastructure and physical data centers within the AWS cloud.

**Explaination**: AWS provides a shared security model in which both AWS and the customer are responsible for different aspects of security and compliance. AWS is responsible for securing the underlying infrastructure and physical data centers within the AWS cloud, while the customer is responsible for securing the actual applications and data within the infrastructure. AWS provides numerous security features and services such as network firewalls, encryption, and identity and access management to assist the customer in securing their data and applications within AWS.

**Documentation Link**: https://aws.amazon.com/compliance/shared-responsibility-model/

**42. Which AWS service can aid in auditing and reporting on each action taken for  resources in your environment?**


* [ ] AWS CloudTrail
* [ ] AWS Inspector
* [ ] AWS Shield
* [ ] AWS Security Hub

**Correct answer:**
* [x] AWS CloudTrail

**Explaination**: AWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account. It enables you to log, monitor, and retain account activity related to actions across your AWS infrastructure, and it can aid in auditing and reporting on each execution of resources in your environment. Using CloudTrail, you can create detailed activity logs that can help you detect security threats and monitor your AWS account for undesirable activity. 

**Documentation Link**: https://aws.amazon.com/cloudtrail/ 

**43. Which AWS service is used for managing user access and permissions to AWS resources?**


* [ ] AWS CloudFormation
* [ ] AWS Identity and Access Management (IAM)
* [ ] Amazon S3
* [ ] Amazon EC2

**Correct answer:**
* [x] AWS Identity and Access Management (IAM)

**Explaination**: AWS Identity and Access Management (IAM) is used to manage user access and permissions to AWS resources. It allows you to create and manage users, groups, and roles, and to grant permissions to these entities to access or manage resources within your AWS account. CloudFormation is a service for deploying a set of AWS resources as a single unit called a stack. Amazon S3 is an object storage service, and Amazon EC2 is a virtual machine service.

**Documentation Link**: https://docs.aws.amazon.com/IAM/latest/UserGuide/introduction.html

**44. What is the AWS recommended approach for performing tasks that require the use of root accounts on each execution?**


* [ ] Use the root account for all tasks
* [ ] Create an IAM user with full administrative privileges
* [ ] Assign task-specific permissions to an IAM user
* [ ] Use Multi-Factor Authentication (MFA) to protect the root account

**Correct answer:**
* [x] Assign task-specific permissions to an IAM user

**Explaination**: AWS best practice recommends that root accounts should be used only for account and service management tasks and never for day-to-day operational tasks. Assigning task-specific permissions to an IAM user allows an administrator to perform specific actions in the AWS account without the need for root account access. This approach provides an additional layer of control to prevent unauthorized or accidental access to resources.

“Use the root account…” is incorrect because using the root account for all tasks is a security risk and violates AWS best practices for security and compliance. “Create an IAM user with full administrative privileges” is incorrect because creating an IAM user with full administrative privileges grants privileged access to all resources and services in an account, which increases the potential risk of unauthorized access or misuse. “Use Multi-Factor Authentication (MFA) to protect the root account” is incorrect because while using MFA is highly recommended for protecting root accounts, it does not address the need for delegating permissions and limiting access.

**Documentation Link**: https://aws.amazon.com/iam/features/manage-permissions/ 

**45. Which of the following best describes the purpose of IAM in AWS?**


* [ ] IAM is responsible for monitoring and auditing your AWS resources.
* [ ] IAM enables you to manage and control access to AWS services and resources.
* [ ] IAM provides real-time threat detection and protection for your AWS environment.
* [ ] IAM automates the encryption of data at rest for your AWS resources.

**Correct answer:**
* [x] IAM enables you to manage and control access to AWS services and resources.

**Explaination**: The correct answer is "IAM enables you to manage and control access to AWS services and resources". AWS Identity and Access Management (IAM) is a service that allows you to securely control access to AWS resources. It enables you to create and manage users, groups, and permissions, allowing you to grant or deny access to AWS services and resources based on the principle of least privilege. IAM helps you enforce security and compliance by providing centralized control over user identities and their permissions within your AWS account.

**Documentation Link**: https://aws.amazon.com/iam/

**46. Which of the following best describes Elastic File System (EFS) in AWS?**


* [ ] A relational database service
* [ ] A managed network file storage service
* [ ] A content delivery network
* [ ] A distributed caching service

**Correct answer:**
* [x] A managed network file storage service

**Explaination**: Elastic File System (EFS) in AWS is a fully managed network file storage service that provides scalable and shared file storage for use with EC2 instances. It allows multiple EC2 instances to access the same file system concurrently. For more information, refer to the official AWS documentation on EFS: Elastic File System (EFS)

**Documentation Link**: https://aws.amazon.com/efs/

**47. Which AWS service is used to manage and control the encryption keys used to encrypt your data stored in AWS services?**


* [ ] AWS Key Management Service (KMS)
* [ ] AWS CloudTrail
* [ ] AWS Config
* [ ] Amazon GuardDuty

**Correct answer:**
* [x] AWS Key Management Service (KMS)

**Explaination**: The correct answer is AWS Key Management Service (KMS). AWS KMS is a managed service that allows you to create and control the encryption keys used to encrypt your data stored in AWS services. It provides a highly secure and scalable key management solution.

**Documentation Link**: https://aws.amazon.com/kms/

**48. Which statement best describes the concept of least privileged access on each execution in AWS?**


* [ ] Providing users with access privileges beyond what they need
* [ ] Providing the same access privileges to all users
* [ ] Providing users with the minimum access privileges necessary to perform their job functions
* [ ] Providing users with unlimited access privileges

**Correct answer:**
* [x] Providing users with the minimum access privileges necessary to perform their job functions

**Explaination**: The concept of least privileged access on each execution in AWS means providing users with the minimum access privileges necessary to perform their job functions. This ensures that users have access only to the resources they need to perform their work and nothing more, reducing the risk of accidental or intentional misuse of resources. Providing users with access privileges beyond what they need can lead to security vulnerabilities. Providing the same access privileges to all users is not an effective approach since not all users require the same level of access. Providing users with unlimited access privileges is not a viable option since this could result in unauthorized access and misuse of resources.

**Documentation Link**: https://aws.amazon.com/blogs/security/techniques-for-writing-least-privilege-iam-policies/

**49. What is the best definition of scalability in the AWS context?**


* [ ] The ability of an application to handle more user requests by adding more servers
* [ ] The ability of an application to handle more user requests by improving the code
* [ ] The ability of an application to handle more user requests by reducing the amount of data stored
* [ ] The ability of an application to handle more user requests by restricting access to certain users

**Correct answer:**
* [x] The ability of an application to handle more user requests by adding more servers

**Explaination**: Scalability is the ability of an application or system to handle more user requests by adding more resources, such as servers, to the system. AWS provides various services, such as Auto Scaling, to automatically add or remove resources based on demand. “...adding more servers” and “...reducing the amount of data stored” may seem similar, but the latter is more related to data management and storage rather than handling user requests. “...improving the code” may also be a valid action to improve scalability, but it refers to improving code rather than adding resources to the system. “...restricting access to certain users” is not a scalable solution, as it does not increase resources to handle more users but rather restricts access.
Link: https://aws.amazon.com/what-is-scalability/ 

**Documentation Link**: https://wa.aws.amazon.com/wellarchitected/2020-07-02T19-33-23/wat.concept.scalability.en.html

**50. Which of the following is not a standard design principle or pattern used in AWS?**


* [ ] Design for Failure
* [ ] Decouple Components
* [ ] Implement Elasticity
* [ ] Think Serial

**Correct answer:**
* [x] Think Serial

**Explaination**: Common design principles in AWS include:
• Design for Failure - Expect failures and design systems to handle them.
• Decouple Components - Build components that are loosely coupled. 
• Implement Elasticity - Scale up and down based on demand. 
• Think Parallel - Perform operations in parallel to be efficient. 
"Think Serial" is not a standard AWS design principle. Other options, refer to core principles in the AWS Well-Architected Framework.


**Documentation Link**: https://docs.aws.amazon.com/wellarchitected/latest/framework/define-implement.html

**51. Fill in the missing word from this official definition of Cloud Computing: “Cloud Computing is the ________delivery of IT resources.”**


* [ ] On-demand
* [ ] Instant 
* [ ] Utility
* [ ] Rapid

**Correct answer:**
* [x] On-demand

**Explaination**: The full definition of cloud computing from the National Institute of Standards and Cloud Computing Concepts is: "Cloud computing is a model for enabling ubiquitous, convenient, on-demand network access to a shared pool of configurable computing resources." On-demand delivery of IT resources is a key characteristic of cloud computing. The options "instant,” “utility," and "rapid" do not match the official definition.

**Documentation Link**: https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-145.pdf

**52. What is the primary benefit of automation in the cloud?**


* [ ] Increases manual errors
* [ ] Decreases cost and time for deployment
* [ ] Increases human intervention
* [ ] Decreases scalability

**Correct answer:**
* [x] Decreases cost and time for deployment

**Explaination**: Automation in the cloud helps in reducing human errors, improving efficiency, and decreasing deployment time and cost, making the system scalable enough to handle changes. Thus, “Decreases cost and time for deployment” is correct. “Increases manual errors” is incorrect as Automation reduces manual errors rather than increasing them. “Increases human intervention” is incorrect as Automation reduces human intervention. “Decreases scalability” is incorrect as Automation increases scalability.  

**Documentation Link**: https://aws.amazon.com/what-is-cloud-computing/

**53. What is one of the key benefits of shifting technical resources to revenue-generating activities on AWS?**


* [ ] Increased infrastructure management tasks
* [ ] Decreased ability to scale and adapt to changing business needs
* [ ] Increased focus on innovation and revenue growth
* [ ] Decreased agility and time to market

**Correct answer:**
* [x] Increased focus on innovation and revenue growth

**Explaination**: One of the key benefits of shifting technical resources to revenue-generating activities on AWS is increased focus on innovation and revenue growth. By allowing AWS to manage the infrastructure, technical resources can be freed up to focus on areas that add value to the business. This can include developing new products and features, improving existing ones, and finding new ways to drive revenue. The other answers are all incorrect, as they each lead to negative consequences that are the opposite of the intended benefit.

**Documentation Link**: https://aws.amazon.com/cloud-concepts/

**54. What are the different types of cloud deployment models that can be used for each execution?**


* [ ] On-Premise, Public, and Hybrid Cloud
* [ ] Infrastructure as a Service (IaaS), Platform as a Service (PaaS), and Software as a Service (SaaS)
* [ ] Cloud Native, Multi-Cloud, and Hybrid Cloud
* [ ] Infrastructure as Code (IaC), Functions as a Service (FaaS), and Containers as a Service (CaaS)

**Correct answer:**
* [x] On-Premise, Public, and Hybrid Cloud

**Explaination**: The different types of cloud deployment models are On-Premise, Public, and Hybrid Cloud. In private cloud, services are provided on a private network, while in public cloud, services are provided over a public network. In hybrid cloud, the services are maintained on both private and public networks at the same time. Hence, option A is the correct answer. “Infrastructure as a Service (IaaS), Platform as a Service (PaaS)...” refers to the different types of cloud service models, not cloud deployment models. Cloud Native, Multi-Cloud, and Hybrid Cloud are cloud strategies and not deployment models. Infrastructure as Code (IaC), Functions as a Service (FaaS), and Containers as a Service (CaaS) , which are cloud application development services, not cloud deployment models.

**Documentation Link**: https://aws.amazon.com/types-of-cloud-computing/

**55. Which of the following is NOT a benefit of using Elasticity in AWS?**


* [ ] Cost savings
* [ ] Scalability
* [ ] High availability
* [ ] Security

**Correct answer:**
* [x] Security

**Explaination**: Elasticity refers to the ability of an IT infrastructure to quickly and easily scale resources up or down depending on demand. The benefits of Elasticity in AWS include cost savings, scalability, and high availability. However, Security is not a benefit of Elasticity. While Elasticity can help mitigate against failure, it is not the same as increasing security. 

**Documentation Link**: https://aws.amazon.com/elasticity/

**56. Which of the following statements accurately defines the role of operational expenses (OpEx) on each execution in AWS?**


* [ ] Operational expenses (OpEx) are the initial costs associated with setting up a workload on AWS.
* [ ] Operational expenses (OpEx) are the costs associated with running and maintaining a workload on AWS.
* [ ] Operational expenses (OpEx) are the costs associated with purchasing new hardware for a workload on AWS.
* [ ] Operational expenses (OpEx) are the costs associated with upgrading the software for a workload on AWS.

**Correct answer:**
* [x] Operational expenses (OpEx) are the costs associated with running and maintaining a workload on AWS.

**Explaination**: Operational expenses (OpEx) are the costs associated with running and maintaining a workload on AWS. These expenses include the costs of compute resources, storage, data transfer, and other operational costs. OpEx costs are often used to measure the ongoing costs of a workload or service on AWS. These costs can be managed and optimized using AWS tools such as AWS Cost Explorer, AWS Budgets, and AWS Trusted Advisor.
“...with setting up a workload on AWS.” is incorrect because it defines the initial costs associated with setting up a workload on AWS as OpEx costs; these costs are actually termed as capital expenses (CapEx). “...with purchasing new hardware for a  workload on AWS.” is incorrect as it talks about the costs associated with purchasing new hardware for a workload which is not related to OpEx costs. Finally, “...with upgrading the software for a workload on AWS.”  is incorrect because it talks about the costs associated with upgrading the software which is not related to OpEx costs.


**57. What is the primary benefit of using parallel computing in AWS?**


* [ ] Increased cost efficiency
* [ ] Improved security measures
* [ ] Faster data backups
* [ ] Enhanced performance

**Correct answer:**
* [x] Enhanced performance

**Explaination**: Parallel computing involves breaking down a large task into smaller tasks that can be executed simultaneously, thus reducing the overall processing time. The main benefit of using this approach in AWS is a significant increase in system performance. Other potential benefits include improved accuracy, reduced costs, and greater data processing capacity.

Increased cost efficiency is not the primary benefit of using parallel computing in AWS. While it may be a secondary or indirect benefit, the primary focus is on improved performance.

Improved security measures, has no direct relation to parallel computing. While security measures are important to consider when implementing any AWS solution, they are not directly related to the use of parallel computing.

Faster data backups, may be a potential benefit of parallel computing since it can accelerate data processing time, but it is not the primary benefit.


**Documentation Link**: https://docs.aws.amazon.com/wellarchitected/latest/framework/welcome.html?pg=cloudessentials

**58. What is the primary purpose of designing for failure in AWS?**


* [ ] To reduce costs by minimizing the need for backups and redundancies
* [ ] To ensure that system downtime and data loss are minimized if a component or service fails
* [ ] To increase system performance and improve overall scalability
* [ ] To reduce the risk of security breaches and protect sensitive data

**Correct answer:**
* [x] To ensure that system downtime and data loss are minimized if a component or service fails

**Explaination**: By designing for failure in AWS, you can minimize the impact of any failures or issues within your system. This is achieved through the use of features such as redundancy, backups, and fault tolerance, which help ensure that your system remains operational even if one or more components fail. This is particularly important in AWS, where complex systems can involve multiple services and components that could be affected by isolated failures. By designing for failure, you can reduce downtime, prevent data loss, and maintain the availability and reliability of your system.

**Documentation Link**: https://aws.amazon.com/architecture/well-architected/ 

**59. Which of the following is a benefit of using Traditional IT over Cloud Computing?**


* [ ] Offload Responsibility to AWS 
* [ ] Increase Focus on what is Core to your business
* [ ] Go Global with global data centers in minutes
* [ ] Increase Control over Infrastructure 

**Correct answer:**
* [x] Increase Control over Infrastructure 

**Explaination**: A benefit of traditional IT compared to cloud computing is having more direct control over the infrastructure. With on-premises data centers, organizations have full control and responsibility over hardware, software, networking, security, and operations. Cloud computing provides less direct control as the service provider manages much of the infrastructure. The remaining options are benefits of cloud computing, not traditional IT.

**Documentation Link**: https://aws.amazon.com/what-is-cloud-computing/ 

**60. Which of the following is a major advantage of migrating to the cloud in terms of software licensing costs?**


* [ ] The cloud reduces cost by eliminating the need for software licenses altogether.
* [ ] Cloud providers include the cost of software licenses in their subscription fees, resulting in lower costs for users.
* [ ] The cloud offers pay-as-you-go pricing models that allow users to pay only for software licenses they use.
* [ ] Migrating to the cloud increases the cost of software licensing due to additional fees charged by cloud providers.

**Correct answer:**
* [x] The cloud offers pay-as-you-go pricing models that allow users to pay only for software licenses they use.

**Explaination**: Moving to the cloud can drastically reduce software licensing costs by offering flexible pricing models that enable users to pay only for what they use. The pay-as-you-go pricing model allows users to subscribe to software licenses on a monthly or hourly basis instead of purchasing expensive perpetual licenses upfront. This translates into significant cost savings as cloud customers have the flexibility to scale their usage up or down as needed.

“Eliminating the need” is incorrect because not all software licensing arrangements are eliminated when moving to the cloud. “Include the cost of software”  is incorrect because cloud providers do not offer software licenses for free. It inflates the costs to the end user. . “Migrating to the cloud”  is incorrect because most cloud providers do not charge additional licensing fees for enterprise software.  The customer is already paying for those


**Documentation Link**: https://aws.amazon.com/getting-started/cloud-essentials/

**61. What Cloud Deployment model is sometimes called “Private Cloud”?**


* [ ] Cloud 
* [ ] On-Premise 
* [ ] Hybrid 
* [ ] Community Cloud 

**Correct answer:**
* [x] On-Premise 

**Explaination**: An on-premises or "on-prem" deployment model refers to a private data center. It is sometimes called a "private cloud." This is infrastructure dedicated to a single organization. The cloud deployment model refers to the public cloud. A community cloud is a public cloud for a dedicated community. A hybrid cloud combines public cloud and on-premises infrastructure.

**Documentation Link**: https://aws.amazon.com/what-is-cloud-computing/ 

**62. What is the definition of economy of scale in AWS?**


* [ ] The increased cost of operating additional resources as production grows
* [ ] The ability to decrease cost per unit of computing power as the size of an operation increases
* [ ] The cost of computing power remains constant regardless of the size of the operation
* [ ] The increased cost of hosting larger quantities of data for an operation

**Correct answer:**
* [x] The ability to decrease cost per unit of computing power as the size of an operation increases

**Explaination**: Economy of scale is a concept in which the average cost of production decreases as the volume of output increases. In AWS, this concept refers to the ability to decrease cost per unit of computing power as the size of an operation increases. This is made possible by the fact that AWS operates on a pay-per-use model, which means businesses only pay for the resources they use. As more resources are used, the overall cost decreases. Therefore, “The ability to decrease cost per unit…” is the correct answer.

“The increased cost of operating…” is incorrect because it suggests that the cost of operating additional resources increases, which is not the case with economy of scale. “The cost of computing power…” is incorrect because the cost of computing power is not constant and actually decreases with the increase in an operation's size. Lastly, “the increased cost of hosting larger quantities…” is incorrect because hosting larger quantities of data does not directly relate to economy of scale in AWS.


**Documentation Link**: https://docs.aws.amazon.com/whitepapers/latest/aws-overview/six-advantages-of-cloud-computing.html

**63. Clients who want to access AWS will use this kind of architectural model to access AWS.**


* [ ] The Peer-to-Peer model 
* [ ] The Client-Server model 
* [ ] The Asynchronous model
* [ ] The Remote Procedure Call model

**Correct answer:**
* [x] The Client-Server model 

**Explaination**: To access AWS services, clients interact with AWS servers using APIs and web interfaces. This follows a client-server architecture where clients request information or resources from centralized servers. Peer-to-peer architectures involve direct communication between network nodes. Asynchronous and Remote Procedure Calls are communication models but not full architectural models themselves.

**Documentation Link**: https://www.educba.com/client-server-model-vs-peer-to-peer-model/

**64. Name the AWS benefit that allows you to easily fix either over-sizing of an application server or under-sizing of an application server?**


* [ ] Trade Upfront Expense for Variable Expense
* [ ] Increase Provisioning Speed and Business Agility
* [ ] Go Global in Minutes
* [ ] Stop Guessing Capacity

**Correct answer:**
* [x] Stop Guessing Capacity

**Explaination**: The "Stop Guessing Capacity" benefit of AWS means the ability to scale computing resources up and down based on demand. This allows quickly fixing issues with over-provisioning or under-provisioning resources. The remaining options refer to other benefits of AWS but not the ability to adjust capacity easily.

**Documentation Link**: https://aws.amazon.com/what-is-cloud-computing/

**65. AWS has a framework for thinking about Architecture Principles and how they apply to AWS. What is the name of this framework?**


* [ ] AWS Cloud Adoption Framework
* [ ] AWS Well-Architected Framework
* [ ] AWS White Paper Framework  
* [ ] AWS Serverless Framework

**Correct answer:**
* [x] AWS Well-Architected Framework

**Explaination**: The AWS Well-Architected Framework provides guidelines and best practices for building secure, high-performing, resilient, and efficient infrastructure for applications in the cloud. It covers Five Pillars: Security, Reliability, Performance Efficiency, Cost Optimization, and Operational Excellence. The remaining options either refer to other AWS frameworks but not architecture best practices or to specific whitepapers.

**Documentation Link**: https://aws.amazon.com/architecture/well-architected/ 

---


## Quiz - CI/CD Workflow for AKS

**1. What is the primary purpose of Flux in the context of GitOps?**


* [ ] Continuous Integration (CI)
* [ ] Continuous Deployment (CD)
* [ ] Continuous Monitoring (CM)
* [ ] Continuous Integration and Continuous Deployment (CI/CD)

**Correct answer:**
* [x] Continuous Deployment (CD)

**Explaination**: Flux is primarily designed to enable continuous deployment in the GitOps workflow. It automates the process of pulling changes from a Git repository and applying them to the Kubernetes cluster, ensuring that the actual state matches the desired state defined in the repository.

**2. Which of the following statements accurately describes Kustomize in relation to Azure Kubernetes Service (AKS)?**


* [ ] Kustomize is a built-in feature of AKS for managing container registries.
* [ ] Kustomize is a declarative approach for deploying applications in AKS.
* [ ] Kustomize is a tool used for monitoring and autoscaling AKS clusters.
* [ ] Kustomize is an alternative container runtime used by AKS.

**Correct answer:**
* [x] Kustomize is a declarative approach for deploying applications in AKS.

**Explaination**: Kustomize is not a built-in feature of AKS but rather a standalone tool. It allows you to define and manage Kubernetes application configurations in a declarative manner. With Kustomize, you can customize and overlay configurations for different environments or variations of the same application, making it a powerful tool for managing deployments in AKS.

**3. Which workflow promotes a "pull request" model for reviewing and approving changes before they are applied to the AKS cluster?**


* [ ] Push-Based CI/CD Workflow
* [ ] Pull-Based CI/CD Workflow
* [ ] GitOps Workflow
* [ ] Event-Driven Workflow

**Correct answer:**
* [x] Pull-Based CI/CD Workflow

**Explaination**: In a pull-based workflow, changes are submitted as pull requests, and they go through a review and approval process before being merged and applied to the AKS cluster. This workflow ensures that changes are thoroughly reviewed before deployment.

**4. In a pull-based deployment workflow using Azure Kubernetes Service (AKS), which component is responsible for continuously monitoring the Git repository and applying the desired state changes to the AKS cluster?**


* [ ] Flux Operator
* [ ] Flux Controller
* [ ] Azure Container Registry
* [ ] Azure Monitor

**Correct answer:**
* [x] Flux Operator

**Explaination**: In a pull-based deployment workflow, the Flux Operator is responsible for continuously monitoring the Git repository. It pulls the desired state changes from the repository and applies them to the AKS cluster, ensuring that the actual state of the cluster matches the desired state specified in the repository. The Flux Operator acts as the bridge between the Git repository and the AKS cluster, orchestrating the synchronization of changes in a pull-based manner.

**5. Which method involves manually specifying each step to create and manage resources in AKS?**


* [ ] Imperative method
* [ ] Procedural method
* [ ] Declarative method
* [ ] Sequential method

**Correct answer:**
* [x] Imperative method

**Explaination**: In the imperative method, you explicitly specify each step and command to create and manage resources in AKS. This approach requires more manual intervention and is less preferred compared to the declarative method.

**6. Which method involves the use of Kubernetes manifests (YAML files) to define the desired state of applications in AKS?**


* [ ] Imperative method
* [ ] Procedural method
* [ ] Declarative method
* [ ] Sequential method

**Correct answer:**
* [x] Declarative method

**Explaination**: Using Kubernetes manifests, you can define the desired state of your applications, such as deployment specifications, service configurations, and other resources. The Kubernetes control plane then ensures the cluster matches the desired state.

**7. Which component of Flux is responsible for synchronizing the desired state defined in a Git repository with the actual state of an AKS cluster?**


* [ ] Flux Controller
* [ ] Flux Operator
* [ ] Flux Helm Operator
* [ ] Flux Syncer

**Correct answer:**
* [x] Flux Controller

**Explaination**: The Flux Controller is responsible for synchronizing the desired state defined in a Git repository with the actual state of an AKS cluster. It continuously monitors the Git repository, pulls changes, and applies them to the cluster, ensuring that the cluster remains in sync with the desired state as specified in the repository.

**8. Which Azure DevOps feature enables push-based deployment of containerized applications to Azure Kubernetes Service (AKS)?**


* [ ] Azure Pipelines
* [ ] Azure Boards
* [ ] Azure Repos
* [ ] Azure Artifacts

**Correct answer:**
* [x] Azure Pipelines

**Explaination**: Azure Pipelines is a feature of Azure DevOps that enables push-based deployment of containerized applications to Azure Kubernetes Service (AKS). It allows you to define CI/CD pipelines that automatically build, test, and deploy container images to AKS when changes are pushed to the repository. Azure Pipelines integrates well with AKS and provides a seamless workflow for continuous deployment to Kubernetes clusters.

**9. Which workflow is characterized by triggering deployments through events, such as code pushes or pull requests?**


* [ ] Push-Based CI/CD Workflow
* [ ] Pull-Based CI/CD Workflow or GitOps Workflow
* [ ] Event-Driven Workflow
* [ ] A platform for managing container images

**Correct answer:**
* [x] A platform for managing container images

**Explaination**: In a push-based or gitops workflow, whenever there is a code push or a pull request, the CI/CD pipeline is triggered, and the new changes are built, tested, and deployed to the AKS cluster.

---


## Quiz - Pulumi Essentials

**1. Where can you track the changes in your Pulumi stack?**


* [ ] Under stack overview dashboard
* [ ] Under stack activity dashboard 
* [ ] Under stack resources dashboard
* [ ] Under stack settings dashboard

**Correct answer:**
* [x] Under stack activity dashboard 

**2. Which parameter allows us to associate a security group with an EC2 instance in Pulumi (assume that we’re using Python)?**


* [ ] Use security_group
* [ ] Use vpc_security_group_ids
* [ ] Use security_groups
* [ ] Use vpc_security_group_id

**Correct answer:**
* [x] Use vpc_security_group_ids

**3. What is the best way to create multiple EC2 instances with the same configurations but different names in Pulumi?**


* [ ] Define instance’s name as an array and loop over the array to declare the instance definition
* [ ] Duplicate the EC2 instance definition multiple times
* [ ] Each Pulumi stack only allows a single EC2 instance

**Correct answer:**
* [x] Define instance’s name as an array and loop over the array to declare the instance definition

**4. How can we delete all AWS resource(s) created by Pulumi?**


* [ ] Use pulumi destroy command
* [ ] Use pulumi delete command 
* [ ] Delete all pulumi configuration files and push to the master branch
* [ ] Go to AWS console and manually delete all resources created by Pulumi

**Correct answer:**
* [x] Use pulumi destroy command

**5. Which Pulumi command allows you to list all output properties exported from a stack?**


* [ ] Use pulumi stack output
* [ ] Use pulumi up 
* [ ] Use pulumi outputs
* [ ] Use pulumi exports

**Correct answer:**
* [x] Use pulumi stack output

**6. Which of the following code allows you to export the public IP address of an EC2 instance in Pulumi (assume that we’re using Python)?**


* [ ] pulumi.export('public_ip', ec2_instance.public_ip)
* [ ] pulumi.export('public_ip', ec2_instance.ipv6_addresses)
* [ ] pulumi.export('public_ip', ec2_instance.private_ip)
* [ ] pulumi.export('public_ip', ec2_instance.associate_public_ip_address)

**Correct answer:**
* [x] pulumi.export('public_ip', ec2_instance.public_ip)

**7. If I have multiple AWS credentials, how should I configure them in Pulumi?**


* [ ] Create multiple AWS profiles in ~/.aws/credentials and set the AWS profile using the pulumi config set aws:profile <profilename> command
* [ ] Create multiple AWS profiles in ~/.aws/credentials and set the AWS profile using the pulumi config use aws:profile <profilename> command
* [ ] Create multiple AWS profiles in ~/.aws/credentials and set the AWS profile using the pulumi config current-aws-profile <profilename> command
* [ ] You can only use the default AWS credentials with Pulumi, as the platform doesn’t support multi-profile configuration

**Correct answer:**
* [x] Create multiple AWS profiles in ~/.aws/credentials and set the AWS profile using the pulumi config set aws:profile <profilename> command

**8. Which of the following code allows you to export the public DNS of an EC2 instance in Pulumi (assume that we’re using Python)?**


* [ ] pulumi.export('ec2_url', ec2_instance.public_dns)
* [ ] pulumi.export('ec2_url', ec2_instance.private_dns)
* [ ] pulumi.export('ec2_url', ec2_instance.private_dns_name_options)
* [ ] pulumi.export('ec2_url', ec2_instance.urn)

**Correct answer:**
* [x] pulumi.export('ec2_url', ec2_instance.public_dns)

**9. If we ask Pulumi to create an S3 bucket using the pulumi_aws package “bucket = s3.Bucket('my-bucket')”, what is the correct behavior?**


* [ ] Pulumi takes input parameter as bucket name; if the name is not globally unique, Pulumi will throw an exception
* [ ] Pulumi takes input parameter as bucket name and automatically appends random characters to create the S3 bucket
* [ ] Pulumi takes input parameter as bucket name and automatically prepends random characters to create the S3 bucket
* [ ] You can’t define bucket name declaratively; you need to supply a unique S3 bucket name at runtime

**Correct answer:**
* [x] Pulumi takes input parameter as bucket name and automatically appends random characters to create the S3 bucket

**10. Can we run the pulumi up command multiple times against the same Pulumi source codes?**


* [ ] Yes, Pulumi will check and let you know if there are any resource changes before you apply again
* [ ] No, everytime you run the command, new resources will be created, and you will end up with many duplicated resources on your infrastructure

**Correct answer:**
* [x] Yes, Pulumi will check and let you know if there are any resource changes before you apply again

**11. Which of the following commands allows you to perform an initial deployment after bootstrapping the Pulumi project?**


* [ ] Use pulumi run
* [ ] Use pulumi up
* [ ] Can’t be achieved via CLI; need to access Pulumi dashboard for that
* [ ] Use pulumi refresh

**Correct answer:**
* [x] Use pulumi up

**12. Which programming languages are supported by Pulumi CLI (assumed that we’re using AWS infrastructure)?**


* [ ] Golang
* [ ] Python
* [ ] PHP
* [ ] All options are correct

**Correct answer:**
* [x] Golang
* [x] Python

**13. Which command can be used for bootstrapping a new Pulumi project?**


* [ ] Use pulumi initialize 
* [ ] Use pulumi start
* [ ] Use pulumi new
* [ ] Use pulumi create

**Correct answer:**
* [x] Use pulumi new

**14. How can we configure AWS credentials (AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY) to use with Pulumi?**


* [ ] Export as environment variables following the naming convention
* [ ] Create a shared credentials file using the AWS CLI 
* [ ] Configure in the Pulumi.yaml file
* [ ] Pulumi doesn’t support Amazon Web Services (AWS)

**Correct answer:**
* [x] Export as environment variables following the naming convention
* [x] Create a shared credentials file using the AWS CLI 

**15. What is/are the drawback(s) of traditional IaC tool languages?**


* [ ] User needs to learn new language (like HCL for Terraform)
* [ ] Lack of flexibility compared to popular programming languages like Python, Java, or Golang
* [ ] Supported by limited ecosystem
* [ ] All options are correct

**Correct answer:**
* [x] All options are correct

**16. Which of the following OS are supported by Pulumi?**


* [ ] Windows only
* [ ] MacOS and Linux only
* [ ] Windows, MacOS, and Linux
* [ ] Linux only

**Correct answer:**
* [x] Windows, MacOS, and Linux

**17. What is/are the key differences between Pulumi and other Infrastructure as Code (IaC) tools?**


* [ ] IaC can be written in your favorite programming languages like Python, Java, Javascript, and Golang
* [ ] Full benefits of using IDE like VS Code, Eclipse along with 3rd packages & libraries
* [ ] Pulumi write codes for your infrastructure;, you only need to manage it
* [ ] Pulumi makes it easy to set up CI/CD just like the traditional applications 

**Correct answer:**
* [x] IaC can be written in your favorite programming languages like Python, Java, Javascript, and Golang
* [x] Full benefits of using IDE like VS Code, Eclipse along with 3rd packages & libraries
* [x] Pulumi makes it easy to set up CI/CD just like the traditional applications 

**18. Which of the following environment variables are required to perform the deployment of the Pulumi project (assume that we’re using AWS infrastructure)?**


* [ ] AWS_ACCESS_KEY_ID
* [ ] AWS_SECRET_ACCESS_KEY
* [ ] AWS_REGION
* [ ] All options are correct

**Correct answer:**
* [x] AWS_ACCESS_KEY_ID
* [x] AWS_SECRET_ACCESS_KEY

**19. What are the benefit(s) of Infrastructure as Code (IaC)?**


* [ ] Infrastructure can be defined as codes and stored in the Git repository
* [ ] Infrastructure can be defined as codes and shared with any members of the team
* [ ] Infrastructure can be defined as codes and makes versioning, tracking, reverting easy 
* [ ] All options are correct

**Correct answer:**
* [x] All options are correct

**20. What does IaC stand for?**


* [ ] Infrastructure as Config
* [ ] Infrastructure as Code
* [ ] Inter Application Communication
* [ ] Interpret as Command

**Correct answer:**
* [x] Infrastructure as Code

---


## Quiz - Postman Essentials

**1. What is the correct execution order of the script in Postman?**


* [ ] Request -> Response -> Pre-request Script -> Test Script
* [ ] Pre-request Script -> Request -> Response -> Test Script
* [ ] Request -> Response -> Test Script -> Pre-request Script
* [ ] Test Script -> Request -> Response -> Pre-request Script

**Correct answer:**
* [x] Pre-request Script -> Request -> Response -> Test Script

**2. Which of the following is/are correct about pre-request script?**


* [ ] Pre-request scripts are written in Javascript 
* [ ] Pre-request scripts run before the request is sent to the server
* [ ] Pre-request scripts run before the response is received by the client
* [ ] Pre-request scripts are written in Golang

**Correct answer:**
* [x] Pre-request scripts are written in Javascript 
* [x] Pre-request scripts run before the request is sent to the server

**3. How should we use the environment in Postman to maximize the benefit?**


* [ ] Define environment in Postman for each application environment (Dev, Staging, Production) in real world
* [ ] Define the environment in Postman for each application release
* [ ] Define the environment in Postman for each individual API
* [ ] Do not use the environment in Postman; it is obsolete and will be removed in the near future

**Correct answer:**
* [x] Define environment in Postman for each application environment (Dev, Staging, Production) in real world

**4. In the timeline of request in Postman, when is the test script executed?**


* [ ] Before the response is received
* [ ] After the response is received
* [ ] Before the request is sent
* [ ] After server receives the request

**Correct answer:**
* [x] After the response is received

**5. How can I use a variable named my_variable in my GET request with an initial value of Hello?**


* [ ] {{my_variable}}
* [ ] {my_variable}
* [ ] [[my_variable]]
* [ ] [my_variable]

**Correct answer:**
* [x] {{my_variable}}

**6. In which of the following languages can Postman test script be programmed?**


* [ ] Golang
* [ ] Python
* [ ] JavaScript 
* [ ] Java

**Correct answer:**
* [x] JavaScript 

**7. What is the outcome of this code “pm.environment.get("variable_key");” in Postman test script?**


* [ ] Get an environment variable
* [ ] Get a variable
* [ ] Get a collection variable
* [ ] Get a global variable

**Correct answer:**
* [x] Get an environment variable

**8. If the DEV and the PRODUCTION base url is different, what is the best way to test both environment APIs in Postman?**


* [ ] Hardcode all APIs with Dev server base url, finish the test, and then manually update value to Production server information
* [ ] Hardcode all APIs with Production server base url, finish the test, and then manually update value to DEV server information
* [ ] Define a variable for base url, set its value with Dev server, finish the test, and then update variable value with Production server one
* [ ] Create 2 environments in Postman; each individual environment has a variable for base url; switch between environments to test

**Correct answer:**
* [x] Create 2 environments in Postman; each individual environment has a variable for base url; switch between environments to test

**9. To manage non-global variable(s) in Postman, does the user need to create an environment?**


* [ ] YES
* [ ] NO

**Correct answer:**
* [x] YES

**10. What do you call a set of variables that you can use in your Postman requests?**


* [ ] Collections
* [ ] Environments
* [ ] Scripts
* [ ] Sessions

**Correct answer:**
* [x] Environments

**11. In Postman, which features allow you to store and reuse data?**


* [ ] Variables 
* [ ] Collections
* [ ] Environments
* [ ] Requests

**Correct answer:**
* [x] Variables 

**12. Which of the following HTTP request methods does Postman support?**


* [ ] GET
* [ ] POST
* [ ] PUT
* [ ] DELETE

**Correct answer:**
* [x] GET
* [x] POST
* [x] PUT
* [x] DELETE

**13. Postman supports what types of authentication?**


* [ ] Basic AUTH
* [ ] API Key
* [ ] Bearer Token
* [ ] All options are correct

**Correct answer:**
* [x] All options are correct

**14. What is the best way to organize and group API requests in Postman?**


* [ ] Use environment
* [ ] Use workplace
* [ ] Use collection
* [ ] Use mock server

**Correct answer:**
* [x] Use collection

**15. How can we submit JSON data in the POST request body?**


* [ ] Use x-www-form-urlencoded
* [ ] Use raw (JSON)
* [ ] Use binary
* [ ] Use form-data

**Correct answer:**
* [x] Use raw (JSON)

**16. Which of the following is/are correct about cookies in Postman?**


* [ ] A computer cookie is more formally known as an HTTP cookie, a web cookie, an Internet cookie, or a browser cookie.
* [ ] A cookie typically has two pieces of data: a unique ID for each user and user credential in base64 format.
* [ ] Cookies enable websites to retrieve your information when you revisit them, so that they can remember you and your preferences and tailor page content for you based on this information.
* [ ] Postman doesn’t support cookies

**Correct answer:**
* [x] A computer cookie is more formally known as an HTTP cookie, a web cookie, an Internet cookie, or a browser cookie.
* [x] Cookies enable websites to retrieve your information when you revisit them, so that they can remember you and your preferences and tailor page content for you based on this information.

**17. What version of Postman is appropriate for large-scale deployments?**


* [ ] Postman app
* [ ] Postman on the web
* [ ] Postman Enterprise
* [ ] Postman desktop agent

**Correct answer:**
* [x] Postman Enterprise

**18. What kinds of tests can be done with Postman?**


* [ ] UI tests
* [ ] Unit tests
* [ ] Functional test
* [ ] Integration tests

**Correct answer:**
* [x] Unit tests
* [x] Functional test
* [x] Integration tests

**19. What is Postman's primary function?**


* [ ] A framework to design, develop, and build APIs on any programming language
* [ ] A tool used to generate HTTP request so that you can test API
* [ ] A powerful yet easy-to-use suite of API developer tools for teams and individuals
* [ ] A specification language for HTTP APIs that provides a standardized means to define your API to others

**Correct answer:**
* [x] A tool used to generate HTTP request so that you can test API

**20. For API testing, what is/are the benefit(s) of using Postman over curl?**


* [ ] Easy integration graphic user interface
* [ ] Helps create, manage, and share your API testing requests
* [ ] Built-in test for API verification
* [ ] All options are correct

**Correct answer:**
* [x] All options are correct

---


## What you know at the end! - Exam 2

**1. Which of the following AWS pricing models provides the most significant discount compared to On-Demand Instance pricing in exchange for a longer-term commitment?**


* [ ] On-Demand Instances
* [ ] Reserved Instances
* [ ] Spot Instances
* [ ] Savings Plans

**Correct answer:**
* [x] Reserved Instances

**Explaination**: Correct answer is “Reserved Instances”: Reserved Instances provide a significant discount (up to 75%) compared to On-Demand Instance pricing and provide a capacity reservation when used in a specific Availability Zone. You can choose between one-year or three-year terms, and you have the option to pay for the Reserved Instance upfront, partially upfront, or with no upfront payment.

**2. Which of the following AWS pricing models allows customers to use compute capacity without upfront commitment and pay for the compute time that they consume?**


* [ ] On-Demand Instances
* [ ] Reserved Instances
* [ ] Spot Instances
* [ ] Dedicated Hosts

**Correct answer:**
* [x] On-Demand Instances

**Explaination**: Correct answer is “On-Demand Instances”: On-Demand Instances let you pay for compute capacity by the hour or second (minimum of 60 seconds) with no long-term commitments. This frees you from the costs and complexities of planning, purchasing, and maintaining hardware and transforms what are commonly large fixed costs into much smaller variable costs.

**3. Which of the following benefits of cloud computing is best demonstrated by the ability to access AWS services from anywhere in the world?**


* [ ] Go global in minutes
* [ ] Stop guessing capacity
* [ ] Increase speed and agility
* [ ] Trade capital expense for variable expense

**Correct answer:**
* [x] Go global in minutes

**Explaination**: Correct answer is “Go global in minutes”:The ability to access AWS services from anywhere in the world and to deploy solutions globally in minutes is a key benefit of cloud computing. AWS has data centers around the world, allowing you to deploy your application in multiple physical locations with just a few clicks.


**4. Which of the following benefits of cloud computing is best demonstrated by the ability to quickly scale up or scale down AWS resources based on demand?**


* [ ] Trade capital expense for variable expense
* [ ] Benefit from massive economies of scale
* [ ] Stop guessing capacity
* [ ] Increase speed and agility

**Correct answer:**
* [x] Stop guessing capacity

**Explaination**: Correct answer is “Stop guessing capacity”: The ability to quickly scale up or scale down AWS resources based on demand demonstrates the benefit of "Stop guessing capacity". In the cloud, you can provision the amount and type of resources that you actually need. If you need more, you can easily scale up. If you don't need as much, you can scale down.

**5. Which of the following benefits of cloud computing is best demonstrated by the ability to pay only for the compute power, storage, and other resources you use, with no long-term contracts or upfront commitments?**


* [ ] Trade capital expense for variable expense
* [ ] Increase speed and agility
* [ ] Stop guessing capacity
* [ ] Go global in minutes

**Correct answer:**
* [x] Trade capital expense for variable expense

**Explaination**: Correct answer is “Trade capital expense for variable expense”: The ability to pay only for the compute power, storage, and other resources you use, with no long-term contracts or upfront commitments, demonstrates the benefit of "Trade capital expense for variable expense". With cloud computing, you can pay as you go and only pay for what you use.

**6. Which of the following AWS pricing models allows customers to bid on unused EC2 capacity and run those instances for as long as their bid exceeds the current Spot Price?**


* [ ] On-Demand Instances
* [ ] Reserved Instances
* [ ] Spot Instances
* [ ] Savings Plans

**Correct answer:**
* [x] Spot Instances

**Explaination**: Correct answer is “Spot Instances” : Spot Instances allow you to bid on unused EC2 capacity and run those instances for as long as your bid exceeds the current Spot Price. Spot Instances are available at up to a 90% discount compared to On-Demand instance prices.

**7. Which of the following benefits of cloud computing is best demonstrated by the ability to quickly launch new services and applications on AWS?**


* [ ] Increase speed and agility
* [ ] Trade capital expense for variable expense
* [ ] Stop guessing capacity
* [ ] Go global in minutes

**Correct answer:**
* [x] Increase speed and agility

**Explaination**: Correct answer is “ Increase speed and agility”: The ability to quickly launch new services and applications on AWS demonstrates the benefit of "Increase speed and agility". Cloud services like AWS allow businesses to move faster and be more agile by enabling them to quickly spin up resources as they need them, deploying hundreds or even thousands of servers in minutes.

**8. Which of the following AWS pricing models provides a significant discount in exchange for a one- or three-year commitment?**


* [ ] On-Demand Instances
* [ ] Reserved Instances
* [ ] Spot Instances
* [ ] Savings Plans

**Correct answer:**
* [x] Reserved Instances

**Explaination**: Correct answer is “Reserved Instances”: Reserved Instances provide you with a significant discount (up to 75%) compared to On-Demand instance pricing. In exchange, you commit to use a specific instance type for a term of 1 or 3 years.

**9. Which of the following benefits of cloud computing is best demonstrated by the ability to deploy your applications in multiple regions around the world with just a few clicks?**


* [ ] Trade capital expense for variable expense
* [ ] Increase speed and agility
* [ ] Stop guessing capacity
* [ ] Go global in minutes

**Correct answer:**
* [x] Go global in minutes

**Explaination**: Correct answer is “Go global in minutes”: The ability to deploy your applications in multiple regions around the world with just a few clicks demonstrates the benefit of "Go global in minutes". With the AWS Cloud, you can easily deploy your application in multiple regions around the world with just a few clicks. This means you can provide lower latency and a better experience for your customers at minimal cost.

**10. Which of the following best describes the AWS Global Infrastructure?**


* [ ] A network of AWS management consoles located around the world
* [ ]  A collection of AWS services available in specific regions
* [ ] A worldwide network of Regions and Availability Zones
* [ ] A global system of AWS data centers

**Correct answer:**
* [x] A worldwide network of Regions and Availability Zones

**Explaination**: Correct answer is “A worldwide network of Regions and Availability Zones”: The AWS Global Infrastructure is a worldwide network of Regions and Availability Zones. Regions are separate geographic areas that have multiple, isolated locations known as Availability Zones. Availability Zones are physically separate locations within an AWS Region that are engineered to be isolated from failures in other Availability Zones and provide inexpensive, low-latency network connectivity to other zones in the same Region.

**11. Which of the following AWS services provides a scalable, high-speed, low-latency platform for transferring data between users and data stored over the Internet?**


* [ ] Amazon S3
* [ ] Amazon EC2
* [ ] Amazon CloudFront
* [ ] Amazon RDS

**Correct answer:**
* [x] Amazon CloudFront

**Explaination**: Correct answer is “Amazon CloudFront”: Amazon CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, all within a developer-friendly environment.

**12. Which of the following best describes the principle of "Loose Coupling" in cloud design?**


* [ ] Designing systems to operate in parallel to increase performance
* [ ] Designing systems to handle failure without impacting the entire system
* [ ] Designing systems with components that are independent and interact through specific interfaces
* [ ] Designing systems to automatically scale resources based on demand

**Correct answer:**
* [x] Designing systems with components that are independent and interact through specific interfaces

**Explaination**: Correct answer is “Designing systems with components that are independent and interact through specific interfaces”:  "Loose Coupling" refers to designing systems where each component is independent and interacts with the others through specific, defined interfaces. This design principle helps ensure that a change or a failure in one component does not cascade to other components.

**13. Which of the following is a key characteristic of cloud computing?**


* [ ] Limited scalability
* [ ] High upfront costs
* [ ] Manual management of resources
* [ ] Rapid elasticity

**Correct answer:**
* [x] Rapid elasticity

**Explaination**: Correct answer is “Rapid elasticity”: Rapid elasticity is a key characteristic of cloud computing. It refers to the ability to quickly scale out and in, often automatically, as workload demands change.

**14. Which of the following best describes cloud computing?**


* [ ] The practice of using a network of remote servers hosted on the Internet to store, manage, and process data rather than a local server or a personal computer
* [ ] The use of hardware and software to deliver a service over a network (typically the Internet)
* [ ] The delivery of computing services—including servers, storage, databases, networking, software, analytics, and intelligence—over the Internet (“the cloud”) to offer faster innovation, flexible resources, and economies of scale
* [ ] All of the options are correct

**Correct answer:**
* [x] All of the options are correct

**Explaination**: All the options provided are different ways of describing cloud computing. 


**15. Which AWS service allows you to manage access and permissions for your AWS resources?**


* [ ] AWS Organizations
* [ ] Amazon S3
* [ ] AWS Identity and Access Management (IAM)
* [ ] Amazon EC2

**Correct answer:**
* [x] AWS Identity and Access Management (IAM)

**Explaination**: Correct answer is “AWS IAM”: AWS Identity and Access Management (IAM) is a web service that helps you manage access to your AWS resources. It enables you to create and manage AWS users and groups and assign granular permissions to AWS resources. With IAM, you can control who can access your resources and what actions they can perform on those resources.

**16. Which of the following best describes the role of AWS in cloud computing?**


* [ ] AWS is a hardware manufacturing company that provides servers for cloud computing.
* [ ] AWS is a software company that provides the operating systems used in cloud computing.
* [ ] AWS is a cloud services provider that offers a broad set of global compute, storage, database, analytics, application, and deployment services.
* [ ] AWS is a telecommunications company that provides internet connectivity for cloud services.

**Correct answer:**
* [x] AWS is a cloud services provider that offers a broad set of global compute, storage, database, analytics, application, and deployment services.

**Explaination**: Amazon Web Services (AWS) is a secure cloud services platform, offering a broad set of global compute, storage, database, analytics, application, and deployment services that help organizations move faster, lower IT costs, and scale applications.

**17. Which of the following statements best describes multi-factor authentication (MFA) in the context of AWS security?**


* [ ] MFA is an optional security feature that adds an extra layer of protection by requiring users to enter a secondary password in addition to their primary password.
* [ ] MFA is a feature that allows users to access AWS resources from multiple devices without having to log in separately on each device.
* [ ] MFA is a feature that enables users to reset their passwords if they forget them.
* [ ] MFA is an optional security feature that requires users to provide two or more forms of authentiaction before accessing AWS resources.

**Correct answer:**
* [x] MFA is an optional security feature that requires users to provide two or more forms of authentiaction before accessing AWS resources.

**Explaination**: Multi-factor authentication (MFA) is a security feature that requires users to provide two or more forms of authentication before being granted access to AWS resources. This additional layer of security helps prevent unauthorized access to sensitive data and applications. With MFA, users are required to provide their primary password as well as a secondary form of authentication such as a security token, biometric information, or a smart card. MFA is an optional security feature for many AWS services, including AWS Management Console access.

**18. Which of the following is an AWS service that provides managed security and compliance controls for your AWS resources?**


* [ ] AWS Config
* [ ] Amazon S3
* [ ] AWS Certificate Manager
* [ ] AWS Artifact

**Correct answer:**
* [x] AWS Artifact

**Explaination**: Correct answer is “AWS Artifact”: AWS Artifact is a service that provides on-demand access to AWS compliance reports and other documents that help you meet regulatory and compliance requirements. This includes documents such as Service Organization Control (SOC) reports, Payment Card Industry (PCI) reports, and Health Insurance Portability and Accountability Act (HIPAA) reports. Artifact also provides access to the AWS Compliance Center, which contains information about AWS's compliance programs and certifications.

**19. Which AWS service provides a way to monitor and detect security incidents and vulnerabilities in your AWS environment?**


* [ ] AWS Key Management Service (KMS)
* [ ] Amazon Inspector
* [ ] AWS Shield
* [ ] Amazon GuardDuty

**Correct answer:**
* [x] Amazon GuardDuty

**Explaination**: Correct answer is “ Amazon GuardDuty”:  Amazon GuardDuty is a threat detection service that continuously monitors and analyzes your AWS accounts and workloads for malicious or unauthorized activity. It uses machine learning, anomaly detection, and integrated threat intelligence to identify potential security issues. GuardDuty can detect a wide range of threats, including network-based attacks, compromised instances, and data exfiltration attempts.

**20. Which AWS service allows you to automate the process of applying security patches and updates to EC2 instances?**


* [ ] AWS Systems Manager
* [ ] Amazon CloudWatch
* [ ] AWS Lambda
* [ ] AWS Config

**Correct answer:**
* [x] AWS Systems Manager

**Explaination**: Correct answer is “AWS Systems Manager”: AWS Systems Manager is a service that enables you to manage and configure your EC2 instances and other AWS resources at scale. It includes a suite of tools for automating common administrative tasks, including patch management. With Systems Manager, you can define patch baselines that specify which patches should be installed on your instances, and then apply those patches automatically.

**21. Which of the following is an AWS service that enables you to manage and control access to AWS resources across multiple accounts?**


* [ ] Amazon GuardDuty
* [ ] AWS Organizations
* [ ] AWS Identity and Access Management (IAM)
* [ ] AWS Config

**Correct answer:**
* [x] AWS Organizations

**Explaination**: Correct answer is “AWS Organizations”: AWS Organizations is a service that enables you to centralize management of multiple AWS accounts. With Organizations, you can create groups of accounts and apply policies to those groups to manage access to resources and services across your organization. You can also use Organizations to automate account creation and management tasks.

**22. Which of the following best describes the Least Privilege Principle in the context of AWS security?**


* [ ] Only users with administrator-level access should be able to modify AWS resources.
* [ ] Access to AWS resources should be granted on a need-to-know basis.
* [ ] All AWS resources should be publicly accessible unless explicitly restricted.
* [ ] Users should share their AWS credentials with other users to simplify resource access

**Correct answer:**
* [x] Access to AWS resources should be granted on a need-to-know basis.

**Explaination**: Correct answer is “Access to AWS resources should be granted on a need-to-know basis”: The Least Privilege Principle is a security principle that states that users should only be given the minimum level of access necessary to perform their job function. This means that access to AWS resources should be granted on a need-to-know basis, and that users should not be given more permissions than they require. By following this principle, you can limit the potential damage that can be caused by a compromised account and reduce the risk of accidental or intentional misuse of resources.

**23. What is the correct workflow for granting access to AWS resources using AWS Identity and Access Management (IAM)?**


* [ ] Create an IAM user, attach a policy to the user, and then assign the user to a group.
* [ ] Create an IAM group, create an IAM policy, and then attach the policy to the group.
* [ ] Create an IAM policy, attach the policy to an instance profile, and then assign the instance profile to an EC2 instance.
* [ ] Create an IAM role, attach a policy to the role, and then assign the role to a user.

**Correct answer:**
* [x] Create an IAM group, create an IAM policy, and then attach the policy to the group.

**Explaination**: Correct answer is “Create an IAM group, create an IAM policy, and then attach the policy to the group”: When granting access to AWS resources using IAM, the best practice is to create an IAM group first, and then assign users to that group. Once you have created a group, you can then create an IAM policy that specifies the permissions that members of the group should have. Finally, you attach the policy to the group.

**24. Which of the following statements best describes the AWS Shared Responsibility Model?**


* [ ] AWS is responsible for securing all aspects of the cloud, including the physical infrastructure, while the customer is responsible for securing their data and applications.
* [ ] The customer is responsible for securing all aspects of the cloud, including the physical infrastructure and AWS services, while AWS is responsible for securing customer data.
* [ ] The customer is responsible for securing all aspects of the cloud, including the physical infrastructure, while AWS is responsible for securing customer data and applications.
* [ ] AWS and the customer are equally responsible for securing all aspects of the cloud.

**Correct answer:**
* [x] AWS is responsible for securing all aspects of the cloud, including the physical infrastructure, while the customer is responsible for securing their data and applications.

**Explaination**: Correct answer is “AWS is responsible for securing all aspects of the cloud, including the physical infrastructure, while the customer is responsible for securing their data and applications”: The AWS Shared Responsibility Model is a security model that defines the division of labor between AWS and the customer in terms of security responsibilities. In this model, AWS is responsible for securing the underlying cloud infrastructure, while the customer is responsible for securing their data, applications, and the services they use in the cloud. This includes managing access to AWS services, configuring security settings, and patching software.

**25. What is the purpose of AWS Security Hub?**


* [ ] To provide a centralized view of security alerts and compliance status across AWS accounts
* [ ] To manage access to AWS resources across multiple accounts and services
* [ ] To scan your AWS resources for vulnerabilities and security threats
* [ ] To encrypt data at rest and in transit in your AWS environment

**Correct answer:**
* [x] To provide a centralized view of security alerts and compliance status across AWS accounts

**Explaination**: Correct answer is “To provide a centralized view of security alerts and compliance status across AWS accounts.”: AWS Security Hub is a security service that provides a centralized view of security alerts and compliance status across your AWS accounts. It aggregates findings from other AWS security services, such as Amazon GuardDuty and AWS Config, as well as from third-party partner solutions. This allows you to quickly identify potential security issues in your AWS environment and take action to remediate them.

**26. What is the purpose of AWS Artifact?**


* [ ] To provide a dashboard for monitoring your AWS resources and services
* [ ] To provide access to AWS compliance reports and certifications
* [ ]  To scan your AWS resources and services for vulnerabilities and security threats
* [ ] To manage access to AWS resources across multiple accounts and services

**Correct answer:**
* [x] To provide access to AWS compliance reports and certifications

**Explaination**: Correct answer is “To provide access to AWS compliance reports and certifications.”: AWS Artifact is a service that provides access to AWS compliance reports and certifications. These reports and certifications are produced by third-party auditors and attest to the security, compliance, and privacy of AWS services. Customers can use these reports and certifications to demonstrate compliance with various regulations and standards.


**27. Which of the following is a best practice for securing data stored in Amazon S3?**


* [ ] Enable server-side encryption using AWS KMS-managed keys.
* [ ] Use publicly accessible S3 buckets to simplify access to your data.
* [ ] Grant bucket-level permissions to all IAM users in your AWS account.
* [ ] Store access keys and secret access keys in plaintext in your code.

**Correct answer:**
* [x] Enable server-side encryption using AWS KMS-managed keys.

**Explaination**: Correct answer is “ Enable server-side encryption using AWS KMS-managed keys” : Server-side encryption is a method of encrypting data at rest in S3 buckets. When you use server-side encryption, your data is encrypted before it is saved to disk and decrypted when it is retrieved. AWS Key Management Service (KMS) provides a fully managed encryption service that makes it easy to create and control the encryption keys used to encrypt your data. By enabling server-side encryption with KMS-managed keys, you can ensure that your data is protected even if it is compromised.

**28. What is the purpose of AWS Shield?**


* [ ] To manage access to AWS resources across multiple accounts and services
* [ ] To encrypt data at rest and in transit for your AWS environment
* [ ] To provide protection against Distributed Denial of Service (DDoS) attacks
* [ ]  To scan your AWS resources for vulnerabilities and security threats

**Correct answer:**
* [x] To provide protection against Distributed Denial of Service (DDoS) attacks

**Explaination**: Correct answer is “To provide protection against Distributed Denial of Service (DDoS) attacks”: AWS Shield is a managed DDoS protection service that safeguards web applications running on AWS. It provides automatic inline mitigations that minimize application downtime and latency caused by DDoS attacks. There are two levels of AWS Shield: Standard and Advanced.

**29. What is the purpose of AWS Web Application Firewall (WAF)?**


* [ ] To monitor and analyze traffic to your web applications for security threats
* [ ] To encrypt data at rest and in transit for your web applications
* [ ] To manage access to AWS resources across multiple accounts and services
* [ ] To scan your web applications for vulnerabilities and security threats

**Correct answer:**
* [x] To monitor and analyze traffic to your web applications for security threats

**Explaination**: Correct answer is “ To monitor and analyze traffic to your web applications for security threats.”: AWS WAF is a web application firewall that helps protect your web applications from common web exploits and attacks, such as SQL injection and cross-site scripting (XSS). It allows you to monitor and analyze traffic to your web applications and provides rules that you can use to filter out malicious traffic. You can also create custom rules to block specific types of requests.

**30. What is the AWS Shared Responsibility Model?**


* [ ] A model that defines the division of responsibilities between AWS and AWS customers for the security of AWS services
* [ ] A model that defines the process for creating IAM policies to grant access to AWS resources
* [ ] A model that defines the encryption options available for data stored in Amazon S3
* [ ] A model that defines the process for monitoring and analyzing traffic to web applications using AWS WAF

**Correct answer:**
* [x] A model that defines the division of responsibilities between AWS and AWS customers for the security of AWS services

**Explaination**: Correct answer is “A model that defines the division of responsibilities between AWS and AWS customers for the security of AWS services”: The AWS Shared Responsibility Model is a model that defines the division of responsibilities between AWS and AWS customers for the security of AWS services. According to the model, AWS is responsible for the security of the cloud, which includes the underlying infrastructure and services such as EC2 and S3. Customers are responsible for the security in the cloud, which means they are responsible for securing their applications and data that are running on AWS.

**31. What is the purpose of AWS Config?**


* [ ] To provide a dashboard for monitoring your AWS resources and services
* [ ] To manage access to AWS resources across multiple accounts and services
* [ ] To scan your AWS resources for vulnerabilities and security threats
* [ ] To provide a detailed inventory of your AWS resources and track changes to them over time

**Correct answer:**
* [x] To provide a detailed inventory of your AWS resources and track changes to them over time

**Explaination**: Correct answer is “To provide a detailed inventory of your AWS resources and track changes to them over time”: AWS Config is a service that provides a detailed inventory of your AWS resources and tracks changes to those resources over time. It allows you to monitor resource configurations for compliance with internal policies, industry regulations, and security best practices. By using AWS Config, you can simplify compliance auditing, security analysis, change management, and operational troubleshooting.

**32. What is the purpose of AWS Key Management Service (KMS)?**


* [ ] To manage access to AWS resources across multiple accounts and services
* [ ] To provide a dashboard for monitoring your AWS resources and services
* [ ] To encrypt data at rest and in transit for your AWS environment
* [ ] To create and control encryption keys used to encrypt your data

**Correct answer:**
* [x] To create and control encryption keys used to encrypt your data

**Explaination**: Correct answer is “To create and control encryption keys used to encrypt your data”: AWS Key Management Service (KMS) is a fully managed encryption service that makes it easy to create and control the encryption keys used to encrypt your data. With AWS KMS, you can create, rotate, and disable encryption keys, as well as define and enforce policies for their use. AWS KMS integrates with other AWS services, such as Amazon S3 and Amazon EBS, to make it easy to protect your data.

**33. What is the purpose of AWS CloudTrail?**


* [ ] To manage access to AWS resources across multiple accounts and services
* [ ] To provide a dashboard for monitoring your AWS resources and services
* [ ] To scan your AWS resources for vulnerabilities and security threats
* [ ] To log, continuously monitor, and retain account activity related to actions performed by users, roles, and services

**Correct answer:**
* [x] To log, continuously monitor, and retain account activity related to actions performed by users, roles, and services

**Explaination**: Correct answer is “To log, continuously monitor, and retain account activity related to actions performed by users, roles, and services”: AWS CloudTrail is a service that logs, continuously monitors, and retains account activity related to actions performed by users, roles, and services in your AWS account. The information that is captured by CloudTrail includes the identity of the API caller, the time of the API call, the source IP address of the API caller, and more. By using CloudTrail, you can ensure compliance with industry regulations, audit security practices, and troubleshoot operational issues.

**34. What is the purpose of AWS Web Application Firewall (WAF)?**


* [ ] To manage access to AWS resources across multiple accounts and services
* [ ] To provide a dashboard for monitoring your AWS resources and services
* [ ] To scan your AWS resources for vulnerabilities and security threats
* [ ] To protect web applications from common web exploits that could affect application availability, compromise security, or consume excessive resources

**Correct answer:**
* [x] To protect web applications from common web exploits that could affect application availability, compromise security, or consume excessive resources

**Explaination**: Correct answer is “To protect web applications from common web exploits that could affect application availability, compromise security, or consume excessive resources”: AWS Web Application Firewall (WAF) is a service that helps protect web applications from common web exploits that could affect application availability, compromise security, or consume excessive resources. AWS WAF allows you to create custom rules that block common attack patterns, such as SQL injection or cross-site scripting (XSS), and it integrates with other AWS services, such as Amazon CloudFront and Amazon API Gateway, to provide comprehensive protection for your web applications.

**35. What is AWS Elastic Block Store (EBS)?**


* [ ] A managed block-level storage service for EC2 instances
* [ ] A serverless compute service that runs your code in response to events
* [ ] A fully managed container orchestration service
* [ ] A content delivery network that securely delivers data, videos, and applications

**Correct answer:**
* [x] A managed block-level storage service for EC2 instances

**Explaination**: Correct answer is “A managed block-level storage service for EC2 instances”: AWS Elastic Block Store (EBS) is a block-level storage service designed for use with Amazon Elastic Compute Cloud (EC2) instances. It provides persistent block-level storage volumes that can be attached to EC2 instances, providing data persistence even after the instance is terminated. EBS also provides different volume types to meet the needs of different workloads.

**36. What does VPC stand for in the context of AWS?**


* [ ] Virtual Private Configuration
* [ ] Virtual Public Cloud
* [ ] Virtual Private Cloud
* [ ] Virtual Public Configuration

**Correct answer:**
* [x] Virtual Private Cloud

**Explaination**: Correct answer is “Virtual Private Cloud”: AWS VPC is a service that allows you to launch AWS resources into a virtual network. This virtual network is logically isolated from other AWS resources, providing a layer of security and control over your AWS infrastructure. You can configure your VPC with subnets, route tables, and network gateways to create a customized network environment that meets your specific needs.

**37. What is the principle of least privilege?**


* [ ] Granting users and services only the permissions they need to perform their job
* [ ] Giving all users the same level of access to all resources
* [ ] Providing all users access to all AWS services and features by default
* [ ] Assigning the administrator role to all users to simplify management

**Correct answer:**
* [x] Granting users and services only the permissions they need to perform their job

**Explaination**: Correct answer is “Granting users and services only the permissions they need to perform their job”: The principle of least privilege is a security best practice that involves granting users and services only the permissions they need to perform their job. By limiting access to only what is necessary, you can reduce the surface area for potential security threats and minimize the impact of any security incidents. This principle applies to all aspects of access control, including AWS Identity and Access Management (IAM) policies, network access control lists, and resource permissions.

**38. What are the benefits of using AWS Certificate Manager (ACM) for SSL/TLS certificates?**


* [ ] ACM provides a dashboard for monitoring your AWS resources and services.
* [ ] ACM simplifies the process of requesting, managing, and deploying SSL/TLS certificates for your AWS resources.
* [ ] ACM scans your AWS resources for vulnerabilities and security threats.
* [ ] ACM allows you to manage access to AWS resources across multiple accounts and services

**Correct answer:**
* [x] ACM simplifies the process of requesting, managing, and deploying SSL/TLS certificates for your AWS resources.

**Explaination**: Correct answer is “ACM simplifies the process of requesting, managing, and deploying SSL/TLS certificates for your AWS resources”: AWS Certificate Manager (ACM) is a service that makes it easy to request, manage, and deploy SSL/TLS certificates for your AWS resources. With ACM, you can request public and private certificates for use with ACM-integrated services, such as Elastic Load Balancing, CloudFront, and API Gateway. ACM takes care of certificate renewal and provides automated validation to ensure that your certificates are properly configured.

**39. What is AWS SDK?**


* [ ] A graphical user interface for managing AWS resources
* [ ] A command-line interface for interacting with AWS resources
* [ ] A set of APIs for integrating AWS services with third-party applications
* [ ] A programming language for automating AWS infrastructure

**Correct answer:**
* [x] A set of APIs for integrating AWS services with third-party applications

**Explaination**: Correct answer is “A set of APIs for integrating AWS services with third-party applications”: The AWS SDK is a collection of software development kits (SDKs) that provide developers with APIs for integrating their applications with AWS services. The SDKs are available in multiple programming languages, including Java, Python, and Ruby, among others. Developers can use the SDK to build applications that interact with AWS services directly from their code without having to write low-level code to communicate with AWS APIs.

**40. What is Amazon S3?**


* [ ] A serverless compute service that runs your code in response to events
* [ ] An on-demand compute capacity service for running containers
* [ ] A fully managed analytics service to process big data
* [ ] A highly scalable object storage service

**Correct answer:**
* [x] A highly scalable object storage service

**Explaination**: Correct answer is “A highly scalable object storage service”: Amazon Simple Storage Service (S3) is a highly scalable, durable, and secure object storage service. It allows users to store and retrieve data of any size at any time from anywhere on the web. S3 is designed for 99.999999999% durability, meaning it can sustain the concurrent loss of data in two facilities. S3 also offers features such as versioning, lifecycle policies, and access control to ensure the security, compliance, and availability of stored data.

**41. What is Amazon Elastic Compute Cloud (EC2)?**


* [ ]  A managed container orchestration service
* [ ] A fully managed database service
* [ ] A scalable compute capacity service for running applications
* [ ] A serverless compute service that runs your code in response to events

**Correct answer:**
* [x] A scalable compute capacity service for running applications

**Explaination**: Correct answer is “A scalable compute capacity service for running applications”: Amazon Elastic Compute Cloud (EC2) is a web service that provides scalable compute capacity in the cloud. It allows users to launch virtual servers called “instances” on demand with various configurations such as operating systems, instance types, and software packages. EC2 provides scalable computing resources that can be easily scaled up or down to meet the changing demands of applications. EC2 users pay only for the compute capacity they consume, with no upfront costs or long-term commitments required.

**42. What is AWS Lambda?**


* [ ] A fully managed container orchestration service
* [ ] A serverless compute service that runs your code in response to events
* [ ] A scalable compute capacity service for running applications
* [ ] A managed container deployment and management service

**Correct answer:**
* [x] A serverless compute service that runs your code in response to events

**Explaination**: Correct answer is “A serverless compute service that runs your code in response to events”: AWS Lambda is a serverless compute service that allows you to run your code in the cloud without provisioning or managing servers. It runs your code in response to events, such as changes to data in an S3 bucket, updates to a DynamoDB table, or HTTP requests through API Gateway. With Lambda, you only pay for the compute time you consume, with no upfront costs or ongoing commitments.

**43. What is Amazon EC2 Auto Scaling?**


* [ ] A service that provides a simple way to manage and store your data in the cloud
* [ ] A fully managed push notification service that enables you to send individual or bulk messages to multiple endpoints
* [ ] A monitoring and observability service that provides data and insights on your AWS resources and applications
* [ ] A service that automatically adjusts the number of Amazon EC2 instances in a group based on demand

**Correct answer:**
* [x] A service that automatically adjusts the number of Amazon EC2 instances in a group based on demand

**Explaination**: Amazon EC2 Auto Scaling automatically adjusts the number of Amazon EC2 instances in a group based on demand, helping you maintain application availability and allowing you to scale your capacity up or down automatically according to conditions you define. You can use EC2 Auto Scaling to ensure that you have the correct number of Amazon EC2 instances available to handle the load for your application. EC2 Auto Scaling can also automatically launch Amazon EC2 instances in response to increased application traffic and terminate instances in response to decreased demand.

**44. What is Amazon Simple Notification Service (SNS)?**


* [ ] A fully managed messaging and queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications
* [ ] A highly scalable, durable, and secure object storage service that allows you to store and retrieve data of any size at any time from anywhere on the web
* [ ] A serverless compute service that runs your code in response to events
* [ ] A fully managed push notification service that enables you to send individual or bulk messages to multiple endpoints

**Correct answer:**
* [x] A fully managed push notification service that enables you to send individual or bulk messages to multiple endpoints

**Explaination**: Correct answer is “A fully managed push notification service that enables you to send individual or bulk messages to multiple endpoints” :  Amazon Simple Notification Service (SNS) is a fully managed push notification service that enables you to send individual or bulk messages to multiple recipients or endpoints, such as Amazon SQS queues, AWS Lambda functions, HTTP endpoints, email addresses, and mobile devices. SNS provides message fanout, which means that each published message can be delivered to multiple subscribers or endpoints with no additional effort required from the publisher.

**45. What is Amazon S3?**


* [ ] A service that provides a simple way to manage and store your data in the cloud
* [ ] A fully managed container orchestration service
* [ ] A serverless compute service that runs your code in response to events
* [ ] A free, open-source, cross-platform relational database management system

**Correct answer:**
* [x] A service that provides a simple way to manage and store your data in the cloud

**Explaination**: Correct answer is “A service that provides a simple way to manage and store your data in the cloud”: Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. This means customers of all sizes and industries can use it to store and protect any amount of data for a range of use cases, such as websites, mobile applications, backup and restore, archive, enterprise applications, IoT devices, and big data analytics. Amazon S3 provides easy-to-use management features so you can organize your data and configure finely tuned access controls to meet your specific business, organizational, and compliance requirements. Amazon S3 is designed for 99.999999999% (11 9's) of durability, and stores data for millions of applications for companies all around the world.

**46. What is Amazon Elastic Compute Cloud (Amazon EC2)?**


* [ ] A service that provides a simple way to manage and store your data in the cloud
* [ ] A fully managed serverless compute service that automatically scales, provisions, and manages servers for you
* [ ] A monitoring and observability service that provides data and insights on your AWS resources and applications
* [ ] A web service that provides resizable compute capacity in the cloud

**Correct answer:**
* [x] A web service that provides resizable compute capacity in the cloud

**Explaination**: Correct answer is “A web service that provides resizable compute capacity in the cloud”: Amazon Elastic Compute Cloud (Amazon EC2) is a web service that provides resizable compute capacity in the cloud. It is designed to make web-scale cloud computing easier for developers. Amazon EC2's simple web service interface allows you to obtain and configure capacity with minimal friction. It provides you with complete control of your computing resources and lets you run on Amazon's proven computing environment. Amazon EC2 reduces the time required to obtain and boot new server instances to minutes, allowing you to quickly scale capacity, both up and down, as your computing requirements change. Amazon EC2 changes the economics of computing by allowing you to pay only for capacity that you actually use.

**47. What is Amazon CloudWatch?**


* [ ] A fully managed container orchestration service
* [ ] A service that provides a simple way to manage and store your data in the cloud
* [ ] A serverless compute service that runs your code in response to events
* [ ] A monitoring and observability service that provides data and insights on your AWS resources and applications

**Correct answer:**
* [x] A monitoring and observability service that provides data and insights on your AWS resources and applications

**Explaination**: Correct answer is “A monitoring and observability service that provides data and insights on your AWS resources and applications”:  Amazon CloudWatch is a monitoring and observability service that provides data and insights on your AWS resources and applications. It collects and tracks metrics, logs, and events from AWS services and resources, as well as from custom applications, and provides a unified view of operational health and performance. CloudWatch can be used to monitor AWS resources, such as EC2 instances and RDS DB instances, and to monitor custom metrics generated by your applications. Additionally, CloudWatch can be used to set alarms and automate actions based on changes in performance or resource utilization.

**48. Which AWS service is responsible for connecting your VPC to the internet and enabling communication between your VPC and the public internet?**


* [ ] AWS Direct Connect
* [ ] AWS Transit Gateway
* [ ] AWS Elastic Load Balancer
* [ ] AWS Internet Gateway

**Correct answer:**
* [x] AWS Internet Gateway

**Explaination**: Correct answer is “AWS Internet Gateway”: AWS Virtual Private Cloud (VPC) is a service that allows you to create an isolated virtual network in the AWS cloud. By default, a VPC is private and does not have direct access to the internet. To enable communication between your VPC and the public internet, you need to attach an AWS Internet Gateway.

**49. Which of the following is true about AWS DynamoDB?**


* [ ] DynamoDB is a fully managed NoSQL database service.
* [ ] DynamoDB is a relational database service.
* [ ] DynamoDB is primarily used for file storage.
* [ ] DynamoDB is limited to a single region for data storage.

**Correct answer:**
* [x] DynamoDB is a fully managed NoSQL database service.

**Explaination**: Correct answer is “DynamoDB is a fully managed NoSQL database service”: 
AWS DynamoDB is a fully managed NoSQL database service that provides fast and predictable performance with seamless scalability. It is designed to handle large amounts of data and high read and write throughput. DynamoDB offers low-latency access to data and automatic scaling based on demand, making it suitable for various use cases such as web applications, gaming, IoT, and more.


**50. Which AWS service provides scalable object storage for storing and retrieving data?**


* [ ] AWS EC2
* [ ] AWS DynamoDB
* [ ] AWS S3
* [ ] AWS RDS

**Correct answer:**
* [x] AWS S3

**Explaination**: Correct answer is “AWS S3”: AWS Simple Storage Service (S3) is a scalable object storage service provided by Amazon Web Services. It is designed to store and retrieve any amount of data from anywhere on the web. S3 offers durability, high availability, and low latency access to objects, making it suitable for various storage use cases such as backup and restore, data archiving, content distribution, and more.

**51. What is the main benefit of using AWS Lambda?**


* [ ] It allows you to run applications on a managed server.
* [ ] It provides continuous scaling and automatic updates.
* [ ] It enables you to run code without provisioning or managing servers.
* [ ] It offers microservices architecture with API Gateway integration.

**Correct answer:**
* [x] It enables you to run code without provisioning or managing servers.

**Explaination**: Correct answer is “It enables you to run code without provisioning or managing servers”: AWS Lambda lets you run code without provisioning or managing servers, which can save on the cost and complexity of server management. It executes your code only when needed and scales automatically, from a few requests per day to thousands per second.

**52. What is Amazon S3?**


* [ ] A database service for storing and retrieving data
* [ ] A messaging service for sending and receiving messages between distributed systems
* [ ] An object storage service for storing and retrieving any type of data
* [ ] A file system service for storing and managing file systems in the cloud

**Correct answer:**
* [x] An object storage service for storing and retrieving any type of data

**Explaination**: Correct answer is “An object storage service for storing and retrieving any type of data”: Amazon Simple Storage Service (S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. It is designed to store and retrieve any amount of data from anywhere on the web. Amazon S3 is highly durable and provides 99.999999999% durability for objects stored in it. It can store various types of objects, including files, videos, images, documents, and more.

**53. Which AWS service provides a global infrastructure consisting of multiple Regions and Availability Zones?**


* [ ] AWS SNS
* [ ] AWS EC2
* [ ] AWS Snowball
* [ ] AWS Global Accelerator

**Correct answer:**
* [x] AWS Global Accelerator

**Explaination**: Correct answer is “AWS Global Accelerator”: AWS Global Infrastructure is responsible for providing a worldwide network of Regions and Availability Zones. However, the specific service that directly utilizes this global infrastructure is AWS Global Accelerator. Global Accelerator is a networking service that improves the availability and performance of your applications by directing traffic across AWS Regions and Availability Zones. It optimizes the global routing of traffic and reduces latency for end users.

**54. Which component of AWS provides a managed relational database service?**


* [ ] AWS DynamoDB
* [ ] AWS RDS
* [ ] AWS Neptune
* [ ] AWS Redshift

**Correct answer:**
* [x] AWS RDS

**Explaination**: Correct answer is “AWS RDS”: Amazon Relational Database Service (RDS) is a managed relational database service that provides you with six familiar database engines to choose from, including Amazon Aurora, MySQL, MariaDB, Oracle, Microsoft SQL Server, and PostgreSQL.

**55. What is the AWS Global Infrastructure?**


* [ ] A service for managing and deploying code in the cloud
* [ ] A network of physical data centers that host AWS services globally
* [ ] A service for hosting and managing databases in the cloud
* [ ]  A messaging service for sending and receiving messages between distributed systems

**Correct answer:**
* [x] A network of physical data centers that host AWS services globally

**Explaination**: Correct answer is “ A network of physical data centers that host AWS services globally”: The AWS Global Infrastructure is a network of physical data centers spread across different regions and availability zones worldwide. Each region is a separate geographic area with multiple availability zones that are isolated from each other and connected through high-speed, low-latency networks. This infrastructure allows AWS customers to deploy their applications and services closer to their end-users, reducing latency and improving performance.

**56. Which AWS service provides a global infrastructure consisting of multiple Regions and Availability Zones?**


* [ ] AWS SNS
* [ ] AWS EC2
* [ ] AWS Snowball
* [ ] AWS Global Accelerator

**Correct answer:**
* [x] AWS Global Accelerator

**Explaination**: Correct answer is “AWS Global Accelerator”: AWS Global Infrastructure is responsible for providing a worldwide network of Regions and Availability Zones. However, the specific service that directly utilizes this global infrastructure is AWS Global Accelerator. Global Accelerator is a networking service that improves the availability and performance of your applications by directing traffic across AWS Regions and Availability Zones. It optimizes the global routing of traffic and reduces latency for end users.

**57. Under the Technology domain and concerning the AWS Cloudwatch service: What information does AWS CloudWatch provide for AWS resources?**


* [ ] Allows you to migrate data into and out of the AWS Cloud
* [ ] Collects and tracks metrics, collects and monitors log files, sets alarms, and automatically reacts to changes in AWS resources
* [ ] Allows you to securely control access to AWS services and resources for your users
* [ ] Provides durable, block-level storage volumes for use with Amazon EC2 instances

**Correct answer:**
* [x] Collects and tracks metrics, collects and monitors log files, sets alarms, and automatically reacts to changes in AWS resources

**Explaination**: Correct answer is “Collects and tracks metrics, collects and monitors log files, sets alarms, and automatically reacts to changes in AWS resources”: AWS CloudWatch is a monitoring service for AWS resources and the applications you run on Amazon Web Services. It collects and tracks metrics, collects and monitors log files, and sets alarms. It can automatically react to changes in your AWS resources, which can be useful to ensure the performance and availability of your applications.

**58. Which AWS service provides a platform for deploying and managing applications in a fully managed environment?**


* [ ] AWS Elastic Beanstalk
* [ ] AWS CloudFormation
* [ ] AWS Lambda
* [ ] AWS EC2

**Correct answer:**
* [x] AWS Elastic Beanstalk

**Explaination**: Correct answer is “AWS Elastic Beanstalk”: AWS Elastic Beanstalk is a fully managed service that provides an easy-to-use platform for deploying and managing applications. It abstracts the underlying infrastructure and allows you to focus on writing code without worrying about server management. Elastic Beanstalk supports a variety of programming languages, platforms, and frameworks, making it suitable for a wide range of applications.

**59. Which AWS service provides persistent block-level storage volumes for use with Amazon EC2 instances?**


* [ ] AWS EFS
* [ ] AWS S3
* [ ] AWS EBS
* [ ] AWS FSx

**Correct answer:**
* [x] AWS EBS

**Explaination**: Correct answer is “AWS EBS”: AWS Elastic Block Store (EBS) provides persistent block-level storage volumes for use with Amazon EC2 instances. EBS volumes are independent storage devices that can be attached to EC2 instances as virtual hard drives. They provide durable and reliable storage that persists even after an instance is terminated. EBS volumes are designed to be highly available and come in different types such as General Purpose SSD, Provisioned IOPS SSD, and Magnetic volumes.

**60. What does AWS AMI stand for?**


* [ ] Amazon Machine Interface
* [ ] Amazon Machine Image
* [ ] Amazon Managed Instance
* [ ] Amazon Managed Image

**Correct answer:**
* [x] Amazon Machine Image

**Explaination**: Correct answer is “Amazon Machine Image”: AWS AMI stands for Amazon Machine Image. An Amazon Machine Image is a pre-configured template that contains the necessary information to launch an instance (virtual server) in Amazon Elastic Compute Cloud (EC2). It includes the root file system, operating system, applications, libraries, and configuration settings required for the instance. AMIs are used to spin up EC2 instances with specific software configurations or pre-installed applications.

**61. Which AWS service records AWS API calls for your account and delivers log files to you?**


* [ ] AWS CloudFront
* [ ] AWS CloudFormation
* [ ] AWS CloudTrail
* [ ] AWS CloudWatch

**Correct answer:**
* [x] AWS CloudTrail

**Explaination**: Correct answer is “AWS CloudTrail”: AWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account. CloudTrail tracks user activity, API usage, and changes to AWS resources.

**62. Which AWS service is used for storing, managing, and deploying container images?**


* [ ] AWS ECS
* [ ] AWS EKS
* [ ] AWS ECR
* [ ] AWS S3

**Correct answer:**
* [x] AWS ECR

**Explaination**: Correct answer is “AWS ECR”: AWS Elastic Container Registry (ECR) is a fully managed container registry service provided by AWS. It is used for storing, managing, and deploying container images, making it an integral part of container-based application development and deployment. With ECR, you can easily push and pull container images to and from the registry, control access to the images, and integrate it seamlessly with other AWS container services like Elastic Container Service (ECS) and Elastic Kubernetes Service (EKS).

**63. Under the Technology domain and concerning the AWS Elastic Container Service (ECS):  Which AWS Service is a highly scalable, high-performance container orchestration service that supports Docker containers and allows you to easily run and scale containerized applications on AWS?**


* [ ] AWS Elastic Beanstalk
* [ ] AWS EC2 instances
* [ ] AWS Elastic Container Service (ECS)
* [ ] AWS Elastic File System (EFS)

**Correct answer:**
* [x] AWS Elastic Container Service (ECS)

**Explaination**: Correct answer is “AWS Elastic Container Service (ECS)”: Amazon Elastic Container Service (Amazon ECS) is a fully managed container orchestration service that is highly secure and scalable. It allows you to easily run, stop, and manage Docker containers on a cluster.

**64. What is the AWS Snowball family of services?**


* [ ] A family of physical devices that can be used to transfer large amounts of data into and out of AWS
* [ ] A database service that provides fast access to petabyte-scale data in the cloud
* [ ] A messaging service for sending and receiving messages between distributed systems
* [ ] A service for managing and deploying code in the cloud

**Correct answer:**
* [x] A family of physical devices that can be used to transfer large amounts of data into and out of AWS

**Explaination**: Correct answer is “A family of physical devices that can be used to transfer large amounts of data into and out of AWS”: AWS Snowball is a family of physical devices designed to enable secure and efficient transfer of large amounts of data into and out of AWS. Snowball devices come in two types: the original Snowball and the newer, smaller Snowball Edge. These devices are rugged, portable, and can store up to 80 TB of data. They can be used to migrate on-premises data to the cloud, move data between AWS regions, or support big data and machine learning workloads.

**65. Under the Technology domain and concerning the AWS Elastic Kubernetes Service (EKS):  Which AWS service allows you to deploy, manage, and scale containerized applications using Kubernetes on AWS?**


* [ ] AWS Elastic Container Service (ECS)
* [ ] AWS Elastic Kubernetes Service (EKS)
* [ ] AWS Elastic Compute Cloud (EC2)
* [ ] AWS Simple Storage Service (S3)

**Correct answer:**
* [x] AWS Elastic Kubernetes Service (EKS)

**Explaination**: Correct answer is “AWS Elastic Kubernetes Service (EKS)”: Amazon Elastic Kubernetes Service (Amazon EKS) is a fully managed service that allows you to run Kubernetes on AWS without needing to install, operate, and maintain your own Kubernetes control plane or nodes.

**66. Which AWS service helps distribute incoming application traffic across multiple targets, such as EC2 instances, containers, and IP addresses?**


* [ ] AWS EKS
* [ ] AWS REdshift
* [ ] AWS SNS
* [ ] AWS Load Balancing

**Correct answer:**
* [x] AWS Load Balancing

**Explaination**: Correct answer is “AWS Load Balancing”: AWS Load Balancing is a service that helps distribute incoming application traffic across multiple targets, such as EC2 instances, containers, and IP addresses. It improves the availability and fault tolerance of your applications by spreading the traffic evenly across the targets and automatically scaling to handle fluctuations in demand. AWS provides different load balancing options, including Elastic Load Balancer (ELB) and Application Load Balancer (ALB), to cater to different application needs.

**67. What is AWS Autoscaling?**


* [ ] A service that automatically monitors and adjusts the capacity of your EC2 instances based on demand
* [ ] A database service that provides fast access to petabyte-scale data in the cloud
* [ ] A messaging service for sending and receiving messages between distributed systems
* [ ] A service for managing and deploying code in the cloud

**Correct answer:**
* [x] A service that automatically monitors and adjusts the capacity of your EC2 instances based on demand

**Explaination**: Correct answer is “A service that automatically monitors and adjusts the capacity of your EC2 instances based on demand”:  AWS Autoscaling is a service that automatically scales up or down the number of EC2 instances in an Auto Scaling group based on demand. It helps to maintain the desired level of application performance while optimizing costs. With Autoscaling, you can create policies that define when and how to add or remove instances based on metrics such as CPU utilization, network traffic, or custom metrics.

**68. Under the Technology domain and concerning the AWS EBS service:  Which type of AWS Elastic Block Store volume offers the lowest cost per gigabyte?**


* [ ] Provisioned IOPS SSD (io2)
* [ ] General Purpose SSD (gp3)
* [ ] Throughput Optimized HDD (st1)
* [ ] Cold HDD (sc1)

**Correct answer:**
* [x] Cold HDD (sc1)

**Explaination**: Correct answer is “Cold HDD (sc1)”: The Cold HDD (sc1) volumes are designed for infrequent access and offer the lowest cost per gigabyte of all EBS volume types.

**69. Which AWS service is a fully managed, petabyte-scale data warehouse service that makes it simple and cost-effective to analyze all your data?**


* [ ] AWS Athena
* [ ] AWS DynamoDB
* [ ] AWS RDS
* [ ] AWS Redshift

**Correct answer:**
* [x] AWS Redshift

**Explaination**: Correct answer is “AWS Redshift”: Amazon Redshift is a fast, scalable data warehouse that makes it simple and cost-effective to analyze all your data across your data warehouse and data lake.

**70. Which AWS service is used for securely transferring large amounts of data to and from the AWS Cloud?**


* [ ] AWS Redshift
* [ ] AWS Neptune
* [ ] AWS SQS
* [ ] AWS Snowball Family

**Correct answer:**
* [x] AWS Snowball Family

**Explaination**: Correct answer is “AWS Snowball Family”: AWS Snowball Family is a service used for securely transferring large amounts of data to and from the AWS Cloud. It addresses common challenges of data transfer, such as limited network bandwidth, long transfer times, and security concerns. The Snowball devices are rugged, secure, and offline data transfer appliances that help accelerate data migration and data transfer processes. They can be used to import or export data from AWS, providing a physical method for large-scale data transfers.

**71. Which AWS tool helps you visualize, understand, and manage your AWS costs and usage over time?**


* [ ] AWS QuickSight
* [ ] AWS Cost Explorer
* [ ] AWS Management Console
* [ ] AWS CloudFormation

**Correct answer:**
* [x] AWS Cost Explorer

**Explaination**: Correct answer is “AWS Cost Explorer”: AWS Cost Explorer is a tool that enables you to view and analyze your costs and usage. You can explore your cost data, identify trends, isolate cost drivers, and detect anomalies.

**72. Which AWS tool allows you to set custom cost and usage budgets that alert you when your costs or usage exceed (or are forecasted to exceed) your budgeted amount?**


* [ ] AWS Cost Explorer
* [ ] AWS Cost and Usage Report
* [ ] AWS Budgets
* [ ] AWS Organizations

**Correct answer:**
* [x] AWS Budgets

**Explaination**: Correct answer is “AWS Budgets”: AWS Budgets gives you the ability to set custom cost and usage budgets that alert you when your costs or usage exceed (or are forecasted to exceed) your budgeted amount.

**73. Under the Billing and Pricing domain:  Which type of AWS licensing model allows you to use and pay for AWS compute capacity by the hour with no long-term commitments, allowing you to adjust your usage according to the demands of your application?**


* [ ] On-Demand Instances
* [ ] Spot Instances
* [ ] Reserved Instances
* [ ] Savings Plans

**Correct answer:**
* [x] On-Demand Instances

**Explaination**: Correct answer is “On-Demand Instances”: On-Demand Instances let you pay for compute capacity by the hour with no long-term commitments. This frees you from the costs and complexities of planning, purchasing, and maintaining hardware and transforms what are commonly large fixed costs into much smaller variable costs.

**74. Which AWS tool allows you to set custom cost and usage budgets that alert you when your costs or usage exceed (or are forecast to exceed) your budgeted amount?**


* [ ] AWS Cost Explorer
* [ ] AWS Cost and Usage Report
* [ ] AWS Budgets
* [ ] AWS Organizations

**Correct answer:**
* [x] AWS Budgets

**Explaination**: Correct answer is “AWS Budgets”: The AWS Budgets tool gives you the ability to set custom cost and usage budgets that alert you when your costs or usage exceed (or are forecast to exceed) your budgeted amount.

**75. Which AWS Support plan offers 24/7 access to customer service, AWS documentation, whitepapers, and support forums, but lacks full technical support from AWS?**


* [ ] Basic
* [ ] Developer
* [ ] Business
* [ ] Enterprise

**Correct answer:**
* [x] Basic

**Explaination**: Correct answer is “Basic”: The Basic Support plan offers customer service, AWS documentation, whitepapers, and support forums. However, it does not offer access to Technical Support Engineers.

**76. Using which AWS service can you consolidate billing across multiple AWS accounts, allowing easy access to cost reporting while maintaining the highest level of account security?**


* [ ] AWS Management Console
* [ ] AWS Budgets
* [ ] AWS Organizations
* [ ] AWS Cost and Usage Report

**Correct answer:**
* [x] AWS Organizations

**Explaination**: Correct answer is “AWS Organizations”: AWS Organizations allows you to consolidate billing across multiple AWS accounts. This provides you with centralized cost tracking and access to volume discounts, all while maintaining separate and secure resources for each account.

**77. Which AWS Support plan provides access to AWS documentation, whitepapers, and support forums, as well as 24/7 customer service for billing, account, and technical questions?**


* [ ] Basic
* [ ] Developer
* [ ] Business
* [ ] Enterprise

**Correct answer:**
* [x] Basic

**Explaination**: Correct answer is “Basic”: The Basic Support plan provides all customers with access to AWS documentation, whitepapers, and support forums, as well as a 24/7 customer service for billing, account, and technical questions.

**78. Which AWS pricing model allows you to bid on spare Amazon EC2 computing capacity?**


* [ ] On-Demand Instances
* [ ] Spot Instances
* [ ] Savings Plans
* [ ] Reserved Instances

**Correct answer:**
* [x] Spot Instances

**Explaination**: Correct answer is “Spot Instances”: Spot Instances allow you to bid on spare Amazon EC2 computing capacity. This can provide significant cost savings, but your instances can be interrupted if your bid price is lower than the current Spot Price.

**79. Which AWS Support plan provides access to a Technical Account Manager and Infrastructure Event Management?**


* [ ] Basic
* [ ] Developer
* [ ] Business
* [ ] Enterprise

**Correct answer:**
* [x] Enterprise

**Explaination**: Correct answer is “Enterprise”: The Enterprise Support plan provides access to a Technical Account Manager and Infrastructure Event Management, in addition to 24/7 phone, email, and chat access to Cloud Support Engineers.

**80. Which AWS service helps you deliver comprehensive cost and usage information to enable you to effectively manage, report, and allocate costs?**


* [ ]  AWS Cost Explorer
* [ ] AWS Budgets
* [ ] AWS CloudTrail
* [ ] AWS Cost and Usage Report

**Correct answer:**
* [x] AWS Cost and Usage Report

**Explaination**: Correct answer is “AWS Cost and Usage Report”: AWS Cost and Usage Report (CUR) provides detailed information about your AWS costs, down to the hourly level for each service category in your AWS account. The CUR is intended to deliver comprehensive cost and usage data to aid in better cost management and decision making.

**81. Which AWS tool provides comprehensive data about your costs, allowing you to analyze your cost drivers and usage trends?**


* [ ] AWS Budgets
* [ ] AWS Cost Explorer
* [ ] AWS Cost and Usage Report
* [ ] AWS Organizations

**Correct answer:**
* [x] AWS Cost and Usage Report

**Explaination**: Correct answer is “AWS Cost and Usage Report”: The AWS Cost and Usage Report is a tool that provides comprehensive data about your costs, allowing you to analyze your cost drivers and usage trends. It includes additional metadata about AWS services, pricing, and reservations (e.g., Amazon EC2 Reserved Instances).

**82. Which of the following is a basic dimension used to measure and understand billing on AWS?**


* [ ] Time
* [ ] Storage
* [ ] Bandwidth
* [ ] Instances

**Correct answer:**
* [x] Time

**Explaination**: Correct answer is “Time”: Time is a basic dimension used to measure and understand billing on AWS. AWS tracks the usage of various resources and services over time to calculate the cost incurred. The duration of resource usage is a fundamental factor in determining the billing amount. AWS bills customers based on the time for which their resources are utilized, such as the number of hours an EC2 instance is running or the duration of storage usage.

**83. A company is using multiple AWS services to host their application, and they want to ensure that they optimize the environment by adhering to AWS best practices.  Which of the following services is capable of inspecting your AWS environment and making recommendations to lower expenditures, improve system performance and reliability, and close security gaps?**


* [ ] AWS Cost Explorer
* [ ] AWS Trusted Advisor
* [ ] AWS Organizations
* [ ] AWS Budgets

**Correct answer:**
* [x] AWS Trusted Advisor

**Explaination**: Correct answer is “AWS Trusted Advisor”: AWS Trusted Advisor is an online tool that provides real-time guidance to help you provision your resources following AWS best practices. Trusted Advisor checks help optimize your AWS infrastructure, increase security and performance, reduce your overall costs, and monitor service limits.

**84. An e-commerce company wants to migrate its on-premise infrastructure to Amazon Web Services. However, they are worried about ensuring their migration is secure and efficient.  Which of the following AWS Cloud Adoption Framework perspectives would be the most essential for the company’s migration?**


* [ ] Governance Perspective
* [ ] Platform Perspective
* [ ] Operations Perspective
* [ ] Security Perspective

**Correct answer:**
* [x] Security Perspective

**Explaination**: Correct answer is “Security Perspective”: The AWS Cloud Adoption Framework is a set of guidelines designed to help businesses boost their digital transformation using Amazon Web Services. AWS CAF has six essential organizational perspectives for successful cloud transformations: business, people, governance, platform, security, and operations. The security perspective supports the confidentiality, integrity, and availability of company data and cloud workloads. Thus, this perspective focuses on ensuring the migration is done securely and with appropriate security controls to protect the company’s data and infrastructure. By leveraging AWS CAF, companies can identify and prioritize transformation opportunities, evaluate their cloud readiness, and evolve their transformation roadmap iteratively.

**85. Which of the following actions will AWS charge you for?**


* [ ] Transfer of EC2 files between two AWS Regions
* [ ] Provisioning elastic IPs and attaching them to running EC2 instances
* [ ] Setting up additional VPCs in your account
* [ ] Network charges for the transfer of data from your data center to S3 through a VPN

**Correct answer:**
* [x] Transfer of EC2 files between two AWS Regions

**Explaination**: Correct answer is “Transfer of EC2 files between two AWS Regions”: AWS charges for data transfer between two different regions. This is because data transfer out to a different region incurs costs associated with bandwidth and infrastructure.

**86. A customer is choosing the best AWS support plan which includes a designated Technical Account Manager. Which of the following should they choose?**


* [ ] Basic
* [ ] Developer
* [ ] Business
* [ ] Enterprise

**Correct answer:**
* [x] Enterprise

**Explaination**: Correct answer is “Enterprise”: The Enterprise support plan is the only AWS support plan that includes a designated Technical Account Manager (TAM). A TAM is your point of contact for AWS best practices and advice. They are technically capable and are your advocate within AWS to support your ongoing operational success.

**87. Which AWS billing tool provides interactive visualizations and analysis of your AWS costs and usage?**


* [ ] Cost Explorer
* [ ] Cost and Usage Report
* [ ] AWS Budgets
* [ ] AWS Organizations

**Correct answer:**
* [x] Cost Explorer

**Explaination**: Correct answer is “Cost Explorer”: AWS Cost Explorer is a billing tool that provides interactive visualizations and analysis of your AWS costs and usage. It enables you to view, understand, and analyze your AWS spending patterns using customizable charts, graphs, and reports. Cost Explorer allows you to visualize your costs by services, usage types, regions, and more. It helps you identify cost drivers, monitor usage trends, and optimize your AWS spending.


**88. Which AWS pricing model allows you to pay for compute capacity by the hour or second with no long-term commitments?**


* [ ] On-Demand Instances
* [ ] Spot Instances
* [ ] Savings Plans
* [ ] Reserved Instances

**Correct answer:**
* [x] On-Demand Instances

**Explaination**: Correct answer is “On-Demand Instances”: On-Demand Instances let you pay for compute capacity by the hour or second with no long-term commitments. This frees you from the costs and complexities of planning, purchasing, and maintaining hardware and transforms what are commonly large fixed costs into much smaller variable costs.

**89. Which of the following provides you access to Reserved Instance (RI) purchase recommendations based on your past usage and indicate potential opportunities for savings as compared to On-Demand usage?**


* [ ] AWS Management Console
* [ ] AWS Cost Explorer
* [ ] AWS Budgets
* [ ] AWS Organizations

**Correct answer:**
* [x] AWS Cost Explorer

**Explaination**: Correct answer is “AWS Cost Explorer”: AWS Cost Explorer has an RI Report that provides you with purchase recommendations. These recommendations are based on your past usage and can indicate potential opportunities for savings compared to On-Demand usage.

**90. Where can the customer view his Reserved Instance usage for the past month?**


* [ ] AWS Management Console
* [ ] AWS Cost Explorer
* [ ] AWS Budgets
* [ ] AWS Organizations

**Correct answer:**
* [x] AWS Cost Explorer

**Explaination**: Correct answer is “AWS Cost Explorer”: AWS Cost Explorer is a tool that enables you to visualize, understand, and manage your AWS costs and usage over time. This is where you can get detailed information about your Reserved Instance usage.

---


## Quiz - Grafana Loki

**1. Which query language is commonly used to query logs in Grafana Loki?**


* [ ] SQL
* [ ] PromQL
* [ ] GraphQL
* [ ]  LogQL

**Correct answer:**
* [x]  LogQL

**2. Which Grafana feature is used to create dashboards for visualizing log data from Loki?**


* [ ] Grafana Plugins
* [ ] Grafana Alerts
* [ ] Grafana Explore
* [ ] Grafana Dashboards

**Correct answer:**
* [x] Grafana Dashboards

**3. In a Loki setup, what is a "Distributed Trace"?**


* [ ] A type of log file format
* [ ] A method of querying logs across multiple Loki instances
* [ ] A way to track requests across microservices
* [ ] A Loki server cluster

**Correct answer:**
* [x] A way to track requests across microservices

**4. What is the primary function of Promtail's "scrape_config" in its configuration file?**


* [ ] To define alert rules
* [ ] To specify log file locations
* [ ] To configure log rotation
* [ ] To define targets for log scraping

**Correct answer:**
* [x] To define targets for log scraping

**5. When deploying Loki in a Kubernetes cluster, what is the purpose of a service like "loki-query-frontend"?**


* [ ] To collect logs from pods
* [ ] To serve as a load balancer for query requests
* [ ] To store logs in a distributed file system
* [ ] To manage Kubernetes resources

**Correct answer:**
* [x] To serve as a load balancer for query requests

**6. Which of the following is a common storage backend for Loki?**


* [ ] SQLite
* [ ] Cassandra
* [ ] PostgreSQL
* [ ] BoltDB

**Correct answer:**
* [x] Cassandra

**7. What role does Grafana play in the Loki-Prometheus-Grafana (LPG) stack?**


* [ ] Collecting logs from applications
* [ ] Storing time-series data
* [ ] Visualizing logs and metrics
* [ ] Parsing log data

**Correct answer:**
* [x] Visualizing logs and metrics

**8. In the context of Loki's Promtail configuration, what is a "pipeline_stage" used for?**


* [ ] To specify the location of log files
* [ ] To define transformation rules for log data
* [ ] To configure retention policies
* [ ] To set up log file permissions

**Correct answer:**
* [x] To define transformation rules for log data

**9. In the context of Grafana Loki, what does the term "Label" refer to?**


* [ ] A data visualization element in Grafana
* [ ] A key-value pair used to identify log entries
* [ ] A type of chart in Prometheus
* [ ] A function for aggregating log data

**Correct answer:**
* [x] A key-value pair used to identify log entries

**10. Which of the following components is used for querying logs in Loki?**


* [ ] Promtail
* [ ] Grafana
* [ ] Loki Agent
* [ ] Elasticsearch

**Correct answer:**
* [x] Grafana

**11. Which component is responsible for collecting logs from individual applications and forwarding them to Loki in the context of Loki's architecture?**


* [ ] Loki Server
* [ ] Loki Agent
* [ ] Grafana
* [ ] Promtail

**Correct answer:**
* [x] Loki Agent

**12. How can you view Kubernetes logs using Grafana in a Loki setup?**


* [ ] By running 'kubectl logs'
* [ ]  By using 'kubectl get logs'
* [ ] By querying Loki through Grafana
* [ ] By accessing logs directly from the Kubernetes nodes

**Correct answer:**
* [x] By querying Loki through Grafana

**13. In a Kubernetes environment, where is Promtail typically deployed to collect container logs?**


* [ ] On every Kubernetes worker node
* [ ] On the Kubernetes master node
* [ ] Inside each container
* [ ] On a separate monitoring server

**Correct answer:**
* [x] On every Kubernetes worker node

**14. What is the purpose of a Promtail pipeline in Loki?**


* [ ] To collect logs from Kubernetes pods
* [ ] To send logs to Elasticsearch
* [ ] To process and enrich logs before sending them to Loki
* [ ] To create visualizations in Grafana

**Correct answer:**
* [x] To process and enrich logs before sending them to Loki

**15. Which of the following is the recommended tool for collecting logs from applications in a Loki setup?**


* [ ] Fluentd
* [ ] Promtail
* [ ] Prometheus
* [ ] Logstash

**Correct answer:**
* [x] Promtail

**16. Which of the following is NOT a step in the Loki installation process?**


* [ ] Installing the Loki server
* [ ] Configuring Promtail
* [ ] Setting up Grafana
* [ ] Installing Elasticsearch

**Correct answer:**
* [x] Installing Elasticsearch

**17. What is the main component of Loki's architecture responsible for collecting logs?**


* [ ] Loki Agent
* [ ] Promtail
* [ ] Grafana
* [ ] Kubernetes

**Correct answer:**
* [x] Promtail

---


## Quiz: GKE Architecture

**1. Which of the following statements about GKE is false?**


* [ ] GKE manages both control plane and worker machines in Autopilot mode. 
* [ ] GKE manages the control plane and system components, while nodes are managed by the user in Standard mode.
* [ ] GKE launches Compute Engine virtual machine instances and registers them as nodes.
* [ ] GKE uses DaemonSets to automate node creation and management.

**Correct answer:**
* [x] GKE uses DaemonSets to automate node creation and management.

**2. What is the purpose of a GKE node pool?**


* [ ]  It represents a group of GKE instances with similar configuration and capacity.
* [ ] It is a backup storage system for GKE applications.  
* [ ] It enables load balancing between different GKE clusters.  
* [ ] It provides a secure network connection between GKE and other Google Cloud services. 

**Correct answer:**
* [x]  It represents a group of GKE instances with similar configuration and capacity.

**3. What components make up the control plane in a Kubernetes cluster?**


* [ ] Cloud Controller Manager, Kube Controller Manager, Kube API Server, Etcd, Kube Scheduler
* [ ] Resource Controllers, API Server, Scheduler, Storage
* [ ] Node controller, Job controller, EndpointSlice controller, ServiceAccount controller 
* [ ] Kubelet, Kube proxy 

**Correct answer:**
* [x] Cloud Controller Manager, Kube Controller Manager, Kube API Server, Etcd, Kube Scheduler

**4. What is Google Kubernetes Engine (GKE)?**


* [ ] A managed platform for running Docker containers 
* [ ] An open-source container orchestration system developed by Google 
* [ ] A cloud-based data storage service provided by Google  
* [ ] A tool for managing virtual machines on the Google Cloud Platform

**Correct answer:**
* [x] A managed platform for running Docker containers 

**5. How does GKE handle container image management?**


* [ ] GKE automatically pulls the latest container images from a container registry.  
* [ ] GKE requires developers to manually upload container images to the cluster.  
* [ ] GKE uses a proprietary image format that is incompatible with other container platforms.
* [ ] GKE relies on third-party tools for container image management. 

**Correct answer:**
* [x] GKE automatically pulls the latest container images from a container registry.  

**6. What are the benefits of using GKE over open-source Kubernetes?**


* [ ] Managed Kubernetes clusters 
* [ ] Autoscaling capabilities  
* [ ] Load balancing of Kubernetes pods  
* [ ] All of the options are correct

**Correct answer:**
* [x] All of the options are correct

**7. What are the two main components of a GKE cluster?**


* [ ] Control plane and nodes  
* [ ] Pods and services  
* [ ] Containers and virtual machines 
* [ ] None of the options are correct.

**Correct answer:**
* [x] Control plane and nodes  

---


## Quiz: GKE design

**1. Which of the following is not a feature of Anthos Service Mesh?**


* [ ] Traffic management
* [ ] Service registry
* [ ] Load balancing
* [ ] Metrics collection

**Correct answer:**
* [x] Load balancing

**2. What is backed up by the Backup for GKE service in Google Cloud?**


* [ ] GKE cluster configuration
* [ ] Container images
* [ ] Kubernetes resources and persistent volumes
* [ ] External services outside the GKE cluster

**Correct answer:**
* [x] Kubernetes resources and persistent volumes

**3. What layer of resource isolation within Kubernetes can be used to separate tenants in a multi-tenant architecture?**


* [ ] Cluster 
* [ ] Namespace 
* [ ] Pod 
* [ ] Container

**Correct answer:**
* [x] Namespace 

**4. Which of the following is not a feature of GKE cluster multi-tenancy?**


* [ ] Tenants can share resources across the cluster.
* [ ] The cluster control plane is shared by all tenants.
* [ ] Tenants can be isolated from each other.
* [ ] Tenants can use different Kubernetes versions.

**Correct answer:**
* [x] Tenants can use different Kubernetes versions.

**5. Which platform developed by Google Cloud helps with application modernization and management in hybrid and multi-cloud environments?**


* [ ] GKE cluster multi-tenancy
* [ ] Anthos Service Mesh 
* [ ] Backup for GKE
* [ ] GKE regional clusters

**Correct answer:**
* [x] Anthos Service Mesh 

**6. What is multi-tenancy in the context of GKE?**


* [ ] Sharing Kubernetes clusters with multiple users 
* [ ] Running multiple Kubernetes clusters in different regions 
* [ ] Managing multiple GKE clusters with a single control plane 
* [ ] Using multiple namespaces within a single GKE cluster

**Correct answer:**
* [x] Sharing Kubernetes clusters with multiple users 

**7. What is the purpose of a service mesh in Kubernetes?**


* [ ]  Load balancing between Kubernetes services 
* [ ] Resource isolation and security enforcement
* [ ] Service discovery and traffic management 
* [ ] Multi-cloud management and orchestration

**Correct answer:**
* [x] Service discovery and traffic management 

**8. What are some benefits of using GKE cluster multi-tenancy?**


* [ ] Increased security 
* [ ] Reduced costs and simplified management 
* [ ]  Improved performance and scalability 
* [ ] Enhanced container isolation

**Correct answer:**
* [x] Reduced costs and simplified management 

**9. What is one of the recommended use cases for using regional clusters in GKE?**


* [ ] Development environments 
* [ ] Non-critical workloads 
* [ ] Production workloads 
* [ ] Test environments

**Correct answer:**
* [x] Production workloads 

**10. Which of the following is NOT a design consideration when choosing highly available clusters in GKE?**


* [ ] Default node pool configuration 
* [ ] Node-to-node traffic cost 
* [ ] Compatibility with preemptible nodes 
* [ ] Overprovisioning scaling limits

**Correct answer:**
* [x] Compatibility with preemptible nodes 

**11. Across how many zones does the control plane of a regional GKE cluster automatically replicate?**


* [ ] One 
* [ ] Two 
* [ ] Three 
* [ ] Four

**Correct answer:**
* [x] Three 

---


## Quiz: AWS RDS

**1. You need to migrate your existing on-premises Oracle database to AWS RDS. Which database engine should you choose to facilitate a smooth migration with minimal code changes?**


* [ ] Amazon RDS for MySQL
* [ ] Amazon RDS for SQL Server
* [ ] Amazon RDS for PostgreSQL
* [ ]  Amazon RDS for Oracle

**Correct answer:**
* [x]  Amazon RDS for Oracle

**2. You are designing an architecture for a web application that uses Amazon RDS as its backend database. The application experiences sudden spikes in traffic, resulting in connection timeouts and increased database load during peak times. Which AWS service can help alleviate these issues by managing database connections and providing connection pooling?**


* [ ] Amazon RDS Multi-AZ
* [ ] Amazon RDS Performance Insights
* [ ] AWS Elastic Beanstalk
* [ ] Amazon RDS Proxy

**Correct answer:**
* [x] Amazon RDS Proxy

**3. You are designing a healthcare application that must comply with HIPAA (Health Insurance Portability and Accountability Act) regulations. Which Amazon RDS feature can help ensure data-at-rest encryption for this sensitive healthcare data?**


* [ ] Amazon RDS Multi-AZ
* [ ] Amazon RDS Performance Insights
* [ ] Amazon RDS IAM Authentication
* [ ] Amazon RDS Transparent Data Encryption (TDE)

**Correct answer:**
* [x] Amazon RDS Transparent Data Encryption (TDE)

**4. Your company is running an online news portal that experiences unpredictable spikes in traffic during breaking news events. You want to ensure that your RDS database can handle these traffic fluctuations without manual intervention. Which feature of AWS RDS can automatically scale compute capacity based on demand?**


* [ ] Amazon RDS Multi-AZ
* [ ] Amazon RDS Provisioned IOPS
* [ ] Amazon RDS Auto Scaling
* [ ] Amazon RDS Read Replicas

**Correct answer:**
* [x] Amazon RDS Auto Scaling

**5. You have a mobile gaming application that stores player scores and statistics in a relational database. Players from different regions are generating a large number of read requests. Which AWS RDS feature should you use to optimize read performance for players from various geographic locations?**


* [ ] Amazon RDS Cross-Region Replication
* [ ] Amazon RDS Multi-AZ deployment
* [ ] Amazon RDS Read Replicas
* [ ] Amazon RDS Aurora Multi-Master

**Correct answer:**
* [x] Amazon RDS Cross-Region Replication

**6. Your application must have low-latency access to the database to deliver real-time updates to users. Which Amazon RDS engine provides the lowest possible latency for read-intensive workloads?**


* [ ] Amazon RDS for MySQL
* [ ] Amazon RDS for SQL Server
* [ ] Amazon RDS for PostgreSQL
* [ ] Amazon RDS Aurora

**Correct answer:**
* [x] Amazon RDS Aurora

**7. Your web application requires a relational database to store user profiles and preferences. The application expects high read and write traffic. Which AWS RDS feature can help optimize read performance for this use case?**


* [ ] Multi-AZ (Availability Zones) deployment
* [ ] Read Replicas
* [ ] Provisioned IOPS storage
* [ ] Automated backups

**Correct answer:**
* [x] Read Replicas

**8. You need to deploy a highly available RDS database for your application. Which AWS RDS deployment option provides automated database backups, failover support, and high availability?**


* [ ] Single-AZ deployment
* [ ] Multi-AZ deployment
* [ ] Cross-Region Read Replicas
* [ ] Amazon Aurora Global Database

**Correct answer:**
* [x] Multi-AZ deployment

---
